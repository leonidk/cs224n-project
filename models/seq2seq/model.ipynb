{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from load_glove import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOVE_LOC = '../../data/glove/glove.6B.50d.txt'\n",
    "DATASET = '../../data/duc2004/data.json' #nips-abstract-title #squad\n",
    "INPUT_MAX = 120\n",
    "OUTPUT_MAX = 30\n",
    "VOCAB_MAX = 30000\n",
    "\n",
    "starter_learning_rate = 1e-3\n",
    "hs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = glove2dict(GLOVE_LOC)\n",
    "word_counter = defaultdict(int)\n",
    "GLV_DIM = words['the'].shape[0]\n",
    "\n",
    "def clean(text,clip_n=0):\n",
    "    res = text.replace('<d>','').replace('<p>','').replace('<s>','').replace('</d>','').replace('</p>','').replace('</s>','')\n",
    "    \n",
    "    r2 = []\n",
    "    for word in res.split():\n",
    "        if word not in words:\n",
    "            words[word] = np.array([random.uniform(-0.5, 0.5) for i in range(GLV_DIM)])\n",
    "    for word in res.split():\n",
    "        word_counter[word] += 1\n",
    "    if clip_n > 0:\n",
    "        return ' '.join(res.split()[:clip_n])\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(DATASET) as fp:\n",
    "    data = json.load(fp)\n",
    "    train = [x for x in data if x['set'] == 'train']\n",
    "    dev = [x for x in data if x['set'] == 'dev']\n",
    "    test = [x for x in data if x['set'] == 'test']\n",
    "\n",
    "    train = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in train]\n",
    "    dev = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in dev]\n",
    "    test = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in test]\n",
    "\n",
    "    valid_words = (sorted([(v,k) for k,v in word_counter.items()])[::-1])\n",
    "    #print len(valid_words)\n",
    "    valid_words = [x[1] for x in valid_words[:VOCAB_MAX]] + ['<EOS>','<PAD>','<UNK>','<SOS>']\n",
    "\n",
    "    initial_matrix = np.array([words[x] for x in valid_words])\n",
    "    def sent_to_idxs(sentence):\n",
    "        base = [valid_words.index('<SOS>')] + [valid_words.index(word) for word in sentence.split()]\n",
    "        base =  base + [valid_words.index('<EOS>')]\n",
    "        base = base + (OUTPUT_MAX-len(base)+1)*[valid_words.index('<EOS>')]\n",
    "        return base\n",
    "    def sent_to_sum(sentence):\n",
    "        summed= [words[word] for word in sentence.split()]\n",
    "        return np.sum(summed,0)/len(sentence)\n",
    "    train_x = [sent_to_sum(x[0]) for x in train]\n",
    "    train_y = [sent_to_idxs(x[1]) for x in train]\n",
    "    \n",
    "    dev_x = [sent_to_sum(x[0]) for x in dev]\n",
    "    dev_y = [sent_to_idxs(x[1]) for x in dev]\n",
    "    \n",
    "    test_x = [sent_to_sum(x[0]) for x in test]\n",
    "    test_y = [sent_to_idxs(x[1]) for x in test]\n",
    "    \n",
    "#train_x[0],train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "VOCAB_SIZE = len(valid_words)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,15000, 0.1, staircase=True)\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32,(None,GLV_DIM))\n",
    "labels_placeholder = tf.placeholder(tf.int32,(None,OUTPUT_MAX+1))\n",
    "\n",
    "preds = [] # Predicted output at each timestep should go here!\n",
    "\n",
    "hh0 = tf.get_variable(\"hh0\", shape=[GLV_DIM,hs], initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32) \n",
    "hb0 = tf.Variable(tf.constant(0.0, shape=[hs],dtype=tf.float32))\n",
    "cell = tf.contrib.rnn.GRUCell(hs)\n",
    "\n",
    "embedding = tf.Variable(initial_matrix,dtype=tf.float32)\n",
    "looked_up = tf.nn.embedding_lookup(embedding,labels_placeholder)\n",
    "x = tf.reshape(looked_up,[-1,OUTPUT_MAX+1,GLV_DIM])\n",
    "\n",
    "U = tf.get_variable(\"U\", shape=(hs,VOCAB_SIZE), initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"b2\", shape=(VOCAB_SIZE,), initializer=tf.constant_initializer(0.0))\n",
    "state = tf.matmul(input_placeholder,hh0) + hb0\n",
    "\n",
    "with tf.variable_scope(\"RNN\"):\n",
    "    for time_step in range(OUTPUT_MAX):\n",
    "        if time_step >= 1:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "        output, state = cell(x[:,time_step,:], state)\n",
    "        pred = tf.matmul(output,U) + b2\n",
    "        preds.append(pred)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "preds = tf.stack(preds)\n",
    "preds = tf.transpose(preds,perm=[1,0,2])\n",
    "\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds,labels=labels_placeholder[:,1:])\n",
    "loss = tf.reduce_mean(ce)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [((tf.clip_by_value(grad, -1., 1.) if grad != None else None), var)  for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs,global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "    saver =  tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batch_size = 32\n",
    "    data_size = len(train_x)\n",
    "    for i in range(data_size*10):\n",
    "        # cost, label,mask,data_size\n",
    "        start_idx = (i*batch_size)%data_size\n",
    "        end_idx = start_idx+batch_size\n",
    "        \n",
    "        feed_dict = {\n",
    "            input_placeholder: train_x[start_idx:end_idx],\n",
    "            labels_placeholder: train_y[start_idx:end_idx] \n",
    "        }\n",
    "        _, bl, summary = sess.run([train_step, loss, merged], feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary, i)\n",
    "        if i % 100 == 0:\n",
    "            saver.save(sess, 'model-checkpoint-', global_step=i)\n",
    "            summary_writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
