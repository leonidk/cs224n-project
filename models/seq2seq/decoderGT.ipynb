{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from load_glove import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOVE_LOC = '../../data/glove/glove.6B.50d.txt'\n",
    "DATASET = '../../data/squad/data_s.json' #nips-abstract-title #squad #\n",
    "INPUT_MAX = 120\n",
    "OUTPUT_MAX = 15\n",
    "VOCAB_MAX = 30000\n",
    "\n",
    "GLV_RANGE = 0.5\n",
    "LR_DECAY = 100\n",
    "LR_DECAY_AMOUNT = 0.5\n",
    "starter_learning_rate = 1e-1\n",
    "hs = 128\n",
    "batch_size = 32\n",
    "PRINT_EVERY = 25\n",
    "TRAIN_KEEP_PROB = 0.5\n",
    "TRAIN_EMBEDDING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = glove2dict(GLOVE_LOC)\n",
    "word_counter = defaultdict(int)\n",
    "GLV_DIM = words['the'].shape[0]\n",
    "\n",
    "def clean(text,clip_n=0):\n",
    "    res = text.replace('<d>','').replace('<p>','').replace('<s>','').replace('</d>','').replace('</p>','').replace('</s>','')\n",
    "    \n",
    "    r2 = []\n",
    "    for word in res.split():\n",
    "        if word not in words:\n",
    "            words[word] = np.array([random.uniform(-GLV_RANGE, GLV_RANGE) for i in range(GLV_DIM)])\n",
    "    for word in res.split():\n",
    "        word_counter[word] += 1\n",
    "    if clip_n > 0:\n",
    "        return ' '.join(res.split()[:clip_n])\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101885\n"
     ]
    }
   ],
   "source": [
    "with open(DATASET) as fp:\n",
    "    data = json.load(fp)\n",
    "    train = [x for x in data if x['set'] == 'train']\n",
    "    dev = [x for x in data if x['set'] == 'dev']\n",
    "    test = [x for x in data if x['set'] == 'test']\n",
    "\n",
    "    train = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in train]\n",
    "    dev = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in dev]\n",
    "    test = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in test]\n",
    "\n",
    "    valid_words = (sorted([(v,k) for k,v in word_counter.items()])[::-1])\n",
    "    print(len(valid_words))\n",
    "    valid_words = [x[1] for x in valid_words[:VOCAB_MAX]] + ['<EOS>','<PAD>','<UNK>','<SOS>']\n",
    "    unk_idx = valid_words.index('<UNK>')\n",
    "    vwd = defaultdict(lambda : unk_idx)\n",
    "    for idx,word in enumerate(valid_words):\n",
    "        vwd[word] = idx\n",
    "\n",
    "    initial_matrix = np.array([words[x] for x in valid_words])\n",
    "    def sent_to_idxs(sentence):\n",
    "        base =  [vwd[word] for word in sentence.split()]\n",
    "        sen_len = len(base)\n",
    "        base =  [vwd['<SOS>']] + base# + [valid_words.index('<EOS>')]\n",
    "        pad_word = (OUTPUT_MAX-sen_len)\n",
    "        base = base + pad_word*[vwd['<EOS>']]\n",
    "        return base,(sen_len,pad_word)\n",
    "    def sent_to_sum(sentence):\n",
    "        summed= [words[word] for word in sentence.split()]\n",
    "        return np.sum(summed,0)/len(sentence)\n",
    "    train_x = [sent_to_sum(x[0]) for x in train]\n",
    "    train_y = [sent_to_idxs(x[1])[0] for x in train]\n",
    "    train_len = [sent_to_idxs(x[1])[1] for x in train]\n",
    "\n",
    "    dev_x = [sent_to_sum(x[0]) for x in dev]\n",
    "    dev_y = [sent_to_idxs(x[1])[0] for x in dev]\n",
    "    dev_len = [sent_to_idxs(x[1])[1] for x in dev]\n",
    "\n",
    "    test_x = [sent_to_sum(x[0]) for x in test]\n",
    "    test_y = [sent_to_idxs(x[1])[0] for x in test]\n",
    "    test_len = [sent_to_idxs(x[1])[1] for x in test]\n",
    "\n",
    "    \n",
    "#train_x[0],train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "VOCAB_SIZE = len(valid_words)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,LR_DECAY, LR_DECAY_AMOUNT, staircase=True)\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32,(None,GLV_DIM))\n",
    "mask_placeholder = tf.placeholder(tf.float32,(None,OUTPUT_MAX))\n",
    "labels_placeholder = tf.placeholder(tf.int32,(None,OUTPUT_MAX+1))\n",
    "dropout_rate = tf.placeholder(tf.float32,())\n",
    "\n",
    "preds = [] # Predicted output at each timestep should go here!\n",
    "\n",
    "hh0 = tf.get_variable(\"hh0\", shape=[GLV_DIM,hs], initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32) \n",
    "hb0 = tf.Variable(tf.constant(0.0, shape=[hs],dtype=tf.float32))\n",
    "cell = tf.contrib.rnn.GRUCell(hs)\n",
    "\n",
    "embedding = tf.Variable(initial_matrix,dtype=tf.float32,trainable=TRAIN_EMBEDDING)\n",
    "looked_up = tf.nn.embedding_lookup(embedding,labels_placeholder)\n",
    "x = tf.reshape(looked_up,[-1,OUTPUT_MAX+1,GLV_DIM])\n",
    "\n",
    "U = tf.get_variable(\"U\", shape=(hs,VOCAB_SIZE), initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"b2\", shape=(VOCAB_SIZE,), initializer=tf.constant_initializer(0.0))\n",
    "state = tf.matmul(input_placeholder,hh0) + hb0\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, x, initial_state=state,dtype=tf.float32)\n",
    "outputs_batchword = tf.reshape(outputs[:,:OUTPUT_MAX,:],[-1,hs])\n",
    "out_drop = tf.nn.dropout(outputs_batchword,dropout_rate)\n",
    "pred_batchword = tf.matmul(out_drop,U) + b2\n",
    "preds = tf.reshape(pred_batchword,[-1,OUTPUT_MAX,VOCAB_SIZE])\n",
    "\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds,labels=labels_placeholder[:,1:])\n",
    "loss = tf.reduce_mean(mask_placeholder * ce)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [((tf.clip_by_value(grad, -1., 1.) if grad != None else None), var)  for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs,global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.34319\n",
      "TRAIN_SAMPLE:  dual dame dame dame writers dame writers writers dual dual dame dame dame dame dame\n",
      "TRAIN_LABEL:  architecturally , the school has a catholic character\n",
      "\n",
      "DEV_SAMPLE:  dual dame dame dame writers writers writers dual dual writers dame dame dame dame dame\n",
      "DEV_LABEL:  the broncos finished the regular season with a <UNK> record , and denied the new\n",
      "\n",
      "\n",
      "25 9.45193\n",
      "TRAIN_SAMPLE:  beyoncé was the the the the to the to the to to to to the\n",
      "TRAIN_LABEL:  the design was only slightly modified after montana became a state and adopted it as\n",
      "\n",
      "DEV_SAMPLE:  beyoncé has beyoncé with her ring am with with with with with with with with\n",
      "DEV_LABEL:  they joined the patriots , dallas cowboys , and pittsburgh steelers as one of four\n",
      "\n",
      "\n",
      "50 7.80603\n",
      "TRAIN_SAMPLE:  <UNK> , , , , , , , , , , , , , ,\n",
      "TRAIN_LABEL:  he passed the winter in <UNK> illness , but gave occasional lessons and was visited\n",
      "\n",
      "DEV_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "DEV_LABEL:  the game was played on february 7 , 2016 , at levi 's stadium in\n",
      "\n",
      "\n",
      "75 9.60679\n",
      "TRAIN_SAMPLE:  '' '' '' '' '' '' the the the the the the the the the\n",
      "TRAIN_LABEL:  he must do this by collecting the multiple `` tears of light '' ; once\n",
      "\n",
      "DEV_SAMPLE:  at at at at at at at at at at at at at at at\n",
      "DEV_LABEL:  the game was played on february 7 , 2016 , at levi 's stadium in\n",
      "\n",
      "\n",
      "100 7.87952\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  <UNK> burned the colored orphan asylum to the ground , but more than 200 children\n",
      "\n",
      "DEV_SAMPLE:  , , , , , , , , , , , , , , ,\n",
      "DEV_LABEL:  the game was played on february 7 , 2016 , at levi 's stadium in\n",
      "\n",
      "\n",
      "125 7.00119\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  the book was made into the well-received 1962 film with the same title , starring\n",
      "\n",
      "DEV_SAMPLE:  , , , , , , , , , , , , , , ,\n",
      "DEV_LABEL:  the game was played on february 7 , 2016 , at levi 's stadium in\n",
      "\n",
      "\n",
      "150 7.44121\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  the former , beyond being one of the largest roman settlements in portugal , is\n",
      "\n",
      "DEV_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "DEV_LABEL:  as this was the 50th super bowl , the league emphasized the `` golden anniversary\n",
      "\n",
      "\n",
      "175 6.64099\n",
      "TRAIN_SAMPLE:  `` `` the , , , , , , , , , , , ,\n",
      "TRAIN_LABEL:  no one 's near doing what <UNK> doing , it’s not even on the same\n",
      "\n",
      "DEV_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "DEV_LABEL:  the broncos took an early lead in super bowl 50 and never trailed\n",
      "\n",
      "\n",
      "200 6.61301\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the , , ,\n",
      "TRAIN_LABEL:  this was also the first season without executive producer nigel lythgoe who left to focus\n",
      "\n",
      "DEV_SAMPLE:  the the the the , , , , , , , , , , ,\n",
      "DEV_LABEL:  newton was limited by denver 's defense , which sacked him seven times and forced\n",
      "\n",
      "\n",
      "225 6.76519\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  the combined effect is a `` rapid deterioration '' of relations between india and china\n",
      "\n",
      "DEV_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "DEV_LABEL:  the american football conference ( afc ) champion denver broncos defeated the national football conference\n",
      "\n",
      "\n",
      "250 6.53676\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  senate passed a reform bill in may 2010 , following the house which passed a\n",
      "\n",
      "DEV_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "DEV_LABEL:  super bowl 50 was an american football game to determine the champion of the national\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample(context_vector):\n",
    "    sentence = []\n",
    "    for i in xrange(OUTPUT_MAX):\n",
    "        x = sent_to_idxs(' '.join(sentence))[0]\n",
    "        #print(x)\n",
    "        feed_dict = {\n",
    "            input_placeholder: context_vector.reshape([1,-1]),\n",
    "            labels_placeholder: np.array(x).reshape([1,-1]),\n",
    "            dropout_rate: 1.0\n",
    "            #mask_placeholder: None\n",
    "        }\n",
    "        probs = np.squeeze(preds.eval(feed_dict=feed_dict))\n",
    "        new_word = valid_words[np.argmax(probs[i,:])]\n",
    "        if new_word != '<EOS>':\n",
    "            sentence.append(new_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(sentence)\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    #summary_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "    #saver =  tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    data_size = len(train_x)\n",
    "    for i in range(data_size*10):\n",
    "        # cost, label,mask,data_size\n",
    "        start_idx = (i*batch_size)%data_size\n",
    "        end_idx = start_idx+batch_size\n",
    "        #print(train_len[start_idx:end_idx])\n",
    "        mask = np.array([np.array([1.0]*x[0] + [0.0]*x[1]) for x in train_len[start_idx:end_idx]])\n",
    "        #print(mask.shape)\n",
    "        #print(np.array(train_y[start_idx:end_idx]).shape)\n",
    "        #print(np.array(train_x[start_idx:end_idx]).shape)\n",
    "\n",
    "        feed_dict = {\n",
    "            input_placeholder: train_x[start_idx:end_idx],\n",
    "            labels_placeholder: train_y[start_idx:end_idx],\n",
    "            mask_placeholder: mask,\n",
    "            dropout_rate: TRAIN_KEEP_PROB\n",
    "        }\n",
    "        _, bl, summary = sess.run([train_step, loss, merged], feed_dict=feed_dict)\n",
    "        #summary_writer.add_summary(summary, i)\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            print(i,bl)\n",
    "            print('TRAIN_SAMPLE: ',sample(train_x[start_idx]))\n",
    "            print('TRAIN_LABEL: ',' '.join([x for x in [valid_words[x] for x in train_y[start_idx]] if x not in ['<EOS>','<SOS>']]))\n",
    "            print()\n",
    "            index = int(random.random()*10)\n",
    "            print('DEV_SAMPLE: ',sample(dev_x[index]))\n",
    "            print('DEV_LABEL: ',' '.join([x for x in [valid_words[x] for x in dev_y[index]] if x not in ['<EOS>','<SOS>']]))\n",
    "            print('\\n')\n",
    "        #    saver.save(sess, 'model-checkpoint-', global_step=i)\n",
    "        #    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
