{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from load_glove import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "GLOVE_LOC = '../../data/glove/glove.6B.50d.txt'\n",
    "DATASET = '../../data/squad/data_s.json' #nips-abstract-title #squad #\n",
    "INPUT_MAX = 150\n",
    "OUTPUT_MAX = 15\n",
    "VOCAB_MAX = 30000\n",
    "\n",
    "GLV_RANGE = 0.5\n",
    "LR_DECAY = 1000\n",
    "LR_DECAY_AMOUNT = 0.9\n",
    "starter_learning_rate = 1e-2\n",
    "hs = 128\n",
    "batch_size = 32\n",
    "PRINT_EVERY = 25\n",
    "TRAIN_KEEP_PROB = 0.5\n",
    "TRAIN_EMBEDDING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words = glove2dict(GLOVE_LOC)\n",
    "word_counter = defaultdict(int)\n",
    "GLV_DIM = words['the'].shape[0]\n",
    "\n",
    "def clean(text,clip_n=0):\n",
    "    res = text.replace('<d>','').replace('<p>','').replace('<s>','').replace('</d>','').replace('</p>','').replace('</s>','')\n",
    "    \n",
    "    r2 = []\n",
    "    for word in res.split():\n",
    "        if word not in words:\n",
    "            words[word] = np.array([random.uniform(-GLV_RANGE, GLV_RANGE) for i in range(GLV_DIM)])\n",
    "    for word in res.split():\n",
    "        word_counter[word] += 1\n",
    "    if clip_n > 0:\n",
    "        return ' '.join(res.split()[:clip_n])\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23225\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "with open(DATASET) as fp:\n",
    "    data = json.load(fp)\n",
    "    train = [x for x in data if x['set'] == 'train']\n",
    "    dev = [x for x in data if x['set'] == 'dev']\n",
    "    test = [x for x in data if x['set'] == 'test']\n",
    "\n",
    "    train = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in train]\n",
    "    dev = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in dev]\n",
    "    test = [(clean(x['data'],INPUT_MAX),clean(x['label'][0],OUTPUT_MAX)) for x in test]\n",
    "\n",
    "    valid_words = (sorted([(v,k) for k,v in word_counter.items()])[::-1])\n",
    "    print(len(valid_words))\n",
    "    valid_words = [x[1] for x in valid_words[:VOCAB_MAX]] + ['<EOS>','<PAD>','<UNK>','<SOS>']\n",
    "    unk_idx = valid_words.index('<UNK>')\n",
    "    vwd = defaultdict(lambda : unk_idx)\n",
    "    for idx,word in enumerate(valid_words):\n",
    "        vwd[word] = idx\n",
    "\n",
    "    initial_matrix = np.array([words[x] for x in valid_words])\n",
    "    def sent_to_idxs(sentence):\n",
    "        base =  [vwd[word] for word in sentence.split()]\n",
    "        sen_len = len(base)\n",
    "        base =  [vwd['<SOS>']] + base# + [valid_words.index('<EOS>')]\n",
    "        pad_word = (OUTPUT_MAX-sen_len)\n",
    "        base = base + pad_word*[vwd['<EOS>']]\n",
    "        return base,(sen_len,pad_word)\n",
    "    def sent_to_idxs_nopad(sentence):\n",
    "        base =  [vwd[word] for word in sentence.split()]\n",
    "        return base\n",
    "    train_x = [sent_to_idxs_nopad(x[0]) for x in train]\n",
    "    train_y = [sent_to_idxs(x[1])[0] for x in train]\n",
    "    train_len = [sent_to_idxs(x[1])[1] for x in train]\n",
    "\n",
    "    dev_x = [sent_to_idxs_nopad(x[0]) for x in dev]\n",
    "    dev_y = [sent_to_idxs(x[1])[0] for x in dev]\n",
    "    dev_len = [sent_to_idxs(x[1])[1] for x in dev]\n",
    "\n",
    "    test_x = [sent_to_idxs_nopad(x[0]) for x in test]\n",
    "    test_y = [sent_to_idxs(x[1])[0] for x in test]\n",
    "    test_len = [sent_to_idxs(x[1])[1] for x in test]\n",
    "\n",
    "    \n",
    "#train_x[0],train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "VOCAB_SIZE = len(valid_words)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,LR_DECAY, LR_DECAY_AMOUNT, staircase=True)\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.int32)\n",
    "mask_placeholder = tf.placeholder(tf.float32,(None,OUTPUT_MAX))\n",
    "labels_placeholder = tf.placeholder(tf.int32,(None,OUTPUT_MAX+1))\n",
    "dropout_rate = tf.placeholder(tf.float32,())\n",
    "\n",
    "embedding = tf.Variable(initial_matrix,dtype=tf.float32,trainable=TRAIN_EMBEDDING)\n",
    "input_embed = tf.nn.embedding_lookup(embedding,input_placeholder)\n",
    "input_summed= tf.reduce_mean(input_embed,1)\n",
    "print(input_summed.get_shape())\n",
    "\n",
    "\n",
    "hh0 = tf.get_variable(\"hh0\", shape=[GLV_DIM,hs], initializer=tf.contrib.layers.xavier_initializer(),dtype=tf.float32) \n",
    "hb0 = tf.Variable(tf.constant(0.0, shape=[hs],dtype=tf.float32))\n",
    "cell = tf.contrib.rnn.GRUCell(hs)\n",
    "\n",
    "looked_up = tf.nn.embedding_lookup(embedding,labels_placeholder)\n",
    "x = tf.reshape(looked_up,[-1,OUTPUT_MAX+1,GLV_DIM])\n",
    "\n",
    "U = tf.get_variable(\"U\", shape=(hs,VOCAB_SIZE), initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"b2\", shape=(VOCAB_SIZE,), initializer=tf.constant_initializer(0.0))\n",
    "state = tf.matmul(input_summed,hh0) + hb0\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, x, initial_state=state,dtype=tf.float32)\n",
    "outputs_batchword = tf.reshape(outputs[:,:OUTPUT_MAX,:],[-1,hs])\n",
    "out_drop = tf.nn.dropout(outputs_batchword,dropout_rate)\n",
    "pred_batchword = tf.matmul(out_drop,U) + b2\n",
    "preds = tf.reshape(pred_batchword,[-1,OUTPUT_MAX,VOCAB_SIZE])\n",
    "\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=preds,labels=labels_placeholder[:,1:])\n",
    "loss = tf.reduce_mean(mask_placeholder * ce)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [((tf.clip_by_value(grad, -1., 1.) if grad != None else None), var)  for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs,global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.11814\n",
      "TRAIN_SAMPLE:  the the the the the the the of building building building building building building building\n",
      "TRAIN_LABEL:  architecturally , the school has a catholic character\n",
      "\n",
      "\n",
      "\n",
      "25 8.01973\n",
      "TRAIN_SAMPLE:  the the the the the the the the the the the the the the the\n",
      "TRAIN_LABEL:  the design was only slightly modified after montana became a state and adopted it as\n",
      "\n",
      "\n",
      "\n",
      "50 6.74384\n",
      "TRAIN_SAMPLE:  in , , , , , , , , , , , , , ,\n",
      "TRAIN_LABEL:  he passed the winter in unremitting illness , but gave occasional lessons and was visited\n",
      "\n",
      "\n",
      "\n",
      "75 7.9342\n",
      "TRAIN_SAMPLE:  the the , , , , , , , , , , , , ,\n",
      "TRAIN_LABEL:  he must do this by collecting the multiple `` tears of light '' ; once\n",
      "\n",
      "\n",
      "\n",
      "100 6.65461\n",
      "TRAIN_SAMPLE:  the the the the the the the , the the , the the , the\n",
      "TRAIN_LABEL:  rioters burned the colored orphan asylum to the ground , but more than 200 children\n",
      "\n",
      "\n",
      "\n",
      "125 6.32408\n",
      "TRAIN_SAMPLE:  the city , the , , the , , the , , the , ,\n",
      "TRAIN_LABEL:  the book was made into the well-received 1962 film with the same title , starring\n",
      "\n",
      "\n",
      "\n",
      "150 6.94131\n",
      "TRAIN_SAMPLE:  the of the of the of the of the of the of the of the\n",
      "TRAIN_LABEL:  the former , beyond being one of the largest roman settlements in portugal , is\n",
      "\n",
      "\n",
      "\n",
      "175 6.01582\n",
      "TRAIN_SAMPLE:  west , west , west , west , west , west , west , west\n",
      "TRAIN_LABEL:  no one 's near doing what he’s doing , it’s not even on the same\n",
      "\n",
      "\n",
      "\n",
      "200 6.19956\n",
      "TRAIN_SAMPLE:  the , , the , , the , , the , , the , ,\n",
      "TRAIN_LABEL:  this was also the first season without executive producer nigel lythgoe who left to focus\n",
      "\n",
      "\n",
      "\n",
      "225 6.60364\n",
      "TRAIN_SAMPLE:  the first of the first of the first of the first of the first ,\n",
      "TRAIN_LABEL:  the combined effect is a `` rapid deterioration '' of relations between india and china\n",
      "\n",
      "\n",
      "\n",
      "250 6.15221\n",
      "TRAIN_SAMPLE:  the first first the first the of the first the of the first the of\n",
      "TRAIN_LABEL:  senate passed a reform bill in may 2010 , following the house which passed a\n",
      "\n",
      "\n",
      "\n",
      "275 6.30358\n",
      "TRAIN_SAMPLE:  the island of the minister of the minister of the minister of the minister of\n",
      "TRAIN_LABEL:  since the 1990s , there has been consolidation in new zealand 's state-owned tertiary education\n",
      "\n",
      "\n",
      "\n",
      "300 7.19555\n",
      "TRAIN_SAMPLE:  the prime minister , the is a minister of the , , the , ,\n",
      "TRAIN_LABEL:  lighting design as it applies to the built environment is known as 'architectural lighting design\n",
      "\n",
      "\n",
      "\n",
      "325 6.16651\n",
      "TRAIN_SAMPLE:  the first first first first first first first first first first first first first first\n",
      "TRAIN_LABEL:  forbes magazine also listed her as the most powerful female musician of 2015\n",
      "\n",
      "\n",
      "\n",
      "350 6.40466\n",
      "TRAIN_SAMPLE:  the term of the of the of the of the of the of the of\n",
      "TRAIN_LABEL:  the conditions that lead to genocide provide guidance to early prevention , such as humanizing\n",
      "\n",
      "\n",
      "\n",
      "375 5.97648\n",
      "TRAIN_SAMPLE:  the ming emperor , the ming emperor , the ming emperor , the emperor ,\n",
      "TRAIN_LABEL:  josef kolmaš , a sinologist , tibetologist , and professor of oriental studies at the\n",
      "\n",
      "\n",
      "\n",
      "400 5.73243\n",
      "TRAIN_SAMPLE:  in the million , in the million , ) , the , ) , the\n",
      "TRAIN_LABEL:  starting from wenchuan , the rupture propagated at an average speed of 3\n",
      "\n",
      "\n",
      "\n",
      "425 6.00713\n",
      "TRAIN_SAMPLE:  the city is the new york city , the city , the city , the\n",
      "TRAIN_LABEL:  new york is the only us city in which a majority ( 52 % )\n",
      "\n",
      "\n",
      "\n",
      "450 6.65597\n",
      "TRAIN_SAMPLE:  the united states in the united states in the united states , the united states\n",
      "TRAIN_LABEL:  their arrival in transoxania signaled a definitive shift from iranian to turkic predominance in central\n",
      "\n",
      "\n",
      "\n",
      "475 6.32449\n",
      "TRAIN_SAMPLE:  he was `` `` `` `` `` '' was `` `` '' was `` ''\n",
      "TRAIN_LABEL:  she later commented , `` it was drummed into my head that college is the\n",
      "\n",
      "\n",
      "\n",
      "500 5.80205\n",
      "TRAIN_SAMPLE:  the buddha is the the of the buddha , the buddha , the buddha ,\n",
      "TRAIN_LABEL:  this contradicts the mahasanghikas ' own vinaya , which shows them as on the same\n",
      "\n",
      "\n",
      "\n",
      "525 6.21356\n",
      "TRAIN_SAMPLE:  the dog of the dog , dogs are dogs are dogs are dogs are dogs\n",
      "TRAIN_LABEL:  as the oldest domesticated species , with estimates ranging from 9,000–30,000 years bce , the\n",
      "\n",
      "\n",
      "\n",
      "550 5.54862\n",
      "TRAIN_SAMPLE:  whitehead is not not not be whitehead is not not not be not be not\n",
      "TRAIN_LABEL:  here it is worthwhile to quote whitehead at length :\n",
      "\n",
      "\n",
      "\n",
      "575 6.22015\n",
      "TRAIN_SAMPLE:  the new york city is the of the new york city is the of the\n",
      "TRAIN_LABEL:  congo-brazzaville was formerly part of the french colony of equatorial africa\n",
      "\n",
      "\n",
      "\n",
      "600 6.12133\n",
      "TRAIN_SAMPLE:  the canadian forces were been were provinces were provinces were provinces were provinces were provinces\n",
      "TRAIN_LABEL:  the armed forces ' 115,349 personnel are divided into a hierarchy of numerous ranks of\n",
      "\n",
      "\n",
      "\n",
      "625 5.47488\n",
      "TRAIN_SAMPLE:  the most of a most of a most of a most of a most of\n",
      "TRAIN_LABEL:  architecturally , the school has a catholic character\n",
      "\n",
      "\n",
      "\n",
      "650 5.64541\n",
      "TRAIN_SAMPLE:  in the was a of the was a of the of the of the of\n",
      "TRAIN_LABEL:  the design was only slightly modified after montana became a state and adopted it as\n",
      "\n",
      "\n",
      "\n",
      "675 5.39765\n",
      "TRAIN_SAMPLE:  chopin was chopin , chopin 's composer , he was a composer , in paris\n",
      "TRAIN_LABEL:  he passed the winter in unremitting illness , but gave occasional lessons and was visited\n",
      "\n",
      "\n",
      "\n",
      "700 6.10843\n",
      "TRAIN_SAMPLE:  this is not be not to be not to be not to be not to\n",
      "TRAIN_LABEL:  he must do this by collecting the multiple `` tears of light '' ; once\n",
      "\n",
      "\n",
      "\n",
      "725 5.21738\n",
      "TRAIN_SAMPLE:  the government , the government , the government had been the and the and the\n",
      "TRAIN_LABEL:  rioters burned the colored orphan asylum to the ground , but more than 200 children\n",
      "\n",
      "\n",
      "\n",
      "750 5.38313\n",
      "TRAIN_SAMPLE:  lee was lee , lee was a novel was a novel of the novel ,\n",
      "TRAIN_LABEL:  the book was made into the well-received 1962 film with the same title , starring\n",
      "\n",
      "\n",
      "\n",
      "775 5.76001\n",
      "TRAIN_SAMPLE:  the first of the first of the first of the first of the first of\n",
      "TRAIN_LABEL:  the former , beyond being one of the largest roman settlements in portugal , is\n",
      "\n",
      "\n",
      "\n",
      "800 4.95904\n",
      "TRAIN_SAMPLE:  `` `` `` `` `` i '' was not be be to be be be\n",
      "TRAIN_LABEL:  no one 's near doing what he’s doing , it’s not even on the same\n",
      "\n",
      "\n",
      "\n",
      "825 4.82148\n",
      "TRAIN_SAMPLE:  the first the , the show , the show , the show , the show\n",
      "TRAIN_LABEL:  this was also the first season without executive producer nigel lythgoe who left to focus\n",
      "\n",
      "\n",
      "\n",
      "850 5.39707\n",
      "TRAIN_SAMPLE:  `` `` `` `` `` `` `` '' '' '' is a `` `` ``\n",
      "TRAIN_LABEL:  the combined effect is a `` rapid deterioration '' of relations between india and china\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-cf71f0649813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTRAIN_KEEP_PROB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         }\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#summary_writer.add_summary(summary, i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mPRINT_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 769\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    770\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 967\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1017\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1018\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1022\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1005\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(context_vector):\n",
    "    sentence = []\n",
    "    for i in xrange(OUTPUT_MAX):\n",
    "        x = sent_to_idxs(' '.join(sentence))[0]\n",
    "        #print(x)\n",
    "        feed_dict = {\n",
    "            input_placeholder: np.array(context_vector).reshape([1,-1]),\n",
    "            labels_placeholder: np.array(x).reshape([1,-1]),\n",
    "            dropout_rate: 1.0\n",
    "            #mask_placeholder: None\n",
    "        }\n",
    "        probs = np.squeeze(preds.eval(feed_dict=feed_dict))\n",
    "        new_word = valid_words[np.argmax(probs[i,:])]\n",
    "        if new_word != '<EOS>':\n",
    "            sentence.append(new_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(sentence)\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    #summary_writer = tf.summary.FileWriter('./train', sess.graph)\n",
    "    #saver =  tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    data_size = len(train_x)\n",
    "    for i in range(data_size*10):\n",
    "        # cost, label,mask,data_size\n",
    "        start_idx = (i*batch_size)%data_size\n",
    "        end_idx = start_idx+batch_size\n",
    "        #print(train_len[start_idx:end_idx])\n",
    "        mask = np.array([np.array([1.0]*x[0] + [0.0]*x[1]) for x in train_len[start_idx:end_idx]])\n",
    "        #print(mask.shape)\n",
    "        #print(np.array(train_y[start_idx:end_idx]).shape)\n",
    "        #print(np.array(train_x[start_idx:end_idx]).shape)\n",
    "        train_sizes = [len(x) for x in train_x[start_idx:end_idx]]\n",
    "        train_mat = np.zeros(shape=(len(train_sizes),max(train_sizes)))\n",
    "        for idx,row in enumerate(train_x[start_idx:end_idx]):\n",
    "            train_mat[idx,:train_sizes[idx]] = np.array(row)\n",
    "        feed_dict = {\n",
    "            input_placeholder: train_mat,\n",
    "            labels_placeholder: train_y[start_idx:end_idx],\n",
    "            mask_placeholder: mask,\n",
    "            dropout_rate: TRAIN_KEEP_PROB\n",
    "        }\n",
    "        _, bl, summary = sess.run([train_step, loss, merged], feed_dict=feed_dict)\n",
    "        #summary_writer.add_summary(summary, i)\n",
    "        if i % PRINT_EVERY == 0:\n",
    "            print(i,bl)\n",
    "            print('TRAIN_SAMPLE: ',sample(train_x[start_idx]))\n",
    "            print('TRAIN_LABEL: ',' '.join([x for x in [valid_words[x] for x in train_y[start_idx]] if x not in ['<EOS>','<SOS>']]))\n",
    "            print()\n",
    "            index = int(random.random()*10)\n",
    "            #print('DEV_SAMPLE: ',sample(dev_x[index]))\n",
    "            #print('DEV_LABEL: ',' '.join([x for x in [valid_words[x] for x in dev_y[index]] if x not in ['<EOS>','<SOS>']]))\n",
    "            print('\\n')\n",
    "        #    saver.save(sess, 'model-checkpoint-', global_step=i)\n",
    "        #    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
