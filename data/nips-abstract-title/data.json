[
  {"data": "<d><p><s>non-negative matrix factorization (nmf) has previously been shown to  be a useful decomposition for multivariate data.</s> <s>two different multi-  plicative algorithms for nmf are analyzed.</s> <s>they differ only slightly in  the multiplicative factor used in the update rules.</s> <s>one algorithm can be  shown to minimize the conventional least squares error while the other  minimizes the generalized kullback-leibler divergence.</s> <s>the monotonic  convergence of both algorithms can be proven using an auxiliary func-  tion analogous to that used for proving convergence of the expectation-  maximization algorithm.</s> <s>the algorithms can also be interpreted as diag-  onally rescaled gradient descent, where the rescaling factor is optimally  chosen to ensure convergence.</s></p></d>", "label": ["<d><p><s>algorithms for non-negative matrix factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spike-triggered averaging techniques are effective for linear characterization of neural responses.</s> <s>but neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses.</s> <s>we describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron.</s> <s>we demonstrate the method in simulation and on retinal ganglion cell data.</s> <s>analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities.</s> <s>this method should be applicable to other sensory areas and modalities.</s></p></d>", "label": ["<d><p><s>characterizing neural gain control using spike-triggered covariance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is known that determinining whether a dec-pomdp, namely, a cooperative partially observable stochastic game (posg), has a cooperative strategy with positive expected reward is complete for nexp.</s> <s>it was not known until now how cooperation affected that complexity.</s> <s>we show that, for competitive posgs, the complexity of determining whether one team has a positive-expected-reward strategy is complete for the class nexp with an oracle for np.</s></p></d>", "label": ["<d><p><s>competition adds complexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the first truly polynomial algorithm for learning the structure of bounded-treewidth junction trees -- an attractive subclass of probabilistic graphical models that permits both the compact representation of probability distributions and efficient exact inference.</s> <s>for a constant treewidth, our algorithm has polynomial time and sample complexity, and provides strong theoretical guarantees in terms of $kl$ divergence from the true distribution.</s> <s>we also present a lazy extension of our approach that leads to very significant speed ups in practice, and demonstrate the viability of our method empirically, on several real world datasets.</s> <s>one of our key new theoretical insights is a method for bounding the conditional mutual information of arbitrarily large sets of random variables with only a polynomial number of mutual information computations on fixed-size subsets of variables, when the underlying distribution can be approximated by a bounded treewidth junction tree.</s></p></d>", "label": ["<d><p><s>efficient principled learning of thin junction trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>semi-supervised inductive learning concerns how to learn a decision rule from a data set containing both labeled and unlabeled data.</s> <s>several boosting algorithms have been extended to semi-supervised learning with various strategies.</s> <s>to our knowledge, however, none of them takes local smoothness constraints among data into account during ensemble learning.</s> <s>in this paper, we introduce a local smoothness regularizer to semi-supervised boosting algorithms based on the universal optimization framework of margin cost functionals.</s> <s>our regularizer is applicable to existing semi-supervised boosting algorithms to improve their generalization and speed up their training.</s> <s>comparative results on synthetic, benchmark and real world tasks demonstrate the effectiveness of our local smoothness regularizer.</s> <s>we discuss relevant issues and relate our regularizer to previous work.</s></p></d>", "label": ["<d><p><s>regularized boost for semi-supervised learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show that under suitable assumptions (primarily linearization) a simple and perspicuous online learning rule for information bottleneck optimization with spiking neurons can be derived.</s> <s>this rule performs on common benchmark tasks as well as a rather complex rule that has previously been proposed \\cite{klampfletal:07b}.</s> <s>furthermore, the transparency of this new learning rule makes a theoretical analysis of its convergence properties feasible.</s> <s>a variation of this learning rule (with sign changes) provides a theoretically founded method for performing principal component analysis {(pca)} with spiking neurons.</s> <s>by applying this rule to an ensemble of neurons, different principal components of the input can be extracted.</s> <s>in addition, it is possible to preferentially extract those principal components from incoming signals $x$ that are related or are not related to some additional target signal $y_t$.</s> <s>in a biological interpretation, this target signal $y_t$ (also called relevance variable) could represent proprioceptive feedback, input from other sensory modalities, or top-down signals.</s></p></d>", "label": ["<d><p><s>simplified rules and theoretical analysis for information bottleneck optimization and pca with spiking neurons</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>under natural viewing conditions, human observers shift their gaze to allocate processing resources to subsets of the visual input.</s> <s>many computational models have aimed at predicting such voluntary attentional shifts.</s> <s>although the importance of high level stimulus properties (higher order statistics, semantics) stands undisputed, most models are based on low-level features of the input alone.</s> <s>in this study we recorded eye-movements of human observers while they viewed photographs of natural scenes.</s> <s>about two thirds of the stimuli contained at least one person.</s> <s>we demonstrate that a combined model of face detection and low-level saliency clearly outperforms a low-level model in predicting locations humans fixate.</s> <s>this is reflected in our finding fact that observes, even when not instructed to look for anything particular, fixate on a face with a probability of over 80% within their first two fixations (500ms).</s> <s>remarkably, the model's predictive performance in images that do not contain faces is not impaired by spurious face detector responses, which is suggestive of a bottom-up mechanism for face detection.</s> <s>in summary, we provide a novel computational approach which combines high level object knowledge (in our case: face locations) with low-level features to successfully predict the allocation of attentional resources.</s></p></d>", "label": ["<d><p><s>predicting human gaze using low-level saliency combined with face detection</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>large repositories of source code create new challenges and opportunities for statistical machine learning.</s> <s>here we first develop an infrastructure for the automated crawling, parsing, and database storage of open source software.</s> <s>the infrastructure allows us to gather internet-scale source code.</s> <s>for instance, in one experiment, we gather 4,632 java projects from sourceforge and apache totaling over 38 million lines of code from 9,250 developers.</s> <s>simple statistical analyses of the data first reveal robust power-law behavior for package, sloc, and method call distributions.</s> <s>we then develop and apply unsupervised author-topic, probabilistic models to automatically discover the topics embedded in the code and extract topic-word and author-topic distributions.</s> <s>in addition to serving as a convenient summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic basis for quantifying and analyzing developer similarity and competence, topic scattering, and document tangling, with direct applications to software engineering.</s> <s>finally, by combining software textual content with structural information captured by our coderank approach, we are able to significantly improve software retrieval performance, increasing the auc metric to 0.86-- roughly 10-30% better than previous approaches based on text alone.</s></p></d>", "label": ["<d><p><s>mining internet-scale software repositories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we construct a biologically motivated stochastic differential model of the neural and hemodynamic activity underlying the observed blood oxygen level dependent (bold) signal in functional magnetic resonance imaging (fmri).</s> <s>the model poses a difficult parameter estimation problem, both theoretically due to the nonlinearity and divergence of the differential system, and computationally due to its time and space complexity.</s> <s>we adapt a particle filter and smoother to the task, and discuss some of the practical approaches used to tackle the difficulties, including use of sparse matrices and parallelisation.</s> <s>results demonstrate the tractability of the approach in its application to an effective connectivity study.</s></p></d>", "label": ["<d><p><s>continuous time particle filtering for fmri</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>independent component analysis (ica) is a powerful method to decouple signals.</s> <s>most of the algorithms performing ica do not consider the temporal correlations of the signal, but only higher moments of its amplitude distribution.</s> <s>moreover, they require some preprocessing of the data (whitening) so as to remove second order correlations.</s> <s>in this paper, we are interested in understanding the neural mechanism responsible for solving ica.</s> <s>we present an online learning rule that exploits delayed correlations in the input.</s> <s>this rule performs ica by detecting joint variations in the firing rates of pre- and postsynaptic neurons, similar to a local rate-based hebbian learning rule.</s></p></d>", "label": ["<d><p><s>an online hebbian learning rule that performs independent component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a theoretical study on the discriminative clustering framework, recently proposed for simultaneous subspace selection via linear discriminant analysis (lda) and clustering.</s> <s>empirical results have shown its favorable performance in comparison with several other popular clustering algorithms.</s> <s>however, the inherent relationship between subspace selection and clustering in this framework is not well understood, due to the iterative nature of the algorithm.</s> <s>we show in this paper that this iterative subspace selection and clustering is equivalent to kernel k-means with a specific kernel gram matrix.</s> <s>this provides significant and new insights into the nature of this subspace selection procedure.</s> <s>based on this equivalence relationship, we propose the discriminative k-means (diskmeans) algorithm for simultaneous lda subspace selection and clustering, as well as an automatic parameter estimation procedure.</s> <s>we also present the nonlinear extension of diskmeans using kernels.</s> <s>we show that the learning of the kernel matrix over a convex set of pre-specified kernel matrices can be incorporated into the clustering formulation.</s> <s>the connection between diskmeans and several other clustering algorithms is also analyzed.</s> <s>the presented theories and algorithms are evaluated through experiments on a collection of benchmark data sets.</s></p></d>", "label": ["<d><p><s>discriminative k-means for clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present epoch-greedy, an algorithm for multi-armed bandits with observable side information.</s> <s>epoch-greedy has the following properties: no knowledge of a time horizon $t$ is necessary.</s> <s>the regret incurred by epoch-greedy is controlled by a sample complexity bound for a hypothesis class.</s> <s>the regret scales as $o(t^{2/3} s^{1/3})$ or better (sometimes, much better).</s> <s>here $s$ is the complexity term in a sample complexity bound for standard supervised learning.</s></p></d>", "label": ["<d><p><s>the epoch-greedy algorithm for multi-armed bandits with side information</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a new local approximation algorithm for computing map and log-partition function for arbitrary exponential family distribution represented by a finite-valued pair-wise markov random field (mrf), say g. our algorithm is based on decomposing g into appropriately chosen small components; computing estimates locally in each of these components and then producing a good global solution.</s> <s>we prove that the algorithm can provide approximate solution within arbitrary accuracy when $g$ excludes some finite sized graph as its minor and g has bounded degree: all planar graphs with bounded degree are examples of such graphs.</s> <s>the running time of the algorithm is $\\theta(n)$ (n is the number of nodes in g), with constant dependent on accuracy, degree of graph and size of the graph that is excluded as a minor (constant for planar graphs).</s> <s>our algorithm for minor-excluded graphs uses the decomposition scheme of klein, plotkin and rao (1993).</s> <s>in general, our algorithm works with any decomposition scheme and provides quantifiable approximation guarantee that depends on the decomposition scheme.</s></p></d>", "label": ["<d><p><s>local algorithms for approximate inference in minor-excluded graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise.</s> <s>our approach consists in reframing this task in a variable selection context.</s> <s>we use a penalized least-squares criterion with a l1-type penalty for this purpose.</s> <s>we prove that, in an appropriate asymptotic framework, this method provides consistent estimators of the change-points.</s> <s>then, we explain how to implement this method in practice by combining the lar algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.</s></p></d>", "label": ["<d><p><s>catching change-points with lasso</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a model that leverages the millions of clicks received by web search engines, to predict document relevance.</s> <s>this allows the comparison of ranking functions when clicks are available but complete relevance judgments are not.</s> <s>after an initial training phase using a set of relevance judgments paired with click data, we show that our model can predict the relevance score of documents that have not been judged.</s> <s>these predictions can be used to evaluate the performance of a search engine, using our novel formalization of the confidence of the standard evaluation metric discounted cumulative gain (dcg), so comparisons can be made across time and datasets.</s> <s>this contrasts with previous methods which can provide only pair-wise relevance judgements between results shown for the same query.</s> <s>when no relevance judgments are available, we can identify the better of two ranked lists up to 82% of the time, and with only two relevance judgments for each query, we can identify the better ranking up to 94% of the time.</s> <s>while our experiments are on sponsored search results, which is the financial backbone of web search, our method is general enough to be applicable to algorithmic web search results as well.</s> <s>furthermore, we give an algorithm to guide the selection of additional documents to judge to improve confidence.</s></p></d>", "label": ["<d><p><s>evaluating search engines by modeling the relationship between relevance and clicks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel method for {\\em linear} dimensionality reduction of manifold modeled data.</s> <s>first, we show that with a small number $m$ of {\\em random projections} of sample points in $\\reals^n$ belonging to an unknown $k$-dimensional euclidean manifold, the intrinsic dimension (id) of the sample set can be estimated to high accuracy.</s> <s>second, we rigorously prove that using only this set of random projections, we can estimate the structure of the underlying manifold.</s> <s>in both cases, the number random projections required is linear in $k$ and logarithmic in $n$, meaning that $k<m\\ll n$.</s> <s>to handle practical situations, we develop a greedy algorithm to estimate the smallest size of the projection space required to perform manifold learning.</s> <s>our method is particularly relevant in distributed sensing systems and leads to significant potential savings in data acquisition, storage and transmission costs.</s></p></d>", "label": ["<d><p><s>random projections for manifold learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data.</s></p></d>", "label": ["<d><p><s>learning the structure of manifolds using random projections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a probabilistic approach to language change in which word forms are represented by phoneme sequences that undergo stochastic edits along the branches of a phylogenetic tree.</s> <s>our framework combines the advantages of the classical comparative method with the robustness of corpus-based probabilistic models.</s> <s>we use this framework to explore the consequences of two different schemes for defining probabilistic models of phonological change, evaluating these schemes using the reconstruction of ancient word forms in romance languages.</s> <s>the result is an efficient inference procedure for automatically inferring ancient word forms from modern languages, which can be generalized to support inferences about linguistic phylogenies.</s></p></d>", "label": ["<d><p><s>a probabilistic approach to language change</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a provably efficient algorithm for learning markov decision processes (mdps) with continuous state and action spaces in the online setting.</s> <s>specifically, we take a model-based approach and show that a special type of online linear regression allows us to learn mdps with (possibly kernalized) linearly parameterized dynamics.</s> <s>this result builds on kearns and singh's work that provides a provably efficient algorithm for finite state mdps.</s> <s>our approach is not restricted to the linear setting, and is applicable to other classes of continuous mdps.</s></p></d>", "label": ["<d><p><s>online linear regression and its application to model-based reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of adaptive sensor control in dynamic resource-constrained sensor networks.</s> <s>we focus on a meteorological sensing network comprising radars that can perform sector scanning rather than always scanning 360 degrees.</s> <s>we compare three sector scanning strategies.</s> <s>the sit-and-spin strategy always scans 360 degrees.</s> <s>the limited lookahead strategy additionally uses the expected environmental state k decision epochs in the future, as predicted from kalman filters, in its decision-making.</s> <s>the full lookahead strategy uses all expected future states by casting the problem as a markov decision process and using reinforcement learning to estimate the optimal scan strategy.</s> <s>we show that the main benefits of using a lookahead strategy are when there are multiple meteorological phenomena in the environment, and when the maximum radius of any phenomenon is sufficiently smaller than the radius of the radars.</s> <s>we also show that there is a trade-off between the average quality with which a phenomenon is scanned and the number of decision epochs before which a phenomenon is rescanned.</s></p></d>", "label": ["<d><p><s>scan strategies for meteorological radars</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel message passing algorithm for approximating the map problem in graphical models.</s> <s>the algorithm is similar in structure to max-product but unlike max-product it always converges, and can be proven to find the exact map solution in various settings.</s> <s>the algorithm is derived via block coordinate descent in a dual of the lp relaxation of map, but does not require any tunable parameters such as step size or tree weights.</s> <s>we also describe a generalization of the method to cluster based potentials.</s> <s>the new method is tested on synthetic and real-world problems, and compares favorably with previous approaches.</s></p></d>", "label": ["<d><p><s>fixing max-product: convergent message passing algorithms for map lp-relaxations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>support vector machines (svms) suffer from a widely recognized scalability problem in both memory use and computational time.</s> <s>to improve scalability, we have developed a parallel svm algorithm (psvm), which reduces memory use through performing a row-based, approximate matrix factorization, and which loads only essential data to each machine to perform parallel computation.</s> <s>let $n$ denote the number of training instances, $p$ the reduced matrix dimension after factorization ($p$ is significantly smaller than $n$), and $m$ the number of machines.</s> <s>psvm reduces the memory requirement from $\\mo$($n^2$) to $\\mo$($np/m$), and improves computation time to $\\mo$($np^2/m$).</s> <s>empirical studies on up to $500$ computers shows psvm to be effective.</s></p></d>", "label": ["<d><p><s>parallelizing support vector machines on distributed computers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is becoming increasingly important to learn from a partially-observed random matrix and predict its missing elements.</s> <s>we assume that the entire matrix is a single sample drawn from a matrix-variate t distribution and suggest a matrix-variate t model (mvtm) to predict those missing elements.</s> <s>we show that mvtm generalizes a range of known probabilistic models, and automatically performs model selection to encourage sparse predictive models.</s> <s>due to the non-conjugacy of its prior, it is difficult to make predictions by computing the mode or mean of the posterior distribution.</s> <s>we suggest an optimization method that sequentially minimizes a convex upper-bound of the log-likelihood, which is very efficient and scalable.</s> <s>the experiments on a toy data and eachmovie dataset show a good predictive accuracy of the model.</s></p></d>", "label": ["<d><p><s>predictive matrix-variate t models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>biological movement is built up of sub-blocks or motion primitives.</s> <s>such primitives provide a compact representation of movement which is also desirable in robotic control applications.</s> <s>we analyse handwriting data to gain a better understanding of use of primitives and their timings in biological movements.</s> <s>inference of the shape and the timing of primitives can be done using a factorial hmm based model, allowing the handwriting to be represented in primitive timing space.</s> <s>this representation provides a distribution of spikes corresponding to the primitive activations, which can also be modelled using hmm architectures.</s> <s>we show how the coupling of the low level primitive model, and the higher level timing model during inference can produce good reconstructions of handwriting, with shared primitives for all characters modelled.</s> <s>this coupled model also captures the variance profile of the dataset which is accounted for by spike timing jitter.</s> <s>the timing code provides a compact representation of the movement while generating a movement without an explicit timing model produces a scribbling style of output.</s></p></d>", "label": ["<d><p><s>modelling motion primitives and their timing in biologically executed movements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images?</s> <s>if someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it?</s> <s>for example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images?</s> <s>the surprising result presented here is that not only the answer is yes but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels.</s> <s>this is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities.</s> <s>we compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited.</s></p></d>", "label": ["<d><p><s>learning the 2-d topology of images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new algorithm for on-line learning linear-threshold functions is proposed which efficiently combines second-order statistics about the data with the logarithmic behavior\" of multiplicative/dual-norm algorithms.</s> <s>an initial theoretical analysis is provided suggesting that our algorithm might be viewed as a standard perceptron algorithm operating on a transformed sequence of examples with improved margin properties.</s> <s>we also report on experiments carried out on datasets from diverse domains, with the goal of comparing to known perceptron algorithms (first-order, second-order, additive, multiplicative).</s> <s>our learning procedure seems to generalize quite well, and converges faster than the corresponding multiplicative baseline algorithms.\"</s></p></d>", "label": ["<d><p><s>on higher-order perceptron algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fair discriminative pedestrian finders are now available.</s> <s>in fact, these pedestrian finders make most errors on pedestrians in configurations that are uncommon in the training data, for example, mounting a bicycle.</s> <s>this is undesirable.</s> <s>however, the human configuration can itself be estimated discriminatively using structure learning.</s> <s>we demonstrate a pedestrian finder which first finds the most likely human pose in the window using a discriminative procedure trained with structure learning on a small dataset.</s> <s>we then present features (local histogram of oriented gradient and local pca of gradient) based on that configuration to an svm classifier.</s> <s>we show, using the inria person dataset, that estimates of configuration significantly improve the accuracy of a discriminative pedestrian finder.</s></p></d>", "label": ["<d><p><s>configuration estimates improve pedestrian finding</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we show how to use unlabeled data and a deep belief net (dbn) to learn a good covariance kernel for a gaussian process.</s> <s>we first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by hinton et.al.</s> <s>if the data is high-dimensional and highly-structured, a gaussian kernel applied to the top layer of features in the dbn works much better than a similar kernel applied to the raw input.</s> <s>performance at both regression and classification can then be further improved by using backpropagation through the dbn to discriminatively fine-tune the covariance kernel.</s></p></d>", "label": ["<d><p><s>using deep belief nets to learn covariance kernels for gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>empirical risk minimization offers well-known learning guarantees when training and test data come from the same domain.</s> <s>in the real world, though, we often wish to adapt a classifier from a source domain with a large amount of training data to different target domain with very little training data.</s> <s>in this work we give uniform convergence bounds for algorithms that minimize a convex combination of source and target empirical risk.</s> <s>the bounds explicitly model the inherent trade-off between training on a large but inaccurate source data set and a small but accurate target training set.</s> <s>our theory also gives results when we have multiple source domains, each of which may have a different number of instances, and we exhibit cases in which minimizing a non-uniform combination of source risks can achieve much lower target error than standard empirical risk minimization.</s></p></d>", "label": ["<d><p><s>learning bounds for domain adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>on-line handwriting recognition is unusual among sequence labelling tasks in that the underlying generator of the observed data, i.e.</s> <s>the movement of the pen, is recorded directly.</s> <s>however, the raw data can be difficult to interpret because each letter is spread over many pen locations.</s> <s>as a consequence, sophisticated pre-processing is required to obtain inputs suitable for conventional sequence labelling algorithms, such as hmms.</s> <s>in this paper we describe a system capable of directly transcribing raw on-line handwriting data.</s> <s>the system consists of a recurrent neural network trained for sequence labelling, combined with a probabilistic language model.</s> <s>in experiments on an unconstrained on-line database, we record excellent results using either raw or pre-processed data, well outperforming a benchmark hmm in both cases.</s></p></d>", "label": ["<d><p><s>unconstrained on-line handwriting recognition with recurrent neural networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the peristimulus time historgram (psth) and its more continuous cousin, the spike density function (sdf) are staples in the analytic toolkit of neurophysiologists.</s> <s>the former is usually obtained by binning spiketrains, whereas the standard method for the latter is smoothing with a gaussian kernel.</s> <s>selection of a bin with or a kernel size is often done in an relatively arbitrary fashion, even though there have been recent attempts to remedy this situation \\cite{shimazakibinningnips2006,shimazakibinningneco2007}.</s> <s>we develop an exact bayesian, generative model approach to estimating pshts and demonstate its superiority to competing methods.</s> <s>further advantages of our scheme include automatic complexity control and error bars on its predictions.</s></p></d>", "label": ["<d><p><s>bayesian binning beats approximate alternatives: estimating peri-stimulus time histograms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an active learning algorithm that learns a continuous valuation model from discrete preferences.</s> <s>the algorithm automatically decides what items are best presented to an individual in order to find the item that they value highly in as few trials as possible, and exploits quirks of human psychology to minimize time and cognitive burden.</s> <s>to do this, our algorithm maximizes the expected improvement at each query without accurately modelling the entire valuation surface, which would be needlessly expensive.</s> <s>the problem is particularly difficult because the space of choices is infinite.</s> <s>we demonstrate the effectiveness of the new algorithm compared to related active learning methods.</s> <s>we also embed the algorithm within a decision making tool for assisting digital artists in rendering materials.</s> <s>the tool finds the best parameters while minimizing the number of queries.</s></p></d>", "label": ["<d><p><s>active preference learning with discrete choice data</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>stimulus selectivity of sensory neurons is often characterized by estimating their receptive field properties such as orientation selectivity.</s> <s>receptive fields are usually derived from the mean (or covariance) of the spike-triggered stimulus ensemble.</s> <s>this approach treats each spike as an independent message but does not take into account that information might be conveyed through patterns of neural activity that are distributed across space or time.</s> <s>can we find a concise description for the processing of a whole population of neurons analogous to the receptive field for single neurons?</s> <s>here, we present a generalization of the linear receptive field which is not bound to be triggered on individual spikes but can be meaningfully linked to distributed response patterns.</s> <s>more precisely, we seek to identify those stimulus features and the corresponding patterns of neural activity that are most reliably coupled.</s> <s>we use an extension of reverse-correlation methods based on canonical correlation analysis.</s> <s>the resulting population receptive fields span the subspace of stimuli that is most informative about the population response.</s> <s>we evaluate our approach using both neuronal models and multi-electrode recordings from rabbit retinal ganglion cells.</s> <s>we show how the model can be extended to capture nonlinear stimulus-response relationships using kernel canonical correlation analysis, which makes it possible to test different coding mechanisms.</s> <s>our technique can also be used to calculate receptive fields from multi-dimensional neural measurements such as those obtained from dynamic imaging methods.</s></p></d>", "label": ["<d><p><s>receptive fields without spike-triggering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reliably recovering 3d human pose from monocular video requires constraints that bias the estimates towards typical human poses and motions.</s> <s>we define priors for people tracking using a laplacian eigenmaps latent variable model (lelvm).</s> <s>lelvm is a probabilistic dimensionality reduction model that naturally combines the advantages of latent variable models---definining a multimodal probability density for latent and observed variables, and globally differentiable nonlinear mappings for reconstruction and dimensionality reduction---with those of spectral manifold learning methods---no local optima, ability to unfold highly nonlinear manifolds, and good practical scaling to latent spaces of high dimension.</s> <s>lelvm is computationally efficient, simple to learn from sparse training data, and compatible with standard probabilistic trackers such as particle filters.</s> <s>we analyze the performance of a lelvm-based probabilistic sigma point mixture tracker in several real and synthetic human motion sequences and demonstrate that lelvm provides sufficient constraints for robust operation in the presence of missing, noisy and ambiguous image measurements.</s></p></d>", "label": ["<d><p><s>people tracking with the laplacian eigenmaps latent variable model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy.</s> <s>we study a variant of fitted q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values.</s> <s>we provide a rigorous theoretical analysis of this algorithm, proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.</s></p></d>", "label": ["<d><p><s>fitted q-iteration in continuous action-space mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>guided by the goal of obtaining an optimization algorithm that is both fast and yielding good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error.</s> <s>the surprising result is that from both the bayesian and frequentist perspectives this can yield the natural gradient direction.</s> <s>although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems.</s> <s>we report experimental results showing much faster convergence in computation time and in number of iterations with tonga (topmoumoute online natural gradient algorithm) than with stochastic gradient descent, even on very large datasets.</s></p></d>", "label": ["<d><p><s>topmoumoute online natural gradient algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important problem in many fields is the analysis of counts data to extract meaningful latent components.</s> <s>methods like probabilistic latent semantic analysis (plsa) and latent dirichlet allocation (lda) have been proposed for this purpose.</s> <s>however, they are limited in the number of components they can extract and also do not have a provision to control the expressiveness\" of the extracted components.</s> <s>in this paper, we present a learning formulation to address these limitations by employing the notion of sparsity.</s> <s>we start with the plsa framework and use an entropic prior in a maximum a posteriori formulation to enforce sparsity.</s> <s>we show that this allows the extraction of overcomplete sets of latent components which better characterize the data.</s> <s>we present experimental evidence of the utility of such representations.\"</s></p></d>", "label": ["<d><p><s>sparse overcomplete latent variable decomposition of counts data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>traditional analysis methods for single-trial classification of electro-encephalography (eeg) focus on two types of paradigms: phase locked methods, in which the amplitude of the signal is used as the feature for classification, i.e.</s> <s>event related potentials; and second order methods, in which the feature of interest is the power of the signal, i.e event related (de)synchronization.</s> <s>the process of deciding which paradigm to use is ad hoc and is driven by knowledge of neurological findings.</s> <s>here we propose a unified method in which the algorithm learns the best first and second order spatial and temporal features for classification of eeg based on a bilinear model.</s> <s>the efficiency of the method is demonstrated in simulated and real eeg from a benchmark data set for brain computer interface.</s></p></d>", "label": ["<d><p><s>second order bilinear discriminant analysis for single trial eeg analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of v1 (primary visual cortex) receptive fields.</s> <s>however, the resulting sparse coefficients still exhibit pronounced statistical dependencies, thus violating the independence assumption of the sparse coding model.</s> <s>here, we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states.</s> <s>when adapted to the statistics of natural images, the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions.</s> <s>these learned interactions may offer an explanation for the function of horizontal connections in v1, and we discuss the implications of our findings for physiological experiments.</s></p></d>", "label": ["<d><p><s>learning horizontal connections in a sparse coding model of natural images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the notion of algorithmic stability has been used effectively in the past to derive tight generalization bounds.</s> <s>a key advantage of these bounds is that they are de- signed for specific learning algorithms, exploiting their particular properties.</s> <s>but, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed (i.i.d.).</s> <s>in many machine learning applications, however, this assumption does not hold.</s> <s>the observations received by the learning algorithm often have some inherent temporal dependence, which is clear in system diagnosis or time series prediction problems.</s> <s>this paper studies the scenario where the observations are drawn from a station- ary beta-mixing sequence, which implies a dependence between observations that weaken over time.</s> <s>it proves novel stability-based generalization bounds that hold even with this more general setting.</s> <s>these bounds strictly generalize the bounds given in the i.i.d.</s> <s>case.</s> <s>we also illustrate their application in the case of several general classes of learning algorithms, including support vector regression and kernel ridge regression.</s></p></d>", "label": ["<d><p><s>stability bounds for non-i.i.d. processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper explores the use of a maximal average margin (mam) optimality principle for the design of learning algorithms.</s> <s>it is shown that the application of this risk minimization principle results in a class of (computationally) simple learning machines similar to the classical parzen window classifier.</s> <s>a direct relation with the rademacher complexities is established, as such facilitating analysis and providing a notion of certainty of prediction.</s> <s>this analysis is related to support vector machines by means of a margin transformation.</s> <s>the power of the mam principle is illustrated further by application to ordinal regression tasks, resulting in an $o(n)$ algorithm able to process large datasets in reasonable time.</s></p></d>", "label": ["<d><p><s>a risk minimization principle for a class of parzen estimators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sound localization by barn owls is commonly modeled as a matching procedure where localization cues derived from auditory inputs are compared to stored templates.</s> <s>while the matching models can explain properties of neural responses, no model explains how the owl resolves spatial ambiguity in the localization cues to produce accurate localization near the center of gaze.</s> <s>here, we examine two models for the barn owl's sound localization behavior.</s> <s>first, we consider a maximum likelihood estimator in order to further evaluate the cue matching model.</s> <s>second, we consider a maximum a posteriori estimator to test if a bayesian model with a prior that emphasizes directions near the center of gaze can reproduce the owl's localization behavior.</s> <s>we show that the maximum likelihood estimator can not reproduce the owl's behavior, while the maximum a posteriori estimator is able to match the behavior.</s> <s>this result suggests that the standard cue matching model will not be sufficient to explain sound localization behavior in the barn owl.</s> <s>the bayesian model provides a new framework for analyzing sound localization in the barn owl and leads to predictions about the owl's localization behavior.</s></p></d>", "label": ["<d><p><s>optimal models of sound localization by barn owls</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when training and test samples follow different input distributions (i.e., the situation called \\emph{covariate shift}), the maximum likelihood estimator is known to lose its consistency.</s> <s>for regaining consistency, the log-likelihood terms need to be weighted according to the \\emph{importance} (i.e., the ratio of test and training input densities).</s> <s>thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation.</s> <s>a naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates.</s> <s>however, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases.</s> <s>in this paper, we propose a direct importance estimation method that does not require the input density estimates.</s> <s>our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized.</s> <s>this is an advantage over a recently developed method of direct importance estimation.</s> <s>simulations illustrate the usefulness of our approach.</s></p></d>", "label": ["<d><p><s>direct importance estimation with model selection and its application to covariate shift adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations, and empirical equilibria which are reached by rational players.</s> <s>rational players are modelled by players using no regret algorithms, which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm's suggested action.</s> <s>we show that for a given set of deviations over the strategy set of a player, it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations.</s> <s>further, we show that if all players use a no regret algorithm, then the empirical distribution of their plays converges to an equilibrium.</s></p></d>", "label": ["<d><p><s>computational equivalence of fixed points and no regret algorithms, and convergence to equilibria</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>planning in partially observable environments remains a challenging problem, despite significant recent advances in offline approximation techniques.</s> <s>a few online methods have also been proposed recently, and proven to be remarkably scalable, but without the theoretical guarantees of their offline counterparts.</s> <s>thus it seems natural to try to unify offline and online techniques, preserving the theoretical properties of the former, and exploiting the scalability of the latter.</s> <s>in this paper, we provide theoretical guarantees on an anytime algorithm for pomdps which aims to reduce the error made by approximate offline value iteration algorithms through the use of an efficient online searching procedure.</s> <s>the algorithm uses search heuristics based on an error analysis of lookahead search, to guide the online search towards reachable beliefs with the most potential to reduce error.</s> <s>we provide a general theorem showing that these search heuristics are admissible, and lead to complete and epsilon-optimal algorithms.</s> <s>this is, to the best of our knowledge, the strongest theoretical result available for online pomdp solution methods.</s> <s>we also provide empirical evidence showing that our approach is also practical, and can find (provably) near-optimal solutions in reasonable time.</s></p></d>", "label": ["<d><p><s>theoretical analysis of heuristic search methods for online pomdps</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in a multiple instance (mi) learning problem, instances are naturally organized into bags and it is the bags, instead of individual instances, that are labeled for training.</s> <s>mi learners assume that every instance in a bag labeled negative is actually negative, whereas at least one instance in a bag labeled positive is actually positive.</s> <s>we present a framework for active learning in the multiple-instance setting.</s> <s>in particular, we consider the case in which an mi learner is allowed to selectively query unlabeled instances in positive bags.</s> <s>this approach is well motivated in domains in which it is inexpensive to acquire bag labels and possible, but expensive, to acquire instance labels.</s> <s>we describe a method for learning from labels at mixed levels of granularity, and introduce two active query selection strategies motivated by the mi setting.</s> <s>our experiments show that learning from instance labels can significantly improve performance of a basic mi learning algorithm in two multiple-instance domains: content-based image recognition and text classification.</s></p></d>", "label": ["<d><p><s>multiple-instance active learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we describe a novel noisy-logical distribution for representing the distribution of a binary output variable conditioned on multiple binary input variables.</s> <s>the distribution is represented in terms of noisy-or's and noisy-and-not's of causal features which are conjunctions of the binary inputs.</s> <s>the standard noisy-or and noisy-and-not models, used in causal reasoning and artificial intelligence, are special cases of the noisy-logical distribution.</s> <s>we prove that the noisy-logical distribution is complete in the sense that it can represent all conditional distributions provided a sufficient number of causal factors are used.</s> <s>we illustrate the noisy-logical distribution by showing that it can account for new experimental findings on how humans perform causal reasoning in more complex contexts.</s> <s>finally, we speculate on the use of the noisy-logical distribution for causal reasoning and artificial intelligence.</s></p></d>", "label": ["<d><p><s>the noisy-logical distribution and its application to causal inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new analysis for the combination of binary classifiers.</s> <s>we propose a theoretical framework based on the neyman-pearson lemma to analyze combinations of classifiers.</s> <s>in particular, we give a method for finding the optimal decision rule for a combination of classifiers and prove that it has the optimal roc curve.</s> <s>we also show how our method generalizes and improves on previous work on combining classifiers and generating roc curves.</s></p></d>", "label": ["<d><p><s>optimal roc curve for a combination of classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the classical hypothesis, that bottom-up saliency is a center-surround process, is combined with a more recent hypothesis that all saliency decisions are optimal in a decision-theoretic sense.</s> <s>the combined hypothesis is denoted as discriminant center-surround saliency, and the corresponding optimal saliency architecture is derived.</s> <s>this architecture equates the saliency of each image location to the discriminant power of a set of features with respect to the classification problem that opposes stimuli at center and surround, at that location.</s> <s>it is shown that the resulting saliency detector makes accurate quantitative predictions for various aspects of the psychophysics of human saliency, including non-linear properties beyond the reach of previous saliency models.</s> <s>furthermore, it is shown that discriminant center-surround saliency can be easily generalized to various stimulus modalities (such as color, orientation and motion), and provides optimal solutions for many other saliency problems of interest for computer vision.</s> <s>optimal solutions, under this hypothesis, are derived for a number of the former (including static natural images, dense motion fields, and even dynamic textures), and applied to a number of the latter (the prediction of human eye fixations, motion-based saliency in the presence of ego-motion, and motion-based saliency in the presence of highly dynamic backgrounds).</s> <s>in result, discriminant saliency is shown to predict eye fixations better than previous models, and produce background subtraction algorithms that outperform the state-of-the-art in computer vision.</s></p></d>", "label": ["<d><p><s>the discriminant center-surround hypothesis for bottom-up saliency</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cascade detectors have been shown to operate extremely rapidly, with high accuracy, and have important applications such as face detection.</s> <s>driven by this success, cascade earning has been an area of active research in recent years.</s> <s>nevertheless, there are still challenging technical problems during the training process of cascade detectors.</s> <s>in particular, determining the optimal target detection rate for each stage of the cascade remains an unsolved issue.</s> <s>in this paper, we propose the multiple instance pruning (mip) algorithm for soft cascades.</s> <s>this algorithm computes a set of thresholds which aggressively terminate computation with no reduction in detection rate or increase in false positive rate on the training dataset.</s> <s>the algorithm is based on two key insights: i) examples that are destined to be rejected by the complete classifier can be safely pruned early; ii) face detection is a multiple instance learning problem.</s> <s>the mip process is fully automatic and requires no assumptions of probability distributions, statistical independence, or ad hoc intermediate rejection targets.</s> <s>experimental results on the mit+cmu dataset demonstrate significant performance advantages.</s></p></d>", "label": ["<d><p><s>multiple-instance pruning for learning efficient cascade detectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel linear clustering framework (diffrac) which relies on a linear discriminative cost function and a convex relaxation of a combinatorial optimization problem.</s> <s>the large convex optimization problem is solved through a sequence of lower dimensional singular value decompositions.</s> <s>this framework has several attractive properties: (1) although apparently similar to k-means, it exhibits superior clustering performance than k-means, in particular in terms of robustness to noise.</s> <s>(2) it can be readily extended to non linear clustering if the discriminative cost function is based on positive definite kernels, and can then be seen as an alternative to spectral clustering.</s> <s>(3) prior information on the partition is easily incorporated, leading to state-of-the-art performance for semi-supervised learning, for clustering or classification.</s> <s>we present empirical evaluations of our algorithms on synthetic and real medium-scale datasets.</s></p></d>", "label": ["<d><p><s>diffrac: a discriminative and flexible framework for clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many tasks in speech processing involve classification of long term characteristics of a speech segment such as language, speaker, dialect, or topic.</s> <s>a natural technique for determining these characteristics is to first convert the input speech into a sequence of tokens such as words, phones, etc.</s> <s>from these tokens, we can then look for distinctive phrases, keywords, that characterize the speech.</s> <s>in many applications, a set of distinctive keywords may not be known a priori.</s> <s>in this case, an automatic method of building up keywords from short context units such as phones is desirable.</s> <s>we propose a method for construction of keywords based upon support vector machines.</s> <s>we cast the problem of keyword selection as a feature selection problem for n-grams of phones.</s> <s>we propose an alternating filter-wrapper method that builds successively longer keywords.</s> <s>application of this method on a language recognition task shows that the technique produces interesting and significant qualitative and quantitative results.</s></p></d>", "label": ["<d><p><s>discriminative keyword selection using support vector machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the estimation problem in gaussian graphical models with arbitrary structure.</s> <s>we analyze the embedded trees algorithm, which solves a sequence of problems on tractable subgraphs thereby leading to the solution of the estimation problem on an intractable graph.</s> <s>our analysis is based on the recently developed walk-sum interpretation of gaussian estimation.</s> <s>we show that non-stationary iterations of the embedded trees algorithm using any sequence of subgraphs converge in walk-summable models.</s> <s>based on walk-sum calculations, we develop adaptive methods that optimize the choice of subgraphs used at each iteration with a view to achieving maximum reduction in error.</s> <s>these adaptive procedures provide a significant speedup in convergence over stationary iterative methods, and also appear to converge in a larger class of models.</s></p></d>", "label": ["<d><p><s>adaptive embedded subgraph algorithms using walk-sum analysis</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in recent years, the language model latent dirichlet allocation (lda), which clusters co-occurring words into topics, has been widely appled in the computer vision field.</s> <s>however, many of these applications have difficulty with modeling the spatial and temporal structure among visual words, since lda assumes that a document is a ``bag-of-words''.</s> <s>it is also critical to properly design ``words'' and ?documents?</s> <s>when using a language model to solve vision problems.</s> <s>in this paper, we propose a topic model spatial latent dirichlet allocation (slda), which better encodes spatial structure among visual words that are essential for solving many vision problems.</s> <s>the spatial information is not encoded in the value of visual words but in the design of documents.</s> <s>instead of knowing the partition of words into documents \\textit{a priori}, the word-document assignment becomes a random hidden variable in slda.</s> <s>there is a generative procedure, where knowledge of spatial structure can be flexibly added as a prior, grouping visual words which are close in space into the same document.</s> <s>we use slda to discover objects from a collection of images, and show it achieves better performance than lda.</s></p></d>", "label": ["<d><p><s>spatial latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe an efficient learning procedure for multilayer generative models that combine the best aspects of markov random fields and deep, directed belief nets.</s> <s>the generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers.</s> <s>each hidden layer has its own mrf whose energy function is modulated by the top-down directed connections from the layer above.</s> <s>to generate from the model, each layer in turn must settle to equilibrium given its top-down input.</s> <s>we show that this type of model is good at capturing the statistics of patches of natural images.</s></p></d>", "label": ["<d><p><s>modeling image patches with a directed hierarchy of markov random fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent research has studied the role of sparsity in high dimensional regression and signal reconstruction, establishing theoretical limits for recovering sparse models from sparse data.</s> <s>in this paper we study a variant of this problem where the original $n$ input variables are compressed by a random linear transformation to $m \\ll n$ examples in $p$ dimensions, and establish conditions under which a sparse linear model can be successfully recovered from the compressed data.</s> <s>a primary motivation for this compression procedure is to anonymize the data and preserve privacy by revealing little information about the original data.</s> <s>we characterize the number of random projections that are required for $\\ell_1$-regularized compressed regression to identify the nonzero coefficients in the true model with probability approaching one, a property called ``sparsistence.''</s> <s>in addition, we show that $\\ell_1$-regularized compressed regression asymptotically predicts as well as an oracle linear model, a property called ``persistence.''</s> <s>finally, we characterize the privacy properties of the compression procedure in information-theoretic terms, establishing upper bounds on the rate of information communicated between the compressed and uncompressed data that decay to zero.</s></p></d>", "label": ["<d><p><s>compressed regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>diffusion processes are a family of continuous-time continuous-state stochastic processes that are in general only partially observed.</s> <s>the joint estimation of the forcing parameters and the system noise (volatility) in these dynamical systems is a crucial, but non-trivial task, especially when the system is nonlinear and multi-modal.</s> <s>we propose a variational treatment of diffusion processes, which allows us to estimate these parameters by simple gradient techniques and which is computationally less demanding than most mcmc approaches.</s> <s>furthermore, our parameter inference scheme does not break down when the time step gets smaller, unlike most current approaches.</s> <s>finally, we show how a cheap estimate of the posterior over the parameters can be constructed based on the variational free energy.</s></p></d>", "label": ["<d><p><s>variational inference for diffusion processes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we develop a gaussian process (gp) framework to model a collection of reciprocal random variables defined on the \\emph{edges} of a network.</s> <s>we show how to construct gp priors, i.e.,~covariance functions, on the edges of directed, undirected, and bipartite graphs.</s> <s>the model suggests an intimate connection between \\emph{link prediction} and \\emph{transfer learning}, which were traditionally considered two separate research topics.</s> <s>though a straightforward gp inference has a very high complexity, we develop an efficient learning algorithm that can handle a large number of observations.</s> <s>the experimental results on several real-world data sets verify superior learning capacity.</s></p></d>", "label": ["<d><p><s>gaussian process models for link analysis and transfer learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>loopy belief propagation has been employed in a wide variety of applications with great empirical success, but it comes with few theoretical guarantees.</s> <s>in this paper we investigate the use of the max-product form of belief propagation for weighted matching problems on general graphs.</s> <s>we show that max-product converges to the correct answer if the linear programming (lp) relaxation of the weighted matching problem is tight and does not converge if the lp relaxation is loose.</s> <s>this provides an exact characterization of max-product performance and reveals connections to the widely used optimization technique of lp relaxation.</s> <s>in addition, we demonstrate that max-product is effective in solving practical weighted matching problems in a distributed fashion by applying it to the problem of self-organization in sensor networks.</s></p></d>", "label": ["<d><p><s>linear programming analysis of loopy belief propagation for weighted matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>using multiple regularization hyperparameters is an effective method for managing model complexity in problems where input features have varying amounts of noise.</s> <s>while algorithms for choosing multiple hyperparameters are often used in neural networks and support vector machines, they are not common in structured prediction tasks, such as sequence labeling or parsing.</s> <s>in this paper, we consider the problem of learning regularization hyperparameters for log-linear models, a class of probabilistic models for structured prediction tasks which includes conditional random fields (crfs).</s> <s>using an implicit differentiation trick, we derive an efficient gradient-based method for learning gaussian regularization priors with multiple hyperparameters.</s> <s>in both simulations and the real-world task of computational rna secondary structure prediction, we find that multiple hyperparameter learning provides a significant boost in accuracy compared to models learned using only a single regularization hyperparameter.</s></p></d>", "label": ["<d><p><s>efficient multiple hyperparameter learning for log-linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes a new model for human visual classification that enables the recovery of image features that explain human subjects' performance on different visual classification tasks.</s> <s>unlike previous methods, this algorithm does not model their performance with a single linear classifier operating on raw image pixels.</s> <s>instead, it models classification as the combination of multiple feature detectors.</s> <s>this approach extracts more information about human visual classification than has been previously possible with other methods and provides a foundation for further exploration.</s></p></d>", "label": ["<d><p><s>grift: a graphical model for inferring visual classification features from human data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>web servers on the internet need to maintain high reliability, but the cause of intermittent failures of web transactions is non-obvious.</s> <s>we use bayesian inference to diagnose problems with web services.</s> <s>this diagnosis problem is far larger than any previously attempted: it requires inference of 10^4 possible faults from 10^5 observations.</s> <s>further, such inference must be performed in less than a second.</s> <s>inference can be done at this speed by combining a variational approximation, a mean-field approximation, and the use of stochastic gradient descent to optimize a variational cost function.</s> <s>we use this fast inference to diagnose a time series of anomalous http requests taken from a real web service.</s> <s>the inference is fast enough to analyze network logs with billions of entries in a matter of hours.</s></p></d>", "label": ["<d><p><s>fast variational inference for large-scale internet diagnosis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this article discusses a latent variable model for inference and prediction of symmetric relational data.</s> <s>the model, based on the idea of the eigenvalue decomposition, represents the relationship between two nodes as the weighted inner-product of node-specific vectors of latent characteristics.</s> <s>this ``eigenmodel'' generalizes other popular latent variable models, such as latent class and distance models: it is shown mathematically that any latent class or distance model has a representation as an eigenmodel, but not vice-versa.</s> <s>the practical implications of this are examined in the context of three real datasets, for which the eigenmodel has as good or better out-of-sample predictive performance than the other two models.</s></p></d>", "label": ["<d><p><s>modeling homophily and stochastic equivalence in symmetric relational data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>active learning sequentially selects unlabeled instances to label with the goal of reducing the effort needed to learn a good classifier.</s> <s>most previous studies in active learning have focused on selecting one unlabeled instance at one time while retraining in each iteration.</s> <s>however, single instance selection systems are unable to exploit a parallelized labeler when one is available.</s> <s>recently a few batch mode active learning approaches have been proposed that select a set of most informative unlabeled instances in each iteration, guided by some heuristic scores.</s> <s>in this paper, we propose a discriminative batch mode active learning approach that formulates the instance selection task as a continuous optimization problem over auxiliary instance selection variables.</s> <s>the optimization is formuated to maximize the discriminative classification performance of the target classifier, while also taking the unlabeled data into account.</s> <s>although the objective is not convex, we can manipulate a quasi-newton method to obtain a good local solution.</s> <s>our empirical studies on uci datasets show that the proposed active learning is more effective than current state-of-the art batch mode active learning algorithms.</s></p></d>", "label": ["<d><p><s>discriminative batch mode active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov jump processes play an important role in a large number of application domains.</s> <s>however, realistic systems are analytically intractable and they have traditionally been analysed using simulation based techniques, which do not provide a framework for statistical inference.</s> <s>we propose a mean field approximation to perform posterior inference and parameter estimation.</s> <s>the approximation allows a practical solution to the inference problem, {while still retaining a good degree of accuracy.}</s> <s>we illustrate our approach on two biologically motivated systems.</s></p></d>", "label": ["<d><p><s>variational inference for markov jump processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the control of high-dimensional, continuous, non-linear systems is a key problem in reinforcement learning and control.</s> <s>local, trajectory-based methods, using techniques such as differential dynamic programming (ddp) are not directly subject to the curse of dimensionality, but generate only local controllers.</s> <s>in this paper, we introduce receding horizon ddp (rh-ddp), an extension to the classic ddp algorithm, which allows us to construct stable and robust controllers based on a library of local-control trajectories.</s> <s>we demonstrate the effectiveness of our approach on a series of high-dimensional control problems using a simulated multi-link swimming robot.</s> <s>these experiments show that our approach effectively circumvents dimensionality issues, and is capable of dealing effectively with problems with (at least) 34 state and 14 action dimensions.</s></p></d>", "label": ["<d><p><s>receding horizon differential dynamic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the learning task consisting in predicting as well as the best function in a finite reference set g up to the smallest possible additive term.</s> <s>if r(g) denotes the generalization error of a prediction function g, under reasonable assumptions on the loss function (typically satisfied by the least square loss when the output is bounded), it is known that the progressive mixture rule g_n satisfies e r(g_n) < min_{g in g} r(g) + cst (log|g|)/n where n denotes the size of the training set, e denotes the expectation wrt the training set distribution.</s> <s>this work shows that, surprisingly, for appropriate reference sets g, the deviation convergence rate of the progressive mixture rule is only no better than cst / sqrt{n}, and not the expected cst / n. it also provides an algorithm which does not suffer from this drawback.</s></p></d>", "label": ["<d><p><s>progressive mixture rules are deviation suboptimal</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces kernels on attributed pointsets, which are sets of vectors embedded in an euclidean space.</s> <s>the embedding gives the notion of neighborhood, which is used to define positive semidefinite kernels on pointsets.</s> <s>two novel kernels on neighborhoods are proposed, one evaluating the attribute similarity and the other evaluating shape similarity.</s> <s>shape similarity function is motivated from spectral graph matching techniques.</s> <s>the kernels are tested on three real life applications: face recognition, photo album tagging, and shot annotation in video sequences, with encouraging results.</s></p></d>", "label": ["<d><p><s>kernels on attributed pointsets with applications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a general boosting method extending functional gradient boosting to optimize complex loss functions that are encountered in many machine learning problems.</s> <s>our approach is based on optimization of quadratic upper bounds of the loss functions which allows us to present a rigorous convergence analysis of the algorithm.</s> <s>more importantly, this general framework enables us to use a standard regression base learner such as decision trees for fitting any loss function.</s> <s>we illustrate an application of the proposed method in learning ranking functions for web search by combining both preference data and labeled data for training.</s> <s>we present experimental results for web search using data from a commercial search engine that show significant improvements of our proposed methods over some existing methods.</s></p></d>", "label": ["<d><p><s>a general boosting method and its application to learning ranking functions for web search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of factorial learning which associates a set of latent causes or features with the observed data.</s> <s>factorial models usually assume that each feature has a single occurrence in a given data point.</s> <s>however, there are data such as images where latent features have multiple occurrences, e.g.</s> <s>a visual object class can have multiple instances shown in the same image.</s> <s>to deal with such cases, we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns.</s> <s>this model can play the role of the prior in an nonparametric bayesian learning scenario where both the latent features and the number of their occurrences are unknown.</s> <s>we use this prior together with a likelihood model for unsupervised learning from images using a markov chain monte carlo inference algorithm.</s></p></d>", "label": ["<d><p><s>the infinite gamma-poisson feature model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computational models of visual cortex, and in particular those based on sparse coding, have enjoyed much recent attention.</s> <s>despite this currency, the question of how sparse or how over-complete a sparse representation should be, has gone without principled answer.</s> <s>here, we use bayesian model-selection methods to address these questions for a sparse-coding model based on a student-t prior.</s> <s>having validated our methods on toy data, we find that natural images are indeed best modelled by extremely sparse distributions; although for the student-t prior, the associated optimal basis size is only modestly overcomplete.</s></p></d>", "label": ["<d><p><s>on sparsity and overcompleteness in image models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>motivated in part by the hierarchical organization of cortex, a number of algorithms have recently been proposed that try to learn hierarchical, or ``deep,'' structure from unlabeled data.</s> <s>while several authors have formally or informally compared their algorithms to computations performed in visual area v1 (and the cochlea), little attempt has been made thus far to evaluate these algorithms in terms of their fidelity for mimicking computations at deeper levels in the cortical hierarchy.</s> <s>this paper presents an unsupervised learning model that faithfully mimics certain properties of visual area v2.</s> <s>specifically, we develop a sparse variant of the deep belief networks of hinton et al.</s> <s>(2006).</s> <s>we learn two layers of nodes in the network, and demonstrate that the first layer, similar to prior work on sparse coding and ica, results in localized, oriented, edge filters, similar to the gabor functions known to model v1 cell receptive fields.</s> <s>further, the second layer in our model encodes correlations of the first layer responses in the data.</s> <s>specifically, it picks up both collinear (``contour'') features as well as corners and junctions.</s> <s>more interestingly, in a quantitative comparison, the encoding of these more complex ``corner'' features matches well with the results from the ito & komatsu's study of biological v2 responses.</s> <s>this suggests that our sparse variant of deep belief networks holds promise for modeling more higher-order features.</s></p></d>", "label": ["<d><p><s>sparse deep belief net model for visual area v2</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we summarize the implementation of an analog vlsi chip hosting a network of 32 integrate-and-fire (if) neurons with spike-frequency adaptation and 2,048 hebbian plastic bistable spike-driven stochastic synapses endowed with a self-regulating mechanism which stops unnecessary synaptic changes.</s> <s>the synaptic matrix can be flexibly configured and provides both recurrent and aer-based connectivity with external, aer compliant devices.</s> <s>we demonstrate the ability of the network to efficiently classify overlapping patterns, thanks to the self-regulating mechanism.</s></p></d>", "label": ["<d><p><s>a configurable analog vlsi neural network with spiking neurons and self-regulating plastic synapses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>functional magnetic resonance imaging (fmri) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels during hundreds of sequential time points.</s> <s>unfortunately, the interpretation of fmri is complicated due both to the relatively unknown connection between the hemodynamic response and neural activity and the unknown spatiotemporal characteristics of the cognitive patterns themselves.</s> <s>here, we use data from the experience based cognition competition to compare global and local methods of prediction applying both linear and nonlinear techniques of dimensionality reduction.</s> <s>we build global low dimensional representations of an fmri dataset, using linear and nonlinear methods.</s> <s>we learn a set of time series that are implicit functions of the fmri data, and predict the values of these times series in the future from the knowledge of the fmri data only.</s> <s>we find effective, low-dimensional models based on the principal components of cognitive activity in classically-defined anatomical regions, the brodmann areas.</s> <s>furthermore for some of the stimuli, the top predictive regions were stable across subjects and episodes, including wernicke?s area for verbal instructions, visual cortex for facial and body features, and visual-temporal regions (brodmann area 7) for velocity.</s> <s>these interpretations and the relative simplicity of our approach provide a transparent and conceptual basis upon which to build more sophisticated techniques for fmri decoding.</s> <s>to our knowledge, this is the first time that classical areas have been used in fmri for an effective prediction of complex natural experience.</s></p></d>", "label": ["<d><p><s>locality and low-dimensions in the prediction of natural experience from fmri</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms.</s> <s>the analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems.</s> <s>small-scale learning problems are subject to the usual approximation--estimation tradeoff.</s> <s>large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.</s></p></d>", "label": ["<d><p><s>the tradeoffs of large scale learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a method for reconstruction of human brain states directly from functional neuroimaging data.</s> <s>the method extends the traditional multivariate regression analysis of discretized fmri data to the domain of stochastic functional measurements, facilitating evaluation of brain responses to naturalistic stimuli and boosting the power of functional imaging.</s> <s>the method searches for sets of voxel timecourses that optimize a multivariate functional linear model in terms of rsquare-statistic.</s> <s>population based incremental learning is used to search for spatially distributed voxel clusters, taking into account the variation in haemodynamic lag across brain areas and among subjects by voxel-wise non-linear registration of stimuli to fmri data.</s> <s>the method captures spatially distributed brain responses to naturalistic stimuli without attempting to localize function.</s> <s>application of the method for prediction of naturalistic stimuli from new and unknown fmri data shows that the approach is capable of identifying distributed clusters of brain locations that are highly predictive of a specific stimuli.</s></p></d>", "label": ["<d><p><s>predicting brain states from fmri data: incremental functional principal component regression</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we introduce supervised latent dirichlet allocation (slda), a statistical model of labelled documents.</s> <s>the model accommodates a variety of response types.</s> <s>we derive a maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations.</s> <s>prediction problems motivate this research: we use the fitted model to predict response values for new documents.</s> <s>we test slda on two real-world problems: movie ratings predicted from reviews, and web page popularity predicted from text descriptions.</s> <s>we illustrate the benefits of slda versus modern regularized regression, as well as versus an unsupervised lda analysis followed by a separate regression.</s></p></d>", "label": ["<d><p><s>supervised topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an algorithm called optimistic linear programming (olp) for learning to optimize average reward in an irreducible but otherwise unknown markov decision process (mdp).</s> <s>olp uses its experience so far to estimate the mdp.</s> <s>it chooses actions by optimistically maximizing estimated future rewards over a set of next-state transition probabilities that are close to the estimates: a computation that corresponds to solving linear programs.</s> <s>we show that the total expected reward obtained by olp up to time $t$ is within $c(p)\\log t$ of the reward obtained by the optimal policy, where $c(p)$ is an explicit, mdp-dependent constant.</s> <s>olp is closely related to an algorithm proposed by burnetas and katehakis with four key differences: olp is simpler, it does not require knowledge of the supports of transition probabilities and the proof of the regret bound is simpler, but our regret bound is a constant factor larger than the regret of their algorithm.</s> <s>olp is also similar in flavor to an algorithm recently proposed by auer and ortner.</s> <s>but olp is simpler and its regret bound has a better dependence on the size of the mdp.</s></p></d>", "label": ["<d><p><s>optimistic linear programming gives logarithmic regret for irreducible mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>binocular fusion takes place over a limited region smaller than one degree of visual angle (panum's fusional area), which is on the order of the range of preferred disparities measured in populations of disparity-tuned neurons in the visual cortex.</s> <s>however, the actual range of binocular disparities encountered in natural scenes ranges over tens of degrees.</s> <s>this discrepancy suggests that there must be a mechanism for detecting whether the stimulus disparity is either inside or outside of the range of the preferred disparities in the population.</s> <s>here, we present a statistical framework to derive feature in a population of v1 disparity neuron to determine the stimulus disparity within the preferred disparity range of the neural population.</s> <s>when optimized for natural images, it yields a feature that can be explained by the normalization which is a common model in v1 neurons.</s> <s>we further makes use of the feature to estimate the disparity in natural images.</s> <s>our proposed model generates more correct estimates than coarse-to-fine multiple scales approaches and it can also identify regions with occlusion.</s> <s>the approach suggests another critical role for normalization in robust disparity estimation.</s></p></d>", "label": ["<d><p><s>estimating disparity with confidence from energy neurons</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose to test for the homogeneity of two samples by using kernel fisher discriminant analysis.</s> <s>this provides us with a consistent nonparametric test statistic, for which we derive the asymptotic distribution under the null hypothesis.</s> <s>we give experimental evidence of the relevance of our method on both artificial and real datasets.</s></p></d>", "label": ["<d><p><s>testing for homogeneity with kernel fisher discriminant analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a method for support vector machine classification using indefinite kernels.</s> <s>instead of directly minimizing or stabilizing a nonconvex loss function, our method simultaneously finds the support vectors and a proxy kernel matrix used in computing the loss.</s> <s>this can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true positive semidefinite kernel.</s> <s>our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method.</s> <s>we compare the performance of our technique with other methods on several data sets.</s></p></d>", "label": ["<d><p><s>support vector machine classification with indefinite kernels</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a gaussian process (gp) framework for robust inference in which a gp prior on the mixing weights of a two-component noise model augments the standard process over latent function values.</s> <s>this approach is a generalization of the mixture likelihood used in traditional robust gp regression, and a specialization of the gp mixture models suggested by tresp (2000) and rasmussen and ghahramani (2002).</s> <s>the value of this restriction is in its tractable expectation propagation updates, which allow for faster inference and model selection, and better convergence than the standard mixture.</s> <s>an additional benefit over the latter method lies in our ability to incorporate knowledge of the noise domain to influence predictions, and to recover with the predictive distribution information about the outlier distribution via the gating process.</s> <s>the model has asymptotic complexity equal to that of conventional robust methods, but yields more confident predictions on benchmark problems than classical heavy-tailed models and exhibits improved stability for data with clustered corruptions, for which they fail altogether.</s> <s>we show further how our approach can be used without adjustment for more smoothly heteroscedastic data, and suggest how it could be extended to more general noise models.</s> <s>we also address similarities with the work of goldberg et al.</s> <s>(1998), and the more recent contributions of tresp, and rasmussen and ghahramani.</s></p></d>", "label": ["<d><p><s>robust regression with twinned gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>adaptation to other initially unknown agents often requires computing an effective counter-strategy.</s> <s>in the bayesian paradigm, one must find a good counter-strategy to the inferred posterior of the other agents' behavior.</s> <s>in the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior.</s> <s>in this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms.</s> <s>the strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed.</s> <s>the technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games.</s> <s>we demonstrate the effectiveness of the technique in two-player texas hold'em.</s> <s>we show that the computed poker strategies are substantially more robust than best response counter-strategies, while still exploiting a suspected tendency.</s> <s>we also compose the generated strategies in an experts algorithm showing a dramatic improvement in performance over using simple best responses.</s></p></d>", "label": ["<d><p><s>computing robust counter-strategies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new and efficient semi-supervised training method for parameter estimation and feature selection in conditional random fields (crfs).</s> <s>in real-world applications such as activity recognition, unlabeled sensor traces are relatively easy to obtain whereas labeled examples are expensive and tedious to collect.</s> <s>furthermore, the ability to automatically select a small subset of discriminatory features from a large pool can be advantageous in terms of computational speed as well as accuracy.</s> <s>in this paper, we introduce the semi-supervised virtual evidence boosting (sveb) algorithm for training crfs -- a semi-supervised extension to the recently developed virtual evidence boosting (veb) method for feature selection and parameter learning.</s> <s>semi-supervised veb takes advantage of the unlabeled data via minimum entropy regularization -- the objective function combines the unlabeled conditional entropy with labeled conditional pseudo-likelihood.</s> <s>the sveb algorithm reduces the overall system cost as well as the human labeling cost required during training, which are both important considerations in building real world inference systems.</s> <s>in a set of experiments on synthetic data and real activity traces collected from wearable sensors, we illustrate that our algorithm benefits from both the use of unlabeled data and automatic feature selection, and outperforms other semi-supervised training approaches.</s></p></d>", "label": ["<d><p><s>fast and scalable training of semi-supervised crfs with application to activity recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we combine two threads of research on approximate dynamic programming: random sampling of states and using local trajectory optimizers to globally optimize a policy and associated value function.</s> <s>this combination allows us to replace a dense multidimensional grid with a much sparser adaptive sampling of states.</s> <s>our focus is on finding steady state policies for the deterministic time invariant discrete time control problems with continuous states and actions often found in robotics.</s> <s>in this paper we show that we can now solve problems we couldn't solve previously with regular grid-based approaches.</s></p></d>", "label": ["<d><p><s>random sampling of states in dynamic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a randomized algorithm for large scale svm learning which solves the problem by iterating over random subsets of the data.</s> <s>crucial to the algorithm for scalability is the size of the subsets chosen.</s> <s>in the context of text classification we show that, by using ideas from random projections, a sample size of o(log n) can be used to obtain a solution which is close to the optimal with a high probability.</s> <s>experiments done on synthetic and real life data sets demonstrate that the algorithm scales up svm learners, without loss in accuracy.</s></p></d>", "label": ["<d><p><s>a randomized algorithm for large scale support vector learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational methods are frequently used to approximate or bound the partition or likelihood function of a markov random field.</s> <s>methods based on mean field theory are guaranteed to provide lower bounds, whereas certain types of convex relaxations provide upper bounds.</s> <s>in general, loopy belief propagation (bp) provides (often accurate) approximations, but not bounds.</s> <s>we prove that for a class of attractive binary models, the value specified by any fixed point of loopy bp always provides a lower bound on the true likelihood.</s> <s>empirically, this bound is much better than the naive mean field bound, and requires no further work than running bp.</s> <s>we establish these lower bounds using a loop series expansion due to chertkov and chernyak, which we show can be derived as a consequence of the tree reparameterization characterization of bp fixed points.</s></p></d>", "label": ["<d><p><s>loop series and bethe variational bounds in attractive graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of support vector machine transduction, which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples.</s> <s>although several studies are devoted to transductive svm, they suffer either from the high computation complexity or from the solutions of local optimum.</s> <s>to address this problem, we propose solving transductive svm via a convex relaxation, which converts the np-hard problem to a semi-definite programming.</s> <s>compared with the other sdp relaxation for transductive svm, the proposed algorithm is computationally more efficient with the number of free parameters reduced from o(n2) to o(n) where n is the number of examples.</s> <s>empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of transductive svm.</s></p></d>", "label": ["<d><p><s>efficient convex relaxation for transductive support vector machine</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>can we leverage learning techniques to build a fast nearest-neighbor (nn) retrieval data structure?</s> <s>we present a general learning framework for the nn problem in which sample queries are used to learn the parameters of a data structure that minimize the retrieval time and/or the miss rate.</s> <s>we explore the potential of this novel framework through two popular nn data structures: kd-trees and the rectilinear structures employed by locality sensitive hashing.</s> <s>we derive a generalization theory for these data structure classes and present simple learning algorithms for both.</s> <s>experimental results reveal that learning often improves on the already strong performance of these data structures.</s></p></d>", "label": ["<d><p><s>a learning framework for nearest neighbor search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes constraint propagation relaxation (cpr), a probabilistic approach to classical constraint propagation that provides another view on the whole parametric family of survey propagation algorithms sp(&#961;), ranging from belief propagation (&#961; = 0) to (pure) survey propagation(&#961; = 1).</s> <s>more importantly, the approach elucidates the implicit, but fundamental assumptions underlying sp(&#961;), thus shedding some light on its effectiveness and leading to applications beyond k-sat.</s></p></d>", "label": ["<d><p><s>cpr for csps: a probabilistic relaxation of constraint propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a hierarchical bayesian model for the discovery of putative regulators from gene expression data only.</s> <s>the hierarchy incorporates the knowledge that there are just a few regulators that by themselves only regulate a handful of genes.</s> <s>this is implemented through a so-called spike-and-slab prior, a mixture of gaussians with different widths, with mixing weights from a hierarchical bernoulli model.</s> <s>for efficient inference we implemented expectation propagation.</s> <s>running the model on a malaria parasite data set, we found four genes with significant homology to transcription factors in an amoebe, one rna regulator and three genes of unknown function (out of the top ten genes considered).</s></p></d>", "label": ["<d><p><s>regulator discovery from gene expression time series of malaria parasites: a hierachical approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input.</s> <s>many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g.</s> <s>low dimension, sparsity, etc).</s> <s>others are based on approximating density by stochastically reconstructing the input from the representation.</s> <s>we describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machines trained probabilistically, namely a restricted boltzmann machine.</s> <s>we propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation.</s> <s>we demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches.</s> <s>we show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input variables can be captured.</s></p></d>", "label": ["<d><p><s>sparse feature learning for deep belief networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel paradigm for statistical machine translation (smt), based on joint modeling of word alignment and the topical aspects underlying bilingual document pairs via a hidden markov bilingual topic admixture (hm-bitam).</s> <s>in this new paradigm, parallel sentence-pairs from a parallel document-pair are coupled via a certain semantic-flow, to ensure coherence of topical context in the alignment of matching words between languages, during likelihood-based training of topic-dependent translational lexicons, as well as topic representations in each language.</s> <s>the resulting trained hm-bitam can not only display topic patterns like other methods such as lda, but now for bilingual corpora; it also offers a principled way of inferring optimal translation in a context-dependent way.</s> <s>our method integrates the conventional ibm models based on hmm --- a key component for most of the state-of-the-art smt systems, with the recently proposed bitam model, and we report an extensive empirical analysis (in many way complementary to the description-oriented of our method in three aspects: word alignment, bilingual topic representation, and translation.</s></p></d>", "label": ["<d><p><s>hm-bitam: bilingual topic exploration, word alignment, and translation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>natural sounds are structured on many time-scales.</s> <s>a typical segment of speech, for example, contains features that span four orders of magnitude: sentences (~1s); phonemes (~0.1s); glottal pulses (~0.01s); and formants (<0.001s).</s> <s>the auditory system uses information from each of these time-scales to solve complicated tasks such as auditory scene analysis.</s> <s>one route toward understanding how auditory processing accomplishes this analysis is to build neuroscience-inspired algorithms which solve similar tasks and to compare the properties of these algorithms with properties of auditory processing.</s> <s>there is however a discord: current machine-audition algorithms largely concentrate on the shorter time-scale structures in sounds, and the longer structures are ignored.</s> <s>the reason for this is two-fold.</s> <s>firstly, it is a difficult technical problem to construct an algorithm that utilises both sorts of information.</s> <s>secondly, it is computationally demanding to simultaneously process data both at high resolution (to extract short temporal information) and for long duration (to extract long temporal information).</s> <s>the contribution of this work is to develop a new statistical model for natural sounds that captures structure across a wide range of time-scales, and to provide efficient learning and inference algorithms.</s> <s>we demonstrate the success of this approach on a missing data task.</s></p></d>", "label": ["<d><p><s>modeling natural sounds with modulation cascade processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>assessing similarity between features is a key step in object recognition and scene categorization tasks.</s> <s>we argue that knowledge on the distribution of distances generated by similarity functions is crucial in deciding whether features are similar or not.</s> <s>intuitively one would expect that similarities between features could arise from any distribution.</s> <s>in this paper, we will derive the contrary, and report the theoretical result that $l_p$-norms --a class of commonly applied distance metrics-- from one feature vector to other vectors are weibull-distributed if the feature values are correlated and non-identically distributed.</s> <s>besides these assumptions being realistic for images, we experimentally show them to hold for various popular feature extraction algorithms, for a diverse range of images.</s> <s>this fundamental insight opens new directions in the assessment of feature similarity, with projected improvements in object and scene recognition algorithms.</s> <s>erratum: the authors of paper have declared that they have become convinced that the reasoning in the reference is too simple as a proof of their claims.</s> <s>as a consequence, they withdraw their theorems.</s></p></d>", "label": ["<d><p><s>the distribution family of similarity distances</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when we have several related tasks, solving them simultaneously is shown to be more effective than solving them individually.</s> <s>this approach is called multi-task learning (mtl) and has been studied extensively.</s> <s>existing approaches to mtl often treat all the tasks as \\emph{uniformly related to each other and the relatedness of the tasks is controlled globally.</s> <s>for this reason, the existing methods can lead to undesired solutions when some tasks are not highly related to each other, and some pairs of related tasks can have significantly different solutions.</s> <s>in this paper, we propose a novel mtl algorithm that can overcome these problems.</s> <s>our method makes use of a task network, which describes the relation structure among tasks.</s> <s>this allows us to deal with intricate relation structures in a systematic way.</s> <s>furthermore, we control the relatedness of the tasks locally, so all pairs of related tasks are guaranteed to have similar solutions.</s> <s>we apply the above idea to support vector machines (svms) and show that the optimization problem can be cast as a second order cone program, which is convex and can be solved efficiently.</s> <s>the usefulness of our approach is demonstrated through simulations with protein super-family classification and ordinal regression problems.</s></p></d>", "label": ["<d><p><s>multi-task learning via conic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an extended probabilistic model for human perception.</s> <s>we argue that in many circumstances, human observers simultaneously evaluate sensory evidence under different hypotheses regarding the underlying physical process that might have generated the sensory information.</s> <s>within this context, inference can be optimal if the observer weighs each hypothesis according to the correct belief in that hypothesis.</s> <s>but if the observer commits to a particular hypothesis, the belief in that hypothesis is converted into subjective certainty, and subsequent perceptual behavior is suboptimal, conditioned only on the chosen hypothesis.</s> <s>we demonstrate that this framework can explain psychophysical data of a recently reported decision-estimation experiment.</s> <s>the model well accounts for the data, predicting the same estimation bias as a consequence of the preceding decision step.</s> <s>the power of the framework is that it has no free parameters except the degree of the observer's uncertainty about its internal sensory representation.</s> <s>all other parameters are defined by the particular experiment which allows us to make quantitative predictions of human perception to two modifications of the original experiment.</s></p></d>", "label": ["<d><p><s>a bayesian model of conditioned perception</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>social tags are user-generated keywords associated with some resource on the web.</s> <s>in the case of music, social tags have become an important component of web2.0\" recommender systems, allowing users to generate playlists based on use-dependent terms such as \"chill\" or \"jogging\" that have been applied to particular songs.</s> <s>in this paper, we propose a method for predicting these social tags directly from mp3 files.</s> <s>using a set of boosted classifiers, we map audio features onto social tags collected from the web.</s> <s>the resulting automatic tags (or \"autotags\") furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender.</s> <s>this avoids the ''cold-start problem'' common in such systems.</s> <s>autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system.\"</s></p></d>", "label": ["<d><p><s>automatic generation of social tags for music recommendation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>automatic relevance determination (ard), and the closely-related sparse bayesian learning (sbl) framework, are effective tools for pruning large numbers of irrelevant features.</s> <s>however, popular update rules used for this process are either prohibitively slow in practice and/or heuristic in nature without proven convergence properties.</s> <s>this paper furnishes an alternative means of optimizing a general ard cost function using an auxiliary function that can naturally be solved using a series of re-weighted l1 problems.</s> <s>the result is an efficient algorithm that can be implemented using standard convex programming toolboxes and is guaranteed to converge to a stationary point unlike existing methods.</s> <s>the analysis also leads to additional insights into the behavior of previous ard updates as well as the ard cost function.</s> <s>for example, the standard fixed-point updates of mackay (1992) are shown to be iteratively solving a particular min-max problem, although they are not guaranteed to lead to a stationary point.</s> <s>the analysis also reveals that ard is exactly equivalent to performing map estimation using a particular feature- and noise-dependent \\textit{non-factorial} weight prior with several desirable properties over conventional priors with respect to feature selection.</s> <s>in particular, it provides a tighter approximation to the l0 quasi-norm sparsity measure than the l1 norm.</s> <s>overall these results suggests alternative cost functions and update procedures for selecting features and promoting sparse solutions.</s></p></d>", "label": ["<d><p><s>a new view of automatic relevance determination</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we show that classical survival analysis involving censored data can naturally be cast as a ranking problem.</s> <s>the concordance index (ci), which quantifies the quality of rankings, is the standard performance measure for model \\emph{assessment} in survival analysis.</s> <s>in contrast, the standard approach to \\emph{learning} the popular proportional hazard (ph) model is based on cox's partial likelihood.</s> <s>in this paper we devise two bounds on ci--one of which emerges directly from the properties of ph models--and optimize them \\emph{directly}.</s> <s>our experimental results suggest that both methods perform about equally well, with our new approach giving slightly better results than the cox's method.</s> <s>we also explain why a method designed to maximize the cox's partial likelihood also ends up (approximately) maximizing the ci.</s></p></d>", "label": ["<d><p><s>on ranking in survival analysis: bounds on the concordance index</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>brain-computer interfaces (bcis), as any other interaction modality based on physiological signals and body channels (e.g., muscular activity, speech and gestures), are prone to errors in the recognition of subject's intent.</s> <s>an elegant approach to improve the accuracy of bcis consists in a verification procedure directly based on the presence of error-related potentials (errp) in the eeg recorded right after the occurrence of an error.</s> <s>six healthy volunteer subjects with no prior bci experience participated in a new human-robot interaction experiment where they were asked to mentally move a cursor towards a target that can be reached within a few steps using motor imagination.</s> <s>this experiment confirms the previously reported presence of a new kind of errp.</s> <s>these interaction errp\" exhibit a first sharp negative peak followed by a positive peak and a second broader negative peak (~290, ~350 and ~470 ms after the feedback, respectively).</s> <s>but in order to exploit these errp we need to detect them in each single trial using a short window following the feedback associated to the response of the classifier embedded in the bci.</s> <s>we have achieved an average recognition rate of correct and erroneous single trials of 81.8% and 76.2%, respectively.</s> <s>furthermore, we have achieved an average recognition rate of the subject's intent while trying to mentally drive the cursor of 73.1%.</s> <s>these results show that it's possible to simultaneously extract useful information for mental control to operate a brain-actuated device as well as cognitive states such as error potentials to improve the quality of the brain-computer interaction.</s> <s>finally, using a well-known inverse model (sloreta), we show that the main focus of activity at the occurrence of the errp are, as expected, in the pre-supplementary motor area and in the anterior cingulate cortex.\"</s></p></d>", "label": ["<d><p><s>eeg-based brain-computer interaction: improved accuracy by automatic single-trial error detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gates are a new notation for representing mixture models and context-sensitive independence in factor graphs.</s> <s>factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation.</s> <s>however, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure.</s> <s>gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized.</s> <s>different variational approximations for mixture models can be understood as different ways of drawing the gates in a model.</s> <s>we present general equations for expectation propagation and variational message passing in the presence of gates.</s></p></d>", "label": ["<d><p><s>gates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases.</s> <s>sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions.</s> <s>we derive a variational expectation-maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques.</s> <s>we illustrate how the proposed method can be applied in the context of cryptoanalysis as a pre-processing tool for the construction of template attacks.</s></p></d>", "label": ["<d><p><s>sparse probabilistic projections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>before the age of 4 months, infants make inductive inferences about the motions of physical objects.</s> <s>developmental psychologists have provided verbal accounts of the knowledge that supports these inferences, but often these accounts focus on categorical rather than probabilistic principles.</s> <s>we propose that infant object perception is guided in part by probabilistic principles like persistence: things tend to remain the same, and when they change they do so gradually.</s> <s>to illustrate this idea, we develop an ideal observer model that includes probabilistic formulations of rigidity and inertia.</s> <s>like previous researchers, we suggest that rigid motions are expected from an early age, but we challenge the previous claim that expectations consistent with inertia are relatively slow to develop (spelke et al., 1992).</s> <s>we support these arguments by modeling four experiments from the developmental literature.</s></p></d>", "label": ["<d><p><s>an ideal observer model of infant object perception</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>semantic hashing seeks compact binary codes of datapoints so that the hamming distance between codewords correlates with semantic similarity.</s> <s>hinton et al.</s> <s>used a clever implementation of autoencoders to find such codes.</s> <s>in this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be np hard.</s> <s>by relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresh- olded eigenvectors of the graph laplacian.</s> <s>by utilizing recent results on convergence of graph laplacian eigenvectors to the laplace-beltrami eigen- functions of manifolds, we show how to efficiently calculate the code of a novel datapoint.</s> <s>taken together, both learning the code and applying it to a novel point are extremely simple.</s> <s>our experiments show that our codes significantly outperform the state-of-the art.</s></p></d>", "label": ["<d><p><s>spectral hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets.</s> <s>recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent.</s> <s>models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientific interest in species such as human and mouse.</s> <s>we present a statistical model that makes a number of significant extensions to previous models to enable characterization of changes in expression among highly complex organisms.</s> <s>we demonstrate the efficacy of our method on a microarray dataset containing diverse tissues from multiple vertebrate species.</s> <s>we anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects.</s></p></d>", "label": ["<d><p><s>a mixture model for the evolution of gene expression in non-homogeneous datasets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneficial to be able to learn this function for adaptive control.</s> <s>a given robot manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem.</s> <s>we show how the structure of the inverse dynamics problem gives rise to a multi-task gaussian process prior over functions, where the inter-task similarity depends on the underlying dynamic parameters.</s> <s>experiments demonstrate that this multi-task formulation generally improves performance over either learning only on single tasks or pooling the data over all tasks.</s></p></d>", "label": ["<d><p><s>multi-task gaussian process learning of robot inverse dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many popular optimization algorithms, like the levenberg-marquardt algorithm (lma), use heuristic-based controllers'' that modulate the behavior of the optimizer during the optimization process.</s> <s>for example, in the lma a damping parameter is dynamically modified based on a set rules that were developed using various heuristic arguments.</s> <s>reinforcement learning (rl) is a machine learning approach to learn optimal controllers by examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms.</s> <s>improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems.</s> <s>for example the lma algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame.</s> <s>here we show that a popular modern reinforcement learning technique using a very simply state space can dramatically improve the performance of general purpose optimizers, like the lma.</s> <s>most surprisingly the controllers learned for a particular domain appear to work very well also on very different optimization domains.</s> <s>for example we used rl methods to train a new controller for the damping parameter of the lma.</s> <s>this controller was trained on a collection of classic, relatively small, non-linear regression problems.</s> <s>the modified lma performed better than the standard lma on these problems.</s> <s>most surprisingly, it also dramatically outperformed the standard lma on a difficult large scale computer vision problem for which it had not been trained before.</s> <s>thus the controller appeared to have extracted control rules that were not just domain specific but generalized across a wide range of optimization domains.\"</s></p></d>", "label": ["<d><p><s>optimization on a budget: a reinforcement learning approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of estimating the ratio of two probability density functions (a.k.a.~the importance).</s> <s>the importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection.</s> <s>in this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically.</s> <s>therefore, the proposed method is computationally very efficient and numerically stable.</s> <s>we also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound.</s> <s>numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.</s></p></d>", "label": ["<d><p><s>efficient direct density ratio estimation for non-stationarity adaptation and outlier detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive risk bounds for the randomized classifiers in sample compressions settings where the classifier-specification utilizes two sources of information viz.</s> <s>the compression set and the message string.</s> <s>by extending the recently proposed occam??</s> <s>?s hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classifiers and also recover the corresponding classical pac-bayes bound.</s> <s>we further show how these compare favorably to the existing results.</s></p></d>", "label": ["<d><p><s>risk bounds for randomized sample compressed classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>polysemy is a problem for methods that exploit image search engines to build object category models.</s> <s>existing unsupervised approaches do not take word sense into consideration.</s> <s>we propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data.</s> <s>the use of lda to discover a latent sense space makes the model robust despite the very limited nature of dictionary definitions.</s> <s>the definitions are used to learn a distribution in the latent space that best represents a sense.</s> <s>the algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense.</s> <s>an object classifier is trained on the resulting sense-specific images.</s> <s>we evaluate our method on a dataset obtained by searching the web for polysemous words.</s> <s>category classification experiments show that our dictionary-based approach outperforms baseline methods.</s></p></d>", "label": ["<d><p><s>unsupervised learning of visual sense models for polysemous words</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present polynomial-time algorithms for the exact computation of lowest- energy states, worst margin violators, partition functions, and marginals in binary undirected graphical models.</s> <s>our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph.</s> <s>maximum-margin parameter estimation for a boundary detection task shows our approach to be ef&#64257;cient and effective.</s></p></d>", "label": ["<d><p><s>efficient exact inference in planar ising models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>uncertainty is omnipresent when we perceive or interact with our environment, and the bayesian framework provides computational methods for dealing with it.</s> <s>mathematical models for bayesian decision making typically require datastructures that are hard to implement in neural networks.</s> <s>this article shows that even the simplest and experimentally best supported type of synaptic plasticity, hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal bayesian decisions.</s> <s>we present a concrete hebbian learning rule operating on log-probability ratios.</s> <s>modulated by reward-signals, this hebbian plasticity rule also provides a new perspective for understanding how bayesian inference could support fast reinforcement learning in the brain.</s> <s>in particular we show that recent experimental results by yang and shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way.</s></p></d>", "label": ["<d><p><s>hebbian learning of bayes optimal decisions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the following instance of transfer learning: given a pair of regression problems, suppose that the regression coefficients share a partially common support, parameterized by the overlap fraction $\\overlap$ between the two supports.</s> <s>this set-up suggests the use of $1, \\infty$-regularized linear regression for recovering the support sets of both regression vectors.</s> <s>our main contribution is to provide a sharp characterization of the sample complexity of this $1,\\infty$ relaxation, exactly pinning down the minimal sample size $n$ required for joint support recovery as a function of the model dimension $\\pdim$, support size $\\spindex$ and overlap $\\overlap \\in [0,1]$.</s> <s>for measurement matrices drawn from standard gaussian ensembles, we prove that the joint $1,\\infty$-regularized method undergoes a phase transition characterized by order parameter $\\orpar(\\numobs, \\pdim, \\spindex, \\overlap) = \\numobs{(4 - 3 \\overlap) s \\log(p-(2-\\overlap)s)}$.</s> <s>more precisely, the probability of successfully recovering both supports converges to $1$ for scalings such that $\\orpar > 1$, and converges to $0$ to scalings for which $\\orpar < 1$.</s> <s>an implication of this threshold is that use of $1, \\infty$-regularization leads to gains in sample complexity if the overlap parameter is large enough ($\\overlap > 2/3$), but performs worse than a naive approach if $\\overlap < 2/3$.</s> <s>we illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations.</s> <s>thus, our results illustrate both the benefits and dangers associated with block-$1,\\infty$ regularization in high-dimensional inference.</s></p></d>", "label": ["<d><p><s>phase transitions for high-dimensional joint support recovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise (i.e., heteroscedasticity) varies spatially.</s> <s>unfortunately, it presents a complex computational problem as the danger of overfitting is high and the individual optimization of every kernel in a learning system may be overly expensive due to the introduction of too many open learning parameters.</s> <s>previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters.</s> <s>in this paper, we focus on nonparametric regression and introduce a bayesian formulation that, with the help of variational approximations, results in an em-like algorithm for simultaneous estimation of regression and kernel parameters.</s> <s>the algorithm is computationally efficient (suitable for large data sets), requires no sampling, automatically rejects outliers and has only one prior to be specified.</s> <s>it can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with gaussian processes.</s> <s>our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning.</s> <s>we evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law.</s></p></d>", "label": ["<d><p><s>bayesian kernel shaping for learning control</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>mixture of gaussian processes models extended a single gaussian process with ability of modeling multi-modal data and reduction of training complexity.</s> <s>previous inference algorithms for these models are mostly based on gibbs sampling, which can be very slow, particularly for large-scale data sets.</s> <s>we present a new generative mixture of experts model.</s> <s>each expert is still a gaussian process but is reformulated by a linear model.</s> <s>this breaks the dependency among training outputs and enables us to use a much faster variational bayesian algorithm for training.</s> <s>our gating network is more flexible than previous generative approaches as inputs for each expert are modeled by a gaussian mixture model.</s> <s>the number of experts and number of gaussian components for an expert are inferred automatically.</s> <s>a variety of tests show the advantages of our method.</s></p></d>", "label": ["<d><p><s>variational mixture of gaussian process experts</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>embeddings of random variables in reproducing kernel hilbert spaces (rkhss) may be used to conduct statistical inference based on higher order moments.</s> <s>for sufficiently rich (characteristic) rkhss, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration.</s> <s>necessary and sufficient conditions for an rkhs to be characteristic exist for $\\r^n$.</s> <s>in the present work, conditions are established for an rkhs to be characteristic on groups and semigroups.</s> <s>illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and $\\r^n_+$.</s></p></d>", "label": ["<d><p><s>characteristic kernels on groups and semigroups</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our setting is a partially observable markov decision process with continuous state, observation and action spaces.</s> <s>decisions are based on a particle filter for estimating the belief state given past observations.</s> <s>we consider a policy gradient approach for parameterized policy optimization.</s> <s>for that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on finite difference (fd) techniques.</s> <s>we show that the naive fd is subject to variance explosion because of the non-smoothness of the resampling procedure.</s> <s>we propose a more sophisticated fd method which overcomes this problem and establish its consistency.</s></p></d>", "label": ["<d><p><s>particle filter-based policy gradient in pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop \\name\\ (stm), a nonparametric bayesian model of parsed documents.</s> <s>\\shortname\\ generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees.</s> <s>each word of a sentence is generated by a distribution that combines document-specific topic weights and parse-tree specific syntactic transitions.</s> <s>words are assumed generated in an order that respects the parse tree.</s> <s>we derive an approximate posterior inference method based on variational methods for hierarchical dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents.</s></p></d>", "label": ["<d><p><s>syntactic topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data.</s> <s>while likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem.</s> <s>in this paper, we present a partially observed maximum entropy discrimination markov network (pomen) model that attempts to combine the advantages of bayesian and margin based paradigms for learning markov networks from partially labeled data.</s> <s>pomen leads to an averaging prediction rule that resembles a bayes predictor that is more robust to overfitting, but is also built on the desirable discriminative laws resemble those of the m$^3$n.</s> <s>we develop an em-style algorithm utilizing existing convex optimization algorithms for m$^3$n as a subroutine.</s> <s>we demonstrate competent performance of pomen over existing methods on a real-world web data extraction task.</s></p></d>", "label": ["<d><p><s>partially observed maximum entropy discrimination markov networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that the empirical minimizer of a stochastic strongly convex objective, where the stochastic component is linear, converges to the population minimizer with rate $o(1/n)$.</s> <s>the result applies, in particular, to the svm objective.</s> <s>thus, we get a rate of $o(1/n)$ on the convergence of the svm objective to its infinite data limit.</s> <s>we demonstrate how this is essential for obtaining tight oracle inequalities for svms.</s> <s>the results extend also to strong convexity with respect to other $\\ellnorm_p$ norms, and so also to objectives regularized using other norms.</s></p></d>", "label": ["<d><p><s>fast rates for regularized objectives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for undiscounted reinforcement learning in markov decision processes (mdps) we consider the total regret of a learning algorithm with respect to an optimal policy.</s> <s>in order to describe the transition structure of an mdp we propose a new parameter: an mdp has diameter d if for any pair of states s1,s2 there is a policy which moves from s1 to s2 in at most d steps (on average).</s> <s>we present a reinforcement learning algorithm with total regret o(dsat) after t steps for any unknown mdp with s states, a actions per state, and diameter d. this bound holds with high probability.</s> <s>we also present a corresponding lower bound of omega(dsat) on the total regret of any learning algorithm.</s> <s>both bounds demonstrate the utility of the diameter as structural parameter of the mdp.</s></p></d>", "label": ["<d><p><s>near-optimal regret bounds for reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies global ranking problem by learning to rank methods.</s> <s>conventional learning to rank methods are usually designed for `local ranking', in the sense that the ranking model is defined on a single object, for example, a document in information retrieval.</s> <s>for many applications, this is a very loose approximation.</s> <s>relations always exist between objects and it is better to define the ranking model as a function on all the objects to be ranked (i.e., the relations are also included).</s> <s>this paper refers to the problem as global ranking and proposes employing a continuous conditional random fields (crf) for conducting the learning task.</s> <s>the continuous crf model is defined as a conditional probability distribution over ranking scores of objects conditioned on the objects.</s> <s>it can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking.</s> <s>taking two specific information retrieval tasks as examples, the paper shows how the continuous crf method can perform global ranking better than baselines.</s></p></d>", "label": ["<d><p><s>global ranking using continuous conditional random fields</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques.</s> <s>inspired by local learning, we propose a method to speed up standard gaussian process regression (gpr) with local gp models (lgp).</s> <s>the training data is partitioned in local regions, for each an individual gp model is trained.</s> <s>the prediction for a query point is performed by weighted estimation using nearby local models.</s> <s>unlike other gp approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction.</s> <s>the proposed method achieves online learning and prediction in real-time.</s> <s>comparisons with other nonparametric regression methods show that lgp has higher accuracy than lwpr and close to the performance of standard gpr and nu-svr.</s></p></d>", "label": ["<d><p><s>local gaussian process regression for real time online model learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is devoted to thoroughly investigating how to bootstrap the roc curve, a widely used visual tool for evaluating the accuracy of test/scoring statistics in the bipartite setup.</s> <s>the issue of confidence bands for the roc curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the smoothed bootstrap\" is introduced.</s> <s>theoretical arguments and simulation results are presented to show that the \"smoothed bootstrap\" is preferable to a \"naive\" bootstrap in order to construct accurate confidence bands.\"</s></p></d>", "label": ["<d><p><s>on bootstrapping the roc curve</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>accurate and efficient inference in evolutionary trees is a central problem in computational biology.</s> <s>realistic models require tracking insertions and deletions along the phylogenetic tree, making inference challenging.</s> <s>we propose new sampling techniques that speed up inference and improve the quality of the samples.</s> <s>we compare our method to previous approaches and show performance improvement on metrics evaluating multiple sequence alignment and reconstruction of ancestral sequences.</s></p></d>", "label": ["<d><p><s>efficient inference in phylogenetic indel trees</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the problem of domain transfer for a supervised classification task in mrna splicing.</s> <s>we consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance.</s> <s>we find that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classification performance.</s></p></d>", "label": ["<d><p><s>an empirical analysis of domain adaptation algorithms for genomic sequence analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes?</s> <s>one could think of using template matching or matching pursuit to find the arbitrarily shifted linear components.</s> <s>however, traditional matching approaches require that the templates be known a priori.</s> <s>to overcome this restriction we use instead semi non-negative matrix factorization (semi-nmf) that we extend to allow for time shifts when matching the templates to the signal.</s> <s>the algorithm estimates templates directly from the data along with their non-negative amplitudes.</s> <s>the resulting method can be thought of as an adaptive template matching procedure.</s> <s>we demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings.</s> <s>on these data the algorithm essentially performs spike detection and unsupervised spike clustering.</s> <s>results on simulated data and extracellular recordings indicate that the method performs well for signal-to-noise ratios of 6db or higher and that spike templates are recovered accurately provided they are sufficiently different.</s></p></d>", "label": ["<d><p><s>adaptive template matching with shift-invariant semi-nmf</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning due to limited sample size.</s> <s>in this paper, we propose a new approach to covariance estimation, which is based on constrained maximum likelihood (ml) estimation of the covariance.</s> <s>specifically, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (smt).</s> <s>the smt is formed by a product of pairwise coordinate rotations known as givens rotations.</s> <s>using this framework, the covariance can be efficiently estimated using greedy minimization of the log likelihood function, and the number of givens rotations can be efficiently computed using a cross-validation procedure.</s> <s>the estimator obtained using this method is always positive definite and well-conditioned even with limited sample size.</s> <s>experiments on hyperspectral data show that smt covariance estimation results in consistently better estimates of the covariance for a variety of different classes and sample sizes compared to traditional shrinkage estimators.</s></p></d>", "label": ["<d><p><s>covariance estimation for high dimensional data vectors using the sparse matrix transform</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the gaussian process density sampler (gpds), an exchangeable generative model for use in nonparametric bayesian density estimation.</s> <s>samples drawn from the gpds are consistent with exact, independent samples from a fixed density function that is a transformation of a function drawn from a gaussian process prior.</s> <s>our formulation allows us to infer an unknown density from data using markov chain monte carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space.</s> <s>we can also infer the hyperparameters of the gaussian process.</s> <s>we compare this density modeling technique to several existing techniques on a toy problem and a skull-reconstruction task.</s></p></d>", "label": ["<d><p><s>the gaussian process density sampler</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a characterization of a useful class of skills based on a graphical representation of an agent's interaction with its environment.</s> <s>our characterization uses betweenness, a measure of centrality on graphs.</s> <s>it may be used directly to form a set of skills suitable for a given environment.</s> <s>more importantly, it serves as a useful guide for developing online, incremental skill discovery algorithms that do not rely on knowing or representing the environment graph in its entirety.</s></p></d>", "label": ["<d><p><s>skill characterization based on betweenness</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>detecting underlying clusters from large-scale data plays a central role in machine learning research.</s> <s>in this paper, we attempt to tackle clustering problems for complex data of multiple distributions and large multi-scales.</s> <s>to this end, we develop an algorithm named zeta $l$-links, or zell which consists of two parts: zeta merging with a similarity graph and an initial set of small clusters derived from local $l$-links of the graph.</s> <s>more specifically, we propose to structurize a cluster using cycles in the associated subgraph.</s> <s>a mathematical tool, zeta function of a graph, is introduced for the integration of all cycles, leading to a structural descriptor of the cluster in determinantal form.</s> <s>the popularity character of the cluster is conceptualized as the global fusion of variations of the structural descriptor by means of the leave-one-out strategy in the cluster.</s> <s>zeta merging proceeds, in the agglomerative fashion, according to the maximum incremental popularity among all pairwise clusters.</s> <s>experiments on toy data, real imagery data, and real sensory data show the promising performance of zell.</s> <s>the $98.1\\%$ accuracy, in the sense of the normalized mutual information, is obtained on the frgc face data of 16028 samples and 466 facial clusters.</s> <s>the matlab codes of zell will be made publicly available for peer evaluation.</s></p></d>", "label": ["<d><p><s>cyclizing clusters via zeta function of a graph</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user??</s> <s>?s original query with additional related terms.</s> <s>current algorithms for automatic query expansion have been shown to consistently improve retrieval accuracy on average, but are highly unstable and have bad worst-case performance for individual queries.</s> <s>we introduce a novel risk framework that formulates query model estimation as a constrained metric labeling problem on a graph of term relations.</s> <s>themodel combines assignment costs based on a baseline feedback algorithm, edge weights based on term similarity, and simple constraints to enforce aspect balance, aspect coverage, and term centrality.</s> <s>results across multiple standard test collections show consistent and dramatic reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm.</s></p></d>", "label": ["<d><p><s>estimating robust query models with convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sampling functions in gaussian process (gp) models is challenging because of the highly correlated posterior distribution.</s> <s>we describe an efficient markov chain monte carlo algorithm for sampling from the posterior process of the gp model.</s> <s>this algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function.</s> <s>at each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional gp prior.</s> <s>the control variable input locations are found by continuously minimizing an objective function.</s> <s>we demonstrate the algorithm on regression and classification problems and we use it to estimate the parameters of a differential equation model of gene regulation.</s></p></d>", "label": ["<d><p><s>efficient sampling for gaussian process inference using control variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables.</s> <s>the bound is obtained by propagating bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest.</s> <s>by construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate belief propagation marginal (``belief'').</s> <s>thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by belief propagation.</s> <s>we show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis.</s></p></d>", "label": ["<d><p><s>bounds on marginal probability distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable organized in a hierarchy.</s> <s>model fitting is challenging, especially for hierarchies with large number of nodes.</s> <s>we provide a novel algorithm based on a multi-scale kalman filter that is both scalable and easy to implement.</s> <s>for non-gaussian responses, quadratic approximation to the log-likelihood results in biased estimates.</s> <s>we suggest a bootstrap strategy to correct such biases.</s> <s>our method is illustrated through simulation studies and analyses of real world data sets in health care and online advertising.</s></p></d>", "label": ["<d><p><s>fast computation of posterior mode in multi-level hierarchical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or kullback-leibler divergence.</s> <s>the objective of this paper is two-fold.</s> <s>first, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with fixed k converge almost surely, even though the k-nearest-neighbor density estimation with fixed k does not converge to its true measure.</s> <s>second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples.</s> <s>nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent.</s> <s>we show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the hilbert schmidt independence criterion, respectively.</s></p></d>", "label": ["<d><p><s>estimation of information theoretic measures for continuous random variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations.</s> <s>this is usually done through the penalization of predictor functions by euclidean or hilbertian norms.</s> <s>in this paper, we explore penalizing by sparsity-inducing norms such as the l1-norm or the block l1-norm.</s> <s>we assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels.</s> <s>this framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the uci repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</s></p></d>", "label": ["<d><p><s>exploring large feature spaces with hierarchical multiple kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many machine learning algorithms require the summation of gaussian kernel functions, an expensive operation if implemented straightforwardly.</s> <s>several methods have been proposed to reduce the computational complexity of evaluating such sums, including tree and analysis based methods.</s> <s>these achieve varying speedups depending on the bandwidth, dimension, and prescribed error, making the choice between methods difficult for machine learning tasks.</s> <s>we provide an algorithm that combines tree methods with the improved fast gauss transform (ifgt).</s> <s>as originally proposed the ifgt suffers from two problems: (1) the taylor series expansion does not perform well for very low bandwidths, and (2) parameter selection is not trivial and can drastically affect performance and ease of use.</s> <s>we address the first problem by employing a tree data structure, resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth.</s> <s>to solve the second problem, we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data, desired accuracy, and bandwidth.</s> <s>in addition, the new ifgt parameter selection approach allows for tighter error bounds.</s> <s>our approach chooses the fastest method at negligible additional cost, and has superior performance in comparisons with previous approaches.</s></p></d>", "label": ["<d><p><s>automatic online tuning for fast gaussian summation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel application of formal concept analysis (fca) to neural decoding: instead of just trying to figure out which stimulus was presented, we demonstrate how to explore the semantic relationships between the neural representation of large sets of stimuli.</s> <s>fca provides a way of displaying and interpreting such relationships via concept lattices.</s> <s>we explore the effects of neural code sparsity on the lattice.</s> <s>we then analyze neurophysiological data from high-level visual cortical area stsa, using an exact bayesian approach to construct the formal context needed by fca.</s> <s>prominent features of the resulting concept lattices are discussed, including indications for a product-of-experts code in real neurons.</s></p></d>", "label": ["<d><p><s>interpreting the neural code with formal concept analysis</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we define a metric for measuring behavior similarity between states in a markov decision process (mdp), in which action similarity is taken into account.</s> <s>we show that the kernel of our metric corresponds exactly to the classes of states defined by mdp homomorphisms (ravindran \\& barto, 2003).</s> <s>we prove that the difference in the optimal value function of different states can be upper-bounded by the value of this metric, and that the bound is tighter than that provided by bisimulation metrics (ferns et al.</s> <s>2004, 2005).</s> <s>our results hold both for discrete and for continuous actions.</s> <s>we provide an algorithm for constructing approximate homomorphisms, by using this metric to identify states that can be grouped together, as well as actions that can be matched.</s> <s>previous research on this topic is based mainly on heuristics.</s></p></d>", "label": ["<d><p><s>bounding performance loss in approximate mdp homomorphisms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions.</s> <s>for $d$ covariates, there are $2^d$ basis coefficients to estimate, which renders conventional approaches computationally prohibitive when $d$ is large.</s> <s>however, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities.</s> <s>our method also allows for flexible control of the trade-off between mean-squared error and computational complexity.</s></p></d>", "label": ["<d><p><s>near-minimax recursive density estimation on the binary hypercube</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide statistical performance guarantees for a recently introduced kernel classifier that optimizes the $l_2$ or integrated squared error (ise) of a difference of densities.</s> <s>the classifier is similar to a support vector machine (svm) in that it is the solution of a quadratic program and yields a sparse classifier.</s> <s>unlike svms, however, the $l_2$ kernel classifier does not involve a regularization parameter.</s> <s>we prove a distribution free concentration inequality for a cross-validation based estimate of the ise, and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ise and probability of error.</s> <s>our results can also be specialized to give performance guarantees for an existing method of $l_2$ kernel density estimation.</s></p></d>", "label": ["<d><p><s>performance analysis for l\\_2 kernel classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an efficient sequential monte carlo inference scheme for the recently proposed coalescent clustering model (teh et al, 2008).</s> <s>our algorithm has a quadratic runtime while those in (teh et al, 2008) is cubic.</s> <s>in experiments, we were surprised to find that in addition to being more efficient, it is also a better sequential monte carlo sampler than the best in (teh et al, 2008), when measured in terms of variance of estimated likelihood and effective sample size.</s></p></d>", "label": ["<d><p><s>an efficient sequential monte carlo algorithm for coalescent clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable.</s> <s>in predicting the topic of a document, we might know that two words are synonyms, or when performing image recognition, we know which pixels are adjacent.</s> <s>such synonymous or neighboring features are near-duplicates and should therefore be expected to have similar weights in a good model.</s> <s>here we present a framework for regularized learning in settings where one has prior knowledge about which features are expected to have similar and dissimilar weights.</s> <s>this prior knowledge is encoded as a graph whose vertices represent features and whose edges represent similarities and dissimilarities between them.</s> <s>during learning, each feature's weight is penalized by the amount it differs from the average weight of its neighbors.</s> <s>for text classification, regularization using graphs of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods.</s> <s>for sentiment analysis, feature graphs constructed from declarative human knowledge, as well as from auxiliary task learning, significantly improve prediction accuracy.</s></p></d>", "label": ["<d><p><s>regularized learning with networks of features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning.</s> <s>the method involves the learning of two mappings of the heterogeneous objects to a unified euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer.</s> <s>the algorithm can be formulated as an optimization problem in a reproducing kernel hilbert space.</s> <s>we report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data.</s></p></d>", "label": ["<d><p><s>supervised bipartite graph inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning.</s> <s>recent impressive results range from humanoid robot movement generation to timing models of human motions.</s> <s>the automatic generation of skill libraries containing multiple motion templates is an important step in robot learning.</s> <s>such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system.</s> <s>in this paper, we show how human trajectories captured as multidimensional time-series can be clustered using bayesian mixtures of linear gaussian state-space models based on the similarity of their dynamics.</s> <s>the appropriate number of templates is automatically determined by enforcing a parsimonious parametrization.</s> <s>as the resulting model is intractable, we introduce a novel approximation method based on variational bayes, which is especially designed to enable the use of efficient inference algorithms.</s> <s>on recorded human balero movements, this method is not only capable of finding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic sarcos arm.</s></p></d>", "label": ["<d><p><s>using bayesian dynamical systems for motion template libraries</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a new interpretation of multiscale random fields (msrfs) that admits efficient optimization in the framework of regular (single level) random fields (rfs).</s> <s>it is based on a new operator, called append, that combines sets of random variables (rvs) to single rvs.</s> <s>we assume that a msrf can be decomposed into disjoint trees that link rvs at different pyramid levels.</s> <s>the append operator is then applied to map rvs in each tree structure to a single rv.</s> <s>we demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images.</s> <s>msrfs provide a natural representation of multiscale contour models, which are needed in order to cope with unstable contour decompositions.</s> <s>the append operator allows us to find optimal image labels using the classical framework of relaxation labeling, alternative methods like markov chain monte carlo (mcmc) could also be used.</s></p></d>", "label": ["<d><p><s>multiscale random fields with application to contour grouping</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been shown that the problem of $\\ell_1$-penalized least-square regression commonly referred to as the lasso or basis pursuit denoising leads to solutions that are sparse and therefore achieves model selection.</s> <s>we propose in this paper an algorithm to solve the lasso with online observations.</s> <s>we introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point.</s> <s>we compare our method to lars and present an application to compressed sensing with sequential observations.</s> <s>our approach can also be easily extended to compute an homotopy from the current solution to the solution after removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation.</s></p></d>", "label": ["<d><p><s>an homotopy algorithm for the lasso with online observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>subspace-based learning problems involve data whose elements are linear subspaces of a vector space.</s> <s>to handle such data structures, grassmann kernels have been proposed and used previously.</s> <s>in this paper, we analyze the relationship between grassmann kernels and probabilistic similarity measures.</s> <s>firstly, we show that the kl distance in the limit yields the projection kernel on the grassmann manifold, whereas the bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems.</s> <s>secondly, based on our analysis of the kl distance, we propose extensions of the projection kernel which can be extended to the set of affine as well as scaled subspaces.</s> <s>we demonstrate the advantages of these extended kernels for classification and recognition tasks with support vector machines and kernel discriminant analysis using synthetic and real image databases.</s></p></d>", "label": ["<d><p><s>extended grassmann kernels for subspace-based learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (icu).</s> <s>in particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the icu and make the raw data almost useless for automated decision making.</s> <s>the problem is complicated by the fact that the sensor data are acquired at fixed intervals whereas the events causing data artifacts may occur at any time and have durations that may be significantly shorter than the data collection interval.</s> <s>we show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables effective detection of artifacts and accurate estimation of the underlying blood pressure values.</s></p></d>", "label": ["<d><p><s>probabilistic detection of short events, with application to critical care monitoring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases.</s> <s>examining a large set of manually segmented scenes, we use chi--square tests to show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the pitman--yor (py) process.</s> <s>this nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image.</s> <s>generalizing previous applications of py processes, we use gaussian processes to discover spatially contiguous segments which respect image boundaries.</s> <s>using a novel family of variational approximations, our approach produces segmentations which compare favorably to state--of--the--art methods, while simultaneously discovering categories shared among natural scenes.</s></p></d>", "label": ["<d><p><s>shared segmentation of natural scenes using dependent pitman-yor processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the roc curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis.</s> <s>most practical performance measures used in scoring applications such as the auc, the local auc, the p-norm push, the dcg and others, can be seen as summaries of the roc curve.</s> <s>this paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics.</s> <s>we investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process.</s></p></d>", "label": ["<d><p><s>empirical performance maximization for linear rank statistics</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>clustering stability is an increasingly popular family of methods for performing model selection in data clustering.</s> <s>the basic idea is that the chosen model should be stable under perturbation or resampling of the data.</s> <s>despite being reasonably effective in practice, these methods are not well understood theoretically, and present some difficulties.</s> <s>in particular, when the data is assumed to be sampled from an underlying distribution, the solutions returned by the clustering algorithm will usually become more and more stable as the sample size increases.</s> <s>this raises a potentially serious practical difficulty with these methods, because it means there might be some hard-to-compute sample size, beyond which clustering stability estimators 'break down' and become unreliable in detecting the most stable model.</s> <s>namely, all models will be relatively stable, with differences in their stability measures depending mostly on random and meaningless sampling artifacts.</s> <s>in this paper, we provide a set of general sufficient conditions, which ensure the reliability of clustering stability estimators in the large sample regime.</s> <s>in contrast to previous work, which concentrated on specific toy distributions or specific idealized clustering frameworks, here we make no such assumptions.</s> <s>we then exemplify how these conditions apply to several important families of clustering algorithms, such as maximum likelihood clustering, certain types of kernel clustering, and centroid-based clustering with any bregman divergence.</s> <s>in addition, we explicitly derive the non-trivial asymptotic behavior of these estimators, for any framework satisfying our conditions.</s> <s>this can help us understand what is considered a 'stable' model by these estimators, at least for large enough samples.</s></p></d>", "label": ["<d><p><s>on the reliability of clustering stability in the large sample regime</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the visual and auditory map alignment in the superior colliculus (sc) of barn owl is important for its accurate localization for prey behavior.</s> <s>prism learning or blindness may interfere this alignment and cause loss of the capability of accurate prey.</s> <s>however, juvenile barn owl could recover its sensory map alignment by shifting its auditory map.</s> <s>the adaptation of this map alignment is believed based on activity dependent axon developing in inferior colliculus (ic).</s> <s>a model is built to explore this mechanism.</s> <s>in this model, axon growing process is instructed by an inhibitory network in sc while the strength of the inhibition adjusted by spike timing dependent plasticity (stdp).</s> <s>we test and analyze this mechanism by application of the neural structures involved in spatial localization in a robotic system.</s></p></d>", "label": ["<d><p><s>bio-inspired real time sensory map realignment in a robotic barn owl</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many machine learning algorithms can be formulated in the framework of statistical independence such as the hilbert schmidt independence criterion.</s> <s>in this paper, we extend this criterion to deal with with structured and interdependent observations.</s> <s>this is achieved by modeling the structures using undirected graphical models and comparing the hilbert space embeddings of distributions.</s> <s>we apply this new criterion to independent component analysis and sequence clustering.</s></p></d>", "label": ["<d><p><s>kernel measures of independence for non-iid data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new family of linear time algorithms based on sufficient statistics for string comparison with mismatches under the string kernels framework.</s> <s>our algorithms improve theoretical complexity bounds of existing approaches while scaling well with respect to the sequence alphabet size, the number of allowed mismatches and the size of the dataset.</s> <s>in particular, on large alphabets with loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure.</s> <s>we evaluate our algorithms on synthetic data and real applications in music genre classification, protein remote homology detection and protein fold prediction.</s> <s>the scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with significantly reduced running times.</s></p></d>", "label": ["<d><p><s>scalable algorithms for string kernels with inexact matching</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and strongly suggest important underlying structures in the data.</s> <s>in this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family pca and provide a simple but novel form to project new testing data into the embedded space.</s> <s>this convex approach successfully avoids the local optima of the em learning.</s> <s>moreover, by introducing a sample-based multinomial approximation to exponential family models, it avoids the limitation of the prevailing gaussian assumptions of standard pca, and produces a kernelized formulation for nonlinear supervised dimensionality reduction.</s> <s>a training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained through a coordinate descent procedure.</s> <s>the advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data.</s></p></d>", "label": ["<d><p><s>supervised exponential family principal component analysis via convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we present two transductive bounds on the risk of the majority vote estimated over partially labeled training sets.</s> <s>our first bound is tight when the additional unlabeled training data are used in the cases where the voted classifier makes its errors on low margin observations and where the errors of the associated gibbs classifier can accurately be estimated.</s> <s>in semi-supervised learning, considering the margin as an indicator of confidence constitutes the working hypothesis of algorithms which search the decision boundary on low density regions.</s> <s>in this case, we propose a second bound on the joint probability that the voted classifier makes an error over an example having its margin over a fixed threshold.</s> <s>as an application we are interested on self-learning algorithms which assign iteratively pseudo-labels to unlabeled training examples having margin above a threshold obtained from this bound.</s> <s>empirical results on different datasets show the effectiveness of our approach compared to the same algorithm and the tsvm in which the threshold is fixed manually.</s></p></d>", "label": ["<d><p><s>a transductive bound for the voted classifier with an application to semi-supervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we consider approximate policy-iteration-based reinforcement learning algorithms.</s> <s>in order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator.</s> <s>we propose two novel regularized policy iteration algorithms by adding l2-regularization to two widely-used policy evaluation methods: bellman residual minimization (brm) and least-squares temporal difference learning (lstd).</s> <s>we derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel hilbert space.</s> <s>we also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions.</s></p></d>", "label": ["<d><p><s>regularized policy iteration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once.</s> <s>however, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric.</s> <s>existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their offline counterparts.</s> <s>we present a new online metric learning algorithm that updates a learned mahalanobis metric based on logdet regularization and gradient descent.</s> <s>we prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms.</s> <s>to further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efficient updates for approximate similarity search data structures.</s> <s>we demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.</s></p></d>", "label": ["<d><p><s>online metric learning and fast similarity search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent research suggests that neural systems employ sparse coding.</s> <s>however, there is limited theoretical understanding of fundamental resolution limits in such sparse coding.</s> <s>this paper considers a general sparse estimation problem of detecting the sparsity pattern of a $k$-sparse vector in $\\r^n$ from $m$ random noisy measurements.</s> <s>our main results provide necessary and sufficient conditions on the problem dimensions, $m$, $n$ and $k$, and the signal-to-noise ratio (snr) for asymptotically-reliable detection.</s> <s>we show a necessary condition for perfect recovery at any given snr for all algorithms, regardless of complexity, is $m = \\omega(k\\log(n-k))$ measurements.</s> <s>this is considerably stronger than all previous necessary conditions.</s> <s>we also show that the scaling of $\\omega(k\\log(n-k))$ measurements is sufficient for a trivial ``maximum correlation'' estimator to succeed.</s> <s>hence this scaling is optimal and does not require lasso, matching pursuit, or more sophisticated methods, and the optimal scaling can thus be biologically plausible.</s></p></d>", "label": ["<d><p><s>resolution limits of sparse coding in high dimensions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is now well established that sparse signal models are well suited to restoration tasks and can effectively be learned from audio, image, and video data.</s> <s>recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones.</s> <s>this paper proposes a new step in that direction with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and multiple decision functions.</s> <s>it is shown that the linear variant of the model admits a simple probabilistic interpretation, and that its most general variant also admits a simple interpretation in terms of kernels.</s> <s>an optimization framework for learning all the components of the proposed model is presented, along with experiments on standard handwritten digit and texture classification tasks.</s></p></d>", "label": ["<d><p><s>supervised dictionary learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>offline handwriting recognition---the transcription of images of handwritten text---is an interesting task, in that it combines computer vision with sequence learning.</s> <s>in most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as hmms used to provide the transcriptions.</s> <s>by combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input.</s> <s>unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language.</s> <s>evidence of its generality and power is provided by data from a recent international arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of arabic.</s></p></d>", "label": ["<d><p><s>offline handwriting recognition with multidimensional recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large-margin structured estimation methods work by minimizing a convex upper bound of loss functions.</s> <s>while they allow for efficient optimization algorithms, these convex formulations are not tight and sacrifice the ability to accurately model the true loss.</s> <s>we present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classification to structured estimation.</s> <s>we show that a small modification of existing optimization algorithms suffices to solve this modified problem.</s> <s>on structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy.</s></p></d>", "label": ["<d><p><s>tighter bounds for structured estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider multi-armed bandit problems where the number of arms is larger than the possible number of experiments.</s> <s>we make a stochastic assumption on the mean-reward of a new selected arm which characterizes its probability of being a near-optimal arm.</s> <s>our assumption is weaker than in previous works.</s> <s>we describe algorithms based on upper-confidence-bounds applied to a restricted set of randomly selected arms and provide upper-bounds on the resulting expected regret.</s> <s>we also derive a lower-bound which matchs (up to logarithmic factors) the upper-bound in some cases.</s></p></d>", "label": ["<d><p><s>algorithms for infinitely many-armed bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel method for inducing synchronous context free grammars (scfgs) from a corpus of parallel string pairs.</s> <s>scfgs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings.</s> <s>we develop a non-parametric bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this field.</s> <s>using a variational bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over previously proposed maximum likelihood models.</s></p></d>", "label": ["<d><p><s>bayesian synchronous grammar induction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we tackle the computational problem of query-conditioned search.</s> <s>given a machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries.</s> <s>the predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements.</s> <s>the general approach is applicable to webpage ranking, internet advertisement, and approximate nearest neighbor search.</s> <s>it is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable.</s> <s>we experimentally find substantial improvement over existing methods for internet advertisement and approximate nearest neighbors.</s></p></d>", "label": ["<d><p><s>predictive indexing for fast search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis.</s> <s>posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or gibbs sampling.</s> <s>there has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings.</s> <s>in this paper we provide the beginnings of such understanding.</s> <s>we analyze the improvement that the recently proposed collapsed variational inference (cvb) provides over mean field variational inference (vb) in latent dirichlet allocation.</s> <s>we prove that the difference in the tightness of the bound on the likelihood of a document decreases as $o(k-1) + \\log m /m$, where $k$ is the number of topics in the model and $m$ is the number of words in a document.</s> <s>as a consequence, the advantage of cvb over vb is lost for long documents but increases with the number of topics.</s> <s>we demonstrate empirically that the theory holds, using simulated text data and two text corpora.</s> <s>we provide practical guidelines for choosing an approximation.</s></p></d>", "label": ["<d><p><s>relative performance guarantees for approximate inference in latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate a topic at the interface of machine learning and cognitive science.</s> <s>human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples.</s> <s>furthermore, we compare human active learning performance with predictions from statistical learning theory.</s> <s>we conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct.</s> <s>our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory.</s> <s>however, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms.</s> <s>to the best of our knowledge, this is the first quantitative study comparing human category learning in active versus passive settings.</s></p></d>", "label": ["<d><p><s>human active learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper examines the generalization properties of online convex programming algorithms when the loss function is lipschitz and strongly convex.</s> <s>our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret.</s> <s>this allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability.</s> <s>the bound also solves an open problem regarding the convergence rate of {\\pegasos}, a recently proposed method for solving the svm optimization problem.</s></p></d>", "label": ["<d><p><s>on the generalization ability of online strongly convex programming algorithms</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>psychophysical experiments show that humans are better at perceiving rotation and expansion than translation.</s> <s>these findings are inconsistent with standard models of motion integration which predict best performance for translation [6].</s> <s>to explain this discrepancy, our theory formulates motion perception at two levels of inference: we first perform model selection between the competing models (e.g.</s> <s>translation, rotation, and expansion) and then estimate the velocity using the selected model.</s> <s>we define novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g.</s> <s>green functions of differential operators).</s> <s>the theory gives good agreement with the trends observed in human experiments.</s></p></d>", "label": ["<d><p><s>model selection and velocity estimation using novel priors for motion patterns</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents.</s> <s>the multi-agent filtering problem is to efficiently represent and update these beliefs through time as the agents act in the world.</s> <s>in this paper, we formally define an infinite sequence of nested beliefs about the state of the world at the current time $t$ and present a filtering algorithm that maintains a finite representation which can be used to generate these beliefs.</s> <s>in some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex.</s> <s>in experiments, we demonstrate efficient filtering in a range of multi-agent domains.</s></p></d>", "label": ["<d><p><s>multi-agent filtering with infinitely nested beliefs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>regularized least squares (rls) algorithms have the ability to avoid over-fitting problems and to express solutions as kernel expansions.</s> <s>however, we observe that the current rls algorithms cannot provide a satisfactory interpretation even on a constant function.</s> <s>on the other hand, while kernel-based algorithms have been developed in such a tendency that almost all learning algorithms are kernelized or being kernelized, a basic fact is often ignored: the learned function from the data and the kernel fits the data well, but may not be consistent with the kernel.</s> <s>based on these considerations and on the intuition that a good kernel-based inductive function should be consistent with both the data and the kernel, a novel learning scheme is proposed.</s> <s>the advantages of this scheme lie in its corresponding representer theorem, its strong interpretation ability about what kind of functions should not be penalized, and its promising accuracy improvements shown in a number of experiments.</s> <s>furthermore, we provide a detailed technical description about heat kernels, which serves as an example for the readers to apply similar techniques for other kernels.</s> <s>our work provides a preliminary step in a new direction to explore the varying consistency between inductive functions and kernels under various distributions.</s></p></d>", "label": ["<d><p><s>learning with consistency between inductive functions and kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a crucial part of developing mathematical models of how the brain works is the quantification of their success.</s> <s>one of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model.</s> <s>unfortunately, this metric is biased due to the intrinsic variability in the data.</s> <s>this variability is in principle unexplainable by the model.</s> <s>we derive a simple analytical modification of the traditional formula that significantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying variance explained by the model class.</s> <s>our estimator advances on previous work by a) accounting for the uncertainty in the noise estimate, b) accounting for overfitting due to free model parameters mitigating the need for a separate validation data set and c) adding a conditioning term.</s> <s>we apply our new estimator to binocular disparity tuning curves of a set of macaque v1 neurons and find that on a population level almost all of the variance unexplained by gabor functions is attributable to noise.</s></p></d>", "label": ["<d><p><s>an improved estimator of variance explained in the presence of noise</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>kernel supervised learning methods can be unified by utilizing the tools from regularization theory.</s> <s>the duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated bayesian interpretations of kernel methods.</s> <s>in this paper we pursue a bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as ``silverman's g-prior.''</s> <s>we provide a theoretical analysis of the posterior consistency of a bayesian model choice procedure based on this prior.</s> <s>we also establish the asymptotic relationship between this procedure and the bayesian information criterion.</s></p></d>", "label": ["<d><p><s>posterior consistency of the silverman g-prior in bayesian model choice</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption.</s> <s>however, real shapes from image datasets, even when expected to be related by almost isometric\" transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale.</s> <s>in this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction.</s> <s>the outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations.</s> <s>our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times.\"</s></p></d>", "label": ["<d><p><s>robust near-isometric matching via structured learning of graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in most cognitive and motor tasks, speed-accuracy tradeoffs are observed: individuals can respond slowly and accurately, or quickly yet be prone to errors.</s> <s>control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history.</s> <s>when stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials.</s> <s>we propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difficulty corresponds to the drift rate for the correct response.</s> <s>the model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence.</s> <s>we derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding.</s> <s>trial-by-trial tracking of difficulty thus leads to sequential effects in speed and accuracy.</s> <s>simulations show the model explains a variety of phenomena in human speeded decision making.</s> <s>we argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures.</s></p></d>", "label": ["<d><p><s>optimal response initiation: why recent experience matters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper lower and upper bounds for the number of support vectors are derived for support vector machines (svms) based on the epsilon-insensitive loss function.</s> <s>it turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution.</s> <s>finally, we briefly discuss a trade-off in epsilon between sparsity and accuracy if the svm is used to estimate the conditional median.</s></p></d>", "label": ["<d><p><s>sparsity of svms that use the epsilon-insensitive loss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>roc curves are one of the most widely used displays to evaluate performance of scoring functions.</s> <s>in the paper, we propose a statistical method for directly optimizing the roc curve.</s> <s>the target is known to be the regression function up to an increasing transformation and this boils down to recovering the level sets of the latter.</s> <s>we propose to use classifiers obtained by empirical risk minimization of a weighted classification error and then to construct a scoring rule by overlaying these classifiers.</s> <s>we show the consistency and rate of convergence to the optimal roc curve of this procedure in terms of supremum norm and also, as a byproduct of the analysis, we derive an empirical estimate of the optimal roc curve.</s></p></d>", "label": ["<d><p><s>overlaying classifiers: a practical approach for optimal ranking</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we introduce a novel framework for estimating vector fields using sparse basis field expansions (s-flex).</s> <s>the notion of basis fields, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement.</s> <s>we consider a regression setting as well as inverse problems.</s> <s>all variants discussed lead to second-order cone programming formulations.</s> <s>while our framework is generally applicable to any type of vector field, we focus in this paper on applying it to solving the eeg/meg inverse problem.</s> <s>it is shown that significantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from eeg/meg measurements become possible with our method when comparing to the state-of-the-art.</s></p></d>", "label": ["<d><p><s>estimating vector fields using sparse basis field expansions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the original goals of computer vision was to fully understand a natural scene.</s> <s>this requires solving several problems simultaneously, including object detection, labeling of meaningful regions, and 3d reconstruction.</s> <s>while great progress has been made in tackling each of these problems in isolation, only recently have researchers again been considering the difficult task of assembling various methods to the mutual benefit of all.</s> <s>we consider learning a set of such classification models in such a way that they both solve their own problem and help each other.</s> <s>we develop a framework known as cascaded classification models (ccm), where repeated instantiations of these classifiers are coupled by their input/output variables in a cascade that improves performance at each level.</s> <s>our method requires only a limited ??</s> <s>?black box???</s> <s>interface with the models, allowing us to use very sophisticated, state-of-the-art classifiers without having to look under the hood.</s> <s>we demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d scene reconstruction.</s></p></d>", "label": ["<d><p><s>cascaded classification models: combining models for holistic scene understanding</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the singular value decomposition is a key operation in many machine learning methods.</s> <s>its computational cost, however, makes it unscalable and impractical for the massive-sized datasets becoming common in applications.</s> <s>we present a new method, quic-svd, for fast approximation of the full svd with automatic sample size minimization and empirical relative error control.</s> <s>previous monte carlo approaches have not addressed the full svd nor benefited from the efficiency of automatic, empirically-driven sample sizing.</s> <s>our empirical tests show speedups of several orders of magnitude over exact svd.</s> <s>such scalability should enable quic-svd to meet the needs of a wide array of methods and applications.</s></p></d>", "label": ["<d><p><s>quic-svd: fast svd using cosine trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cognitive control refers to the flexible deployment of memory and attention in response to task demands and current goals.</s> <s>control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping.</s> <s>in these tasks, participants must maintain information about the current stimulus-response mapping in working memory.</s> <s>prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning.</s> <s>we present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference.</s> <s>we show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing.</s> <s>moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efficiently refined with subsequent task experience.</s></p></d>", "label": ["<d><p><s>temporal dynamics of cognitive control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>current on-line learning algorithms for predicting the labelling of a graph have an important limitation in the case of large diameter graphs; the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems.</s> <s>we overcome this problem with an efficient algorithm which achieves a logarithmic mistake bound.</s> <s>furthermore, current algorithms are optimised for data which exhibits cluster-structure; we give an additional algorithm which performs well locally in the presence of cluster structure and on large diameter graphs.</s></p></d>", "label": ["<d><p><s>online prediction on large diameter graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>from an information-theoretic perspective, a noisy transmission system such as a visual brain-computer interface (bci) speller could benefit from the use of error-correcting codes.</s> <s>however, optimizing the code solely according to the maximal minimum-hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a significantly reduced average target-to-target interval (tti), leading to difficulties in classifying the individual event-related potentials (erps) due to overlap and refractory effects.</s> <s>clearly any change to the stimulus setup must also respect the possible psychophysiological consequences.</s> <s>here we report new eeg data from experiments in which we explore stimulus types and codebooks in a within-subject design, finding an interaction between the two factors.</s> <s>our data demonstrate that the traditional, row-column code has particular spatial properties that lead to better performance than one would expect from its ttis and hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used.</s></p></d>", "label": ["<d><p><s>effects of stimulus type and of error-correcting code design on bci speller performance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a new analysis of an efficient margin-based algorithm for selective sampling in classification problems.</s> <s>using the so-called tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm.</s> <s>our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the bayes risk at rate $n^{-(1+\\alpha)/(3+\\alpha)}$, with labels being sampled at the same rate (here $n$ denotes the sample size, and $\\alpha > 0$ is the exponent in the low noise condition).</s> <s>we compare this convergence rate to the rate $n^{-(1+\\alpha)/(2+\\alpha)}$ achieved by the fully supervised algorithm using all labels.</s> <s>experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efficient competitors.</s></p></d>", "label": ["<d><p><s>linear classification and selective sampling under low noise conditions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a novel center-based clustering algorithm is proposed in this paper.</s> <s>we first formulate clustering as an np-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem.</s> <s>this leads to an efficient and very general algorithm, which works in the dual domain, and can cluster data based on an arbitrary set of distances.</s> <s>despite its generality, it is independent of initialization (unlike em-like methods such as k-means), has guaranteed convergence, and can also provide online optimality bounds about the quality of the estimated clustering solutions.</s> <s>to deal with the most critical issue in a center-based clustering algorithm (selection of cluster centers), we also introduce the notion of stability of a cluster center, which is a well defined lp-based quantity that plays a key role to our algorithm's success.</s> <s>furthermore, we also introduce, what we call, the margins (another key ingredient in our algorithm), which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efficient approximations to the latter.</s> <s>promising experimental results demonstrate the potentials of our method.</s></p></d>", "label": ["<d><p><s>clustering via lp-based stabilities</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a multiplicative approximation scheme (mas) for inference problems in graphical models, which can be applied to various inference algorithms.</s> <s>the method uses $\\epsilon$-decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error $\\epsilon$.</s> <s>mas translates these local approximations into bounds on the accuracy of the results.</s> <s>we show how to optimize $\\epsilon$-decompositions and provide a fast closed-form solution for an $l_2$ approximation.</s> <s>applying mas to the variable elimination inference algorithm, we introduce an algorithm we call dynadecomp which is extremely fast in practice and provides guaranteed error bounds on the result.</s> <s>the superior accuracy and efficiency of dynadecomp is demonstrated.</s></p></d>", "label": ["<d><p><s>mas: a multiplicative approximation scheme for probabilistic inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining.</s> <s>however, the computational and/or communication resources required by the method in processing large-scale data sets are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm.</s> <s>in this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering.</s> <s>we show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the laplacian matrix.</s> <s>from this result we derive approximate upper bounds on the clustering error.</s> <s>we show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a specification of permitted loss in clustering performance.</s></p></d>", "label": ["<d><p><s>spectral clustering with perturbed data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel hierarchical, nonlinear model that predicts brain activity in area v1 evoked by natural images.</s> <s>in the study reported here brain activity was measured by means of functional magnetic resonance imaging (fmri), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (~ 2mm cube) of brain tissue.</s> <s>our model, which we call the spam v1 model, is based on the reasonable assumption that fmri measurements reflect the (possibly nonlinearly) pooled, rectified output of a large population of simple and complex cells in v1.</s> <s>it has a hierarchical filtering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called ???pooled-complex???</s> <s>cells).</s> <s>the pooling stage then obtains the measured fmri signals as a sparse additive model (spam) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed.</s> <s>our results show that the spam v1 model predicts fmri responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells.</s> <s>furthermore, the spatial receptive fields, frequency tuning and orientation tuning curves of the spam v1 model estimated for each voxel appears to be consistent with the known properties of v1, and with previous analyses of this data set.</s> <s>a visualization procedure applied to the spam v1 model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities.</s></p></d>", "label": ["<d><p><s>nonparametric sparse hierarchical models describe v1 fmri responses to natural images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a way of learning matrix representations of objects and relationships.</s> <s>the goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method.</s> <s>we demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships.</s> <s>we show that the same system can learn first-order propositions such as $(2, 5) \\member +\\!3$ or $(christopher, penelope)\\member has\\_wife$, and higher-order propositions such as $(3, +\\!3) \\member plus$ and $(+\\!3, -\\!3) \\member inverse$ or $(has\\_husband, has\\_wife)\\in higher\\_oppsex$.</s> <s>we further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations $+\\!3$ or $has\\_wife$ even though it has not been trained on any first-order examples involving these relations.</s></p></d>", "label": ["<d><p><s>using matrices to model symbolic relationship</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new co-clustering problem of images and visual features.</s> <s>the problem involves a set of non-object images in addition to a set of object images and features to be co-clustered.</s> <s>co-clustering is performed in a way of maximising discrimination of object images from non-object images, thus emphasizing discriminative features.</s> <s>this provides a way of obtaining perceptual joint-clusters of object images and features.</s> <s>we tackle the problem by simultaneously boosting multiple strong classifiers which compete for images by their expertise.</s> <s>each boosting classifier is an aggregation of weak-learners, i.e.</s> <s>simple visual features.</s> <s>the obtained classifiers are useful for multi-category and multi-view object detection tasks.</s> <s>experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classifiers in object detection tasks.</s></p></d>", "label": ["<d><p><s>mcboost: multiple classifier boosting for perceptual co-clustering of images and visual features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms.</s> <s>our framework yields the tightest known logarithmic regret bounds for follow-the-leader and for the gradient descent algorithm proposed in hazankakaag06.</s> <s>we then show that one can interpolate between these two extreme cases.</s> <s>in particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations.</s> <s>finally, we further extend our framework for generalized strongly convex functions.</s></p></d>", "label": ["<d><p><s>mind the duality gap: logarithmic regret algorithms for online optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bartlett et al (2006) recently proved that a ground condition for convex surrogates, classification calibration, ties up the minimization of the surrogates and classification risks, and left as an important problem the algorithmic questions about the minimization of these surrogates.</s> <s>in this paper, we propose an algorithm which provably minimizes any classification calibrated surrogate strictly convex and differentiable --- a set whose losses span the exponential, logistic and squared losses ---, with boosting-type guaranteed convergence rates under a weak learning assumption.</s> <s>a particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zero-sum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning.</s> <s>we report experiments on more than 50 readily available domains of 11 flavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates.</s></p></d>", "label": ["<d><p><s>on the efficient minimization of classification calibrated surrogates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases.</s> <s>first we apply an idea of dwork et al.</s> <s>to design a specific privacy-preserving machine learning algorithm, logistic regression.</s> <s>this involves bounding the sensitivity of logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity.</s> <s>noting that the approach of dwork et al.</s> <s>has limitations when applied to other machine learning algorithms, we then present another privacy-preserving logistic regression algorithm.</s> <s>the algorithm is based on solving a perturbed objective, and does not depend on the sensitivity.</s> <s>we prove that our algorithm preserves privacy in the model due to dwork et al., and we provide a learning performance guarantee.</s> <s>our work also reveals an interesting connection between regularization and privacy.</s></p></d>", "label": ["<d><p><s>privacy-preserving logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>compressive sensing (cs) combines sampling and compression into a single sub-nyquist linear measurement process for sparse and compressible signals.</s> <s>in this paper, we extend the theory of cs to include signals that are concisely represented in terms of a graphical model.</s> <s>in particular, we use markov random fields (mrfs) to represent sparse signals whose nonzero coefficients are clustered.</s> <s>our new model-based reconstruction algorithm, dubbed lattice matching pursuit (lamp), stably recovers mrf-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>sparse signal recovery using markov random fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the cluster assumption is exploited by most semi-supervised learning (ssl) methods.</s> <s>however, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classification.</s> <s>in such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classification accuracy becomes a challenge.</s> <s>we introduce semi-supervised learning with weakly-related unlabeled data\" (sslw), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information.</s> <s>although the sslw could improve a wide range of classification tasks, in this paper, we focus on text categorization with a small training pool.</s> <s>the key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent.</s> <s>to this end, sslw estimates the optimal word-correlation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents.</s> <s>for empirical evaluation, we present a direct comparison with a number of state-of-the-art methods for inductive semi-supervised learning and text categorization; and we show that sslw results in a significant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test beds.\"</s></p></d>", "label": ["<d><p><s>semi-supervised learning with weakly-related unlabeled data : towards better text categorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents the first data-dependent generalization bounds for non-i.i.d.</s> <s>settings based on the notion of rademacher complexity.</s> <s>our bounds extend to the non-i.i.d.</s> <s>case existing rademacher complexity bounds derived for the i.i.d.</s> <s>setting.</s> <s>these bounds provide a strict generalization of the ones found in the i.i.d.</s> <s>case, and can also be used within the standard i.i.d.</s> <s>scenario.</s> <s>they apply to the standard scenario of beta-mixing stationary sequences examined in many previous studies of non-i.i.d.</s> <s>settings and benefit form the crucial advantages of rademacher complexity over other measures of the complexity of hypothesis classes.</s> <s>in particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample.</s> <s>the empirical rademacher complexity can be estimated from finite samples and lead to tighter bounds.</s></p></d>", "label": ["<d><p><s>rademacher complexity bounds for non-i.i.d. processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text.</s> <s>we suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model.</s> <s>this semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any specific task in the same space through regularization.</s> <s>in an empirical study, we construct 190 different text classification tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks.</s> <s>we test the ability of various algorithms to use the mixed unlabeled text to enhance all classification tasks.</s> <s>empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the specific task to be enhanced, and the prediction model used.</s></p></d>", "label": ["<d><p><s>learning the semantic correlation: an alternative way to gain from unlabeled text</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering.</s> <s>in this respect, we follow up on the work of kelinberg, (kleinberg) that showed an impossibility result for such axiomatization.</s> <s>we argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the specific formalism used in kleinberg.</s> <s>as opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the primitive object to be axiomatized.</s> <s>we show that principles like those formulated in kleinberg's axioms can be readily expressed in the latter framework without leading to inconsistency.</s> <s>a clustering-quality measure is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how `strong' or `conclusive' the clustering is.</s> <s>we analyze what clustering-quality measures should look like and introduce a set of requirements (`axioms') that express these requirement and extend the translation of kleinberg's axioms to our framework.</s> <s>we propose several natural clustering quality measures, all satisfying the proposed axioms.</s> <s>in addition, we show that the proposed clustering quality can be computed in polynomial time.</s></p></d>", "label": ["<d><p><s>measures of clustering quality: a working set of axioms for clustering</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>existing approaches to nonrigid structure from motion assume that the instantaneous 3d shape of a deforming object is a linear combination of basis shapes, which have to be estimated anew for each video sequence.</s> <s>in contrast, we propose that the evolving 3d structure be described by a linear combination of basis trajectories.</s> <s>the principal advantage of this lateral approach is that we do not need to estimate any basis vectors during computation.</s> <s>instead, we show that generic bases over trajectories, such as the discrete cosine transform (dct) bases, can be used to effectively describe most real motions.</s> <s>this results in a significant reduction in unknowns, and corresponding stability, in estimation.</s> <s>we report empirical performance, quantitatively using motion capture data and qualitatively on several video sequences exhibiting nonrigid motions including piece-wise rigid motion, articulated motion, partially nonrigid motion (such as a facial expression), and highly nonrigid motion (such as a person dancing).</s></p></d>", "label": ["<d><p><s>nonrigid structure from motion in trajectory space</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of extracting smooth low-dimensional ``neural trajectories'' that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials.</s> <s>beyond the benefit of visualizing the high-dimensional noisy spiking activity in a compact denoised form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity.</s> <s>current methods for extracting neural trajectories involve a two-stage process: the data are first ``denoised'' by smoothing over time, then a static dimensionality reduction technique is applied.</s> <s>we first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time.</s> <s>we then present a novel method for extracting neural trajectories, gaussian-process factor analysis (gpfa), which unifies the smoothing and dimensionality reduction operations in a common probabilistic framework.</s> <s>we applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution.</s> <s>by adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that gpfa provided a better characterization of the population activity than the two-stage methods.</s> <s>from the extracted single-trial neural trajectories, we directly observed a convergence in neural state during motor planning, an effect suggestive of attractor dynamics that was shown indirectly by previous studies.</s></p></d>", "label": ["<d><p><s>gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>randomized neural networks are immortalized in this ai koan: in the days when sussman was a novice, minsky once came to him as he sat hacking at the pdp-6.</s> <s>``what are you doing?''</s> <s>asked minsky.</s> <s>``i am training a randomly wired neural net to play tic-tac-toe,'' sussman replied.</s> <s>``why is the net wired randomly?''</s> <s>asked minsky.</s> <s>sussman replied, ``i do not want it to have any preconceptions of how to play.''</s> <s>minsky then shut his eyes.</s> <s>``why do you close your eyes?''</s> <s>sussman asked his teacher.</s> <s>``so that the room will be empty,'' replied minsky.</s> <s>at that moment, sussman was enlightened.</s> <s>we analyze shallow random networks with the help of concentration of measure inequalities.</s> <s>specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities.</s> <s>we identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities.</s></p></d>", "label": ["<d><p><s>weighted sums of random kitchen sinks: replacing minimization with randomization in learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>identification and comparison of nonlinear dynamical systems using noisy and sparse experimental data is a vital task in many fields, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced.</s> <s>we present an accelerated sampling procedure which enables bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of gaussian processes (gp).</s> <s>our method involves gp regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time.</s> <s>we demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods.</s></p></d>", "label": ["<d><p><s>accelerating bayesian inference over nonlinear differential equations with gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>metal binding is important for the structural and functional characterization of proteins.</s> <s>previous prediction efforts have only focused on bonding state, i.e.</s> <s>deciding which protein residues act as metal ligands in some binding site.</s> <s>identifying the geometry of metal-binding sites, i.e.</s> <s>deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone.</s> <s>in this paper, we formulate it in the framework of learning with structured outputs.</s> <s>our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs.</s> <s>on a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75\\%/46\\% correct ligand-ion assignments, which improves to 88\\%/88\\% in the setting where the metal binding state is known.</s></p></d>", "label": ["<d><p><s>predicting the geometry of metal binding sites from protein sequence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in multi-task learning several related tasks are considered simultaneously, with the hope that by an appropriate sharing of information across tasks, each task may benefit from the others.</s> <s>in the context of learning linear functions for supervised classification or regression, this can be achieved by including a priori information about the weight vectors associated with the tasks, and how they are expected to be related to each other.</s> <s>in this paper, we assume that tasks are clustered into groups, which are unknown beforehand, and that tasks within a group have similar weight vectors.</s> <s>we design a new spectral norm that encodes this a priori assumption, without the prior knowledge of the partition of tasks into groups, resulting in a new convex optimization formulation for multi-task learning.</s> <s>we show in simulations on synthetic examples and on the iedb mhc-i binding dataset, that our approach outperforms well-known convex methods for multi-task learning, as well as related non convex methods dedicated to the same problem.</s></p></d>", "label": ["<d><p><s>clustered multi-task learning: a convex formulation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper we introduce the meannn approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence.</s> <s>as opposed to other nonparametric approaches the meannn results in smooth differentiable functions of the data samples with clear geometrical interpretation.</s> <s>then we apply the proposed estimators to the ica problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods.</s> <s>the improved performance on the proposed ica algorithm is demonstrated on standard tests in comparison with state-of-the-art techniques.</s></p></d>", "label": ["<d><p><s>ica based on a smooth estimation of the differential entropy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, fitted q-iteration (fqi) based methods have become more popular due to their increased sample efficiency, a more stable learning process and the higher quality of the resulting policy.</s> <s>however, these methods remain hard to use for continuous action spaces which frequently occur in real-world tasks, e.g., in robotics and other technical applications.</s> <s>the greedy action selection commonly used for the policy improvement step is particularly problematic as it is expensive for continuous actions, can cause an unstable learning process, introduces an optimization bias and results in highly non-smooth policies unsuitable for real-world systems.</s> <s>in this paper, we show that by using a soft-greedy action selection the policy improvement step used in fqi can be simplified to an inexpensive advantage-weighted regression.</s> <s>with this result, we are able to derive a new, computationally efficient fqi algorithm which can even deal with high dimensional action spaces.</s></p></d>", "label": ["<d><p><s>fitted q-iteration by advantage weighted regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>ranking is at the heart of many information retrieval applications.</s> <s>unlike standard regression or classification, in which we predict outputs independently, in ranking, we are interested in predicting structured outputs so that misranking one object can significantly affect whether we correctly rank the other objects.</s> <s>in practice, the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required, or assumptions of independence between object scores must be made in order to make the problem tractable.</s> <s>we present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks (cdns), where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions (cdfs) over multiple pairwise preferences.</s> <s>we apply our framework to the problem of document retrieval in the case of the ohsumed benchmark dataset.</s> <s>we will show that the ranknet, listnet and listmle probabilistic models can be viewed as particular instances of cdns and that our proposed framework allows for the exploration of a broad class of flexible structured loss functionals for ranking learning.</s></p></d>", "label": ["<d><p><s>structured ranking learning using cumulative distribution networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel mathematical formalism for the idea of a local model,'' a model of a potentially complex dynamical system that makes only certain predictions in only certain situations.</s> <s>as a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system.</s> <s>we then show how one might combine several local models to produce a more detailed model.</s> <s>we demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods.\"</s></p></d>", "label": ["<d><p><s>simple local models for complex dynamical systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov decision processes (mdps) have been extensively studied and used in the context of planning and decision-making, and many methods exist to find the optimal policy for problems modelled as mdps.</s> <s>although finding the optimal policy is sufficient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), finding all possible near-optimal policies might be useful as it provides more flexibility to the person executing the policy.</s> <s>in this paper we introduce the new concept of non-deterministic mdp policies, and address the question of finding near-optimal non-deterministic policies.</s> <s>we propose two solutions to this problem, one based on a mixed integer program and the other one based on a search algorithm.</s> <s>we include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system.</s></p></d>", "label": ["<d><p><s>mdps with non-deterministic policies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>eeg connectivity measures could provide a new type of feature space for inferring a subject's intention in brain-computer interfaces (bcis).</s> <s>however, very little is known on eeg connectivity patterns for bcis.</s> <s>in this study, eeg connectivity during motor imagery (mi) of the left and right is investigated in a broad frequency range across the whole scalp by combining beamforming with transfer entropy and taking into account possible volume conduction effects.</s> <s>observed connectivity patterns indicate that modulation intentionally induced by mi is strongest in the gamma-band, i.e., above 35 hz.</s> <s>furthermore, modulation between mi and rest is found to be more pronounced than between mi of different hands.</s> <s>this is in contrast to results on mi obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in bcis.</s> <s>it is concluded that future studies on connectivity based bcis should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions.</s></p></d>", "label": ["<d><p><s>understanding brain connectivity patterns during motor imagery for brain-computer interfacing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from specific noise models.</s> <s>we demonstrate this approach on the challenging problem of natural image denoising.</s> <s>using a test set with a hundred natural images, we find that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and markov random field (mrf) methods.</s> <s>moreover, we find that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting.</s> <s>we also show how convolutional networks are mathematically related to mrf approaches by presenting a mean field theory for an mrf specially designed for image denoising.</s> <s>although these approaches are related, convolutional networks avoid computational difficulties in mrf approaches that arise from probabilistic learning and inference.</s> <s>this makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is significantly less than that associated with inference in mrf approaches with even hundreds of parameters.</s></p></d>", "label": ["<d><p><s>natural image denoising with convolutional networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>compared to invasive brain-computer interfaces (bci), non-invasive bci systems based on electroencephalogram (eeg) signals have not been applied successfully for complex control tasks.</s> <s>in the present study, however, we demonstrate this is possible and report on the interaction of a human subject with a complex real device: a pinball machine.</s> <s>first results in this single subject study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires complex predictive behavior.</s> <s>using machine learning methods for mental state decoding, bci-based pinball control is possible within the first session without the necessity to employ lengthy subject training.</s> <s>while the current study is still of anecdotal nature, it clearly shows that very compelling control with excellent timing and dynamics is possible for a non-invasive bci.</s></p></d>", "label": ["<d><p><s>playing pinball with non-invasive bci</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>working memory is a central topic of cognitive neuroscience because it is critical for solving real world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior.</s> <s>however, an often neglected fact is that learning to use working memory effectively is itself a difficult problem.</s> <s>the gating\" framework is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems.</s> <s>we bring together gating with ideas from machine learning about using finite memory systems in more general problems.</s> <s>thus we present a normative gating model that learns, by online temporal difference methods, to use working memory to maximize discounted future rewards in general partially observable settings.</s> <s>the model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in human experiments.</s> <s>moreover, the model introduces a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards.\"</s></p></d>", "label": ["<d><p><s>learning to use working memory in partially observable environments through dopaminergic reinforcement</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide sharp bounds for rademacher and gaussian complexities of (constrained) linear classes.</s> <s>these bounds make short work of providing a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either $l_2$ or $l_1$ constraints), margin bounds (including both $l_2$ and $l_1$ margins, along with more general notions based on relative entropy), a proof of the pac-bayes theorem, and $l_2$ covering numbers (with $l_p$ norm constraints and relative entropy constraints).</s> <s>in addition to providing a unified analysis, the results herein provide some of the sharpest risk and margin bounds (improving upon a number of previous results).</s> <s>interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction (up to a constant factor of 2).</s></p></d>", "label": ["<d><p><s>on the complexity of linear prediction: risk bounds, margin bounds, and regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>policy gradient (pg) reinforcement learning algorithms have strong (local) convergence guarantees, but their learning performance is typically limited by a large variance in the estimate of the gradient.</s> <s>in this paper, we formulate the variance reduction problem by describing a signal-to-noise ratio (snr) for policy gradient algorithms, and evaluate this snr carefully for the popular weight perturbation (wp) algorithm.</s> <s>we confirm that snr is a good predictor of long-term learning performance, and that in our episodic formulation, the cost-to-go function is indeed the optimal baseline.</s> <s>we then propose two modifications to traditional model-free policy gradient algorithms in order to optimize the snr.</s> <s>first, we examine wp using anisotropic sampling distributions, which introduces a bias into the update but increases the snr; this bias can be interpretted as following the natural gradient of the cost function.</s> <s>second, we show that non-gaussian distributions can also increase the snr, and argue that the optimal isotropic distribution is a ???shell???</s> <s>distribution with a constant magnitude and uniform distribution in direction.</s> <s>we demonstrate that both modifications produce substantial improvements in learning performance in challenging policy gradient experiments.</s></p></d>", "label": ["<d><p><s>signal-to-noise ratio analysis of policy gradient algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one major role of primary visual cortex (v1) in vision is the encoding of the orientation of lines and contours.</s> <s>the role of the local recurrent network in these computations is, however, still a matter of debate.</s> <s>to address this issue, we analyze intracellular recording data of cat v1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map.</s> <s>for the analysis, we consider a network model of hodgkin-huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map.</s> <s>we then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input.</s> <s>each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances.</s> <s>a quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition.</s> <s>this recurrent regime is close to a regime of 'instability', where strong, self-sustained activity of the network occurs.</s> <s>the firing rate of neurons in the best-fitting network is particularly sensitive to small modulations of model parameters, which could be one of the functional benefits of a network operating in this particular regime.</s></p></d>", "label": ["<d><p><s>dependence of orientation tuning on recurrent excitation and inhibition in a network model of v1</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present cutoff averaging\", a technique for converting any conservative online learning algorithm into a batch learning algorithm.</s> <s>most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted.</s> <s>an attractive property of our technique is that it preserves the efficiency of the original online algorithm, making it approporiate for large-scale learning problems.</s> <s>we provide a statistical analysis of our technique and back our theoretical claims with experimental results.\"</s></p></d>", "label": ["<d><p><s>from online to batch learning with cutoff-averaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of learning classifiers for several related tasks that may differ in their joint distribution of input and output variables.</s> <s>for each task, small - possibly even empty - labeled samples and large unlabeled samples are available.</s> <s>while the unlabeled samples reflect the target distribution, the labeled samples may be biased.</s> <s>we derive a solution that produces resampling weights which match the pool of all examples to the target distribution of any given task.</s> <s>our work is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed.</s> <s>here, questionnaires offered to a small portion of each portal's users produce biased samples.</s> <s>transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy.</s></p></d>", "label": ["<d><p><s>transfer learning by distribution matching for targeted advertising</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms.</s> <s>a critical idea is that of limited capacity, the allocation of which has produced half a century's worth of conflict about such phenomena as early and late selection.</s> <s>an influential resolution of this debate is based on the notion of perceptual load (lavie, 2005, tics, 9: 75), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difficult tasks grab all resources for themselves, leaving distractors high and dry.</s> <s>we argue that this theory presents a challenge to bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data.</s></p></d>", "label": ["<d><p><s>load and attentional bayes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>actor-critic algorithms for reinforcement learning are achieving renewed popularity due to their good convergence properties in situations where other approaches often fail (e.g., when function approximation is involved).</s> <s>interestingly, there is growing evidence that actor-critic approaches based on phasic dopamine signals play a key role in biological learning through the cortical and basal ganglia.</s> <s>we derive a temporal difference based actor critic learning algorithm, for which convergence can be proved without assuming separate time scales for the actor and the critic.</s> <s>the approach is demonstrated by applying it to networks of spiking neurons.</s> <s>the established relation between phasic dopamine and the temporal difference signal lends support to the biological relevance of such algorithms.</s></p></d>", "label": ["<d><p><s>temporal difference based actor critic learning - convergence and neural implementation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduces a new probability distribution over a potentially infinite number of binary markov chains which we call the markov indian buffet process.</s> <s>this process extends the ibp to allow temporal dependencies in the hidden variables.</s> <s>we use this stochastic process to build a nonparametric extension of the factorial hidden markov model.</s> <s>after working out an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden markov model can be used for blind source separation.</s></p></d>", "label": ["<d><p><s>the infinite factorial hidden markov model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a variety of behavioral tasks, subjects exhibit an automatic and apparently sub-optimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern.</s> <s>this is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no predictive power.</s> <s>in this work, we use a normative bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of fundamental mechanisms critical for adapting to changing statistics in the natural environment.</s> <s>we show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise bayes-optimal algorithm.</s> <s>the bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations, a feature also apparent in the behavioral data.</s> <s>we derive an explicit relationship between the parameters and computations of the exact bayesian algorithm and those of the approximate linear-exponential filter.</s> <s>since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful.</s> <s>we also show that near-optimal tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs.</s> <s>this is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities.</s></p></d>", "label": ["<d><p><s>sequential effects: superstition or rational behavior?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of multiple kernel learning (mkl), which can be formulated as a convex-concave problem.</s> <s>in the past, two efficient methods, i.e., semi-infinite linear programming (silp) and subgradient descent (sd), have been proposed for large-scale multiple kernel learning.</s> <s>despite their success, both methods have their own shortcomings: (a) the sd method utilizes the gradient of only the current solution, and (b) the silp method does not regularize the approximate solution obtained from the cutting plane model.</s> <s>in this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning.</s> <s>the extended level method overcomes the drawbacks of silp and sd by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set.</s> <s>empirical study with eight uci datasets shows that the extended level method can significantly improve efficiency by saving on average 91.9% of computational time over the silp method and 70.3% over the sd method.</s></p></d>", "label": ["<d><p><s>an extended level method for efficient multiple kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of efficiently encoding a signal by transforming it to a new representation whose components are statistically independent.</s> <s>a widely studied linear solution, independent components analysis (ica), exists for the case when the signal is generated as a linear transformation of independent non- gaussian sources.</s> <s>here, we examine a complementary case, in which the source is non-gaussian but elliptically symmetric.</s> <s>in this case, no linear transform suffices to properly decompose the signal into independent components, but we show that a simple nonlinear transformation, which we call radial gaussianization (rg), is able to remove all dependencies.</s> <s>we then demonstrate this methodology in the context of natural signal statistics.</s> <s>we first show that the joint distributions of bandpass filter responses, for both sound and images, are better described as elliptical than linearly transformed independent sources.</s> <s>consistent with this, we demonstrate that the reduction in dependency achieved by applying rg to either pairs or blocks of bandpass filter responses is significantly greater than that achieved by pca or ica.</s></p></d>", "label": ["<d><p><s>reducing statistical dependencies in natural signals using radial gaussianization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most algorithms for solving markov decision processes rely on a discount factor, which ensures their convergence.</s> <s>in fact, it is often used in problems with is no intrinsic motivation.</s> <s>in this paper, we show that when used in approximate dynamic programming, an artificially low discount factor may significantly improve the performance on some problems, such as tetris.</s> <s>we propose two explanations for this phenomenon.</s> <s>our first justification follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds.</s> <s>however, we also show that these bounds are loose, a thus their decrease does not entirely justify a better practical performance.</s> <s>we thus propose another justification: when the rewards are received only sporadically (as it is the case in tetris), we can derive tighter bounds, which support a significant performance increase with a decrease in the discount factor.</s></p></d>", "label": ["<d><p><s>biasing approximate dynamic programming with a lower discount factor</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>distributed learning is a problem of fundamental interest in machine learning and cognitive science.</s> <s>in this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: latent dirichlet allocation (lda) and hierarchical dirichlet processes (hdp).</s> <s>in the proposed approach, the data are distributed across p processors, and processors independently perform gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors.</s> <s>we demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard lda and hdp samplers, but with significant improvements in computation time and memory.</s> <s>we show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors.</s> <s>as a stepping stone in the development of asynchronous hdp, a parallel hdp sampler is also introduced.</s></p></d>", "label": ["<d><p><s>asynchronous distributed learning of topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>causal structure-discovery techniques usually assume that all causes of more than one variable are observed.</s> <s>this is the so-called causal sufficiency assumption.</s> <s>in practice, it is untestable, and often violated.</s> <s>in this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data.</s> <s>similar to algorithms such as ic* and fci, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables.</s> <s>assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditional-independence tests w.r.t.</s> <s>the number of variables.</s> <s>we show with experiments that our algorithm is comparable to the state-of-the-art fci algorithm in accuracy, while being several orders of magnitude faster on large problems.</s> <s>we conclude that mbcs* makes a new range of causally insufficient problems computationally tractable.</s></p></d>", "label": ["<d><p><s>finding latent causes in causal networks: an efficient approach based on markov blankets</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study learning formulations with non-convex regularizaton that are natural for sparse linear models.</s> <s>there are two approaches to this problem: (1) heuristic methods such as gradient descent that only find a local minimum.</s> <s>a drawback of this approach is the lack of theoretical guarantee showing that the local minimum gives a good solution.</s> <s>(2) convex relaxation such as $l_1$-regularization that solves the problem under some conditions.</s> <s>however it often leads to sub-optimal sparsity in reality.</s> <s>this paper tries to remedy the above gap between theory and practice.</s> <s>in particular, we investigate a multi-stage convex relaxation scheme for solving problems with non-convex regularization.</s> <s>theoretically, we analyze the behavior of a resulting two-stage relaxation scheme for the capped-$l_1$ regularization.</s> <s>our performance bound shows that the procedure is superior to the standard $l_1$ convex relaxation for learning sparse targets.</s> <s>experiments confirm the effectiveness of this method on some simulation and real data.</s></p></d>", "label": ["<d><p><s>multi-stage convex relaxation for learning with sparse regularization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of variable-resolution arithmetic vector processing elements (vpe).</s> <s>groups of vpes operate in simd (single instruction multiple data) mode, and each group is connected to an independent memory bank.</s> <s>in this way memory bandwidth scales with the number of vpe, and the main data flows are local, keeping power dissipation low.</s> <s>with 256 vpes, implemented on two fpga (field programmable gate array) chips, we obtain a sustained speed of 19 gmacs (billion multiply-accumulate per sec.)</s> <s>for svm training, and 86 gmacs for svm classification.</s> <s>this performance is more than an order of magnitude higher than that of any fpga implementation reported so far.</s> <s>the speed on one fpga is similar to the fastest speeds published on a graphics processor for the mnist problem, despite a clock rate of the fpga that is six times lower.</s> <s>high performance at low clock rates makes this massively parallel architecture particularly attractive for embedded applications, where low power dissipation is critical.</s> <s>tests with convolutional neural networks and other learning algorithms are under way now.</s></p></d>", "label": ["<d><p><s>a massively parallel digital learning processor</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>continuous attractor neural networks (canns) are emerging as promising models for describing the encoding of continuous stimuli in neural systems.</s> <s>due to the translational invariance of their neuronal interactions, canns can hold a continuous family of neutrally stable states.</s> <s>in this study, we systematically explore how neutral stability of a cann facilitates its tracking performance, a capacity believed to have wide applications in brain functions.</s> <s>we develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space.</s> <s>we quantify the distortions of the bump shape during tracking, and study their effects on the tracking performance.</s> <s>results are obtained on the maximum speed for a moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus.</s></p></d>", "label": ["<d><p><s>tracking changing stimuli in continuous attractor neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are simply performing associative learning supported by similarity.</s> <s>we provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics.</s> <s>using the equivalence of bayesian linear regression and gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem.</s> <s>we use this insight to define a gaussian process model of human function learning that combines the strengths of both approaches.</s></p></d>", "label": ["<d><p><s>modeling human function learning with gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bandpass filtering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of v1 simple cells.</s> <s>while the effect of bandpass filtering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation.</s> <s>here we employ the class of $l_p$ elliptically contoured distributions to investigate the extent to which the two features---orientation selectivity and contrast gain control---are suited to model the statistics of natural images.</s> <s>within this framework we find that contrast gain control can play a significant role for the removal of redundancies in natural images.</s> <s>orientation selectivity, in contrast, has only a very limited potential for redundancy reduction.</s></p></d>", "label": ["<d><p><s>the conjoint effect of divisive normalization and orientation selectivity on redundancy reduction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs.</s> <s>in this paper, a dynamic visual attention model based on the rarity of features is proposed.</s> <s>we introduce the incremental coding length (icl) to measure the perspective entropy gain of each feature.</s> <s>the objective of our model is to maximize the entropy of the sampled visual features.</s> <s>in order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their incremental coding length.</s> <s>by selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes.</s> <s>we demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation.</s> <s>moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return.</s></p></d>", "label": ["<d><p><s>dynamic visual attention: searching for coding length increments</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>principal components analysis (pca) has become established as one of the key tools for dimensionality reduction when dealing with real valued data.</s> <s>approaches such as exponential family pca and non-negative matrix factorisation have successfully extended pca to non-gaussian data types, but these techniques fail to take advantage of bayesian inference and can suffer from problems of overfitting and poor generalisation.</s> <s>this paper presents a fully probabilistic approach to pca, which is generalised to the exponential family, based on hybrid monte carlo sampling.</s> <s>we describe the model which is based on a factorisation of the observed data matrix, and show performance of the model on both synthetic and real data.</s></p></d>", "label": ["<d><p><s>bayesian exponential family pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>integrating semantic and syntactic analysis is essential for document analysis.</s> <s>using an analogous reasoning, we present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context.</s> <s>we argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufficient for representing context.</s> <s>learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or ''informative'' background(context).</s> <s>labeling.</s> <s>we present a ''shape-aware'' model which utilizes contour information for efficient and accurate labeling of features in the image.</s> <s>our approach iterates between an mcmc-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity.</s></p></d>", "label": ["<d><p><s>a ``shape aware'' model for semi-supervised learning of objects and its context</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in classification problems, support vector machines maximize the margin of separation between two classes.</s> <s>while the paradigm has been successful, the solution obtained by svms is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions.</s> <s>this article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data.</s> <s>the proposed formulation can be efficiently solved and experiments on digit datasets show drastic performance improvements over svms.</s></p></d>", "label": ["<d><p><s>relative margin machines</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a mixture model whose components are restricted boltzmann machines (rbms).</s> <s>this possibility has not been considered before because computing the partition function of an rbm is intractable, which appears to make learning a mixture of rbms intractable as well.</s> <s>surprisingly, when formulated as a third-order boltzmann machine, such a mixture model can be learned tractably using contrastive divergence.</s> <s>the energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden multinomial unit that represents the cluster labels.</s> <s>the distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized.</s> <s>instead, they are defined implicitly via the energy function and depend on all the parameters in the model.</s> <s>we present results for the mnist and norb datasets showing that the implicit mixture of rbms learns clusters that reflect the class structure in the data.</s></p></d>", "label": ["<d><p><s>implicit mixtures of restricted boltzmann machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new class of consistency constraints for linear programming (lp) relaxations for finding the most probable (map) configuration in graphical models.</s> <s>usual cluster-based lp relaxations enforce joint consistency of the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters.</s> <s>by partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters.</s> <s>we show how to solve the cluster selection and partitioning problem monotonically in the dual lp, using the current beliefs to guide these choices.</s> <s>we obtain a dual message-passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly.</s></p></d>", "label": ["<d><p><s>clusters and coarse partitions in lp relaxations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new fast gaussian summation algorithm for high-dimensional datasets with high accuracy.</s> <s>first, we extend the original fast multipole-type methods to use approximation schemes with both hard and probabilistic error.</s> <s>second, we utilize a new data structure called subspace tree which maps each data point in the node to its lower dimensional mapping as determined by any linear dimension reduction method such as pca.</s> <s>this new data structure is suitable for reducing the cost of each pairwise distance computation, the most dominant cost in many kernel methods.</s> <s>our algorithm guarantees probabilistic relative error on each kernel sum, and can be applied to high-dimensional gaussian summations which are ubiquitous inside many kernel methods as the key computational bottleneck.</s> <s>we provide empirical speedup results on low to high-dimensional datasets up to 89 dimensions.</s></p></d>", "label": ["<d><p><s>fast high-dimensional kernel summations using the monte carlo multipole method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we focus on training deep neural networks for visual recognition tasks.</s> <s>one challenge is the lack of an informative regularization on the network parameters, to imply a meaningful control on the computed function.</s> <s>we propose a training strategy that takes advantage of kernel methods, where an existing kernel function represents useful prior knowledge about the learning task of interest.</s> <s>we derive an efficient algorithm using stochastic gradient descent, and demonstrate very positive results in a wide range of visual recognition tasks.</s></p></d>", "label": ["<d><p><s>deep learning with kernel regularization for visual recognition</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points.</s> <s>we argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved.</s> <s>accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace.</s> <s>the problem of solving for the mapping is transformed into one of solving for an eulerian flow field which we compute using ideas from kernel methods.</s> <s>we demonstrate the efficacy of our approach on various real world data sets.</s></p></d>", "label": ["<d><p><s>diffeomorphic dimensionality reduction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many interesting problems, including bayesian network structure-search, can be cast in terms of finding the optimum value of a function over the space of graphs.</s> <s>however, this function is often expensive to compute exactly.</s> <s>we here present a method derived from the study of reproducing-kernel hilbert spaces which takes advantage of the regular structure of the space of all graphs on a fixed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy.</s> <s>we then test this method on both a small testing set and a real-world bayesian network; the results suggest that not only is this method reasonably accurate, but that the bde score itself varies quadratically over the space of all graphs.</s></p></d>", "label": ["<d><p><s>bayesian network score approximation using a metagraph kernel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many motor skills in humanoid robotics can be learned using parametrized motor primitives as done in imitation learning.</s> <s>however, most interesting motor learning problems are high-dimensional reinforcement learning problems often beyond the reach of current methods.</s> <s>in this paper, we extend previous work on policy learning from the immediate reward case to episodic reinforcement learning.</s> <s>we show that this results into a general, common framework also connected to policy gradient methods and yielding a novel algorithm for policy learning by assuming a form of exploration that is particularly well-suited for dynamic motor primitives.</s> <s>the resulting algorithm is an em-inspired algorithm applicable in complex motor learning tasks.</s> <s>we compare this algorithm to alternative parametrized policy search methods and show that it outperforms previous methods.</s> <s>we apply it in the context of motor learning and show that it can learn a complex ball-in-a-cup task using a real barrett wam robot arm.</s></p></d>", "label": ["<d><p><s>policy search for motor primitives in robotics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes.</s> <s>we consider two such models: the switching linear dynamical system (slds) and the switching vector autoregressive (var) process.</s> <s>in this paper, we present a nonparametric approach to the learning of an unknown number of persistent, smooth dynamical modes by utilizing a hierarchical dirichlet process prior.</s> <s>we develop a sampling algorithm that combines a truncated approximation to the dirichlet process with an efficient joint sampling of the mode and state sequences.</s> <s>the utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the ibovespa stock index.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian learning of switching linear dynamical systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>research in animal learning and behavioral neuroscience has distinguished between two forms of action control: a habit-based form, which relies on stored action values, and a goal-directed form, which forecasts and compares action outcomes based on a model of the environment.</s> <s>while habit-based control has been the subject of extensive computational research, the computational principles underlying goal-directed control in animals have so far received less attention.</s> <s>in the present paper, we advance a computational framework for goal-directed control in animals and humans.</s> <s>we take three empirically motivated points as founding premises: (1) neurons in dorsolateral prefrontal cortex represent action policies, (2) neurons in orbitofrontal cortex represent rewards, and (3) neural computation, across domains, can be appropriately understood as performing structured probabilistic inference.</s> <s>on a purely computational level, the resulting account relates closely to previous work using bayesian inference to solve markov decision problems, but extends this work by introducing a new algorithm, which provably converges on optimal plans.</s> <s>on a cognitive and neuroscientific level, the theory provides a unifying framework for several different forms of goal-directed action selection, placing emphasis on a novel form, within which orbitofrontal reward representations directly drive policy selection.</s></p></d>", "label": ["<d><p><s>goal-directed decision making in prefrontal cortex: a computational framework</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the discovery of causal relationships between a set of observed variables is a fundamental problem in science.</s> <s>for continuous-valued data linear acyclic causal models are often used because these models are well understood and there are well-known methods to fit them to data.</s> <s>in reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods.</s> <s>in this contribution we show that in fact the basic linear framework can be generalized to nonlinear models with additive noise.</s> <s>in this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identified.</s> <s>in addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by nonlinearities.</s></p></d>", "label": ["<d><p><s>nonlinear causal discovery with additive noise models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>given an $n$-vertex weighted tree with structural diameter $s$ and a subset of $m$ vertices, we present a technique to compute a corresponding $m \\times m$ gram matrix of the pseudoinverse of the graph laplacian in $o(n+ m^2 + m s)$ time.</s> <s>we discuss the application of this technique to fast label prediction on a generic graph.</s> <s>we approximate the graph with a spanning tree and then we predict with the kernel perceptron.</s> <s>we address the approximation of the graph with either a minimum spanning tree or a shortest path tree.</s> <s>the fast computation of the pseudoinverse enables us to address prediction problems on large graphs.</s> <s>to this end we present experiments on two web-spam classification tasks, one of which includes a graph with 400,000 nodes and more than 10,000,000 edges.</s> <s>the results indicate that the accuracy of our technique is competitive with previous methods using the full graph information.</s></p></d>", "label": ["<d><p><s>fast prediction on a tree</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a theoretical analysis of the problem of adaptation with multiple sources.</s> <s>for each source domain, the distribution over the input points as well as a hypothesis with error at most \\epsilon are given.</s> <s>the problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain.</s> <s>we present several theoretical results relating to this problem.</s> <s>in particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions benefit from favorable theoretical guarantees.</s> <s>our main result shows that, remarkably, for any fixed target function, there exists a distribution weighted combining rule that has a loss of at most \\epsilon with respect to *any* target mixture of the source distributions.</s> <s>we further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3\\epsilon.</s> <s>finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</s></p></d>", "label": ["<d><p><s>domain adaptation with multiple sources</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>empirical evidence shows that in favorable situations semi-supervised learning (ssl) algorithms can capitalize on the abundancy of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound.</s> <s>however, in other situations unlabeled data do not seem to help.</s> <s>recent attempts at theoretically characterizing the situations in which unlabeled data can help have met with little success, and sometimes appear to conflict with each other and intuition.</s> <s>in this paper, we attempt to bridge the gap between practice and theory of semi-supervised learning.</s> <s>we develop a rigorous framework for analyzing the situations in which unlabeled data can help and quantify the improvement possible using finite sample error bounds.</s> <s>we show that there are large classes of problems for which ssl can significantly outperform supervised learning, in finite sample regimes and sometimes also in terms of error convergence rates.</s></p></d>", "label": ["<d><p><s>unlabeled data: now it helps, now it doesn't</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>correlations between spike counts are often used to analyze neural coding.</s> <s>the noise is typically assumed to be gaussian.</s> <s>yet, this assumption is often inappropriate, especially for low spike counts.</s> <s>in this study, we present copulas as an alternative approach.</s> <s>with copulas it is possible to use arbitrary marginal distributions such as poisson or negative binomial that are better suited for modeling noise distributions of spike counts.</s> <s>furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions.</s> <s>we develop a framework to analyze spike count data by means of copulas.</s> <s>methods for parameter inference based on maximum likelihood estimates and for computation of shannon entropy are provided.</s> <s>we apply the method to our data recorded from macaque prefrontal cortex.</s> <s>the data analysis leads to three significant findings: (1) copula-based distributions provide better fits than discretized multivariate normal distributions; (2) negative binomial margins fit the data better than poisson margins; and (3) a dependence model that includes only pairwise interactions overestimates the information entropy by at least 19% compared to the model with higher order interactions.</s></p></d>", "label": ["<d><p><s>modeling short-term noise dependence of spike counts in macaque prefrontal cortex</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a sparse approximation approach for dependent output gaussian processes (gp).</s> <s>employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a gp.</s> <s>based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated.</s> <s>we show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network.</s></p></d>", "label": ["<d><p><s>sparse convolved gaussian processes for multi-output regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>confidence-weighted (cw) learning [6], an online learning method for linear classifiers, maintains a gaussian distributions over weight vectors, with a covariance matrix that represents uncertainty about weights and correlations.</s> <s>confidence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classifies examples with a specified probability.</s> <s>within this framework, we derive a new convex form of the constraint and analyze it in the mistake bound model.</s> <s>empirical evaluation with both synthetic and text data shows our version of cw learning achieves lower cumulative and out-of-sample errors than commonly used first-order and second-order online methods.</s></p></d>", "label": ["<d><p><s>exact convex confidence-weighted learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the odor transduction process has a large time constant and is susceptible to various types of noise.</s> <s>therefore, the olfactory code at the sensor/receptor level is in general a slow and highly variable indicator of the input odor in both natural and artificial situations.</s> <s>insects overcome this problem by using a neuronal device in their antennal lobe (al), which transforms the identity code of olfactory receptors to a spatio-temporal code.</s> <s>this transformation improves the decision of the mushroom bodies (mbs), the subsequent classifier, in both speed and accuracy.here we propose a rate model based on two intrinsic mechanisms in the insect al, namely integration and inhibition.</s> <s>then we present a mb classifier model that resembles the sparse and random structure of insect mb.</s> <s>a local hebbian learning procedure governs the plasticity in the model.</s> <s>these formulations not only help to understand the signal conditioning and classification methods of insect olfactory systems, but also can be leveraged in synthetic problems.</s> <s>among them, we consider here the discrimination of odor mixtures from pure odors.</s> <s>we show on a set of records from metal-oxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors.</s></p></d>", "label": ["<d><p><s>artificial olfactory brain for mixture identification</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a kernel-based method for change-point analysis within a sequence of temporal observations.</s> <s>change-point analysis of an (unlabelled) sample of observations consists in, first, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution.</s> <s>we propose a test statistics based upon the maximum kernel fisher discriminant ratio as a measure of homogeneity between segments.</s> <s>we derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs).</s> <s>this allows to build a statistical hypothesis testing procedure for testing the presence of change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting.</s> <s>if a change actually occurs, the test statistics also yields an estimator of the change-point location.</s> <s>promising experimental results in temporal segmentation of mental tasks from bci data and pop song indexation are presented.</s></p></d>", "label": ["<d><p><s>kernel change-point analysis</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the essence of exploration is acting to try to decrease uncertainty.</s> <s>we propose a new methodology for representing uncertainty in continuous-state control problems.</s> <s>our approach, multi-resolution exploration (mre), uses a hierarchical mapping to identify regions of the state space that would benefit from additional samples.</s> <s>we demonstrate mre's broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method.</s> <s>empirical results show that mre improves upon state-of-the-art exploration approaches.</s></p></d>", "label": ["<d><p><s>multi-resolution exploration in continuous spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show how improved sequences for magnetic resonance imaging can be found through automated optimization of bayesian design scores.</s> <s>combining recent advances in approximate bayesian inference and natural image statistics with high-performance numerical computation, we propose the first scalable bayesian experimental design framework for this problem of high relevance to clinical and brain research.</s> <s>our solution requires approximate inference for dense, non-gaussian models on a scale seldom addressed before.</s> <s>we propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modified to compute primitives in our framework.</s> <s>our approach is evaluated on a realistic setup with raw data from a 3t mr scanner.</s></p></d>", "label": ["<d><p><s>bayesian experimental design of magnetic resonance imaging sequences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a new content publishing system that selects articles to serve to a user, choosing from an editorially programmed pool that is frequently refreshed.</s> <s>it is now deployed on a major internet portal, and selects articles to serve to hundreds of millions of user visits per day, significantly increasing the number of user clicks over the original manual approach, in which editors periodically selected articles to display.</s> <s>some of the challenges we face include a dynamic content pool, short article lifetimes, non-stationary click-through rates, and extremely high traffic volumes.</s> <s>the fundamental problem we must solve is to quickly identify which items are popular(perhaps within different user segments), and to exploit them while they remain current.</s> <s>we must also explore the underlying pool constantly to identify promising alternatives, quickly discarding poor performers.</s> <s>our approach is based on tracking per article performance in near real time through online models.</s> <s>we describe the characteristics and constraints of our application setting, discuss our design choices, and show the importance and effectiveness of coupling online models with a simple randomization procedure.</s> <s>we discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention.</s> <s>our analysis of this application also suggests a number of future research avenues.</s></p></d>", "label": ["<d><p><s>online models for content optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper discusses non-parametric regression between riemannian manifolds.</s> <s>this learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics.</s> <s>we present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization.</s> <s>the regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural.</s> <s>moreover, we demonstrate that our algorithm performs well in a difficult surface registration problem.</s></p></d>", "label": ["<d><p><s>non-parametric regression between manifolds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>by attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, co-clustering algorithms often demonstrate surpris- ingly impressive performance improvements over traditional one-sided (row) clustering techniques.</s> <s>a good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa).</s> <s>in many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering.</s> <s>in this paper, we develop two novel semi-supervised multi-class classification algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation (e.g., non-negative matrix factorization) formulations for co-clustering.</s> <s>these algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical representer theorem applied to regularization problems posed on a collection of reproducing kernel hilbert spaces.</s> <s>empirical results demonstrate the effectiveness and utility of our algorithms.</s></p></d>", "label": ["<d><p><s>regularized co-clustering with dual supervision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose using correlated bigram lsa for unsupervised lm adaptation for automatic speech recognition.</s> <s>the model is trained using efficient variational em and smoothed using the proposed fractional kneser-ney smoothing which handles fractional counts.</s> <s>our approach can be scalable to large training corpora via bootstrapping of bigram lsa from unigram lsa.</s> <s>for lm adaptation, unigram and bigram lsa are integrated into the background n-gram lm via marginal adaptation and linear interpolation respectively.</s> <s>experimental results show that applying unigram and bigram lsa together yields 6%--8% relative perplexity reduction and 0.6% absolute character error rates (cer) reduction compared to applying only unigram lsa on the mandarin rt04 test set.</s> <s>comparing with the unadapted baseline, our approach reduces the absolute cer by 1.2%.</s></p></d>", "label": ["<d><p><s>correlated bigram lsa for unsupervised language model adaptation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a discriminative part-based approach for human action recognition from video sequences using motion features.</s> <s>our model is based on the recently proposed hidden conditional random field~(hcrf) for object recognition.</s> <s>similar to hcrf for object recognition, we model a human action by a flexible constellation of parts conditioned on image observations.</s> <s>different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions.</s> <s>our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition.</s> <s>in particular, our experimental results demonstrate that combining large-scale global features and local patch features performs significantly better than directly applying hcrf on local patches alone.</s></p></d>", "label": ["<d><p><s>learning a discriminative hidden part model for human action recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel principal component analysis (kpca) is a popular generalization of linear pca that allows non-linear feature extraction.</s> <s>in kpca, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled.</s> <s>the feature space is typically induced implicitly by a kernel function, and linear pca in the feature space is performed via the kernel trick.</s> <s>however, due to the implicitness of the feature space, some extensions of pca such as robust pca cannot be directly generalized to kpca.</s> <s>this paper presents a technique to overcome this problem, and extends it to a unified framework for treating noise, missing data, and outliers in kpca.</s> <s>our method is based on a novel cost function to perform inference in kpca.</s> <s>extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods.</s></p></d>", "label": ["<d><p><s>robust kernel principal component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the temporal restricted boltzmann machine (trbm) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box.</s> <s>the major disadvantage of the trbm is that exact inference is extremely hard, since even computing a gibbs update for a single variable of the posterior is exponentially expensive.</s> <s>this difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning.</s> <s>in this paper we introduce the recurrent trbm, which is a very slight modification of the trbm for which exact inference is very easy and exact gradient learning is almost tractable.</s> <s>we demonstrate that the rtrbm is better than an analogous trbm at generating motion capture and videos of bouncing balls.</s></p></d>", "label": ["<d><p><s>the recurrent temporal restricted boltzmann machine</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>with the increased availability of data for complex domains, it is desirable to learn bayesian network structures that are sufficiently expressive for generalization while also allowing for tractable inference.</s> <s>while the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overfitting, particularly when data is scarce.</s> <s>in this work we present a novel method for learning bayesian networks of bounded treewidth that employs global structure modifications and that is polynomial in the size of the graph and the treewidth bound.</s> <s>at the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model's treewidth by at most one.</s> <s>we demonstrate the effectiveness of our ``treewidth-friendly'' method on several real-life datasets.</s> <s>importantly, we also show that by using global operators, we are able to achieve better generalization even when learning bayesian networks of unbounded treewidth.</s></p></d>", "label": ["<d><p><s>learning bounded treewidth bayesian networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present an algorithm for solving a broad class of online resource allocation problems.</s> <s>our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule.</s> <s>we assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v,t), where t is the time invested in activity v. under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time t, for some fixed deadline t > 0, and (ii) the average time required to complete each job.</s> <s>we evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating cpu time among solvers entered in the 2007 sat solver competition.</s></p></d>", "label": ["<d><p><s>an online algorithm for maximizing submodular functions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we report a compact realization of short-term depression (std) in a vlsi stochastic synapse.</s> <s>the behavior of the circuit is based on a subtractive single release model of std.</s> <s>experimental results agree well with simulation and exhibit expected std behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems.</s> <s>the dynamic stochastic synapse could potentially be a powerful addition to existing deterministic vlsi spiking neural systems.</s></p></d>", "label": ["<d><p><s>short-term depression in vlsi stochastic synapse</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a principled mechanism for identifying conditional dependencies in time-series data is provided through structure learning of dynamic bayesian networks (dbns).</s> <s>an important assumption of dbn structure learning is that the data are generated by a stationary process??</s> <s>?an assumption that is not true in many important settings.</s> <s>in this paper, we introduce a new class of graphical models called non-stationary dynamic bayesian networks, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time.</s> <s>non-stationary dynamic bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time.</s> <s>we define the non-stationary dbn model, present an mcmc sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data.</s></p></d>", "label": ["<d><p><s>non-stationary dynamic bayesian networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>conditional random sampling (crs) was originally proposed for efficiently computing pairwise ($l_2$, $l_1$) distances, in static, large-scale, and sparse data sets such as text and web data.</s> <s>it was previously presented using a heuristic argument.</s> <s>this study extends crs to handle dynamic or streaming data, which much better reflect the real-world situation than assuming static data.</s> <s>compared with other known sketching algorithms for dimension reductions such as stable random projections, crs exhibits a significant advantage in that it is ``one-sketch-for-all.''</s> <s>in particular, we demonstrate that crs can be applied to efficiently compute the $l_p$ distance and the hilbertian metrics, both are popular in machine learning.</s> <s>although a fully rigorous analysis of crs is difficult, we prove that, with a simple modification, crs is rigorous at least for an important application of computing hamming norms.</s> <s>a generic estimator and an approximate variance formula are provided and tested on various applications, for computing hamming norms, hamming distances, and $\\chi^2$ distances.</s></p></d>", "label": ["<d><p><s>one sketch for all: theory and application of conditional random sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>language comprehension in humans is significantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input.</s> <s>in contrast, most of the leading psycholinguistic models and fielded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events.</s> <s>we present a new limited-memory model of sentence comprehension which involves an adaptation of the particle filter, a sequential monte carlo method, to the problem of incremental parsing.</s> <s>we show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the first rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information.</s></p></d>", "label": ["<d><p><s>modeling the effects of memory on human online sentence processing with particle filters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a multi-label multiple kernel learning (mkl) formulation, in which the data are embedded into a low-dimensional space directed by the instance-label correlations encoded into a hypergraph.</s> <s>we formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the mkl framework.</s> <s>the proposed learning formulation leads to a non-smooth min-max problem, and it can be cast into a semi-infinite linear program (silp).</s> <s>we further propose an approximate formulation with a guaranteed error bound which involves an unconstrained and convex optimization problem.</s> <s>in addition, we show that the objective function of the approximate formulation is continuously differentiable with lipschitz gradient, and hence existing methods can be employed to compute the optimal solution efficiently.</s> <s>we apply the proposed formulation to the automated annotation of drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</s></p></d>", "label": ["<d><p><s>multi-label multiple kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in analogy to the pca setting, the sparse pca problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deflation.</s> <s>while the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the pca context.</s> <s>in this work, we demonstrate that the standard pca deflation procedure is seldom appropriate for the sparse pca setting.</s> <s>to rectify the situation, we first develop several heuristic deflation alternatives with more desirable properties.</s> <s>we then reformulate the sparse pca optimization problem to explicitly reflect the maximum additional variance objective on each round.</s> <s>the result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.</s></p></d>", "label": ["<d><p><s>deflation methods for sparse pca</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>contours have been established in the biological and computer vision literatures as a compact yet descriptive representation of object shape.</s> <s>while individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure).</s> <s>we present a method for further grouping of contours in an image using their relationship to the contours of a second, related image.</s> <s>stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group.</s> <s>to find matches for contours, we rely only on shape, which applies directly to all three modalities without modification, in constrant to the specialized approaches developed for each independently.</s> <s>visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them.</s> <s>for each transformation, groups of contours with matching shape across the two images are identified to provide a context for evaluating matches of individual contour points across the images.</s> <s>the resulting contexts of contours are used to perform a final grouping on contours in the original image while simultaneously finding matches in the related image, again by shape matching.</s> <s>we demonstrate grouping results on image pairs consisting of stereo, motion, and similar images.</s> <s>our method also produces qualitatively better results against a baseline method that does not use the inferred contexts.</s></p></d>", "label": ["<d><p><s>grouping contours via a related image</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks.</s> <s>analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold.</s> <s>in this paper, we describe a class of latent variable models of such data called mixed membership stochastic blockmodels.</s> <s>this model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation.</s> <s>we develop a general variational inference algorithm for fast approximate posterior inference.</s> <s>we explore applications to social networks and protein interaction networks.</s></p></d>", "label": ["<d><p><s>mixed membership stochastic blockmodels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>young children demonstrate the ability to make inferences about the preferences of other agents based on their choices.</s> <s>however, there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge.</s> <s>we use a rational model of preference learning, drawing on ideas from economics and computer science, to explain the behavior of children in several recent experiments.</s> <s>specifically, we show how a simple econometric model can be extended to capture two- to four-year-olds???</s> <s>use of statistical information in inferring preferences, and their generalization of these preferences.</s></p></d>", "label": ["<d><p><s>a rational model of preference learning and choice prediction by children</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formulate and study a new variant of the $k$-armed bandit problem, motivated by e-commerce applications.</s> <s>in our model, arms have (stochastic) lifetime after which they expire.</s> <s>in this setting an algorithm needs to continuously explore new arms, in contrast to the standard $k$-armed bandit model in which arms are available indefinitely and exploration is reduced once an optimal arm is identified with near-certainty.</s> <s>the main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budget.</s> <s>an algorithm needs to choose among a large collection of ads, more than can be fully explored within the ads' lifetime.</s> <s>we present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case.</s> <s>empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms significantly outperform the standard multi-armed bandit approaches applied to these settings.</s></p></d>", "label": ["<d><p><s>mortal multi-armed bandits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>unexpected stimuli are a challenge to any machine learning algorithm.</s> <s>here we identify distinct types of unexpected events, focusing on 'incongruent events' - when 'general level' and 'specific level' classifiers give conflicting predictions.</s> <s>we define a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies.</s> <s>for each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy .</s> <s>an incongruent event is an event where the probability computed based on some more specific level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conflicting predictions.</s> <s>we derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership.</s> <s>respectively, we show promising results with real data on two specific problems: out of vocabulary words in speech recognition, and the identification of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</s></p></d>", "label": ["<d><p><s>beyond novelty detection: incongruent events, when general and specific classifiers disagree</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>``how is information decoded in the brain?''</s> <s>is one of the most difficult and important questions in neuroscience.</s> <s>whether neural correlation is important or not in decoding neural activities is of special interest.</s> <s>we have developed a general framework for investigating how far the decoding process in the brain can be simplified.</s> <s>first, we hierarchically construct simplified probabilistic models of neural responses that ignore more than $k$th-order correlations by using a maximum entropy principle.</s> <s>then, we compute how much information is lost when information is decoded using the simplified models, i.e., ``mismatched decoders''.</s> <s>we introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders.</s> <s>we applied our proposed framework to spike data for vertebrate retina.</s> <s>we used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies.</s> <s>we found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding.</s> <s>we also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information.</s></p></d>", "label": ["<d><p><s>a general framework for investigating how far the decoding process in the brain can be simplified</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural probabilistic language models (nplms) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models.</s> <s>the main drawback of nplms is their extremely long training and testing times.</s> <s>morin and bengio have proposed a hierarchical language model built around a binary tree of words that was two orders of magnitude faster than the non-hierarchical language model it was based on.</s> <s>however, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge.</s> <s>we introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data.</s> <s>we then show that the resulting models can outperform non-hierarchical models and achieve state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>a scalable hierarchical distributed language model</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a simple new monte carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as deep belief networks.</s> <s>while the method is based on markov chains, estimates based on short runs are formally unbiased.</s> <s>in expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound.</s> <s>the method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest monte carlo methods.</s> <s>we give examples of the new method substantially improving simple variational bounds at modest extra cost.</s></p></d>", "label": ["<d><p><s>evaluating probabilities under high-dimensional latent variable models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss.</s> <s>this method has several essential properties.</s> <s>first, the degree of sparsity is continuous---a parameter controls the rate of sparsification from no sparsification to total sparsification.</s> <s>second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular $l_1$-regularization method in the batch setting.</s> <s>we prove that small rates of sparsification result in only small additional regret with respect to typical online-learning guarantees.</s> <s>finally, the approach works well empirically.</s> <s>we apply it to several datasets and find that for datasets with large numbers of features, substantial sparsity is discoverable.</s></p></d>", "label": ["<d><p><s>sparse online learning via truncated gradient</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider linear prediction models where the target function is a sparse linear combination of a set of basis functions.</s> <s>we are interested in the problem of identifying those basis functions with non-zero coefficients and reconstructing the target function from noisy observations.</s> <s>two heuristics that are widely used in practice are forward and backward greedy algorithms.</s> <s>first, we show that neither idea is adequate.</s> <s>second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneficial.</s> <s>we prove strong theoretical results showing that this procedure is effective in learning sparse representations.</s> <s>experimental results support our theory.</s></p></d>", "label": ["<d><p><s>adaptive forward-backward greedy algorithm for sparse learning with linear models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>adaptation of visually guided reaching movements in novel visuomotor environments (e.g.</s> <s>wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues.</s> <s>previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation.</s> <s>we instead propose a unified model in which sensory and motor adaptation are jointly driven by optimal bayesian estimation of the sensory and motor contributions to perceived errors.</s> <s>our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereffects.</s> <s>this unified model also makes the surprising prediction that force field adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations.</s> <s>we confirm this prediction with an experiment.</s></p></d>", "label": ["<d><p><s>unifying the sensory and motor components of sensorimotor adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved.</s> <s>in the 1930s, sir frederic bartlett explored the influence of memory biases in ??</s> <s>?serial reproduction???</s> <s>of information, in which one person??</s> <s>?s reconstruction of a stimulus from memory becomes the stimulus seen by the next person.</s> <s>these experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reflected the biases inherent in memory.</s> <s>we formally analyze serial reproduction using a bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission.</s> <s>we then test the predictions of this account in two experiments using simple one-dimensional stimuli.</s> <s>our results provide theoretical and empirical justification for the idea that serial reproduction reflects memory biases.</s></p></d>", "label": ["<d><p><s>how memory biases affect information transmission: a rational analysis of serial reproduction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>classical game theoretic approaches that make strong rationality assumptions have difficulty modeling observed behaviour in economic games of human subjects.</s> <s>we investigate the role of finite levels of iterated reasoning and non-selfish utility functions in a partially observable markov decision process model that incorporates game theoretic notions of interactivity.</s> <s>our generative model captures a broad class of characteristic behaviours in a multi-round investment game.</s> <s>we invert the generative process for a recognition model that is used to classify 200 subjects playing an investor-trustee game against randomly matched opponents.</s></p></d>", "label": ["<d><p><s>bayesian model of behaviour in economic games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>continuously-adaptive discretization for message-passing (cad-mp) is a new message-passing algorithm employing adaptive discretization.</s> <s>most previous message-passing algorithms approximated arbitrary continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a fixed, uniform discretization.</s> <s>in contrast, cad-mp uses a discretization that is (i) non-uniform, and (ii) adaptive.</s> <s>the non-uniformity allows cad-mp to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly.</s> <s>we give a principled method for altering the non-uniform discretization according to information-based measures.</s> <s>cad-mp is shown in experiments on simulated data to estimate marginal beliefs much more precisely than competing approaches for the same computational expense.</s></p></d>", "label": ["<d><p><s>continuously-adaptive discretization for message-passing algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics.</s> <s>this shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive.</s> <s>it is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function.</s> <s>this has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems.</s> <s>these points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers.</s> <s>a new boosting algorithm, savageboost, is derived for the minimization of this loss.</s> <s>experimental results show that it is indeed less sensitive to outliers than conventional methods, such as ada, real, or logitboost, and converges in fewer iterations.</s></p></d>", "label": ["<d><p><s>on the design of loss functions for classification: theory, robustness to outliers, and savageboost</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters.</s> <s>the algorithms work by maximizing the dependence between the taxonomy and the original data.</s> <s>the resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-of-the-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach).</s> <s>we demonstrate our algorithm on image and text data.</s></p></d>", "label": ["<d><p><s>learning taxonomies by dependence maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the coding of information by neural populations depends critically on the statistical dependencies between neuronal responses.</s> <s>however, there is no simple model that combines the observations that (1) marginal distributions over single-neuron spike counts are often approximately poisson; and (2) joint distributions over the responses of multiple neurons are often strongly dependent.</s> <s>here, we show that both marginal and joint properties of neural responses can be captured using poisson copula models.</s> <s>copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them.</s> <s>different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefficients.</s> <s>we explore a variety of poisson copula models for joint neural response distributions, and derive an efficient maximum likelihood procedure for estimating them.</s> <s>we apply these models to neuronal data collected in and macaque motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons.</s></p></d>", "label": ["<d><p><s>characterizing neural dependencies with copula models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of binary classification where the classifier may abstain instead of classifying each observation.</s> <s>the bayes decision rule for this setup, known as chow's rule, is defined by two thresholds on posterior probabilities.</s> <s>from simple desiderata, namely the consistency and the sparsity of the classifier, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule.</s> <s>we show that, for suitable kernel machines, our approach is universally consistent.</s> <s>we cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard svm optimization problem and propose an active set method to solve it efficiently.</s> <s>we finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions.</s></p></d>", "label": ["<d><p><s>support vector machines with a reject option</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we developed localized sliced inverse regression for supervised dimension reduction.</s> <s>it has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classification problems.</s> <s>a semisupervised version is proposed for the use of unlabeled data.</s> <s>the utility is illustrated on simulated as well as real data sets.</s></p></d>", "label": ["<d><p><s>localized sliced inverse regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider robust least-squares regression with feature-wise disturbance.</s> <s>we show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to $\\ell_1$ regularized regression (lasso).</s> <s>this provides an interpretation of lasso from a robust optimization perspective.</s> <s>we generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems.</s> <s>therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations.</s> <s>the advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of lasso, as well as to prove a general consistency result for robust regression problems, including lasso, from a unified robustness perspective.</s></p></d>", "label": ["<d><p><s>robust regression and lasso</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>applications of multi-class classification, such as document categorization, often appear in cost-sensitive settings.</s> <s>recent work has significantly improved the state of the art by moving beyond ``flat'' classification through incorporation of class hierarchies [cai and hoffman 04].</s> <s>we present a novel algorithm that goes beyond hierarchical classification and estimates the latent semantic space that underlies the class hierarchy.</s> <s>in this space, each class is represented by a prototype and classification is done with the simple nearest neighbor rule.</s> <s>the optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other.</s> <s>we show that our optimization is convex and can be solved efficiently for large data sets.</s> <s>experiments on the ohsumed medical journal data base yield state-of-the-art results on topic categorization.</s></p></d>", "label": ["<d><p><s>large margin taxonomy embedding for document categorization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples.</s> <s>we propose to allow the category-learner to strategically choose what annotations it receives---based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation.</s> <s>we construct a multiple-instance discriminative classifier based on the initial training data.</s> <s>then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next.</s> <s>after each request, the current classifier is incrementally updated.</s> <s>unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent flag on others).</s> <s>as a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort.</s></p></d>", "label": ["<d><p><s>multi-level active prediction of useful image annotations for recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic topic models (and their extensions) have become popular as models of latent structures in collections of text documents or images.</s> <s>these models are usually treated as generative models and trained using maximum likelihood estimation, an approach which may be suboptimal in the context of an overall classification problem.</s> <s>in this paper, we describe disclda, a discriminative learning framework for such models as latent dirichlet allocation (lda) in the setting of dimensionality reduction with supervised side information.</s> <s>in disclda, a class-dependent linear transformation is introduced on the topic mixture proportions.</s> <s>this parameter is estimated by maximizing the conditional likelihood using monte carlo em.</s> <s>by using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classification.</s> <s>we compare the predictive power of the latent structure of disclda with unsupervised lda on the 20 newsgroup ocument classification task.</s></p></d>", "label": ["<d><p><s>disclda: discriminative learning for dimensionality reduction and classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the profit-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market.</s> <s>the sequential decision problem is hard to solve because the state space is a function.</s> <s>we demonstrate that the belief state is well approximated by a gaussian distribution.</s> <s>we prove a key monotonicity property of the gaussian state update which makes the problem tractable, yielding the first optimal sequential market-making algorithm in an established model.</s> <s>the algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher profits later.</s></p></d>", "label": ["<d><p><s>adapting to a market shock: optimal sequential market-making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance.</s> <s>these representations are typically high dimensional and assume diverse forms.</s> <s>thus finding a way to transform them into a unified space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering.</s> <s>we describe an approach that incorporates multiple kernel learning with dimensionality reduction (mkl-dr).</s> <s>while the proposed framework is flexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding.</s> <s>it follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</s></p></d>", "label": ["<d><p><s>dimensionality reduction for data in multiple feature representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy.</s> <s>we present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level.</s> <s>it also maintains sep- arate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly.</s> <s>we use the model to investigate the sequence evolu- tion of hiv populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy.</s> <s>the detection of biologically rele- vant and plausible signals in both therapy studies demonstrates the effectiveness of the method.</s></p></d>", "label": ["<d><p><s>a spatially varying two-sample recombinant coalescent, with applications to hiv escape response</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset.</s> <s>there are several asymptotically correct, informative algorithms that search for causal information given a single dataset, even with missing values and hidden variables.</s> <s>there are, however, no such reliable procedures for distributed data with overlapping variables, and only a single heuristic procedure (structural em).</s> <s>this paper describes an asymptotically correct procedure, ion, that provides all the information about structure obtainable from the marginal independence relations.</s> <s>using simulated and real data, the accuracy of ion is compared with that of structural em, and with inference on complete, unified data.</s></p></d>", "label": ["<d><p><s>integrating locally learned causal structures with overlapping variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a generalization of stochastic bandit problems where the set of arms, x, is allowed to be a generic topological space.</s> <s>we constraint the mean-payoff function with a dissimilarity function over x in a way that is more general than lipschitz.</s> <s>we construct an arm selection policy whose regret improves upon previous result for a large class of problems.</s> <s>in particular, our results imply that if x is the unit hypercube in a euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally h?lder with a known exponent, then the expected regret is bounded up to a logarithmic factor by $n$, i.e., the rate of the growth of the regret is independent of the dimension of the space.</s> <s>moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider.</s></p></d>", "label": ["<d><p><s>online optimization in x-armed bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle appearance\" and \"disappearance\" of neurons.</s> <s>our approach is to augment a known time-varying dirichlet process that ties together a sequence of infinite gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations.</s> <s>we demonstrate this model by showing results from sorting two publicly available neural data recordings for which the a partial ground truth labeling is known.\"</s></p></d>", "label": ["<d><p><s>dependent dirichlet process spike sorting</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose.</s> <s>first, these tools can be used to allow patients to interact with their environment through a brain-machine interface (bmi).</s> <s>second, analysis of the characteristics of such methods can reveal the significance of various features of neural activity, stimuli and responses to the encoding-decoding task.</s> <s>in this study we adapted, implemented and tested a machine learning method, called kernel auto-regressive moving average (karma), for the task of inferring movements from neural activity in primary motor cortex.</s> <s>our version of this algorithm is used in an on-line learning setting and is updated when feedback from the last inferred sequence become available.</s> <s>we first used it to track real hand movements executed by a monkey in a standard 3d motor control task.</s> <s>we then applied it in a closed-loop bmi setting to infer intended movement, while arms were restrained, allowing a monkey to perform the task using the bmi alone.</s> <s>karma is a recurrent method that learns a nonlinear model of output dynamics.</s> <s>it uses similarity functions (termed kernels) to compare between inputs.</s> <s>these kernels can be structured to incorporate domain knowledge into the method.</s> <s>we compare karma to various state-of-the-art methods by evaluating tracking performance and present results from the karma based bmi experiments.</s></p></d>", "label": ["<d><p><s>kernel-arma for hand tracking and brain-machine interfacing during 3d motor control</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>object matching is a fundamental operation in data analysis.</s> <s>it typically requires the definition of a similarity measure between the classes of objects to be matched.</s> <s>instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes.</s> <s>this is achieved by maximizing the dependency between matched pairs of observations by means of the hilbert schmidt independence criterion.</s> <s>this problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for finding a locally optimal solution.</s></p></d>", "label": ["<d><p><s>kernelized sorting</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the synchronous brain activity measured via meg (or eeg) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex.</s> <s>estimating the number, location, and orientation of these sources remains a challenging task, one that is significantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts.</s> <s>this paper derives an empirical bayesian method for addressing each of these issues in a principled fashion.</s> <s>the resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations.</s> <s>robust interference suppression is also easily incorporated.</s> <s>in a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing bayesian localization methods or common signal processing techniques such as beamforming and sloreta.</s> <s>empirical results on both simulated and real data sets verify the efficacy of this approach.</s></p></d>", "label": ["<d><p><s>estimating the location and orientation of complex, correlated neural activity using meg</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum and hippocampus ca3.</s> <s>here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically.</s> <s>we show by numerical simulations of large asymmetric inhibitory networks with fixed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and firing state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation.</s> <s>we show that cells and assemblies switch between firing and quiescent states with time durations consistent with a power-law.</s> <s>our results are in good qualitative agreement with the experimental studies.</s> <s>the deterministic dynamical behaviour is related to winner-less competition shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points.</s></p></d>", "label": ["<d><p><s>cell assemblies in large sparse inhibitory networks of biologically realistic spiking neurons</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work, we consider the problem of learning a positive semidefinite matrix.</s> <s>the critical issue is how to preserve positive semidefiniteness during the course of learning.</s> <s>our algorithm is mainly inspired by lpboost [1] and the general greedy convex optimization framework of zhang [2].</s> <s>we demonstrate the essence of the algorithm, termed psdboost (positive semidefinite boosting), by focusing on a few different applications in machine learning.</s> <s>the proposed psdboost algorithm extends traditional boosting algorithms in that its parameter is a positive semidefinite matrix with trace being one instead of a classifier.</s> <s>psdboost is based on the observation that any trace-one positive semidefinitematrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of psdboost.</s> <s>numerical experiments are presented.</s></p></d>", "label": ["<d><p><s>psdboost: matrix-generation linear programming for positive semidefinite matrices learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyse matching pursuit for kernel principal components analysis by proving that the sparse subspace it produces is a sample compression scheme.</s> <s>we show that this bound is tighter than the kpca bound of shawe-taylor et al swck-05 and highly predictive of the size of the subspace needed to capture most of the variance in the data.</s> <s>we analyse a second matching pursuit algorithm called kernel matching pursuit (kmp) which does not correspond to a sample compression scheme.</s> <s>however, we give a novel bound that views the choice of subspace of the kmp algorithm as a compression scheme and hence provide a vc bound to upper bound its future loss.</s> <s>finally we describe how the same bound can be applied to other matching pursuit related algorithms.</s></p></d>", "label": ["<d><p><s>theory of matching pursuit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop as series of corrections to expectation propagation (ep), which is one of the most popular methods for approximate probabilistic inference.</s> <s>these corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when ep yields unrealiable results.</s></p></d>", "label": ["<d><p><s>improving on expectation propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning.</s> <s>despite its far-reaching application, there is almost no work on applying stochastic approximation to learning problems with constraints.</s> <s>the reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems.</s> <s>we propose that interior-point methods are a natural solution.</s> <s>we establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via l1 regularization.</s></p></d>", "label": ["<d><p><s>an interior-point stochastic approximation method and an l1-regularized delta rule</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we use graphical models and structure learning to explore how people learn policies in sequential decision making tasks.</s> <s>studies of sequential decision-making in humans frequently find suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment.</s> <s>we argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment.</s> <s>we formulate the structure learning problem using mixtures of reward models, and solve the optimal action selection problem using bayesian reinforcement learning.</s> <s>we show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies.</s> <s>our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure.</s></p></d>", "label": ["<d><p><s>structure learning in human sequential decision-making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints.</s> <s>our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model.</s> <s>the algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator.</s> <s>the framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression.</s> <s>the methods are illustrated with experiments on synthetic data and gene microarray data.</s></p></d>", "label": ["<d><p><s>nonparametric regression and classification with joint sparsity constraints</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we develop new techniques for time series classification based on hierarchical bayesian generative models (called mixed-effect models) and the fisher kernel derived from them.</s> <s>a key advantage of the new formulation is that one can compute the fisher information matrix despite varying sequence lengths and sampling times.</s> <s>we therefore can avoid the ad hoc replacement of fisher information matrix with the identity matrix commonly used in literature, which destroys the geometrical grounding of the kernel construction.</s> <s>in contrast, our construction retains the proper geometric structure resulting in a kernel that is properly invariant under change of coordinates in the model parameter space.</s> <s>experiments on detecting cognitive decline show that classifiers based on the proposed kernel out-perform those based on generative models and other feature extraction routines.</s></p></d>", "label": ["<d><p><s>hierarchical fisher kernels for longitudinal data</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random field characterized by pairwise potentials that form a truncated convex model.</s> <s>for this problem, we propose an improved st-mincut based move making algorithm.</s> <s>unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (lp) relaxation.</s> <s>compared to previous approaches based on the lp relaxation, e.g.</s> <s>interior-point algorithms or tree-reweighted message passing (trw), our method is faster as it uses only the efficient st-mincut algorithm in its design.</s> <s>furthermore, it directly provides us with a primal solution (unlike trw and other related methods which attempt to solve the dual of the lp).</s> <s>we demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems.</s> <s>our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as $\\alpha$-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations.</s> <s>we believe that further explorations in this direction would help design efficient algorithms for more complex relaxations.</s></p></d>", "label": ["<d><p><s>improved moves for truncated convex models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new reinforcement-learning model for the role of the hippocampus in classical conditioning, focusing on the differences between trace and delay conditioning.</s> <s>in the model, all stimuli are represented both as unindividuated wholes and as a series of temporal elements with varying delays.</s> <s>these two stimulus representations interact, producing different patterns of learning in trace and delay conditioning.</s> <s>the model proposes that hippocampal lesions eliminate long-latency temporal elements, but preserve short-latency temporal elements.</s> <s>for trace conditioning, with no contiguity between stimulus and reward, these long-latency temporal elements are vital to learning adaptively timed responses.</s> <s>for delay conditioning, in contrast, the continued presence of the stimulus supports conditioned responding, and the short-latency elements suppress responding early in the stimulus.</s> <s>in accord with the empirical data, simulated hippocampal damage impairs trace conditioning, but not delay conditioning, at medium-length intervals.</s> <s>with longer intervals, learning is impaired in both procedures, and, with shorter intervals, in neither.</s> <s>in addition, the model makes novel predictions about the response topography with extended stimuli or post-training lesions.</s> <s>these results demonstrate how temporal contiguity, as in delay conditioning, changes the timing problem faced by animals, rendering it both easier and less susceptible to disruption by hippocampal lesions.</s></p></d>", "label": ["<d><p><s>a computational model of hippocampal function in trace conditioning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difficult to obtain.</s> <s>we explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction.</s> <s>we propose three alternative formulations for imposing a spatial smoothness prior on the image labels.</s> <s>tests of the new models and some baseline approaches on two real image datasets demonstrate the effectiveness of incorporating the latent structure.</s></p></d>", "label": ["<d><p><s>learning hybrid models for image annotation with partially labeled data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem.</s> <s>we draw on recent work in nonparametric bayesian statistics to define a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features.</s> <s>by comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features.</s></p></d>", "label": ["<d><p><s>analyzing human feature learning as nonparametric bayesian inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a novel stochastic process that can be used to construct a multidimensional generalization of the stick-breaking process and which is related to the classic stick breaking process described by sethuraman1994 in one dimension.</s> <s>we describe how the process can be applied to relational data modeling using the de finetti representation for infinitely and partially exchangeable arrays.</s></p></d>", "label": ["<d><p><s>the mondrian process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent work long and servedio ls05short presented a ``martingale boosting'' algorithm that works by constructing a branching program over weak classifiers and has a simple analysis based on elementary properties of random walks.</s> <s>ls05short showed that this martingale booster can tolerate random classification noise when it is run with a noise-tolerant weak learner; however, a drawback of the algorithm is that it is not adaptive, i.e.</s> <s>it cannot effectively take advantage of variation in the quality of the weak classifiers it receives.</s> <s>in this paper we present a variant of the original martingale boosting algorithm and prove that it is adaptive.</s> <s>this adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage.</s> <s>the new algorithm inherits the desirable properties of the original ls05short algorithm, such as random classification noise tolerance, and has several other advantages besides adaptiveness: it requires polynomially fewer calls to the weak learner than the original algorithm, and it can be used with confidence-rated weak hypotheses that output real values rather than boolean predictions.</s></p></d>", "label": ["<d><p><s>adaptive martingale boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we apply robust bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown.</s> <s>for the generative case, we derive an entropy-based weighting that maximizes expected log likelihood under the worst-case true class proportions.</s> <s>for the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss.</s> <s>we apply our theory to the modeling of species geographic distributions from presence data, an extreme case of label bias since there is no absence data.</s> <s>on a benchmark dataset, we find that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data.</s></p></d>", "label": ["<d><p><s>generative and discriminative learning with unknown labeling bias</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic relational models provide a rich family of choices for learning and predicting dyadic data between two sets of entities.</s> <s>it generalizes matrix factorization to a supervised learning problem that utilizes attributes of objects in a hierarchical bayesian framework.</s> <s>previously empirical bayesian inference was applied, which is however not scalable when the size of either object sets becomes tens of thousands.</s> <s>in this paper, we introduce a markov chain monte carlo (mcmc) algorithm to scale the model to very large-scale dyadic data.</s> <s>both superior scalability and predictive accuracy are demonstrated on a collaborative filtering problem, which involves tens of thousands users and a half million items.</s></p></d>", "label": ["<d><p><s>stochastic relational models for large-scale dyadic data using mcmc</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we introduce the first temporal-difference learning algorithm that is stable with linear function approximation and off-policy training, for any finite markov decision process, target policy, and exciting behavior policy, and whose complexity scales linearly in the number of parameters.</s> <s>we consider an i.i.d.\\ policy-evaluation setting in which the data need not come from on-policy experience.</s> <s>the gradient temporal-difference (gtd) algorithm estimates the expected update vector of the td(0) algorithm and performs stochastic gradient descent on its l_2 norm.</s> <s>our analysis proves that its expected update is in the direction of the gradient, assuring convergence under the usual stochastic approximation conditions to the same least-squares solution as found by the lstd, but without its quadratic computational complexity.</s> <s>gtd is online and incremental, and does not involve multiplying by products of likelihood ratios as in importance-sampling methods.</s></p></d>", "label": ["<d><p><s>a convergent </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.</s> <s>these kernel functions can be used in shallow architectures, such as support vector machines (svms), or in deep kernel-based architectures that we call multilayer kernel machines (mkms).</s> <s>we evaluate svms and mkms with these kernel functions on problems designed to illustrate the advantages of deep architectures.</s> <s>on several problems, we obtain better results than previous, leading benchmarks from both svms with gaussian kernels as well as deep belief nets.</s></p></d>", "label": ["<d><p><s>kernel methods for deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while many advances have already been made on the topic of hierarchical classi-  ?cation learning, we take a step back and examine how a hierarchical classi?ca-  tion problem should be formally de?ned.</s> <s>we pay particular attention to the fact  that many arbitrary decisions go into the design of the the label taxonomy that  is provided with the training data, and that this taxonomy is often unbalanced.</s> <s>we correct this problem by using the data distribution to calibrate the hierarchical  classi?cation loss function.</s> <s>this distribution-based correction must be done with  care, to avoid introducing unmanagable statstical dependencies into the learning  problem.</s> <s>this leads us off the beaten path of binomial-type estimation and into  the uncharted waters of geometric-type estimation.</s> <s>we present a new calibrated  de?nition of statistical risk for hierarchical classi?cation, an unbiased geometric  estimator for this risk, and a new algorithmic reduction from hierarchical classi?-  cation to cost-sensitive classi?cation.</s></p></d>", "label": ["<d><p><s>distribution-calibrated hierarchical classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dependent dirichlet processes (dps) are dependent sets of random measures, each being marginally dirichlet process distributed.</s> <s>they are used in bayesian nonparametric models when the usual exchangebility assumption does not hold.</s> <s>we propose a simple and general framework to construct dependent dps by marginalizing and normalizing a single gamma process over an extended space.</s> <s>the result is a set of dps, each located at a point in a space such that  neighboring dps are more dependent.</s> <s>we describe markov chain monte carlo inference, involving the typical gibbs sampling and three different metropolis-hastings proposals to speed up convergence.</s> <s>we report an empirical study of convergence speeds on a synthetic dataset and demonstrate an application of the model to topic modeling through time.</s></p></d>", "label": ["<d><p><s>spatial normalized gamma processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the relative merits of different population coding schemes have mostly been analyzed in the framework of stimulus reconstruction using fisher information.</s> <s>here, we consider the case of stimulus discrimination in a two alternative forced choice paradigm and compute neurometric functions in terms of the minimal discrimination error and the jensen-shannon information to study neural population codes.</s> <s>we first explore the relationship between minimum discrimination error, jensen-shannon information and fisher information and show that the discrimination framework is more informative about the coding accuracy than fisher information as it defines an error for any pair of possible stimuli.</s> <s>in particular, it includes fisher information as a special case.</s> <s>second, we use the framework to study population codes of angular variables.</s> <s>specifically, we assess the impact of different noise correlations structures on coding accuracy in long versus short decoding time windows.</s> <s>that is, for long time window we use the common gaussian noise approximation.</s> <s>to address the case of short time windows we analyze the ising model with identical noise correlation structure.</s> <s>in this way, we provide a new rigorous framework for assessing the functional consequences of noise correlation structures for the representational accuracy of neural population codes that is in particular applicable to short-time population coding.</s></p></d>", "label": ["<d><p><s>neurometric function analysis of population codes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose new methodologies to detect anomalies in discrete-time processes taking values in a set.</s> <s>the method is based on the inference of functionals whose evaluations on successive states visited by the process have low autocorrelations.</s> <s>deviations from this behavior are used to flag anomalies.</s> <s>the candidate functionals are estimated in a subset of a reproducing kernel hilbert space associated with the set where the process takes values.</s> <s>we provide experimental results which show that these techniques compare favorably with other algorithms.</s></p></d>", "label": ["<d><p><s>white functionals for anomaly detection in dynamical systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>with the advent of the internet it is now possible to collect   hundreds of millions of images.</s> <s>these images come with varying   degrees of label information.</s> <s>``clean labels can be manually obtained on a   small fraction, ``noisy labels may be extracted automatically from  surrounding text, while for most images there are no labels at all.</s> <s>semi-supervised learning is a principled framework for combining these different label sources.</s> <s>however, it scales polynomially with the  number of images, making it impractical for use on gigantic  collections with hundreds of millions of images and thousands of classes.</s> <s>in this paper we show how to utilize recent results in machine   learning to obtain highly efficient approximations for   semi-supervised learning that are linear in the number of images.?</s> <s>specifically, we use the convergence of the eigenvectors of the normalized graph laplacian to eigenfunctions of weighted laplace-beltrami operators.</s> <s>we combine this with a label sharing framework obtained from wordnet to propagate label information to classes lacking manual annotations.</s> <s>our algorithm enables us to apply semi-supervised learning to a  database of 80 million images with 74 thousand classes.</s></p></d>", "label": ["<d><p><s>semi-supervised learning in gigantic image collections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>extensive games are often used to model the interactions of multiple agents within an environment.</s> <s>much recent work has focused on increasing the size of an extensive game that can be feasibly solved.</s> <s>despite these improvements, many interesting games are still too large for such techniques.</s> <s>a common approach for computing strategies in these large games is to first employ an abstraction technique to reduce the original game to an abstract game that is of a manageable size.</s> <s>this abstract game is then solved and the resulting strategy is used in the original game.</s> <s>most top programs in recent aaai computer poker competitions use this approach.</s> <s>the trend in this competition has been that strategies found in larger abstract games tend to beat strategies found in smaller abstract games.</s> <s>these larger abstract games have more expressive strategy spaces and therefore contain better strategies.</s> <s>in this paper we present a new method for computing strategies in large games.</s> <s>this method allows us to compute more expressive strategies without increasing the size of abstract games that we are required to solve.</s> <s>we demonstrate the power of the approach experimentally in both small and large games, while also providing a theoretical justification for the resulting improvement.</s></p></d>", "label": ["<d><p><s>strategy grafting in extensive games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the replica method is a non-rigorous but widely-used technique from statistical physics used in the asymptotic analysis of many large random nonlinear problems.</s> <s>this paper applies the replica method to non-gaussian map estimation.</s> <s>it is shown that with large random linear measurements and gaussian noise, the asymptotic behavior of the map estimate of an n-dimensional vector ``decouples as n scalar map estimators.</s> <s>the result is a counterpart to guo and verdus replica analysis on mmse estimation.</s> <s>the replica map analysis can be readily applied to many estimators used in compressed sensing, including basis pursuit, lasso, linear estimation with thresholding and zero-norm estimation.</s> <s>in the case of lasso estimation, the scalar estimator reduces to a soft-thresholding operator and for zero-norm estimation it reduces to a hard-threshold.</s> <s>among other benefits, the replica method provides a computationally tractable method for exactly computing various performance metrics including mse and sparsity recovery.</s></p></d>", "label": ["<d><p><s>asymptotic analysis of map estimation via the replica method and compressed sensing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study an encoding/decoding mechanism accounting for the relative spike timing of the signals propagating from peripheral nerve fibers to second-order somatosensory neurons in the cuneate nucleus (cn).</s> <s>the cn is modeled as a population of spiking neurons receiving as inputs the spatiotemporal responses of real mechanoreceptors obtained via microneurography recordings in humans.</s> <s>the efficiency of the haptic discrimination process is quantified by a novel definition of entropy that takes into full account the metrical properties of the spike train space.</s> <s>this measure proves to be a suitable decoding scheme for generalizing the classical shannon entropy to spike-based neural codes.</s> <s>it permits an assessment of neurotransmission in the presence of a large output space (i.e.</s> <s>hundreds of spike trains) with 1 ms temporal precision.</s> <s>it is shown that the cn population code performs a complete discrimination of 81 distinct stimuli already within 35 ms of the first afferent spike, whereas a partial discrimination (80% of the maximum information transmission) is possible as rapidly as 15 ms.</s> <s>this study suggests that the cn may not constitute a mere synaptic relay along the somatosensory pathway but, rather, it may convey optimal contextual accounts (in terms of fast and reliable information transfer) of peripheral tactile inputs to downstream structures of the central nervous system.</s></p></d>", "label": ["<d><p><s>optimal context separation of spiking haptic signals by second-order somatosensory neurons</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a sequence of unsupervised, nonparametric bayesian models for clustering  complex linguistic objects.</s> <s>in this approach, we consider a potentially infinite number of features and categorical outcomes.</s> <s>we evaluate these models for the task  of within- and cross-document event coreference on two corpora.</s> <s>all the models we investigated show significant improvements when compared against an existing baseline for this task.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian models for unsupervised event coreference resolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the indian buffet process (ibp) is an exchangeable distribution over binary matrices used in bayesian nonparametric featural models.</s> <s>in this paper we propose a three-parameter generalization of the ibp exhibiting power-law behavior.</s> <s>we achieve this by generalizing the beta process (the de finetti measure of the ibp) to the \\emph{stable-beta process} and deriving the ibp corresponding to it.</s> <s>we find interesting relationships between the stable-beta process and the pitman-yor process (another stochastic process used in bayesian nonparametric models with interesting power-law properties).</s> <s>we show that our power-law ibp is a good model for word occurrences in documents with improved performance over the normal ibp.</s></p></d>", "label": ["<d><p><s>indian buffet processes with power-law behavior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper tackles the problem of selecting among several linear estimators in non-parametric regression; this includes model selection for linear regression, the choice of a regularization parameter in kernel ridge regression or spline smoothing, and the choice of a kernel in multiple kernel learning.</s> <s>we propose a new algorithm which first estimates consistently the variance of the noise, based upon the concept of minimal penalty which was previously introduced in the context of model selection.</s> <s>then, plugging our variance estimate in mallows $c_l$ penalty is proved to lead to an algorithm satisfying an oracle inequality.</s> <s>simulation experiments with kernel ridge regression and multiple kernel learning show that the proposed algorithm often improves significantly existing calibration procedures such as 10-fold cross-validation or generalized cross-validation.</s></p></d>", "label": ["<d><p><s>data-driven calibration of linear estimators with minimal penalties</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>interesting real-world datasets often exhibit nonlinear, noisy, continuous-valued states that are unexplorable, are poorly described by first principles, and are only partially observable.</s> <s>if partial observability can be overcome, these constraints suggest the use of model-based reinforcement learning.</s> <s>we experiment with manifold embeddings as the reconstructed observable state-space of an off-line, model-based reinforcement learning approach to control.</s> <s>we demonstrate the embedding of a system changes as a result of learning and that the best performing embeddings well-represent the dynamics of both the uncontrolled and adaptively controlled system.</s> <s>we apply this approach in simulation to learn a neurostimulation policy that is more efficient in treating epilepsy than conventional policies.</s> <s>we then demonstrate the learned policy completely suppressing seizures in real-world neurostimulation experiments on actual animal brain slices.</s></p></d>", "label": ["<d><p><s>manifold embeddings for model-based reinforcement learning under partial observability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the human brain can be described as containing a number of functional regions.</s> <s>for a given task, these regions, as well as the connections between them, play a key role in information processing in the brain.</s> <s>however, most existing multi-voxel pattern analysis approaches either treat multiple functional regions as one large uniform region or several independent regions, ignoring the connections between regions.</s> <s>in this paper, we propose to model such connections in an hidden conditional random field (hcrf) framework, where the classifier of one region of interest (roi) makes predictions based on not only its voxels but also the classifier predictions from rois that it connects to.</s> <s>furthermore, we propose a structural learning method in the hcrf framework to automatically uncover the connections between rois.</s> <s>experiments on fmri data acquired while human subjects viewing images of natural scenes show that our model can improve the top-level (the classifier combining information from all rois) and roi-level prediction accuracy, as well as uncover some meaningful connections between rois.</s></p></d>", "label": ["<d><p><s>hierarchical mixture of classification experts uncovers interactions between brain regions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new family of distributions, called $l_p${\\em -nested symmetric distributions}, whose densities access the data exclusively through a hierarchical cascade of $l_p$-norms.</s> <s>this class generalizes the family of spherically and $l_p$-spherically symmetric distributions which have recently been successfully used for natural image modeling.</s> <s>similar to those distributions it allows for a nonlinear mechanism to reduce the dependencies between its variables.</s> <s>with suitable choices of the parameters and norms, this family also includes the independent subspace analysis (isa) model, which has been proposed as a means of deriving filters that mimic complex cells found in mammalian primary visual cortex.</s> <s>$l_p$-nested distributions are easy to estimate and allow us to explore the variety of models between isa and the $l_p$-spherically symmetric models.</s> <s>our main findings are that, without a preprocessing step of contrast gain control, the independent subspaces of isa are in fact more dependent than the individual filter coefficients within a subspace and, with contrast gain control, where isa finds more than one subspace, the filter responses were almost independent anyway.</s></p></d>", "label": ["<d><p><s>hierarchical modeling of local image features through </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a bayesian sequential model for category learning.</s> <s>the sequential model updates two category parameters, the mean and the variance, over time.</s> <s>we define conjugate temporal priors to enable closed form solutions to be obtained.</s> <s>this model can be easily extended to supervised and unsupervised learning involving multiple categories.</s> <s>to model the spacing effect, we introduce a generic prior in the temporal updating stage to capture a learning preference, namely, less change for repetition and more change for variation.</s> <s>finally, we show how this approach can be generalized to efficiently performmodel selection to decide whether observations are from one or multiple categories.</s></p></d>", "label": ["<d><p><s>modeling the spacing effect in sequential category learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modern machine learning-based approaches to computer vision require very large databases of labeled images.</s> <s>some contemporary vision systems already require on the order of millions of images for training (e.g., omron face detector).</s> <s>while the collection of these large databases is becoming a bottleneck, new internet-based services that allow labelers from around the world to be easily hired and managed provide a promising solution.</s> <s>however, using these services to label large databases brings with it new theoretical and practical challenges: (1) the labelers may have wide ranging levels of expertise which are unknown a priori, and in some cases may be adversarial; (2) images may vary in their level of difficulty; and (3) multiple labels for the same image must be combined to provide an estimate of the actual label of the image.</s> <s>probabilistic approaches provide a principled way to approach these problems.</s> <s>in this paper we present a probabilistic model and use it to simultaneously infer the label of each image, the expertise of each labeler, and the difficulty of each image.</s> <s>on both simulated and real data, we demonstrate that the model outperforms the commonly used ``majority vote heuristic for inferring image labels, and is robust to both adversarial and noisy labelers.</s></p></d>", "label": ["<d><p><s>whose vote should count more: optimal integration of labels from labelers of unknown expertise</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>imaging techniques such as optical imaging of intrinsic signals, 2-photon calcium imaging and voltage sensitive dye imaging can be used to measure the functional organization of visual cortex across different spatial scales.</s> <s>here, we present bayesian methods based on gaussian processes for extracting topographic maps from functional imaging data.</s> <s>in particular, we focus on the estimation of orientation preference maps (opms) from intrinsic signal imaging data.</s> <s>we model the underlying map as a bivariate gaussian process, with a prior covariance function that reflects known properties of opms, and a noise covariance adjusted to the data.</s> <s>the posterior mean can be interpreted as an optimally smoothed estimate of the map, and can be used for model based interpolations of the map from sparse measurements.</s> <s>by sampling from the posterior distribution, we can get error bars on statistical properties such as preferred orientations,  pinwheel locations or -counts.</s> <s>finally, the use of an explicit probabilistic model facilitates interpretation of parameters and provides the basis for decoding studies.</s> <s>we demonstrate our model both on simulated data and on intrinsic signaling data from ferret visual cortex.</s></p></d>", "label": ["<d><p><s>bayesian estimation of orientation preference maps</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the concave-convex procedure (cccp) is a majorization-minimization algorithm that solves d.c. (difference of convex functions) programs as a sequence of convex programs.</s> <s>in machine learning, cccp is extensively used in many learning algorithms like sparse support vector machines (svms), transductive svms, sparse principal component analysis, etc.</s> <s>though widely used in many applications, the convergence behavior of cccp has not gotten a lot of specific attention.</s> <s>yuille and rangarajan analyzed its convergence in their original paper, however, we believe the analysis is not complete.</s> <s>although the convergence of cccp can be derived from the convergence of the d.c. algorithm (dca), their proof is more specialized and technical than actually required for the specific case of cccp.</s> <s>in this paper, we follow a different reasoning and show how zangwills global convergence theory of iterative algorithms provides a natural framework to prove the convergence of cccp, allowing a more elegant and simple proof.</s> <s>this underlines zangwills theory as a powerful and general framework to deal with the convergence issues of iterative algorithms, after also being used to prove the convergence of algorithms like expectation-maximization, generalized alternating minimization, etc.</s> <s>in this paper, we provide a rigorous analysis of the convergence of cccp by addressing these questions: (i) when does cccp find a local minimum or a stationary point of the d.c. program under consideration?</s> <s>(ii) when does the sequence generated by cccp converge?</s> <s>we also present an open problem on the issue of local convergence of cccp.</s></p></d>", "label": ["<d><p><s>on the convergence of the concave-convex procedure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the use of context is critical for scene understanding in computer vision, where the recognition of an object is driven by both local appearance and the objects relationship to other elements of the scene (context).</s> <s>most current approaches rely on modeling the relationships between object categories as a source of context.</s> <s>in this paper we seek to move beyond categories to provide a richer appearance-based model of context.</s> <s>we present an exemplar-based model of objects and their relationships, the visual memex, that encodes both local appearance and 2d spatial context between object instances.</s> <s>we evaluate our model on torralbas proposed context challenge against a baseline category-based system.</s> <s>our experiments suggest that moving beyond categories for context modeling appears to be quite beneficial, and may be the critical missing ingredient in scene understanding systems.</s></p></d>", "label": ["<d><p><s>beyond categories: the visual memex model for reasoning about object relationships</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper considers a sensitivity analysis in hidden markov models with continuous state and observation spaces.</s> <s>we propose an infinitesimal perturbation analysis (ipa) on the filtering distribution with respect to some parameters of the model.</s> <s>we describe a methodology for using any algorithm that estimates the filtering density, such as sequential monte carlo methods, to design an algorithm that estimates its gradient.</s> <s>the resulting ipa estimator is proven to be asymptotically unbiased, consistent and has computational complexity linear in the number of particles.</s> <s>we consider an application of this analysis to the problem of identifying unknown parameters of the model given a sequence of observations.</s> <s>we derive an ipa estimator for the gradient of the log-likelihood, which may be used in a gradient method for the purpose of likelihood maximization.</s> <s>we illustrate the method with several numerical experiments.</s></p></d>", "label": ["<d><p><s>sensitivity analysis in hmms with application to likelihood maximization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>motivated by recent developments in manifold-valued regression we propose a family of nonparametric kernel-smoothing estimators with metric-space valued output including a robust median type estimator and the classical frechet mean.</s> <s>depending on the choice of the output space and the chosen metric the estimator reduces to partially well-known procedures for multi-class classification, multivariate regression in euclidean space, regression with manifold-valued output and even some cases of structured output learning.</s> <s>in this paper we focus on the case of regression with manifold-valued input and output.</s> <s>we show pointwise and bayes consistency for all estimators in the family for the case of manifold-valued output and illustrate the robustness properties of the estimator with experiments.</s></p></d>", "label": ["<d><p><s>robust nonparametric regression with metric-space valued output</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of zero-shot learning, where the goal is to learn a classifier $f: x \\rightarrow y$ that must predict novel values of $y$ that were omitted from the training set.</s> <s>to achieve this, we define the notion of a semantic output code classifier (soc) which utilizes a knowledge base of semantic properties of $y$ to extrapolate to novel classes.</s> <s>we provide a formalism for this type of classifier and study its theoretical properties in a pac framework, showing conditions under which the classifier can accurately predict novel classes.</s> <s>as a case study, we build a soc classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fmri) of their neural activity, even without training examples for those words.</s></p></d>", "label": ["<d><p><s>zero-shot learning with semantic output codes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>recent advances in neuroimaging techniques provide great potentials for effective diagnosis of alzheimer?s disease (ad), the most common form of dementia.</s> <s>previous studies have shown that ad is closely related to alternation in the functional brain network, i.e., the functional connectivity among different brain regions.</s> <s>in this paper, we consider the problem of learning functional brain connectivity from neuroimaging, which holds great promise for identifying image-based markers used to distinguish normal controls (nc), patients with mild cognitive impairment (mci), and patients with ad.</s> <s>more specifically, we study sparse inverse covariance estimation (sice), also known as exploratory gaussian graphical models, for brain connectivity modeling.</s> <s>in particular, we apply sice to learn and analyze functional brain connectivity patterns from different subject groups, based on a key property of sice, called the ?monotone property?</s> <s>we established in this paper.</s> <s>our experimental results on neuroimaging pet data of 42 ad, 116 mci, and 67 nc subjects reveal several interesting connectivity patterns consistent with literature findings, and also some new patterns that can help the knowledge discovery of ad.</s></p></d>", "label": ["<d><p><s>learning brain connectivity of alzheimer's disease from neuroimaging data</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the behavior of the popular laplacian regularization method for semi-supervised learning at the regime of a fixed number of labeled points but a large number of unlabeled points.</s> <s>we show that in $\\r^d$, $d \\geq 2$, the method is actually not well-posed, and as the number of unlabeled points increases the solution degenerates to a noninformative function.</s> <s>we also contrast the method with the laplacian eigenvector method, and discuss the ``smoothness assumptions associated with this alternate method.</s></p></d>", "label": ["<d><p><s>statistical analysis of semi-supervised learning: the limit of infinite unlabelled data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there has been a clear distinction between induction or training time and diagnosis time active information acquisition.</s> <s>while active learning during induction focuses on acquiring data that promises to provide the best classification model, the goal at diagnosis time focuses completely on next features to observe about the test case at hand in order to make better predictions about the case.</s> <s>we introduce a model and inferential methods that breaks this distinction.</s> <s>the methods can be used to extend case libraries under a budget but, more fundamentally, provide a framework for guiding agents to collect data under scarce resources, focused by diagnostic challenges.</s> <s>this extension to active learning leads to a new class of policies for real-time diagnosis, where recommended information-gathering sequences include actions that simultaneously seek new data for the case at hand and for cases in the training set.</s></p></d>", "label": ["<d><p><s>breaking boundaries between induction time and diagnosis time active information acquisition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discriminatively trained undirected graphical models have had wide empirical success, and there has been increasing interest in toolkits that ease their application to complex relational data.</s> <s>the power in relational models is in their repeated structure and tied parameters; at issue is how to define these structures in a powerful and flexible way.</s> <s>rather than using a declarative language, such as sql or first-order logic, we advocate using an imperative language to express various aspects of model structure, inference, and learning.</s> <s>by combining the traditional, declarative, statistical semantics of factor graphs with imperative definitions of their construction and operation, we allow the user to mix declarative and procedural domain knowledge, and also gain significant efficiencies.</s> <s>we have implemented such imperatively defined factor graphs in a system we call factorie, a software library for an object-oriented, strongly-typed, functional language.</s> <s>in experimental comparisons to markov logic networks on joint segmentation and coreference, we find our approach to be 3-15 times faster while reducing error by 20-25%-achieving a new state of the art.</s></p></d>", "label": ["<d><p><s>factorie: probabilistic programming via imperatively defined factor graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a non-parametric bayesian model is proposed for processing multiple images.</s> <s>the analysis employs image features and, when present, the words associated with accompanying annotations.</s> <s>the model clusters the images into classes, and each image is segmented into a set of objects, also allowing the opportunity to assign a word to each object (localized labeling).</s> <s>each object is assumed to be represented as a heterogeneous mix of components, with this realized via mixture models linking image features to object types.</s> <s>the number of image classes, number of object types, and the characteristics of the object-feature mixture models are inferred non-parametrically.</s> <s>to constitute spatially contiguous objects, a new logistic stick-breaking process is developed.</s> <s>inference is performed efficiently via variational bayesian analysis, with example results presented on two image databases.</s></p></d>", "label": ["<d><p><s>a bayesian model for simultaneous image clustering, annotation and object segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the control of neuroprosthetic devices from the activity of motor cortex neurons benefits from learning effects where the function of these neurons is adapted to the control task.</s> <s>it was recently shown that tuning properties of neurons in monkey motor cortex are adapted selectively in order to compensate for an erroneous interpretation of their activity.</s> <s>in particular, it was shown that the tuning curves of those neurons whose preferred directions had been misinterpreted changed more than those of other neurons.</s> <s>in this article, we show that the experimentally observed self-tuning properties of the system can be explained on the basis of a simple learning rule.</s> <s>this learning rule utilizes neuronal noise for exploration and performs hebbian weight updates that are modulated by a global reward signal.</s> <s>in contrast to most previously proposed reward-modulated hebbian learning rules, this rule does not require extraneous knowledge about what is noise and what is signal.</s> <s>the learning rule is able to optimize the performance of the model system within biologically realistic periods of time and under high noise levels.</s> <s>when the neuronal noise is fitted to experimental data, the model produces learning effects similar to those found in monkey experiments.</s></p></d>", "label": ["<d><p><s>functional network reorganization in motor cortex can be explained by reward-modulated hebbian learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose dirichlet-bernoulli alignment (dba), a generative model for corpora in which each pattern (e.g., a document) contains a set of instances (e.g., paragraphs in the document) and belongs to multiple classes.</s> <s>by casting predefined classes as latent dirichlet variables (i.e., instance level labels), and modeling the multi-label of each pattern as bernoulli variables conditioned on the weighted empirical average of topic assignments, dba automatically aligns the latent topics discovered from data to human-defined classes.</s> <s>dba is useful for both pattern classification and instance disambiguation, which are tested on text classification and named entity disambiguation for web search queries respectively.</s></p></d>", "label": ["<d><p><s>dirichlet-bernoulli alignment: a generative model for multi-class multi-label multi-instance corpora</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the learning of appropriate distance metrics is a critical problem in classification.</s> <s>in this work, we propose a boosting-based technique, termed boostmetric, for learning a mahalanobis distance metric.</s> <s>one of the primary difficulties in learning such a metric is to ensure that the mahalanobis matrix remains positive semidefinite.</s> <s>semidefinite programming is sometimes used to enforce this constraint, but does not scale well.</s> <s>boostmetric is instead based on a key observation that any positive semidefinite matrix can be decomposed into a linear positive combination of trace-one rank-one matrices.</s> <s>boostmetric thus uses rank-one positive semidefinite matrices as weak learners within an efficient and scalable boosting-based learning process.</s> <s>the resulting method is easy to implement, does not require tuning, and can accommodate various types of constraints.</s> <s>experiments on various datasets show that the proposed algorithm compares favorably to those state-of-the-art methods in terms of classification accuracy and running time.</s></p></d>", "label": ["<d><p><s>positive semidefinite metric learning with boosting</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>many categories are better described by providing relational information than listing characteristic features.</s> <s>we present a hierarchical generative model that helps to explain how relational categories are learned and used.</s> <s>our model learns abstract schemata that specify the relational similarities shared by members of a category, and our emphasis on abstraction departs from previous theoretical proposals that focus instead on comparison of concrete instances.</s> <s>our first experiment suggests that our abstraction-based account can address some of the tasks that have previously been used to support comparison-based approaches.</s> <s>our second experiment focuses on one-shot schema learning, a problem that raises challenges for comparison-based approaches but is handled naturally by our abstraction-based account.</s></p></d>", "label": ["<d><p><s>abstraction and relational learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>schizophrenia is a complex psychiatric disorder that has eluded a characterization in terms of local abnormalities of brain activity, and is hypothesized to affect the collective, ``emergent working of the brain.</s> <s>we propose a novel data-driven approach to capture emergent features using functional brain networks [eguiluzet al] extracted from fmri data, and demonstrate its advantage over traditional region-of-interest (roi) and local, task-specific linear activation analyzes.</s> <s>our results suggest that schizophrenia is indeed associated with disruption of global, emergent brain properties related to its functioning as a network, which cannot be explained by alteration of local activation patterns.</s> <s>moreover, further exploitation of interactions by sparse markov random field classifiers shows clear gain over linear methods, such as gaussian naive bayes and svm, allowing to reach 86% accuracy (over 50% baseline - random guess), which is quite remarkable given that it is based on a single fmri experiment using a simple auditory task.</s></p></d>", "label": ["<d><p><s>discriminative network models of schizophrenia</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the recent introduction of indefinite svm by luss and daspremont [15] has effectively demonstrated svm classification with a non-positive semi-definite  kernel (indefinite kernel).</s> <s>this paper studies the properties of the objective function introduced there.</s> <s>in particular, we show that the objective function  is continuously differentiable and its gradient can be explicitly computed.</s> <s>indeed, we further show that its gradient is lipschitz continuous.</s> <s>the main idea behind our analysis is that the objective function is smoothed by the penalty term, in its saddle (min-max) representation, measuring the distance between the indefinite kernel matrix and the proxy positive semi-definite one.</s> <s>our elementary result greatly facilitates the application of gradient-based algorithms.</s> <s>based on our analysis,  we further develop  nesterovs smooth optimization approach [16,17] for indefinite svm which has an optimal convergence rate for smooth problems.</s> <s>experiments on various benchmark datasets validate our analysis and demonstrate the efficiency of our proposed algorithms.</s></p></d>", "label": ["<d><p><s>analysis of svm with indefinite kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the nested chinese restaurant process (ncrp) is a powerful nonparametric bayesian model for learning tree-based hierarchies from data.</s> <s>since its posterior distribution is intractable, current inference methods have all relied on mcmc sampling.</s> <s>in this paper, we develop an alternative inference technique based on variational methods.</s> <s>to employ variational methods, we derive a tree-based stick-breaking construction of the ncrp mixture model, and a novel variational algorithm that efficiently explores a posterior over a large set of combinatorial structures.</s> <s>we demonstrate the use of this approach for text and hand written digits modeling, where we show we can adapt the ncrp to continuous data as well.</s></p></d>", "label": ["<d><p><s>variational inference for the nested chinese restaurant process</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in many domains, humans appear to combine perceptual cues in a near-optimal, probabilistic fashion: two noisy pieces of information tend to be combined linearly with weights proportional to the precision of each cue.</s> <s>here we present a case where structural information plays an important role.</s> <s>the presence of a background cue gives rise to the possibility of occlusion, and places a soft constraint on the location of a target ?</s> <s>in effect propelling it forward.</s> <s>we present an ideal observer model of depth estimation for this situation where structural or ordinal information is important and then fit the model to human data from a stereo-matching task.</s> <s>to test whether subjects are truly using ordinal cues in a probabilistic manner we then vary the uncertainty of the task.</s> <s>we find that the model accurately predicts shifts in subject?s behavior.</s> <s>our results indicate that the nervous system estimates depth ordering in a probabilistic fashion and estimates the structure of the visual scene during depth perception.</s></p></d>", "label": ["<d><p><s>structural inference affects depth perception in the context of potential occlusion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the reconstruction of sparse signals in the multiple measurement vector (mmv) model,in which the signal, represented as a matrix, consists of a set of jointly sparse vectors.</s> <s>mmv is an extension of the single measurement vector (smv) model employed in standard compressive sensing (cs).</s> <s>recent theoretical studies focus on the convex relaxation of the mmv problem based on the $(2,1)$-norm minimization, which is an extension of the well-known $1$-norm minimization employed in smv.</s> <s>however, the resulting convex optimization problem in mmv is significantly much more difficult to solve than the one in smv.</s> <s>existing algorithms reformulate it as a second-order cone programming (socp) or semidefinite programming (sdp), which is computationally expensive to solve for problems of moderate size.</s> <s>in this paper, we propose a new (dual) reformulation of the convex optimization problem in mmv and develop an efficient algorithm based on the prox-method.</s> <s>interestingly, our theoretical analysis reveals the close connection between the proposed reformulation and multiple kernel learning.</s> <s>our simulation studies demonstrate the scalability of the proposed algorithm.</s></p></d>", "label": ["<d><p><s>efficient recovery of jointly sparse vectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there is a growing body of experimental evidence to suggest that the brain is capable of approximating optimal bayesian inference in the face of noisy input stimuli.</s> <s>despite this progress, the neural underpinnings of this computation are still poorly  understood.</s> <s>in this paper we focus on the problem of bayesian filtering of stochastic time series.</s> <s>in particular we introduce a novel neural network, derived from a line attractor architecture, whose dynamics map directly onto those of the kalman filter in the limit where the prediction error is small.</s> <s>when the prediction error is large we show that the network responds robustly to change-points in a way that is qualitatively compatible with the optimal bayesian model.</s> <s>the model suggests ways in which probability distributions are encoded in the brain and makes a number of testable experimental predictions.</s></p></d>", "label": ["<d><p><s>a neural implementation of the kalman filter</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many transductive inference algorithms assume that distributions over training and test estimates should be related, e.g.</s> <s>by providing a large margin of separation on both sets.</s> <s>we use this idea to design a transduction algorithm which can be used without modification for classification, regression, and structured estimation.</s> <s>at its heart we exploit the fact that for a good learner the distributions over the outputs on training and test sets should match.</s> <s>this is a classical two-sample problem which can be solved efficiently in its most general form by using distance measures in hilbert space.</s> <s>it turns out that a number of existing heuristics can be viewed as special cases of our approach.</s></p></d>", "label": ["<d><p><s>distribution matching for transduction</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>fast retrieval methods are increasingly critical for many large-scale analysis tasks, and there have been several recent methods that attempt to learn hash functions for fast and accurate nearest neighbor searches.</s> <s>in this paper, we develop an algorithm for learning hash functions based on explicitly minimizing the reconstruction error between the original distances and the hamming distances of the corresponding binary embeddings.</s> <s>we develop a scalable coordinate-descent algorithm for our proposed hashing objective that is able to efficiently learn hash functions in a variety of settings.</s> <s>unlike existing methods such as semantic hashing and spectral hashing, our method is easily kernelized and does not require restrictive assumptions about the underlying distribution of the data.</s> <s>we present results over several domains to demonstrate that our method outperforms existing state-of-the-art techniques.</s></p></d>", "label": ["<d><p><s>learning to hash with binary reconstructive embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we present an algorithm for separating mixed sounds from  a monophonic recording.</s> <s>our approach makes use of training data which  allows us to learn representations of the types of sounds that compose the  mixture.</s> <s>in contrast to popular methods that attempt to extract com-  pact generalizable models for each sound from training data, we employ  the training data itself as a representation of the sources in the mixture.</s> <s>we show that mixtures of known sounds can be described as sparse com-  binations of the training data itself, and in doing so produce signi?cantly  better separation results as compared to similar systems based on compact  statistical models.</s></p></d>", "label": ["<d><p><s>a sparse non-parametric approach for single channel separation of known sounds</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>nonparametric bayesian models provide a framework for flexible probabilistic modelling of complex datasets.</s> <s>unfortunately, bayesian inference methods often require high-dimensional averages and can be slow to compute, especially with the potentially unbounded representations associated with nonparametric models.</s> <s>we address the challenge of scaling nonparametric bayesian inference to the increasingly large datasets found in real-world applications, focusing on the case of parallelising inference in the indian buffet process (ibp).</s> <s>our approach divides a large data set between multiple processors.</s> <s>the processors use message passing to compute likelihoods in an asynchronous, distributed fashion and to propagate statistics about the global bayesian posterior.</s> <s>this novel mcmc sampler is the first parallel inference scheme for ibp-based models, scaling to datasets orders of magnitude larger than had previously been possible.</s></p></d>", "label": ["<d><p><s>large scale nonparametric bayesian inference: data parallelisation in the indian buffet process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we extend dyna planning architecture for policy evaluation and control in two significant aspects.</s> <s>first, we introduce a multi-step dyna planning that projects the simulated state/feature many steps into the future.</s> <s>our multi-step dyna is based on a multi-step model, which we call the {\\em $\\lambda$-model}.</s> <s>the $\\lambda$-model interpolates between the one-step model and an infinite-step model, and can be learned efficiently online.</s> <s>second, we use for dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run.</s> <s>experimental results show that dyna using the multi-step model evaluates a policy faster than using single-step models; dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further, multi-step dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step dyna algorithms.</s></p></d>", "label": ["<d><p><s>multi-step dyna planning for policy evaluation and control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the quest to make brain computer interfacing (bci) more usable, dry electrodes have emerged that get rid of the initial 30 minutes required for placing an electrode cap.</s> <s>another time consuming step is the required individualized adaptation to the bci user, which involves another 30 minutes calibration for assessing a subjects brain signature.</s> <s>in this paper we aim to also remove this calibration proceedure from bci setup time by means of machine learning.</s> <s>in particular, we harvest a large database of eeg bci motor imagination recordings (83 subjects) for constructing a library of subject-specific spatio-temporal filters and derive a subject independent bci classifier.</s> <s>our offline results indicate that bci-na\\{i}ve users could start real-time bci use with no prior calibration at only a very moderate performance loss.\"</s></p></d>", "label": ["<d><p><s>subject independent eeg-based bci decoding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many models for computations in recurrent networks of neurons assume that the network state moves from some initial state to some fixed point attractor or limit cycle that represents the output of the computation.</s> <s>however experimental data show that in response to a sensory stimulus the network state moves from its initial state through a trajectory of network states and eventually returns to the initial state, without reaching an attractor or limit cycle in between.</s> <s>this type of network response, where salient information about external stimuli is encoded in characteristic trajectories of continuously varying network states, raises the question how a neural system could compute with such code, and arrive for example at a temporally stable classification of the external stimulus.</s> <s>we show that a known unsupervised learning algorithm, slow feature analysis (sfa), could be an important ingredient for extracting stable information from these network trajectories.</s> <s>in fact, if sensory stimuli are more often followed by another stimulus from the same class than by a stimulus from another class, sfa approaches the classification capability of fishers linear discriminant (fld), a powerful algorithm for supervised learning.</s> <s>we apply this principle to simulated cortical microcircuits, and show that it enables readout neurons to learn discrimination of spoken digits and detection of repeating firing patterns within a stream of spike trains with the same firing statistics, without requiring any supervision for learning.</s></p></d>", "label": ["<d><p><s>replacing supervised classification learning by slow feature analysis in spiking neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>canonical correlation analysis (cca) is a useful technique for modeling dependencies between two (or more) sets of variables.</s> <s>building upon the recently suggested probabilistic interpretation of cca, we propose a nonparametric, fully bayesian framework that can automatically select the number of correlation components, and effectively capture the sparsity underlying the projections.</s> <s>in addition, given (partially) labeled data, our algorithm can also be used as a (semi)supervised dimensionality reduction technique, and can be applied to learn useful predictive features in the context of learning a set of related tasks.</s> <s>experimental results demonstrate the efficacy of the proposed approach for both cca as a stand-alone problem, and when applied to multi-label prediction.</s></p></d>", "label": ["<d><p><s>multi-label prediction via sparse infinite cca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, deep learning approaches have gained significant interest as a way of building hierarchical representations from unlabeled data.</s> <s>however, to our knowledge, these deep learning approaches have not been extensively studied for auditory data.</s> <s>in this paper, we apply convolutional deep belief networks to audio data and empirically evaluate them on various audio classification tasks.</s> <s>for the case of speech data, we show that the learned features correspond to phones/phonemes.</s> <s>in addition, our feature representations trained from unlabeled audio data show very good performance for multiple audio classification tasks.</s> <s>we hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks.</s></p></d>", "label": ["<d><p><s>unsupervised feature learning for audio classification using convolutional deep belief networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>learning linear combinations of multiple kernels is an appealing strategy when the right choice of features is unknown.</s> <s>previous approaches to multiple kernel learning (mkl) promote sparse kernel combinations and hence support interpretability.</s> <s>unfortunately, l1-norm mkl is hardly observed to outperform trivial baselines in practical applications.</s> <s>to allow for robust kernel mixtures, we generalize mkl to arbitrary lp-norms.</s> <s>we devise new insights on the connection between  several existing mkl formulations and develop two efficient interleaved optimization strategies for arbitrary p>1.</s> <s>empirically, we demonstrate that the interleaved optimization strategies are much faster compared to the traditionally used wrapper approaches.</s> <s>finally, we apply lp-norm mkl to real-world problems from computational biology, showing that non-sparse mkl achieves accuracies that go beyond the state-of-the-art.</s></p></d>", "label": ["<d><p><s>efficient and accurate lp-norm multiple kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove strong noise-tolerance properties of a potential-based boosting algorithm, similar to madaboost (domingo and watanabe, 2000) and smoothboost (servedio, 2003).</s> <s>our analysis is in the agnostic framework of kearns, schapire and sellie (1994), giving polynomial-time guarantees in presence of arbitrary noise.</s> <s>a remarkable feature of our algorithm is that it can be implemented without reweighting examples, by randomly relabeling them instead.</s> <s>our boosting theorem gives, as easy corollaries, alternative derivations of two recent non-trivial results in computational learning theory: agnostically learning decision trees (gopalan et al, 2008) and agnostically learning halfspaces (kalai et al, 2005).</s> <s>experiments suggest that the algorithm performs similarly to madaboost.</s></p></d>", "label": ["<d><p><s>potential-based agnostic boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new approach to the problem of robust estimation in multiview geometry.</s> <s>inspired by recent  advances in the sparse recovery problem of statistics, our estimator is defined as a bayesian maximum a posteriori  with multivariate laplace prior on the vector describing the outliers.</s> <s>this leads to an estimator in which  the fidelity to the data is measured by the $l_\\infty$-norm while the regularization is done by the $l_1$-norm.</s> <s>the proposed procedure is fairly fast since the outlier removal is done by solving one linear program (lp).</s> <s>an important difference compared to existing algorithms is that for our estimator it is not necessary  to specify neither the number nor the proportion of the outliers.</s> <s>the theoretical results, as well as  the numerical example reported in this work, confirm the efficiency of the proposed approach.</s></p></d>", "label": ["<d><p><s><var>l_1</var></s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning distance functions with side information plays a key role in many machine learning and data mining applications.</s> <s>conventional approaches often assume a mahalanobis distance function.</s> <s>these approaches are limited in two aspects: (i) they are computationally expensive (even infeasible) for high dimensional data because the size of the metric is in the square of dimensionality; (ii) they assume a fixed metric for the entire input space and therefore are unable to handle heterogeneous data.</s> <s>in this paper, we propose a novel scheme that learns nonlinear bregman distance functions from side information using a non-parametric approach that is similar to support vector machines.</s> <s>the proposed scheme avoids the assumption of fixed metric because its local distance metric is implicitly derived from the hessian matrix of a convex function that is used to generate the bregman distance function.</s> <s>we present an efficient learning algorithm for the proposed scheme for distance function learning.</s> <s>the extensive experiments with semi-supervised clustering show the proposed technique (i) outperforms the state-of-the-art approaches for distance function learning, and (ii) is computationally efficient for high dimensional data.</s></p></d>", "label": ["<d><p><s>learning bregman distance functions and its application for semi-supervised clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we address the problem of provably correct feature selection in arbitrary domains.</s> <s>an optimal solution to the problem is a markov boundary, which is a minimal set of features that make the probability distribution of a target variable conditionally invariant to the state of all other features in the domain.</s> <s>while numerous algorithms for this problem have been proposed, their theoretical correctness and practical behavior under arbitrary probability distributions is unclear.</s> <s>we address this by introducing the markov boundary theorem that precisely characterizes the properties of an ideal markov boundary, and use it to develop algorithms that learn a more general boundary that can capture complex interactions that only appear when the values of multiple features are considered together.</s> <s>we introduce two algorithms: an exact, provably correct one as well a more practical randomized anytime version, and show that they perform well on artificial as well as benchmark and real-world data sets.</s> <s>throughout the paper we make minimal assumptions that consist of only a general set of axioms that hold for every probability distribution, which gives these algorithms universal applicability.</s></p></d>", "label": ["<d><p><s>toward provably correct feature selection in arbitrary domains</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a fast and scalable alternating optimization technique to detect regions of interest (rois) in cluttered web images without labels.</s> <s>the proposed approach discovers highly probable regions of  object instances by iteratively repeating the following two functions: (1) choose the exemplar set (i.e.</s> <s>small number of high ranked reference rois) across the dataset and (2) refine the rois of each image with respect to the exemplar set.</s> <s>these two subproblems are formulated as ranking in two different similarity networks of roi hypotheses by link analysis.</s> <s>the experiments with the pascal 06 dataset show that our unsupervised localization performance is better than one of state-of-the-art techniques and comparable to supervised methods.</s> <s>also, we test the scalability of our approach with five objects in flickr dataset consisting of more than 200,000 images.</s></p></d>", "label": ["<d><p><s>unsupervised detection of regions of interest using iterative link analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.</s> <s>while typically the prior is factorial, here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient, globally-convergent reweighted $\\ell_1$ minimization procedure.</s> <s>the first method under consideration arises from the sparse bayesian learning (sbl) framework.</s> <s>although based on a highly non-convex underlying cost function, in the context of canonical sparse estimation problems, we prove uniform superiority of this method over the lasso in that, (i) it can never do worse, and (ii) for any dictionary and sparsity profile, there will always exist cases where it does better.</s> <s>these results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.</s> <s>we then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.</s> <s>for both of these methods, as well as traditional factorial analogs, we demonstrate the effectiveness of reweighted $\\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification, group feature selection, and non-negativity constraints.</s> <s>as a byproduct of this development, a rigorous reformulation of sparse bayesian classification (e.g., the relevance vector machine) is derived that, unlike the original, involves no approximation steps and descends a well-defined objective function.</s></p></d>", "label": ["<d><p><s>sparse estimation using general likelihoods and non-factorial priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>since the development of loopy belief propagation, there has been considerable work on advancing the state of the art for approximate inference over distributions defined on discrete random variables.</s> <s>improvements include guarantees of convergence, approximations that are provably more accurate, and bounds on the results of exact inference.</s> <s>however, extending these methods  to continuous-valued systems has lagged behind.</s> <s>while several methods have been developed to use belief propagation on systems with continuous values, they have not as yet incorporated the recent advances for discrete variables.</s> <s>in this context we extend a recently proposed particle-based belief propagation algorithm to provide a general framework for adapting discrete message-passing algorithms to perform inference in continuous systems.</s> <s>the resulting algorithms behave similarly to their purely discrete counterparts, extending the benefits of these more advanced inference techniques to the continuous domain.</s></p></d>", "label": ["<d><p><s>particle-based variational inference for continuous systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce skill chaining, a skill discovery method for reinforcement learning agents in continuous domains, that builds chains of skills leading to an end-of-task reward.</s> <s>we demonstrate experimentally that it creates skills that result in performance benefits in a challenging continuous domain.</s></p></d>", "label": ["<d><p><s>skill discovery in continuous reinforcement learning domains using skill chaining</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper, we study the manifold regularization for the sliced inverse regression (sir).</s> <s>the manifold regularization improves the standard sir in two aspects: 1) it encodes the local geometry for sir and 2) it enables sir to deal with transductive and semi-supervised learning problems.</s> <s>we prove that the proposed graph laplacian based regularization is convergent at rate root-n.</s> <s>the projection directions of the regularized sir are optimized by using a conjugate gradient method on the grassmann manifold.</s> <s>experimental results support our theory.</s></p></d>", "label": ["<d><p><s>manifold regularization for sir with rate root-n convergence</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a bayesian nonparametric approach to relating multiple time series via a set of latent, dynamical behaviors.</s> <s>using a beta process prior, we allow data-driven selection of the size of this set, as well as the pattern with which behaviors are shared among time series.</s> <s>via the indian buffet process representation of the beta process predictive distributions, we develop an exact markov chain monte carlo inference method.</s> <s>in particular, our approach uses the sum-product algorithm to efficiently compute metropolis-hastings acceptance probabilities, and explores new dynamical behaviors via birth/death proposals.</s> <s>we validate our sampling algorithm using several synthetic datasets, and also demonstrate promising unsupervised segmentation of visual motion capture data.</s></p></d>", "label": ["<d><p><s>sharing features among dynamical systems with beta processes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>when used to guide decisions, linear regression analysis typically involves estimation of regression coefficients via ordinary least squares and their subsequent use to make decisions.</s> <s>when there are multiple response variables and features do not perfectly capture their relationships, it is beneficial to account for the decision objective when computing regression coefficients.</s> <s>empirical optimization does so but sacrifices performance when features are well-chosen or training data are insufficient.</s> <s>we propose directed regression, an efficient algorithm that combines merits of ordinary least squares and empirical optimization.</s> <s>we demonstrate through a computational study that directed regression can generate significant performance gains over either alternative.</s> <s>we also develop a theory that motivates the algorithm.</s></p></d>", "label": ["<d><p><s>directed regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dynamic bayesian networks have been applied widely to reconstruct the structure of regulatory processes from time series data.</s> <s>the standard approach is based on the assumption of a homogeneous markov chain, which is not valid in many real-world scenarios.</s> <s>recent research efforts addressing this shortcoming have considered undirected graphs, directed graphs for discretized data, or over-flexible models that lack any information sharing between time series segments.</s> <s>in the present article, we propose a non-stationary dynamic bayesian network for continuous data, in which parameters are allowed to vary between segments, and in which a common network structure provides essential information sharing across segments.</s> <s>our model is based on a bayesian change-point process, and we apply a variant of the allocation sampler of nobile and fearnside to infer the number and location of the change-points.</s></p></d>", "label": ["<d><p><s>non-stationary continuous dynamic bayesian networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper uses information-theoretic techniques to determine minimax rates for estimating nonparametric sparse additive regression models under high-dimensional scaling.</s> <s>we assume an additive decomposition of the form $f^*(x_1, \\ldots, x_p) = \\sum_{j \\in s} h_j(x_j)$, where each component function $h_j$ lies in some hilbert space $\\hilb$ and $s \\subset \\{1, \\ldots, \\pdim \\}$ is an unknown subset with cardinality $\\s = |s$.</s> <s>given $\\numobs$ i.i.d.</s> <s>observations of $f^*(x)$ corrupted with white gaussian noise where the covariate vectors $(x_1, x_2, x_3,...,x_{\\pdim})$ are drawn with i.i.d.</s> <s>components from some distribution $\\mp$, we determine tight lower bounds on the minimax rate for estimating the regression function with respect to squared $\\ltp$ error.</s> <s>the main result shows that the minimax rates are $\\max{\\big(\\frac{\\s \\log \\pdim / \\s}{n}, \\lowerratesq \\big)}$.</s> <s>the first term reflects the difficulty of performing \\emph{subset selection} and is independent of the hilbert space $\\hilb$; the second term $\\lowerratesq$ is an \\emph{\\s-dimensional estimation} term, depending only on the low dimension $\\s$ but not the ambient dimension $\\pdim$, that captures the difficulty of estimating a sum of $\\s$ univariate functions in the hilbert space $\\hilb$.</s> <s>as a special case, if $\\hilb$ corresponds to the $\\m$-th order sobolev space $\\sobm$ of functions that are $m$-times differentiable, the $\\s$-dimensional estimation term takes the form $\\lowerratesq \\asymp \\s \\; n^{-2\\m/(2\\m+1)}$.</s> <s>the minimax rates are compared with rates achieved by an $\\ell_1$-penalty based approach, it can be shown that a certain $\\ell_1$-based approach achieves the minimax optimal rate.</s></p></d>", "label": ["<d><p><s>lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>despite the large amount of literature on upper bounds on complexity of convex analysis, surprisingly little is known about the fundamental hardness of these problems.</s> <s>the extensive use of convex optimization in machine learning and statistics makes such an understanding critical to understand fundamental computational limits of learning and estimation.</s> <s>in this paper, we study the complexity of stochastic convex optimization in an oracle model of computation.</s> <s>we improve upon known results and obtain tight minimax complexity estimates for some function classes.</s> <s>we also discuss implications of these results to the understanding the inherent complexity of large-scale learning and estimation problems.</s></p></d>", "label": ["<d><p><s>information-theoretic lower bounds on the oracle complexity of convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of learning classifiers when observations have multiple views, some of which may not be observed for all examples.</s> <s>we assume the existence of view generating functions which may complete the missing views in an approximate way.</s> <s>this situation corresponds for example to learning text classifiers from multilingual collections where documents are not available in all languages.</s> <s>in that case, machine translation (mt) systems may be used to translate each document in the missing languages.</s> <s>we derive a generalization error bound for classifiers learned on examples with multiple artificially created views.</s> <s>our result uncovers a trade-off between the size of the training set, the number of views, and the quality of the view generating functions.</s> <s>as a consequence, we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning.</s> <s>an extension of this framework is a natural way to leverage unlabeled multi-view data in semi-supervised learning.</s> <s>experimental results on a subset of the reuters rcv1/rcv2 collections support our findings by showing that additional views obtained from mt may significantly improve the classification performance in the cases identified by our trade-off.</s></p></d>", "label": ["<d><p><s>learning from multiple partially observed views - an application to multilingual text categorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bag-of-words document representations are often used in text, image and video processing.</s> <s>while it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms.</s> <s>the classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded.</s> <s>more robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words.</s> <s>while favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation.</s> <s>in this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary.</s> <s>this approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image representations.</s> <s>experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.</s></p></d>", "label": ["<d><p><s>group sparse coding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the general problem of learning kernels based on a polynomial combination of base kernels.</s> <s>it analyzes this problem in the case of regression and the kernel ridge regression algorithm.</s> <s>it examines the corresponding learning kernel optimization problem, shows how that minimax problem can be reduced to a simpler minimization problem, and proves that the global solution of this problem always lies on the boundary.</s> <s>we give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations.</s> <s>finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.</s></p></d>", "label": ["<d><p><s>learning non-linear combinations of kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many types of regularization schemes have been employed in statistical learning, each one motivated by some assumption about the problem domain.</s> <s>in this paper, we present a unified asymptotic analysis of smooth regularizers, which allows us to see how the validity of these assumptions impacts the success of a particular regularizer.</s> <s>in addition, our analysis motivates an algorithm for optimizing regularization parameters, which in turn can be analyzed within our framework.</s> <s>we apply our analysis to several examples, including hybrid generative-discriminative learning and multi-task learning.</s></p></d>", "label": ["<d><p><s>asymptotically optimal regularization in smooth parametric models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new empirical risk minimization framework for approximating functions from training samples for low-dimensional regression applications where a lattice (look-up table) is stored and interpolated at run-time for an efficient hardware implementation.</s> <s>rather than evaluating a fitted function at the lattice nodes without regard to the fact that samples will be interpolated, the proposed lattice regression approach estimates the lattice to minimize the interpolation error on the given training samples.</s> <s>experiments show that lattice regression can reduce mean test error  compared to gaussian process regression for digital color management of printers, an application for which linearly interpolating a look-up table (lut) is standard.</s> <s>simulations confirm that lattice regression performs consistently better than the naive approach to learning the lattice, particularly when the density of training samples is low.</s></p></d>", "label": ["<d><p><s>lattice regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>studying signal and noise properties of recorded neural data is critical in developing more efficient algorithms to recover the encoded information.</s> <s>important issues exist in this research including the variant spectrum spans of neural spikes that make it difficult to choose a global optimal bandpass filter.</s> <s>also, multiple sources produce aggregated noise that deviates from the conventional white gaussian noise.</s> <s>in this work, the spectrum variability of spikes is addressed, based on which the concept of adaptive bandpass filter that fits the spectrum of individual spikes is proposed.</s> <s>multiple noise sources have been studied through analytical models as well as empirical measurements.</s> <s>the dominant noise source is identified as neuron noise followed by interface noise of the electrode.</s> <s>this suggests that major efforts to reduce noise from electronics are not well spent.</s> <s>the measured noise from in vivo experiments shows a family of 1/f^{x} (x=1.5\\pm 0.5) spectrum that can be reduced using noise shaping techniques.</s> <s>in summary, the methods of adaptive bandpass filtering and noise shaping together result in several db signal-to-noise ratio (snr) enhancement.</s></p></d>", "label": ["<d><p><s>noise characterization, modeling, and reduction for in vivo neural recording</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>by adding a spatial regularization kernel to a standard loss function formulation of the boosting problem, we develop a framework for spatially informed boosting.</s> <s>from this regularized loss framework we derive an efficient boosting algorithm that uses additional weights/priors on the base classifiers.</s> <s>we prove that the proposed algorithm exhibits a ``grouping effect, which encourages the selection of all spatially local, discriminative base classifiers.</s> <s>the algorithms primary advantage is in applications where the trained classifier is used to identify the spatial pattern of discriminative information, e.g.</s> <s>the voxel selection problem in fmri.</s> <s>we demonstrate the algorithms performance on various data sets.</s></p></d>", "label": ["<d><p><s>boosting with spatial regularization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>given $n$ noisy samples with $p$ dimensions, where $n \\ll p$, we show that the multi-stage thresholding procedures can accurately estimate a sparse vector $\\beta \\in \\r^p$ in a linear model, under the restricted eigenvalue conditions (bickel-ritov-tsybakov 09).</s> <s>thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works.</s> <s>more importantly, this method allows very significant values of $s$, which is the number of non-zero elements in the true parameter $\\beta$.</s> <s>for example, it works for cases where the ordinary lasso would have failed.</s> <s>finally, we show that if $x$ obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the gauss-dantzig selector (cand\\{e}s-tao 07) achieves the $\\ell_2$ loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.</s></p></d>", "label": ["<d><p><s>thresholding procedures for high dimensional variable selection and statistical estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning, from k input data, a regression function in a function space of high dimension n using projections onto a random subspace of lower dimension m. from any linear approximation algorithm using empirical risk minimization (possibly penalized), we provide bounds on the excess risk of the estimate computed in the projected subspace (compressed domain) in terms of the excess risk of the estimate built in the high-dimensional space (initial domain).</s> <s>we apply the analysis to the ordinary least-squares regression and show that by choosing m=o(\\sqrt{k}), the estimation error (for the quadratic loss) of the ``compressed least squares regression is o(1/\\sqrt{k}) up to logarithmic factors.</s> <s>we also discuss the numerical complexity of several algorithms (both in initial and compressed domains) as a function of n, k, and m.</s></p></d>", "label": ["<d><p><s>compressed least-squares regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the recently proposed \\emph{additive noise model} has advantages over previous structure learning algorithms, when attempting to recover some true data generating mechanism, since it (i) does not assume linearity or gaussianity and (ii) can recover a unique dag rather than an equivalence class.</s> <s>however, its original extension to the multivariate case required enumerating all possible dags, and for some special distributions, e.g.</s> <s>linear gaussian, the model is invertible and thus cannot be used for structure learning.</s> <s>we present a new approach which combines a pc style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class.</s> <s>this results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible.</s> <s>experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-gaussian.</s></p></d>", "label": ["<d><p><s>nonlinear directed acyclic structure learning with weakly additive noise models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study unsupervised learning in a probabilistic generative model for occlusion.</s> <s>the model uses two types of latent variables: one indicates which objects are present in the image, and the other how they are ordered in depth.</s> <s>this depth order then determines how the positions and appearances of the objects present, specified in the model parameters, combine to form the image.</s> <s>we show that the object parameters can be learnt from an unlabelled set of images in which objects occlude one another.</s> <s>exact maximum-likelihood learning is intractable.</s> <s>however, we show that tractable approximations to expectation maximization (em) can be found if the training images each contain only a small number of objects on average.</s> <s>in numerical experiments it is shown that these approximations recover the correct set of object parameters.</s> <s>experiments on a novel version of the bars test using colored bars, and experiments on more realistic data, show that the algorithm performs well in extracting the generating causes.</s> <s>experiments based on the standard bars benchmark test for object learning show that the algorithm performs well in comparison to other recent component extraction approaches.</s> <s>the model and the learning algorithm thus connect research on occlusion with the research field of multiple-cause component extraction methods.</s></p></d>", "label": ["<d><p><s>occlusive components analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been established that the second-order stochastic gradient descent (2sgd) method can potentially achieve generalization performance as well as empirical optimum in a single pass (i.e., epoch) through the training examples.</s> <s>however, 2sgd requires computing the inverse of the hessian matrix of the loss function, which is prohibitively expensive.</s> <s>this paper presents periodic step-size adaptation (psa), which approximates the jacobian matrix of the mapping function and explores a linear relation between the jacobian and hessian to approximate the hessian periodically and achieve near-optimal results in experiments on a wide variety of models and tasks.</s></p></d>", "label": ["<d><p><s>periodic step size adaptation for single pass on-line learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we examine the generalization error of regularized distance metric learning.</s> <s>we show that with appropriate constraints, the generalization error of regularized distance metric learning could be independent from the dimensionality, making it suitable for handling high dimensional data.</s> <s>in addition, we present an efficient online learning algorithm for regularized distance metric learning.</s> <s>our empirical studies with data classification and face recognition show that the proposed algorithm is (i) effective for distance metric learning when compared to the state-of-the-art methods, and (ii) efficient and robust for high dimensional data.</s></p></d>", "label": ["<d><p><s>regularized distance metric learning:theory and algorithm</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis.</s> <s>however, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations.</s> <s>this paper considers the idealized ?robust principal component analysis?</s> <s>problem of recovering a low rank matrix a from corrupted observations d = a + e. here, the error entries e can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse.</s> <s>we prove that most matrices a can be efficiently and exactly recovered from most error sign-and-support patterns, by solving a simple convex program.</s> <s>our result holds even when the rank of a grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors e grows in proportion to the total number of entries in the matrix.</s> <s>a by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries.</s> <s>simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.</s></p></d>", "label": ["<d><p><s>robust principal component analysis: exact recovery of corrupted low-rank matrices via convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning a measure of similarity between pairs of objects is a fundamental problem in machine learning.</s> <s>it stands in the core of classification methods like kernel machines, and is particularly useful for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video.</s> <s>in these tasks, users look for objects that are not only visually similar but also semantically related to a given object.</s> <s>unfortunately, current approaches for learning similarity may not scale to large datasets with high dimensionality, especially when imposing metric constraints on the learned similarity.</s> <s>we describe oasis, a method for learning pairwise similarity that is fast and scales linearly with the number of objects and the number of non-zero features.</s> <s>scalability is achieved through online learning of a bilinear model over sparse representations using a large margin criterion and an efficient hinge loss cost.</s> <s>oasis is accurate at a wide range of scales: on a standard benchmark with thousands of images, it is more precise than state-of-the-art methods, and faster by orders of magnitude.</s> <s>on 2 million images collected from the web, oasis can be trained within 3 days on a single cpu.</s> <s>the non-metric similarities learned by oasis can be transformed into metric similarities, achieving higher precisions than similarities that are learned as metrics in the first place.</s> <s>this suggests an approach for learning a metric from data that is larger by an order of magnitude than was handled before.</s></p></d>", "label": ["<d><p><s>an online algorithm for large scale image similarity learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multitask learning addressed the problem of learning related tasks whose information can be shared each other.</s> <s>traditional problem usually deal with homogeneous tasks such as regression, classification individually.</s> <s>in this paper we consider the problem learning multiple related tasks where tasks consist of both continuous and discrete outputs from a common set of input variables that lie in a high-dimensional space.</s> <s>all of the tasks are related in the sense that they share the same set of relevant input variables, but the amount of influence of each input on different outputs may vary.</s> <s>we formulate this problem as a combination of linear regression and logistic regression and model the joint sparsity as l1/linf and l1/l2-norm of the model parameters.</s> <s>among several possible applications, our approach addresses an important open problem in genetic association mapping, where we are interested in discovering genetic markers that influence multiple correlated traits jointly.</s> <s>in our experiments, we demonstrate our method in the scenario of association mapping, using simulated and asthma data, and show that the algorithm can effectively recover the relevant inputs with respect to all of the tasks.</s></p></d>", "label": ["<d><p><s>heterogeneous multitask learning with joint sparsity constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the heavy-tailed distribution of gradients in natural scenes have proven effective priors for a range of problems such as denoising, deblurring and super-resolution.</s> <s>however, the use of sparse distributions makes the problem non-convex and impractically slow to solve for multi-megapixel images.</s> <s>in this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use hyper-laplacian priors.</s> <s>we adopt an alternating minimization scheme where one of the two phases is a non-convex problem that is separable over pixels.</s> <s>this per-pixel sub-problem may be solved with a lookup table (lut).</s> <s>alternatively, for two specific values of ?, 1/2 and 2/3 an analytic solution can be found, by finding the roots of a cubic and quartic polynomial, respectively.</s> <s>our approach (using either luts or analytic formulae) is able to deconvolve a 1 megapixel image in less than ?3 seconds, achieving comparable quality to existing methods such as iteratively reweighted least squares (irls) that take ?20 minutes.</s> <s>furthermore, our method is quite general and can easily be extended to related image processing problems, beyond the deconvolution application demonstrated.</s></p></d>", "label": ["<d><p><s>fast image deconvolution using hyper-laplacian priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning to rank has become an important research topic in machine learning.</s> <s>while most learning-to-rank methods learn the ranking function by minimizing the loss functions, it is the ranking measures (such as ndcg and map) that are used to evaluate the performance of the learned ranking function.</s> <s>in this work, we reveal the relationship between ranking measures and loss functions in learning-to-rank methods, such as ranking svm, rankboost, ranknet, and listmle.</s> <s>we show that these loss functions are upper bounds of the measure-based ranking errors.</s> <s>as a result, the minimization of these loss functions will lead to the maximization of the ranking measures.</s> <s>the key to obtaining this result is to model ranking as a sequence of classification tasks, and define a so-called essential loss as the weighted sum of the classification errors of individual tasks in the sequence.</s> <s>we have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods.</s> <s>our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors.</s> <s>experimental results on benchmark datasets show that the modifications can lead to better ranking performance, demonstrating the correctness of our analysis.</s></p></d>", "label": ["<d><p><s>ranking measures and loss functions in learning to rank</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel learning is a powerful framework for nonlinear data modeling.</s> <s>using the kernel trick, a number of problems have been formulated as semidefinite programs (sdps).</s> <s>these include maximum variance unfolding (mvu) (weinberger et al., 2004) in nonlinear dimensionality reduction, and pairwise constraint propagation (pcp) (li et al., 2008) in constrained clustering.</s> <s>although in theory sdps can be efficiently solved, the high computational complexity incurred in numerically processing the huge linear matrix inequality constraints has rendered the sdp approach unscalable.</s> <s>in this paper, we show that a large class of kernel learning problems can be reformulated as semidefinite-quadratic-linear programs (sqlps), which only contain a simple positive semidefinite constraint, a second-order cone constraint and a number of linear constraints.</s> <s>these constraints are much easier to process numerically, and the gain in speedup over previous approaches is at least of the order $m^{2.5}$, where m is the matrix dimension.</s> <s>experimental results are also presented to show the superb computational efficiency of our approach.</s></p></d>", "label": ["<d><p><s>fast graph laplacian regularized kernel learning via semidefinite?quadratic?linear programming</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>pruning can massively accelerate the computation of feature expectations in large models.</s> <s>however, any single pruning mask will introduce bias.</s> <s>we present a novel approach which employs a randomized sequence of pruning masks.</s> <s>formally, we apply auxiliary variable mcmc sampling to generate this sequence of masks, thereby gaining theoretical guarantees about convergence.</s> <s>because each mask is generally able to skip large portions of an underlying dynamic program, our approach is particularly compelling for high-degree algorithms.</s> <s>empirically, we demonstrate our method on bilingual parsing, showing decreasing bias as more masks are incorporated, and outperforming fixed tic-tac-toe pruning.</s></p></d>", "label": ["<d><p><s>randomized pruning: efficiently calculating expectations in large dynamic programs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while many perceptual and cognitive phenomena are well described in terms of bayesian inference, the necessary computations are intractable at the scale of real-world tasks, and it remains unclear how the human mind approximates bayesian inference algorithmically.</s> <s>we explore the proposal that for some tasks, humans use a form of markov chain monte carlo to approximate the posterior distribution over hidden variables.</s> <s>as a case study, we show how several phenomena of perceptual multistability can be explained as mcmc inference in simple graphical models for low-level vision.</s></p></d>", "label": ["<d><p><s>perceptual multistability as markov chain monte carlo inference</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show how to sequentially optimize magnetic resonance imaging measurement designs over stacks of neighbouring image slices, by performing convex variational inference on a large scale non-gaussian linear dynamical system, tracking dominating directions of posterior covariance without imposing any factorization constraints.</s> <s>our approach can be scaled up to high-resolution images by reductions to numerical mathematics primitives and parallelization on several levels.</s> <s>in a first study, designs are found that improve significantly on others chosen independently for each slice or drawn at random.</s></p></d>", "label": ["<d><p><s>speeding up magnetic resonance image acquisition by bayesian multi-slice adaptive compressed sensing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game.</s> <s>one efficient method for computing nash equilibria in large, zero-sum, imperfect information games is counterfactual regret minimization (cfr).</s> <s>in the domain of poker, cfr has proven effective, particularly when using a domain-specific augmentation involving chance outcome sampling.</s> <s>in this paper, we describe a general family of domain independent cfr sample-based algorithms called monte carlo counterfactual regret minimization (mccfr) of which the original and poker-specific versions are special cases.</s> <s>we start by showing that mccfr performs the same regret updates as cfr on expectation.</s> <s>then, we introduce two sampling schemes: {\\it outcome sampling} and {\\it external sampling}, showing that both have bounded overall regret with high  probability.</s> <s>thus, they can compute an approximate equilibrium using self-play.</s> <s>finally, we prove a new tighter bound on the regret for the original cfr algorithm and relate this new bound to mccfrs bounds.</s> <s>we show empirically that, although the sample-based algorithms require more iterations, their lower cost per iteration can lead to dramatically faster convergence in various games.</s></p></d>", "label": ["<d><p><s>monte carlo sampling for regret minimization in extensive games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hypergraph clustering refers to the process of extracting maximally coherent groups from a set of objects using high-order (rather than pairwise) similarities.</s> <s>traditional approaches to this problem are based on the idea of partitioning the input data into a user-defined number of classes, thereby obtaining the clusters as a by-product of the partitioning process.</s> <s>in this paper, we provide a radically different perspective to the problem.</s> <s>in contrast to the classical approach, we attempt to provide a meaningful formalization of the very notion of a cluster and we show that game theory offers an attractive and unexplored perspective that serves well our purpose.</s> <s>specifically, we show that the hypergraph clustering problem can be naturally cast into a non-cooperative multi-player ``clustering game, whereby the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept.</s> <s>from the computational viewpoint, we show that the problem of finding the equilibria of our clustering game is equivalent to locally optimizing a polynomial function over the standard simplex, and we provide a discrete-time dynamics to perform this optimization.</s> <s>experiments are presented which show the superiority of our approach over state-of-the-art hypergraph clustering techniques.</s></p></d>", "label": ["<d><p><s>a game-theoretic approach to hypergraph clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a structured output model for object category detection that explicitly accounts for alignment, multiple aspects and partial truncation in both training and inference.</s> <s>the model is formulated as large margin learning with latent variables and slack rescaling, and both training and inference are computationally efficient.</s> <s>we make the following contributions: (i) we note that extending the structured output regression formulation of blaschko and lampert (eccv 2008) to include a bias term significantly improves performance; (ii) that alignment (to account for small rotations and anisotropic scalings) can be included as a latent variable and efficiently determined and implemented; (iii) that the latent variable extends to multiple aspects (e.g.</s> <s>left facing, right facing, front) with the same formulation; and (iv), most significantly for performance, that truncated and truncated instances can be included in both training and inference with an explicit truncation mask.</s> <s>we demonstrate the method by training and testing on the pascal voc 2007 data set -- training includes the truncated examples, and in testing object instances are detected at multiple scales, alignments, and with significant truncations.</s></p></d>", "label": ["<d><p><s>structured output regression for detection with partial truncation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>directed graphical models such as bayesian networks are a favored formalism to model the dependency structures in complex multivariate systems such as those encountered in biology and neural sciences.</s> <s>when the system is undergoing dynamic transformation, often a temporally rewiring network is needed for capturing the dynamic causal influences between covariates.</s> <s>in this paper, we propose a time-varying dynamic bayesian network (tv-dbn) for modeling the structurally varying directed dependency structures underlying non-stationary biological/neural time series.</s> <s>this is a challenging problem due the non-stationarity and sample scarcity of the time series.</s> <s>we present a kernel reweighted $\\ell_1$ regularized auto-regressive procedure for learning the tv-dbn model.</s> <s>our method enjoys nice properties such as computational efficiency and provable asymptotic consistency.</s> <s>applying tv-dbn to time series measurements during yeast cell cycle and brain response to visual stimuli reveals interesting dynamics underlying the respective biological systems.</s></p></d>", "label": ["<d><p><s>time-varying dynamic bayesian networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov random fields (mrfs), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables.</s> <s>maximum likelihood learning in mrfs is hard due to the presence of the global normalizing constant.</s> <s>in this paper we consider a class of stochastic approximation algorithms of robbins-monro type that uses markov chain monte carlo to do approximate maximum likelihood learning.</s> <s>we show that using mcmc operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large densely-connected mrfs.</s> <s>our results on mnist and norb datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data and perform well on digit and object recognition tasks.</s></p></d>", "label": ["<d><p><s>learning in markov random fields using tempered transitions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we investigate how similar images sharing the same global description can help with unsupervised scene segmentation in an image.</s> <s>in contrast to recent work in semantic alignment of scenes, we allow an input image to be explained by partial matches of similar scenes.</s> <s>this allows for a better explanation of the input scenes.</s> <s>we perform mrf-based segmentation that optimizes over matches, while respecting boundary information.</s> <s>the recovered segments are then used to re-query a large database of images to retrieve better matches for the target region.</s> <s>we show improved performance in detecting occluding boundaries over previous methods on data gathered from the labelme database.</s></p></d>", "label": ["<d><p><s>segmenting scenes by matching image composites</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>which ads should we display in sponsored search in order to maximize our revenue?</s> <s>how should we dynamically rank information sources to maximize value of information?</s> <s>these applications exhibit strong diminishing returns: selection of redundant ads and information sources decreases their marginal utility.</s> <s>we show that these and other problems can be formalized as repeatedly selecting an assignment of items to positions to maximize a sequence of monotone submodular functions that arrive one by one.</s> <s>we present an efficient algorithm for this general problem and analyze it in the no-regret model.</s> <s>our algorithm is equipped with strong theoretical guarantees, with a performance ratio that converges to the optimal constant of 1-1/e.</s> <s>we empirically evaluate our algorithms on two real-world online optimization problems on the web: ad allocation with submodular utilities, and dynamically ranking blogs to detect information cascades.</s></p></d>", "label": ["<d><p><s>online learning of assignments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a probabilistic model of human memory performance in free recall experiments.</s> <s>in these experiments, a subject first studies a list of words and then tries to recall them.</s> <s>to model these data, we draw on both previous psychological research and statistical topic models of text documents.</s> <s>we assume that memories are formed by assimilating the semantic meaning of studied words (represented as a distribution over topics) into a slowly changing latent context (represented in the same space).</s> <s>during recall, this context is reinstated and used as a cue for retrieving studied words.</s> <s>by conceptualizing memory retrieval as a dynamic latent variable model, we are able to use bayesian inference to represent uncertainty and reason about the cognitive processes underlying memory.</s> <s>we present a particle filter algorithm for performing approximate posterior inference, and evaluate our model on the prediction of recalled words in experimental data.</s> <s>by specifying the model hierarchically, we are also able to capture inter-subject variability.</s></p></d>", "label": ["<d><p><s>a bayesian analysis of dynamics in free recall</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper addresses the problem of noisy generalized binary search (gbs).</s> <s>gbs is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries.</s> <s>at each step, a query is selected that most evenly splits the hypotheses under consideration into two disjoint subsets, a natural generalization of the idea underlying classic binary search.</s> <s>gbs is used in many applications, including fault testing, machine diagnostics, disease diagnosis, job scheduling, image processing, computer vision, and active learning.</s> <s>in most of these cases, the responses to queries can be noisy.</s> <s>past work has provided a partial characterization of gbs, but existing noise-tolerant versions of gbs are suboptimal in terms of sample complexity.</s> <s>this paper presents the first optimal algorithm for noisy gbs and demonstrates its application to learning multidimensional threshold functions.</s></p></d>", "label": ["<d><p><s>noisy generalized binary search</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search.</s> <s>our algorithm differs from previous approaches to learning from search, such as samuels checkers player and the td-leaf algorithm, in two key ways.</s> <s>first, we update all nodes in the search tree, rather than a single node.</s> <s>second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function.</s> <s>we implemented our algorithm in a chess program meep, using a linear heuristic function.</s> <s>after initialising its weight vector to small random values, meep was able to learn high quality weights from self-play alone.</s> <s>when tested online against human opponents, meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play.</s></p></d>", "label": ["<d><p><s>bootstrapping from game tree search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel non-parametric adaptive anomaly detection algorithm for high dimensional data based on score functions derived from nearest neighbor graphs on n-point nominal data.</s> <s>anomalies are declared whenever the score of a test sample falls below q, which is supposed to be the desired false alarm level.</s> <s>the resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level, q, for the case when the anomaly density is a mixture of the nominal and a known density.</s> <s>our algorithm is computationally efficient, being linear in dimension and quadratic in data size.</s> <s>it does not require choosing complicated tuning parameters or function approximation classes and it can adapt to local structure such as local change in dimensionality.</s> <s>we demonstrate the algorithm on both artificial and real data sets in high dimensional feature spaces.</s></p></d>", "label": ["<d><p><s>anomaly detection with score functions based on nearest neighbor graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel feature selection algorithm for the $k$-means clustering problem.</s> <s>our algorithm is randomized and, assuming an accuracy parameter $\\epsilon \\in (0,1)$, selects and appropriately rescales in an unsupervised manner $\\theta(k \\log(k / \\epsilon) / \\epsilon^2)$ features from a dataset of arbitrary dimensions.</s> <s>we prove that, if we run any $\\gamma$-approximate $k$-means algorithm ($\\gamma \\geq 1$) on the features selected using our method, we can find a $(1+(1+\\epsilon)\\gamma)$-approximate partition with high probability.</s></p></d>", "label": ["<d><p><s>unsupervised feature selection for the </s></p></d>"], "set": "test"},
  {"data": "<d><p><s>situations in which people with opposing prior beliefs observe the same evidence and then strengthen those existing beliefs are frequently offered as evidence of human irrationality.</s> <s>this phenomenon, termed belief polarization, is typically assumed to be non-normative.</s> <s>we demonstrate, however, that a variety of cases of belief polarization are consistent with a bayesian approach to belief revision.</s> <s>simulation results indicate that belief polarization is not only possible but relatively common within the class of bayesian models that we consider.</s></p></d>", "label": ["<d><p><s>bayesian belief polarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we extend the concept of phase tuning, a ubiquitous mechanism in sensory neurons including motion and disparity detection neurons, to the motion contrast detection.</s> <s>we demonstrate that motion contrast can be detected by phase shifts between motion neuronal responses in different spatial regions.</s> <s>by constructing the differential motion opponency in response to motions in two different spatial regions, varying motion contrasts can be detected, where similar motion is detected by zero phase shifts and differences in motion by non-zero phase shifts.</s> <s>the model can exhibit either enhancement or suppression of responses by either different or similar motion in the surrounding.</s> <s>a primary advantage of the model is that the responses are selective to relative motion instead of absolute motion, which could model neurons found in neurophysiological experiments responsible for motion pop-out detection.</s></p></d>", "label": ["<d><p><s>extending phase mechanism to differential motion opponency for motion pop-out</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when individuals independently recollect events or retrieve facts from memory, how can we aggregate these retrieved memories to reconstruct the actual set of events or facts?</s> <s>in this research, we report the performance of individuals in a series of general knowledge tasks, where the goal is to reconstruct from memory the order of historic events, or the order of items along some physical dimension.</s> <s>we introduce two bayesian models for aggregating order information based on a thurstonian approach and mallows model.</s> <s>both models assume that each individuals reconstruction is based on either a random permutation of the unobserved ground truth, or by a pure guessing strategy.</s> <s>we apply mcmc to make inferences about the underlying truth and the strategies employed by individuals.</s> <s>the models demonstrate a wisdom of crowds\" effect, where the aggregated orderings are closer to the true ordering than the orderings of the best individual.\"</s></p></d>", "label": ["<d><p><s>the wisdom of crowds in the recollection of order information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>alignment of time series is an important problem to solve in many scientific disciplines.</s> <s>in particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale difference between human actions as well as the inter/intra subject variability.</s> <s>in this paper we present canonical time warping (ctw), an extension of canonical correlation analysis (cca) for spatio-temporal alignment of the behavior between two subjects.</s> <s>ctw extends previous work on cca in two ways: (i) it combines cca with dynamic time warping for temporal alignment; and (ii) it extends cca to allow local spatial deformations.</s> <s>we show ctws effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects performing similar actions, and alignment of two people with similar facial expressions.</s> <s>our results demonstrate that ctw provides both visually and qualitatively better alignment than state-of-the-art techniques based on dynamic time warping.</s></p></d>", "label": ["<d><p><s>canonical time warping for alignment of human behavior</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a nonparametric bayesian method for texture learning and synthesis.</s> <s>a texture image is represented by a 2d-hidden markov model (2d-hmm) where the hidden states correspond to the cluster labeling of textons and the transition matrix encodes their spatial layout (the compatibility between adjacent textons).</s> <s>2d-hmm is coupled with the hierarchical dirichlet process (hdp) which allows the number of textons and the complexity of transition matrix grow as the input texture becomes irregular.</s> <s>the hdp makes use of dirichlet process prior which favors regular textures by penalizing the model complexity.</s> <s>this framework (hdp-2d-hmm) learns the texton vocabulary and their spatial layout jointly and automatically.</s> <s>the hdp-2d-hmm results in a compact  representation of textures which allows fast texture synthesis with comparable rendering quality over the state-of-the-art image-based rendering methods.</s> <s>we also show that hdp-2d-hmm can be applied to perform image segmentation and synthesis.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian texture learning and synthesis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work has led to the ability to perform space ef?cient, approximate counting  over large vocabularies in a streaming context.</s> <s>motivated by the existence of data  structures of this type, we explore the computation of associativity scores, other-  wise known as pointwise mutual information (pmi), in a streaming context.</s> <s>we  give theoretical bounds showing the impracticality of perfect online pmi compu-  tation, and detail an algorithm with high expected accuracy.</s> <s>experiments on news  articles show our approach gives high accuracy on real world data.</s></p></d>", "label": ["<d><p><s>streaming pointwise mutual information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when individuals learn facts (e.g., foreign language vocabulary) over multiple study sessions, the temporal spacing of study has a significant impact on memory retention.</s> <s>behavioral experiments have shown a nonmonotonic relationship between spacing and retention:  short or long intervals between study sessions yield lower cued-recall accuracy than intermediate intervals.</s> <s>appropriate spacing of study can double retention on educationally relevant time scales.</s> <s>we introduce a multiscale context model (mcm) that is able to predict the influence of a particular study schedule on retention for specific material.</s> <s>mcms prediction is based on empirical data characterizing forgetting of the material following a single study session.</s> <s>mcm is a synthesis of two existing memory models (staddon, chelaru, & higa, 2002; raaijmakers, 2003).</s> <s>on the surface, these  models are unrelated and incompatible, but we show they share a core feature  that allows them to be integrated.</s> <s>mcm can determine study schedules that  maximize the durability of learning, and has implications for education  and training.</s> <s>mcm can be cast either as a neural network with inputs that  fluctuate over time, or as a cascade of leaky integrators.</s> <s>mcm is  intriguingly similar to a bayesian multiscale model of memory (kording, tenenbaum, shadmehr, 2007), yet mcm is better able to account for human  declarative memory.</s></p></d>", "label": ["<d><p><s>predicting the optimal spacing of study: a multiscale context model of memory</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a goal of central importance in the study of hierarchical models for object recognition -- and indeed the visual cortex -- is that of understanding quantitatively the trade-off between invariance and selectivity, and how invariance and discrimination properties contribute towards providing an improved representation useful for learning from data.</s> <s>in this work we provide a general group-theoretic framework for characterizing and understanding invariance in a family of hierarchical models.</s> <s>we show that by taking an algebraic perspective, one can provide a concise set of conditions which must be met to establish invariance, as well as a constructive prescription for meeting those conditions.</s> <s>analyses in specific cases of particular relevance to computer vision and text processing are given, yielding insight into how and when invariance can be achieved.</s> <s>we find that the minimal sets of transformations intrinsic to the hierarchical model needed to support a particular invariance can be clearly described, thereby encouraging efficient computational implementations.</s></p></d>", "label": ["<d><p><s>on invariance in hierarchical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although it is widely believed that reinforcement learning is a suitable tool for describing behavioral learning, the mechanisms by which it can be implemented in networks of spiking neurons are not fully understood.</s> <s>here, we show that different learning rules emerge from a policy gradient approach depending on which features of the spike trains are assumed to influence the reward signals, i.e., depending on which neural code is in effect.</s> <s>we use the framework of williams (1992) to derive learning rules for arbitrary neural codes.</s> <s>for illustration, we present policy-gradient rules for  three different example codes - a spike count code, a spike timing code and the most general ``full spike train code - and test them on simple model problems.</s> <s>in addition to classical synaptic learning, we derive learning rules for intrinsic parameters that control the excitability of the neuron.</s> <s>the spike count learning rule has structural similarities with established bienenstock-cooper-munro rules.</s> <s>if the distribution of the relevant spike train features  belongs to the natural exponential family, the learning rules have a characteristic shape that raises interesting prediction problems.</s></p></d>", "label": ["<d><p><s>code-specific policy gradient rules for spiking neurons</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a novel and highly effective approach for multi-body motion segmentation.</s> <s>drawing inspiration from robust statistical model fitting, we estimate putative subspace hypotheses from the data.</s> <s>however, instead of ranking them we encapsulate the hypotheses in a novel mercer kernel which elicits the potential of two point trajectories to have emerged from the same subspace.</s> <s>the kernel permits the application of well-established statistical learning methods for effective outlier rejection, automatic recovery of the number of motions and accurate segmentation of the point trajectories.</s> <s>the method operates well under severe outliers arising from spurious trajectories or mistracks.</s> <s>detailed experiments on a recent benchmark dataset (hopkins 155) show that our method is superior to other state-of-the-art approaches in terms of recovering the number of motions, segmentation accuracy, robustness against gross outliers and computational efficiency.</s></p></d>", "label": ["<d><p><s>the ordered residual kernel for robust motion subspace clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in cognitive science, empirical data collected from participants are the arbiters in model selection.</s> <s>model discrimination thus depends on designing maximally informative experiments.</s> <s>it has been shown that adaptive design optimization (ado) allows one to discriminate models as efficiently as possible in simulation experiments.</s> <s>in this paper we use ado in a series of experiments with people to discriminate the power, exponential, and hyperbolic models of memory retention, which has been a long-standing problem in cognitive science, providing an ideal setting in which to test the application of ado for addressing questions about human cognition.</s> <s>using an optimality criterion based on mutual information, ado is able to find designs that are maximally likely to increase our certainty about the true model upon observation of the experiment outcomes.</s> <s>results demonstrate the usefulness of ado and also reveal some challenges in its implementation.</s></p></d>", "label": ["<d><p><s>adaptive design optimization in experiments with people</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\\a$-mixing processes.</s> <s>to illustrate this oracle inequality, we use it to derive learning rates for some learning methods including least squares svms.</s> <s>since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.)</s> <s>processes, it turns out that these learning rates are close to the optimal rates known in the i.i.d.</s> <s>case.</s></p></d>", "label": ["<d><p><s>fast learning from non-i.i.d. observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the general problem of constructing nonparametric   bayesian models on infinite-dimensional random objects, such   as functions, infinite graphs or infinite permutations.</s> <s>the problem has generated much interest in machine learning,   where it is treated heuristically, but has not been    studied in full generality in nonparametric bayesian statistics, which tends to   focus on models over probability distributions.</s> <s>our approach applies a standard tool of stochastic process   theory, the construction of stochastic processes from their   finite-dimensional marginal distributions.</s> <s>the main contribution of the paper is a generalization   of the classic kolmogorov extension theorem to conditional   probabilities.</s> <s>this extension allows a rigorous construction of nonparametric bayesian models   from systems of finite-dimensional, parametric bayes equations.</s> <s>using this approach, we show (i)   how existence of a conjugate posterior for    the nonparametric model can be guaranteed by choosing   conjugate finite-dimensional models in the construction, (ii) how the   mapping to the posterior parameters of the nonparametric   model can be explicitly determined, and (iii) that   the construction of conjugate models in essence requires the   finite-dimensional models to be in the exponential family.</s> <s>as an application of our constructive framework,    we derive a model on infinite   permutations, the nonparametric bayesian analogue of a model   recently proposed for the analysis of rank data.</s></p></d>", "label": ["<d><p><s>construction of nonparametric bayesian models from parametric bayes equations</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a kernel embedding of probability distributions into reproducing kernel hilbert spaces (rkhs) has recently been proposed, which  allows the comparison of two probability measures p and q based on the distance between their respective embeddings: for a sufficiently rich rkhs, this distance is zero if and only if p and q coincide.</s> <s>in using this distance as a statistic for a  test of whether two samples are from different distributions, a major difficulty arises in computing the significance threshold, since the empirical statistic has as its null distribution (where p=q)  an infinite weighted sum of $\\chi^2$ random variables.</s> <s>the main result of the present work is a  novel, consistent estimate of this null distribution, computed from  the eigenspectrum of the gram matrix on the aggregate sample from p and q.</s> <s>this estimate may be computed faster than a previous consistent estimate based on the bootstrap.</s> <s>another prior approach was to compute the null distribution based on fitting a parametric family with the low order moments of the test statistic: unlike the present work, this heuristic has no guarantee of being accurate or consistent.</s> <s>we verify the performance of our null distribution estimate on both   an artificial example and on high dimensional multivariate data.</s></p></d>", "label": ["<d><p><s>a fast, consistent kernel two-sample test</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove certain theoretical properties of a graph-regularized transductive learning objective that is based on minimizing a kullback-leibler divergence based loss.</s> <s>these include showing that the iterative alternating minimization procedure used to minimize the objective converges to the correct solution and deriving a test for convergence.</s> <s>we also propose a graph node ordering algorithm that is cache cognizant and leads to a linear speedup in parallel computations.</s> <s>this ensures that the algorithm scales to large data sets.</s> <s>by making use of empirical evaluation on the timit and switchboard i corpora, we show this approach is able to out-perform other state-of-the-art ssl approaches.</s> <s>in one instance, we solve a problem on a 120 million node graph.</s></p></d>", "label": ["<d><p><s>entropic graph regularization in non-parametric semi-supervised classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work on the statistical modeling of neural responses has focused on modulated renewal processes in which the spike rate is a function of the stimulus and recent spiking history.</s> <s>typically, these models incorporate spike-history dependencies via either: (a) a conditionally-poisson process with rate dependent on a linear projection of the spike train history (e.g., generalized linear model); or (b) a modulated non-poisson renewal process (e.g., inhomogeneous gamma process).</s> <s>here we show that the two approaches can be combined, resulting in a {\\it conditional renewal} (cr) model for neural spike trains.</s> <s>this model captures both real and rescaled-time effects, and can be fit by maximum likelihood using a simple application of the time-rescaling theorem [1].</s> <s>we show that for any modulated renewal process model, the log-likelihood is concave in the linear filter parameters only under certain restrictive conditions on the renewal density (ruling out many popular choices, e.g.</s> <s>gamma with $\\kappa \\neq1$), suggesting that real-time history effects are easier to estimate than non-poisson renewal properties.</s> <s>moreover, we show that goodness-of-fit tests based on the time-rescaling theorem [1] quantify relative-time effects, but do not reliably assess accuracy in spike prediction or stimulus-response modeling.</s> <s>we illustrate the cr model with applications to both real and simulated neural data.</s></p></d>", "label": ["<d><p><s>time-rescaling methods for the estimation and assessment of non-poisson neural encoding models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>semi-supervised regression based on the graph laplacian suffers from the fact that the solution is biased towards a constant and the lack of extrapolating power.</s> <s>outgoing from these observations we propose to use the second-order hessian energy for semi-supervised regression which overcomes both of these problems, in particular, if the data lies on or close to a low-dimensional submanifold in the feature space, the hessian energy prefers functions which vary ``linearly with respect to the natural parameters in the data.</s> <s>this property makes it also particularly suited for the task of semi-supervised dimensionality reduction where the goal is to find the natural parameters in the data based on a few labeled points.</s> <s>the experimental result suggest that our method is superior to semi-supervised regression using laplacian regularization and standard supervised methods and is particularly suited for semi-supervised dimensionality reduction.</s></p></d>", "label": ["<d><p><s>semi-supervised regression using hessian energy with an application to semi-supervised dimensionality reduction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a class of nonlinear (polynomial) models that are discriminatively trained to directly map from the word content in a query-document or document-document pair to a ranking score.</s> <s>dealing with polynomial models on word features is computationally challenging.</s> <s>we propose a low rank (but diagonal preserving) representation of our polynomial models to induce feasible memory and computation requirements.</s> <s>we provide an empirical study on retrieval tasks based on wikipedia documents, where we obtain state-of-the-art performance while providing realistically scalable methods.</s></p></d>", "label": ["<d><p><s>polynomial semantic indexing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most of existing methods for dna motif discovery consider only a single set of sequences to find an over-represented motif.</s> <s>in contrast, we consider multiple sets of sequences where we group sets associated with the same motif into a cluster, assuming that each set involves a single motif.</s> <s>clustering sets of sequences yields clusters of coherent motifs, improving signal-to-noise ratio or enabling us to identify multiple motifs.</s> <s>we present a probabilistic model for dna motif discovery where we identify multiple motifs through searching for patterns which are shared across multiple sets of sequences.</s> <s>our model infers cluster-indicating latent variables and learns motifs simultaneously, where these two tasks interact with each other.</s> <s>we show that our model can handle various motif discovery problems, depending on how to construct multiple sets of sequences.</s> <s>experiments on three different problems for discovering dna motifs emphasize the useful behavior and confirm the substantial gains over existing methods where only single set of sequences is considered.</s></p></d>", "label": ["<d><p><s>clustering sequence sets for motif discovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the principles by which spiking neurons contribute to the astounding computational power of generic cortical microcircuits, and how spike-timing-dependent plasticity (stdp) of synaptic weights could generate and maintain  this computational function, are unknown.</s> <s>we show here that stdp, in conjunction with a stochastic soft winner-take-all (wta) circuit, induces spiking neurons to generate through their synaptic weights implicit internal models for subclasses (or causes\") of the high-dimensional spike patterns of hundreds of pre-synaptic neurons.</s> <s>hence these  neurons will fire after learning whenever the current input best matches their internal model.</s> <s>the resulting computational function of soft wta circuits, a common network motif of cortical microcircuits, could therefore be a drastic dimensionality reduction of information streams, together with the autonomous creation of internal models for the probability distributions of their input patterns.</s> <s>we show that the autonomous generation and maintenance of this computational function can be explained on the basis of rigorous mathematical principles.</s> <s>in particular, we show that stdp is able to approximate a stochastic online expectation-maximization (em) algorithm for modeling the input data.</s> <s>a corresponding result is shown for hebbian learning in artificial neural networks.\"</s></p></d>", "label": ["<d><p><s>stdp enables spiking neurons to detect hidden causes of their inputs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the minimum description length (mdl) principle selects the model that has the shortest code for data plus model.</s> <s>we show that for a countable class of models, mdl predictions are close to the true distribution in a strong sense.</s> <s>the result is completely general.</s> <s>no independence, ergodicity, stationarity, identifiability, or other assumption on the model class need to be made.</s> <s>more formally, we show that for any countable class of models, the distributions selected by mdl (or map) asymptotically predict (merge with) the true measure in the class in total variation distance.</s> <s>implications for non-i.i.d.</s> <s>domains like time-series forecasting, discriminative learning, and reinforcement learning are discussed.</s></p></d>", "label": ["<d><p><s>discrete mdl predicts in total variation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>everyday social interactions are heavily influenced by our snap judgments about others goals.</s> <s>even young infants can infer the goals of intentional agents from observing how they interact with objects and other agents in their environment: e.g., that one agent is `helping or `hindering anothers attempt to get up a hill or open a box.</s> <s>we propose a model for how people can infer these social goals from actions, based on inverse planning in multiagent markov decision problems (mdps).</s> <s>the model infers the goal most likely to be driving an agents behavior by assuming the agent acts approximately rationally given environmental constraints and its model of other agents present.</s> <s>we also present behavioral evidence in support of this model over a simpler, perceptual cue-based alternative.</s></p></d>", "label": ["<d><p><s>help or hinder: bayesian models of social goal inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>resting state activity is brain activation that arises in the absence of any task, and is usually measured in awake subjects during prolonged fmri scanning sessions where the only instruction given is to close the eyes and do nothing.</s> <s>it has been recognized in recent years that resting state activity is implicated in a wide variety of brain function.</s> <s>while certain networks of brain areas have different levels of activation at rest and during a task, there is nevertheless significant similarity between activations in the two cases.</s> <s>this suggests that recordings of resting state activity can be used as a source of unlabeled data to augment discriminative regression techniques in a semi-supervised setting.</s> <s>we evaluate this setting empirically yielding three main results: (i) regression tends to be improved by the use of laplacian regularization even when no additional unlabeled data are available, (ii) resting state data may have a similar marginal distribution to that recorded during the execution of a visual processing task reinforcing the hypothesis that these conditions have similar types of activation, and (iii) this source of information can be broadly exploited to improve the robustness of empirical inference in fmri studies, an inherently data poor domain.</s></p></d>", "label": ["<d><p><s>augmenting feature-driven fmri analyses: semi-supervised learning and resting state activity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of designing binary codes for high-dimensional data such that vectors that are similar in the original space map to similar binary strings.</s> <s>we introduce a simple distribution-free encoding scheme based on random projections, such that the expected hamming distance between the binary codes of two vectors is related to the value of a shift-invariant kernel (e.g., a gaussian kernel) between the vectors.</s> <s>we present a full theoretical analysis of the convergence properties of the proposed scheme, and report favorable experimental performance as compared to a recent state-of-the-art method, spectral hashing.</s></p></d>", "label": ["<d><p><s>locality-sensitive binary codes from shift-invariant kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel multivariate laplace (mvl) distribution as a sparsity promoting prior for bayesian source localization that allows the specification of constraints between and within sources.</s> <s>we represent the mvl distribution as a scale mixture that induces a coupling between source variances instead of their means.</s> <s>approximation of the posterior marginals using expectation propagation is shown to be very efficient due to properties of the scale mixture representation.</s> <s>the computational bottleneck amounts to computing the diagonal elements of a sparse matrix inverse.</s> <s>our approach is illustrated using a mismatch negativity paradigm for which meg data and a structural mri have been acquired.</s> <s>we show that spatial coupling leads to sources which are active over larger cortical areas as compared with an uncoupled prior.</s></p></d>", "label": ["<d><p><s>bayesian source localization with the multivariate laplace prior</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we investigate methods for selecting sets of labeled vertices for use in predicting the labels of vertices on a graph.</s> <s>we specifically study methods which choose a single batch of labeled vertices (i.e.</s> <s>offline, non sequential methods).</s> <s>in this setting, we find common graph smoothness assumptions directly motivate simple label selection methods with interesting theoretical guarantees.</s> <s>these methods bound prediction error in terms of the smoothness of the true labels with respect to the graph.</s> <s>some of these bounds give new motivations for previously proposed algorithms, and some suggest new algorithms which we evaluate.</s> <s>we show improved performance over baseline methods on several real world data sets.</s></p></d>", "label": ["<d><p><s>label selection on graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an algorithm is presented for online learning of rotations.</s> <s>the proposed algorithm involves matrix exponentiated gradient updates and is motivated by the von neumann divergence.</s> <s>the additive updates are skew-symmetric matrices with trace zero which comprise the lie algebra of the rotation group.</s> <s>the orthogonality and unit determinant of the matrix  parameter are preserved using matrix logarithms and exponentials and the algorithm lends itself to interesting interpretations in terms of the computational topology of the compact lie groups.</s> <s>the stability and the computational complexity of the algorithm are discussed.</s></p></d>", "label": ["<d><p><s>on learning rotations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a novel information theoretic approach for semi-supervised learning of conditional random fields.</s> <s>our approach defines a training objective that combines the conditional likelihood on labeled data and the mutual information on unlabeled data.</s> <s>different from previous minimum conditional entropy semi-supervised discriminative learning methods, our approach can be naturally cast into the rate distortion theory framework in information theory.</s> <s>we analyze the tractability of the framework for structured prediction and present a convergent variational training algorithm to defy the combinatorial explosion of terms in the sum over label configurations.</s> <s>our experimental results show that the rate distortion approach outperforms standard $l_2$ regularization and minimum conditional entropy regularization on both multi-class classification and sequence labeling problems.</s></p></d>", "label": ["<d><p><s>a rate distortion approach for semi-supervised conditional random fields</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the standard assumption of identically distributed training and test data can be violated when an adversary can exercise some control over the generation of the test data.</s> <s>in a prediction game, a learner produces a predictive model while an adversary may alter the distribution of input data.</s> <s>we study single-shot prediction games in which the cost functions of learner and adversary are not necessarily antagonistic.</s> <s>we identify conditions under which the prediction game has a unique nash equilibrium, and derive algorithms that will find the equilibrial prediction models.</s> <s>in a case study, we explore properties of nash-equilibrial prediction models for email spam filtering empirically.</s></p></d>", "label": ["<d><p><s>nash equilibria of static prediction games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>graph matching and map inference are essential problems in computer vision and machine learning.</s> <s>we introduce a novel algorithm that can accommodate both problems and solve them efficiently.</s> <s>recent graph matching algorithms are based on a general quadratic programming formulation, that takes in consideration both unary and second-order terms reflecting the similarities in local appearance as well as in the pairwise geometric relationships between the matched features.</s> <s>in this case the problem is np-hard and a lot of effort has been spent in finding efficiently approximate solutions by relaxing the constraints of the original problem.</s> <s>most algorithms find optimal continuous solutions of the modified problem, ignoring during the optimization the original discrete constraints.</s> <s>the continuous solution is quickly binarized at the end, but very little attention is put into this final discretization step.</s> <s>in this paper we argue that the stage in which a discrete solution is found is crucial for good performance.</s> <s>we propose an efficient algorithm, with climbing and convergence properties, that optimizes in the discrete domain the quadratic score, and it gives excellent results either by itself or by starting from the solution returned by any graph matching algorithm.</s> <s>in practice it outperforms state-or-the art algorithms and it also significantly improves their performance if used in combination.</s> <s>when applied to map inference, the algorithm is a parallel extension of iterated conditional modes (icm) with climbing and convergence properties that make it a compelling alternative to the sequential icm.</s> <s>in our experiments on map inference our algorithm proved its effectiveness by outperforming icm and max-product belief propagation.</s></p></d>", "label": ["<d><p><s>an integer projected fixed point method for graph matching and map inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>image representation based on image bases provides a framework for understanding neural representation of visual perception.</s> <s>a recent fmri study has shown that arbitrary contrast-defined visual images can be reconstructed from fmri activity patterns using a combination of multi-scale local image bases.</s> <s>in the reconstruction model, the mapping from an fmri activity pattern to the contrasts of the image bases was learned from measured fmri responses to visual images.</s> <s>but the shapes of the images bases were fixed, and thus may not be optimal for reconstruction.</s> <s>here, we propose a method to build a reconstruction model in which image bases are automatically extracted from the measured data.</s> <s>we constructed a probabilistic model that relates the fmri activity space to the visual image space via a set of latent variables.</s> <s>the mapping from the latent variables to the visual image space can be regarded as a set of image bases.</s> <s>we found that spatially localized, multi-scale image bases were estimated near the fovea, and that the model using the estimated image bases was able to accurately reconstruct novel visual images.</s> <s>the proposed method provides a means to discover a novel functional mapping between stimuli and brain activity patterns.</s></p></d>", "label": ["<d><p><s>estimating image bases for visual image reconstruction from human brain activity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning to rank is a relatively new field of study, aiming to learn a ranking function from a set of training data with relevancy labels.</s> <s>the ranking algorithms are often evaluated using information retrieval measures, such as normalized discounted cumulative gain [1] and mean average precision [2].</s> <s>until recently, most learning to rank algorithms were not using a loss function related to the above mentioned evaluation measures.</s> <s>the main difficulty in direct optimization of these measures is that they depend on the ranks of documents, not the numerical values output by the ranking function.</s> <s>we propose a probabilistic framework that addresses this challenge by optimizing the expectation of ndcg over all the possible permutations of documents.</s> <s>a relaxation strategy is used to approximate the average of ndcg over the space of permutation, and a bound optimization approach is proposed to make the computation efficient.</s> <s>extensive experiments show that the proposed algorithm outperforms state-of-the-art ranking algorithms on several benchmark data sets.</s></p></d>", "label": ["<d><p><s>learning to rank by optimizing ndcg measure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a corpus of news items consisting of images accompanied by text captions, we want to find out ``whos doing what, i.e.</s> <s>associate names and action verbs in the captions to the face and body pose of the persons in the images.</s> <s>we present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus.</s> <s>these models can then be used to recognize people and actions in novel images without captions.</s> <s>we demonstrate experimentally that our joint `face and pose model solves the correspondence problem better than earlier models covering only the face, and that it can perform recognition of new uncaptioned images.</s></p></d>", "label": ["<d><p><s>who?s doing what: joint modeling of names and verbs for simultaneous face and pose annotation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a classic debate in cognitive science revolves around understanding how children learn complex linguistic rules, such as those governing  restrictions on verb alternations, without negative  evidence.</s> <s>traditionally, formal learnability arguments have been used to claim that such learning is impossible without the aid of innate  language-specific knowledge.</s> <s>however, recently, researchers have shown  that statistical models are capable of learning complex rules from only  positive evidence.</s> <s>these two kinds of learnability analyses differ in their assumptions about the role of the distribution from which linguistic  input is generated.</s> <s>the former analyses assume that learners seek to identify grammatical sentences in a way that is robust to the distribution  from which the sentences are generated, analogous to discriminative  approaches in machine learning.</s> <s>the latter assume that learners are trying  to estimate a generative model, with sentences being sampled from that  model.</s> <s>we show that these two learning approaches differ in their use of implicit negative evidence -- the absence of a sentence -- when learning  verb alternations, and demonstrate that human learners can produce results  consistent with the predictions of both approaches, depending on the  context in which the learning problem is presented.</s></p></d>", "label": ["<d><p><s>differential use of implicit negative evidence in generative and discriminative language learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds.</s> <s>we propose approximate bilinear programming, a new formulation of value function approximation that provides strong a priori guarantees.</s> <s>in particular, it provably finds an approximate value function that minimizes the bellman residual.</s> <s>solving a bilinear program optimally is np hard, but this is unavoidable because the bellman-residual minimization itself is np hard.</s> <s>we, therefore, employ and analyze a common approximate algorithm for bilinear programs.</s> <s>the analysis shows that this algorithm offers a convergent generalization of approximate policy iteration.</s> <s>finally, we demonstrate that the proposed approach can consistently minimize the bellman residual on a simple benchmark problem.</s></p></d>", "label": ["<d><p><s>robust value function approximation using bilinear programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several key problems in machine learning, such as feature selection and active learning, can be formulated as submodular set function maximization.</s> <s>we present herein a novel algorithm for maximizing a submodular set function under a cardinality constraint --- the algorithm is based on a cutting-plane method and is implemented as an iterative small-scale binary-integer linear programming procedure.</s> <s>it is well known that this problem is np-hard, and the approximation factor achieved by the greedy algorithm is the theoretical limit for polynomial time.</s> <s>as for (non-polynomial time) exact algorithms that perform reasonably in practice, there has been very little in the literature although the problem is quite important for many applications.</s> <s>our algorithm is guaranteed to find the exact solution in finite iterations, and it converges fast in practice due to the efficiency of the cutting-plane mechanism.</s> <s>moreover, we also provide a method that produces successively decreasing upper-bounds of the optimal solution, while our algorithm provides successively increasing lower-bounds.</s> <s>thus, the accuracy of the current solution can be estimated at any point, and the algorithm can be stopped early once a desired degree of tolerance is met.</s> <s>we evaluate our algorithm on sensor placement and feature selection applications showing good performance.</s></p></d>", "label": ["<d><p><s>submodularity cuts and applications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the low-rank matrix completion problem is a fundamental problem with many important applications.</s> <s>recently, candes & recht, keshavan et al.</s> <s>and candes & tao obtained the first non-trivial theoretical results for the problem assuming that the observed entries are sampled uniformly at random.</s> <s>unfortunately, most real-world datasets do not satisfy this assumption, but instead exhibit power-law distributed samples.</s> <s>in this paper, we propose a graph theoretic approach to matrix completion that solves the problem for more realistic sampling models.</s> <s>our method is easier to analyze than previous methods with the analysis reducing to computing the threshold for complete cascades in random graphs, a problem of independent interest.</s> <s>by analyzing the graph theoretic problem, we show that our method achieves exact recovery when the observed entries are sampled from the chung-lu-vu model, which can generate power-law distributed graphs.</s> <s>we also hypothesize that our algorithm solves the matrix completion problem from an optimal number of entries for the popular preferential attachment model and provide strong empirical evidence for the claim.</s> <s>furthermore, our method is easier to implement and is substantially faster than existing methods.</s> <s>we demonstrate the effectiveness of our method on examples when the low-rank matrix is sampled according to the prevalent random graph models for complex networks and also on the netflix challenge dataset.</s></p></d>", "label": ["<d><p><s>matrix completion from power-law distributed samples</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the inter-subject alignment of functional mri (fmri) data is important for improving the statistical power of fmri group analyses.</s> <s>in contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects.</s> <s>we test our method on fmri data collected during a movie viewing experiment.</s> <s>by cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.</s></p></d>", "label": ["<d><p><s>fmri-based inter-subject cortical alignment using functional connectivity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the estimation of high-dimensional parametric models requires imposing some structure on the models, for instance that they be sparse, or that matrix structured parameters have low rank.</s> <s>a general approach for such structured parametric model estimation is to use regularized m-estimation procedures, which regularize a loss function that measures goodness of fit of the parameters to the data with some regularization function that encourages the assumed structure.</s> <s>in this paper, we aim to provide a unified analysis of such regularized m-estimation procedures.</s> <s>in particular, we report the convergence rates of such estimators in any metric norm.</s> <s>using just our main theorem, we are able to rederive some of the many existing results, but also obtain a wide range of novel convergence rates results.</s> <s>our analysis also identifies key properties of loss and regularization functions such as restricted strong convexity, and decomposability, that ensure the corresponding regularized m-estimators have good convergence rates.</s></p></d>", "label": ["<d><p><s>a unified framework for high-dimensional analysis of </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other.</s> <s>however, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving classification of many parts of the scene ambiguous.</s> <s>in this work, we propose a hierarchical region-based approach to joint object detection and image segmentation.</s> <s>our approach reasons about pixels, regions and objects in a coherent probabilistic model.</s> <s>importantly, our model gives a single unified description of the scene.</s> <s>we explain every pixel in the image and enforce global consistency between all variables in our model.</s> <s>we run experiments on challenging vision datasets and show significant improvement over state-of-the-art object detection accuracy.</s></p></d>", "label": ["<d><p><s>region-based segmentation and object detection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>policy gradient reinforcement learning (rl) algorithms have received much attention in seeking stochastic policies that maximize the average rewards.</s> <s>in addition, extensions based on the concept of the natural gradient (ng) show promising learning efficiency because these regard metrics for the task.</s> <s>though there are two candidate metrics, kakades fisher information matrix (fim) and morimuras fim, all rl algorithms with ng have followed the kakades approach.</s> <s>in this paper, we describe a generalized natural gradient (gng) by linearly interpolating the two fims and propose an efficient implementation for the gng learning based on a theory of the estimating function, generalized natural actor-critic (gnac).</s> <s>the gnac algorithm involves a near optimal auxiliary function to reduce the variance of the gng estimates.</s> <s>interestingly, the gnac can be regarded as a natural extension of the current state-of-the-art nac algorithm, as long as the interpolating parameter is appropriately selected.</s> <s>numerical experiments showed that the proposed gnac algorithm can estimate gng efficiently and outperformed the nac algorithm.</s></p></d>", "label": ["<d><p><s>a generalized natural actor-critic algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new perspective on approximations to the maximum a posteriori (map) task in probabilistic graphical models, that is based on simplifying a given instance, and then tightening the approximation.</s> <s>first, we start with a structural relaxation of the original model.</s> <s>we then infer from the relaxation its deficiencies, and compensate for them.</s> <s>this perspective allows us to identify two distinct classes of approximations.</s> <s>first, we find that max-product belief propagation can be viewed as a way to compensate for a relaxation, based on a particular idealized case for exactness.</s> <s>we identify a second approach to compensation that is based on a more refined idealized case, resulting in a new approximation with distinct properties.</s> <s>we go on to propose a new class of algorithms that, starting with a relaxation, iteratively yields tighter approximations.</s></p></d>", "label": ["<d><p><s>approximating map by compensating for structural relaxations</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>it was recently shown that certain nonparametric regressors can escape the curse of dimensionality in the sense that their convergence rates adapt to the intrinsic dimension of data (\\cite{bl:65, sk:77}).</s> <s>we prove some stronger results in more general settings.</s> <s>in particular, we consider a regressor which, by combining aspects of both tree-based regression and kernel regression, operates on a general metric space, yields a smooth function, and evaluates in time $o(\\log n)$.</s> <s>we derive a tight convergence rate of the form $n^{-2/(2+d)}$ where $d$ is the assouad dimension of the input space.</s></p></d>", "label": ["<d><p><s>fast, smooth and adaptive regression in metric spaces</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>stochastic neighbor embedding (sne) has shown to be quite promising for data visualization.</s> <s>currently, the most popular implementation, t-sne, is restricted to a particular student t-distribution as its embedding distribution.</s> <s>moreover, it uses a gradient descent algorithm that may require users to tune parameters such as the learning step size, momentum, etc., in finding its optimum.</s> <s>in this paper, we propose the heavy-tailed symmetric stochastic neighbor embedding (hssne) method, which is a generalization of the t-sne to accommodate various heavy-tailed embedding similarity functions.</s> <s>with this generalization, we are presented with two difficulties.</s> <s>the first is how to select the best embedding similarity among all heavy-tailed functions and the second is how to optimize the objective function once the heave-tailed function has been selected.</s> <s>our contributions then are: (1) we point out that various heavy-tailed embedding similarities can be characterized by their negative score functions.</s> <s>based on this finding, we present a parameterized subset of similarity functions for choosing the best tail-heaviness for hssne; (2) we present a fixed-point optimization algorithm that can be applied to all heavy-tailed functions and does not require the user to set any parameters; and (3) we present two empirical studies, one for unsupervised visualization showing that our optimization algorithm runs as fast and as good as the best known t-sne implementation and the other for semi-supervised visualization showing quantitative superiority using the homogeneity measure as well as qualitative advantage in cluster separation over t-sne.</s></p></d>", "label": ["<d><p><s>heavy-tailed symmetric stochastic neighbor embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose to use rademacher complexity, originally developed in computational learning theory, as a measure of human learning capacity.</s> <s>rademacher complexity measures a learners ability to fit random data, and can be used to bound the learners true error based on the observed training sample error.</s> <s>we first review the definition of rademacher complexity and its generalization bound.</s> <s>we then describe a learning the noise\" procedure to experimentally measure human rademacher complexities.</s> <s>the results from empirical studies showed that: (i) human rademacher complexity can be successfully measured, (ii) the complexity depends on the domain and training sample size in intuitive ways, (iii) human learning respects the generalization bounds, (iv) the bounds can be useful in predicting the danger of overfitting in human learning.</s> <s>finally, we discuss the potential applications of human rademacher complexity in cognitive science.\"</s></p></d>", "label": ["<d><p><s>human rademacher complexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the central problems in neuroscience is reconstructing synaptic connectivity in neural circuits.</s> <s>synapses onto a neuron can be probed by sequentially stimulating potentially pre-synaptic neurons while monitoring the membrane voltage of the post-synaptic neuron.</s> <s>reconstructing a large neural circuit using such a ?brute force?</s> <s>approach is rather time-consuming and inefficient because the connectivity in neural circuits is sparse.</s> <s>instead, we propose to measure a post-synaptic neuron?s voltage while stimulating simultaneously multiple randomly chosen potentially pre-synaptic neurons.</s> <s>to extract the weights of individual synaptic connections we apply a decoding algorithm recently developed for compressive sensing.</s> <s>compared to the brute force approach, our method promises significant time savings that grow with the size of the circuit.</s> <s>we use computer simulations to find optimal stimulation parameters and explore the feasibility of our reconstruction method under realistic experimental conditions including noise and non-linear synaptic integration.</s> <s>multiple-neuron stimulation allows reconstructing synaptic connectivity just from the spiking activity of post-synaptic neurons, even when sub-threshold voltage is unavailable.</s> <s>by using calcium indicators, voltage-sensitive dyes, or multi-electrode arrays one could monitor activity of multiple post-synaptic neurons simultaneously, thus mapping their synaptic inputs in parallel, potentially reconstructing a complete neural circuit.</s></p></d>", "label": ["<d><p><s>reconstruction of sparse circuits using multi-neuronal excitation (rescume)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services.</s> <s>in these services, since users can attach annotations freely, some annotations do not describe the semantics of the content, thus they are noisy,  i.e.</s> <s>not content-related.</s> <s>the extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition, or can improve information retrieval performance.</s> <s>the proposed model is a generative model for content and annotations, in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content.</s> <s>we demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.</s></p></d>", "label": ["<d><p><s>modeling social annotation data with content relevance using a topic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the proposal that cortical activity in the visual cortex is optimized for sparse neural activity is one of the most established ideas in computational neuroscience.</s> <s>however, direct experimental evidence for optimal sparse coding remains inconclusive, mostly due to the lack of reference values on which to judge the measured sparseness.</s> <s>here we analyze neural responses to natural movies in the primary visual cortex of ferrets at different stages of development, and of rats while awake and under different levels of anesthesia.</s> <s>in contrast with prediction from a sparse coding model, our data shows that population and lifetime sparseness decrease with visual experience, and increase from the awake to anesthetized state.</s> <s>these results suggest that the representation in the primary visual cortex is not actively optimized to maximize sparseness.</s></p></d>", "label": ["<d><p><s>no evidence for active sparsification in the visual cortex</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>representing distributions over permutations can be a daunting task due to the fact that the number of permutations of n objects scales factorially in n. one recent way that has been used to reduce storage complexity has been to exploit probabilistic independence, but as we argue, full independence assumptions impose strong sparsity constraints on distributions and are unsuitable for modeling rankings.</s> <s>we identify a novel class of independence structures, called riffled independence, which encompasses a more expressive family of distributions while retaining many of the properties necessary for performing efficient inference and reducing sample complexity.</s> <s>in riffled independence, one draws two permutations independently, then performs the riffle shuffle, common in card games, to combine the two permutations to form a single permutation.</s> <s>in ranking, riffled independence corresponds to ranking disjoint sets of objects independently, then interleaving those rankings.</s> <s>we provide a formal introduction and present algorithms for using riffled independence within fourier-theoretic frameworks which have been explored by a number of recent papers.</s></p></d>", "label": ["<d><p><s>riffled independence for ranked data</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>humans are typically able to infer how many objects their environment contains and to recognize when the same object is encountered twice.</s> <s>we present a simple statistical model that helps to explain these abilities and evaluate it in three behavioral experiments.</s> <s>our first experiment suggests that humans rely on prior knowledge when deciding whether an object token has been previously encountered.</s> <s>our second and third experiments suggest that humans can infer how many objects they have seen and can learn about categories and their properties even when they are uncertain about which tokens are instances of the same object.</s></p></d>", "label": ["<d><p><s>individuation, identification and object discovery</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>given a matrix m of low-rank, we consider the problem of reconstructing it from noisy observations of a small, random subset of its entries.</s> <s>the problem arises in a variety of applications, from collaborative filtering (the ?netflix problem?)</s> <s>to structure-from-motion and positioning.</s> <s>we study a low complexity algorithm introduced in [1], based on a combination of spectral techniques and manifold optimization, that we call here optspace.</s> <s>we prove performance guarantees that are order-optimal in a number of circumstances.</s></p></d>", "label": ["<d><p><s>matrix completion from noisy entries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one crucial assumption made by both principal component analysis (pca) and probabilistic pca (ppca) is that the instances are independent and identically distributed (i.i.d.).</s> <s>however, this common i.i.d.</s> <s>assumption is unreasonable for relational data.</s> <s>in this paper, by explicitly modeling covariance between instances as derived from the relational information, we propose a novel probabilistic dimensionality reduction method, called probabilistic relational pca (prpca), for relational data analysis.</s> <s>although the i.i.d.</s> <s>assumption is no longer adopted in prpca, the learning algorithms for prpca can still be devised easily like those for ppca which makes explicit use of the i.i.d.</s> <s>assumption.</s> <s>experiments on real-world data sets show that prpca can effectively utilize the relational information to dramatically outperform pca and achieve state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>probabilistic relational pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new approach to the analysis of loopy belief propagation (lbp) by establishing a formula that connects the hessian of the bethe free energy with the edge zeta function.</s> <s>the formula has a number of theoretical implications on lbp.</s> <s>it is applied to give a sufficient condition that the hessian of the bethe free energy is positive definite, which shows non-convexity for graphs with multiple cycles.</s> <s>the formula clarifies the relation between the local stability of a fixed point of lbp and local minima of the bethe free energy.</s> <s>we also propose a new approach to the uniqueness of lbp fixed point, and show various conditions of uniqueness.</s></p></d>", "label": ["<d><p><s>graph zeta function in the bethe free energy and loopy belief propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the partially observable markov decision process (pomdp) framework has proven useful in planning domains that require balancing actions that increase an agents knowledge and actions that increase an agents reward.</s> <s>unfortunately, most pomdps are complex structures with a large number of parameters.</s> <s>in many realworld problems, both the structure and the parameters are difficult to specify from domain knowledge alone.</s> <s>recent work in bayesian reinforcement learning has made headway in learning pomdp models; however, this work has largely focused on learning the parameters of the pomdp model.</s> <s>we define an infinite pomdp (ipomdp) model that does not require knowledge of the size of the state space; instead, it assumes that the number of visited states will grow as the agent explores its world and explicitly models only visited states.</s> <s>we demonstrate the ipomdp utility on several standard problems.</s></p></d>", "label": ["<d><p><s>the infinite partially observable markov decision process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>search engines today present results that are often oblivious to recent shifts in intent.</s> <s>for example, the meaning of the query independence day shifts in early july to a us holiday and to a movie around the time of the box office release.</s> <s>while no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 1/2 the search queries.</s> <s>this paper shows that the signals a search engine receives can be used to both determine that a shift in intent happened, as well as find a result that is now more relevant.</s> <s>we present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions.</s> <s>we provide strong evidence that this regret is close to the best achievable.</s> <s>finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.</s></p></d>", "label": ["<d><p><s>adapting to the shifting intent of search queries</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated.</s> <s>human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection.</s> <s>understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging.</s> <s>here we propose a simple mechanism for bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus.</s> <s>this mechanism is based on a monte carlo method known as importance sampling, commonly used in computer science and statistics.</s> <s>moreover, a simple extension to recursive importance sampling can be used to perform hierarchical bayesian inference.</s> <s>we identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and oblique effect.</s></p></d>", "label": ["<d><p><s>neural implementation of hierarchical bayesian inference by importance sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a general bayesian approach to probabilistic matrix factorization subject to linear constraints.</s> <s>the approach is based on a gaussian observation model and gaussian priors with bilinear equality and inequality constraints.</s> <s>we present an efficient markov chain monte carlo inference procedure based on gibbs sampling.</s> <s>special cases of the proposed model are bayesian formulations of non-negative matrix factorization and factor analysis.</s> <s>the method is evaluated on a blind source separation problem.</s> <s>we demonstrate that our algorithm can be used to extract meaningful and interpretable features that are remarkably different from features extracted using existing related matrix factorization techniques.</s></p></d>", "label": ["<d><p><s>linearly constrained bayesian matrix factorization for blind source separation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>orthogonal matching pursuit (omp) is a widely used greedy algorithm for recovering sparse vectors from linear measurements.</s> <s>a well-known analysis of tropp and gilbert shows that omp can recover a k-sparse n-dimensional real vector from m = 4k log(n) noise-free random linear measurements with a probability that goes to one as n goes to infinity.</s> <s>this work shows strengthens this result by showing that a lower number of measurements, m = 2k log(n-k), is in fact sufficient for asymptotic recovery.</s> <s>moreover, this number of measurements is also sufficient for detection of the sparsity pattern (support) of the vector with measurement errors provided the signal-to-noise ratio (snr) scales to infinity.</s> <s>the scaling m = 2k log(n-k) exactly matches the number of measurements required by the more complex lasso for signal recovery.</s></p></d>", "label": ["<d><p><s>orthogonal matching pursuit from noisy random measurements: a new analysis</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>contrast statistics of the majority of natural images conform to a weibull distribution.</s> <s>this property of natural images may facilitate efficient and very rapid extraction of a scenes visual gist.</s> <s>here we investigate whether a neural response model based on the weibull contrast distribution captures visual information that humans use to rapidly identify natural scenes.</s> <s>in a learning phase, we measure eeg activity  of 32 subjects viewing brief flashes of 800 natural scenes.</s> <s>from these neural measurements and the contrast statistics of the natural image stimuli,  we  derive an across subject  weibull response model.</s> <s>we use this model to predict the responses to a large set of new scenes and estimate which scene the subject viewed by finding the best match between the model predictions and the observed eeg responses.</s> <s>in almost 90 percent of the cases our model accurately predicts the observed scene.</s> <s>moreover, in most failed cases, the scene mistaken for the observed scene is visually similar to the observed scene itself.</s> <s>these results suggest that weibull contrast statistics of natural images contain a considerable amount of scene gist information to warrant rapid identification of natural images.</s></p></d>", "label": ["<d><p><s>a biologically plausible model for rapid natural scene identification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide some insights into how task correlations in multi-task gaussian process (gp) regression affect the generalization error and the learning curve.</s> <s>we analyze the asymmetric two-task case, where a secondary task is to help the learning of a primary task.</s> <s>within this setting, we give bounds on the generalization error and the learning curve of the primary task.</s> <s>our approach admits intuitive understandings of the multi-task gp by relating it to single-task gps.</s> <s>for the case of one-dimensional input-space under optimal sampling with data only for the secondary task, the limitations of multi-task gp can be quantified explicitly.</s></p></d>", "label": ["<d><p><s>generalization errors and learning curves for regression with multi-task gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in most online learning algorithms, the weights assigned to the misclassified examples (or support vectors) remain unchanged during the entire learning process.</s> <s>this is clearly insufficient since when a new misclassified example is added to the pool of support vectors, we generally expect it to affect the weights for the existing support vectors.</s> <s>in this paper, we propose a new online learning method, termed double updating online learning\", or \"duol\" for short.</s> <s>instead of only assigning a fixed weight to the misclassified example received in current trial, the proposed online learning algorithm also tries to update the weight for one of the existing support vectors.</s> <s>we show that the mistake bound can be significantly improved by the proposed online learning method.</s> <s>encouraging experimental results show that the proposed technique is in general considerably more effective than the state-of-the-art online learning algorithms.\"</s></p></d>", "label": ["<d><p><s>duol: a double updating approach for online learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the recent emergence of graphics processing units (gpus) as general-purpose parallel computing devices provides us with new opportunities to develop scalable learning methods for massive data.</s> <s>in this work, we consider the problem of parallelizing two inference methods on gpus for latent dirichlet allocation (lda) models, collapsed gibbs sampling (cgs) and collapsed variational bayesian (cvb).</s> <s>to address limited memory constraints on gpus, we propose a novel data partitioning scheme that effectively reduces the memory cost.</s> <s>furthermore, the partitioning scheme balances the computational cost on each multiprocessor and enables us to easily avoid memory access conflicts.</s> <s>we also use data streaming to handle extremely large datasets.</s> <s>extensive experiments showed that our parallel inference methods consistently produced lda models with the same predictive power as sequential training methods did but with 26x speedup for cgs and 196x speedup for cvb on a gpu with 30 multiprocessors; actually the speedup is almost linearly scalable with the number of multiprocessors available.</s> <s>the proposed partitioning scheme and data streaming can be easily ported to many other models in machine learning.</s></p></d>", "label": ["<d><p><s>parallel inference for latent dirichlet allocation on graphics processing units</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe an algorithm for learning bilinear svms.</s> <s>bilinear classifiers are a discriminative variant of bilinear models, which capture the dependence of data on multiple factors.</s> <s>such models are particularly appropriate for visual data that is better represented as a matrix or tensor, rather than a vector.</s> <s>matrix encodings allow for more natural regularization through rank restriction.</s> <s>for example, a rank-one scanning-window classifier yields a separable filter.</s> <s>low-rank models have fewer parameters and so are easier to regularize and faster to score at run-time.</s> <s>we learn low-rank models with bilinear classifiers.</s> <s>we also use bilinear classifiers for transfer learning by sharing linear factors between different classification tasks.</s> <s>bilinear classifiers are trained with biconvex programs.</s> <s>such programs are optimized with coordinate descent, where each coordinate step requires solving a convex program - in our case, we use a standard off-the-shelf svm solver.</s> <s>we demonstrate bilinear svms on difficult problems of people detection in video sequences and action classification of video sequences, achieving state-of-the-art results in both.</s></p></d>", "label": ["<d><p><s>bilinear classifiers for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many computer vision applications, the ideal image feature would be invariant to multiple confounding image properties, such as illumination and viewing angle.</s> <s>recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features.</s> <s>however, outside of using these learning algorithms in a classi?er, they can be sometimes dif?cult to evaluate.</s> <s>in this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different image transforms.</s> <s>we ?nd that deep autoencoders become invariant to increasingly complex image transformations with depth.</s> <s>this further justi?es the use of ?deep?</s> <s>vs. ?shallower?</s> <s>representations.</s> <s>our performance metrics agree with existing measures of invariance.</s> <s>our evaluation metrics can also be used to evaluate future work in unsupervised deep learning, and thus help the development of future algorithms.</s></p></d>", "label": ["<d><p><s>measuring invariances in deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a method for learning a group of continuous transformation operators  to traverse smooth nonlinear manifolds.</s> <s>the method is applied to model how  natural images change over time and scale.</s> <s>the group of continuous transform  operators is represented by a basis that is adapted to the statistics of the data so  that the in?nitesimal generator for a measurement orbit can be produced by a  linear combination of a few basis elements.</s> <s>we illustrate how the method can be  used to ef?ciently code time-varying images by describing changes across time  and scale in terms of the learned operators.</s></p></d>", "label": ["<d><p><s>learning transport operators for image manifolds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects.</s> <s>the model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions.</s> <s>given a failing test case, the model determines the least likely transitional pattern in the execution trace.</s> <s>the model is designed such that bayesian inference has a closed-form solution.</s> <s>we evaluate the  bernoulli graph model on data of the software projects aspectj and rhino.</s></p></d>", "label": ["<d><p><s>localizing bugs in program executions with graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe, analyze, and experiment with a new framework for empirical loss minimization with regularization.</s> <s>our algorithmic framework alternates between two phases.</s> <s>on each iteration we first perform an {\\em unconstrained} gradient descent step.</s> <s>we then cast and solve an instantaneous optimization problem that trades off minimization of a regularization term while keeping close proximity to the result of the first phase.</s> <s>this yields a simple yet effective algorithm for both batch penalized risk minimization and online learning.</s> <s>furthermore, the two phase approach enables sparse solutions when used in conjunction with regularization functions that promote sparsity, such as $\\ell_1$.</s> <s>we derive concrete and very simple algorithms for minimization of loss functions with $\\ell_1$, $\\ell_2$, $\\ell_2^2$, and $\\ell_\\infty$ regularization.</s> <s>we also show how to construct efficient algorithms for mixed-norm $\\ell_1/\\ell_q$ regularization.</s> <s>we further extend the algorithms and give efficient implementations for very high-dimensional data with sparsity.</s> <s>we demonstrate the potential of the proposed framework in experiments with synthetic and natural datasets.</s></p></d>", "label": ["<d><p><s>efficient learning using forward-backward splitting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a central hypothesis about early visual processing is that it represents inputs in a coordinate system matched to the statistics of natural scenes.</s> <s>simple versions of this lead to gabor-like receptive fields and divisive gain modulation from local surrounds; these have led to influential neural and psychological models of visual processing.</s> <s>however, these accounts are based on an incomplete view of the visual context surrounding each point.</s> <s>here, we consider an approximate model of linear and non-linear correlations between the responses of spatially distributed gabor-like receptive fields, which, when trained on an ensemble of natural scenes, unifies a range of spatial context effects.</s> <s>the full model accounts for neural surround data in primary visual cortex (v1), provides a statistical foundation for perceptual phenomena associated with lis (2002) hypothesis that v1 builds a saliency map, and fits data on the tilt illusion.</s></p></d>", "label": ["<d><p><s>statistical models of linear and nonlinear contextual interactions in early visual processing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in practice, most investing is done assuming a probabilistic model of stock price returns known as the geometric brownian motion (gbm).</s> <s>while it is often an acceptable approximation, the gbm model is not always valid empirically.</s> <s>this motivates a worst-case approach to investing, called universal portfolio management, where the objective is to maximize wealth relative to the wealth earned by the best fixed portfolio in hindsight.</s> <s>in this paper we tie the two approaches, and design an investment strategy which is universal in the worst-case, and yet capable of exploiting the mostly valid gbm model.</s> <s>our method is based on new and improved regret bounds for online convex optimization with exp-concave loss functions.</s></p></d>", "label": ["<d><p><s>on stochastic and worst-case models for investing</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>several key computational bottlenecks in machine learning involve pairwise distance computations, including all-nearest-neighbors (finding the nearest neighbor(s) for each point, e.g.</s> <s>in manifold learning) and kernel summations (e.g.</s> <s>in kernel density estimation or kernel machines).</s> <s>we consider the general, bichromatic case for these problems, in addition to the scientific problem of n-body potential calculation.</s> <s>in this paper we show for the first time o(n) worst case runtimes for practical algorithms for these problems based on the cover tree data structure (beygelzimer, kakade, langford, 2006).</s></p></d>", "label": ["<d><p><s>linear-time algorithms for pairwise statistical problems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this study, we present a method for estimating the mutual information for a localized pattern of fmri data.</s> <s>we show that taking a multivariate information approach to voxel selection leads to a decoding accuracy that surpasses an univariate inforamtion approach and other standard voxel selection methods.</s> <s>furthermore,we extend the multivariate mutual information theory to measure the functional connectivity between distributed brain regions.</s> <s>by jointly estimating the information shared by two sets of voxels we can reliably map out the connectivities in the human brain during experiment conditions.</s> <s>we validated our approach on a 6-way scene categorization fmri experiment.</s> <s>the multivariate information analysis is able to ?nd strong information ?ow between ppa and rsc, which con?rms existing neuroscience studies on scenes.</s> <s>furthermore, by exploring over the whole brain, our method identifies other interesting rois that share information with the ppa, rsc scene network,suggesting interesting future work for neuroscientists.</s></p></d>", "label": ["<d><p><s>exploring functional connectivities of the human brain using multivariate information analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability.</s> <s>specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time --- but are complex to train.</s> <s>conveniently, sequential learning of categories cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the richness of shareability and might depend on ordering in learning.</s> <s>in hierarchical frameworks these issues have been little explored.</s> <s>in this paper, we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows.</s> <s>specifically, we propose, evaluate and compare three important types of multi-class learning: 1.)</s> <s>independent training of individual categories, 2.)</s> <s>joint training of classes, 3.)</s> <s>sequential learning of classes.</s> <s>we explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets.</s></p></d>", "label": ["<d><p><s>evaluating multi-class learning strategies in a generative hierarchical framework for object detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel linear program for the approximation of the dynamic programming cost-to-go function in high-dimensional stochastic control problems.</s> <s>lp approaches to approximate dp naturally restrict attention to approximations that are lower bounds to the optimal cost-to-go function.</s> <s>our program -- the `smoothed approximate linear program -- relaxes this restriction in an appropriate fashion while remaining computationally tractable.</s> <s>doing so appears to have several advantages: first, we demonstrate superior bounds on the quality of approximation to the optimal cost-to-go function afforded by our approach.</s> <s>second, experiments with our approach on a challenging problem (the game of tetris) show that the approach outperforms the existing lp approach (which has previously been shown to be competitive with several adp algorithms) by an order of magnitude.</s></p></d>", "label": ["<d><p><s>a smoothed approximate linear program</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for learning max-weight matching predictors in bipartite graphs.</s> <s>the method consists of performing maximum a posteriori estimation in exponential families with sufficient statistics that encode permutations and data features.</s> <s>although inference is in general hard, we show that for one very relevant application - document ranking - exact inference is efficient.</s> <s>for general model instances, an appropriate sampler is readily available.</s> <s>contrary to existing max-margin matching models, our approach is statistically consistent and, in addition, experiments with increasing sample sizes indicate superior improvement over such models.</s> <s>we apply the method to graph matching in computer vision as well as to a standard benchmark dataset for learning document ranking, in which we obtain state-of-the-art results, in particular improving on max-margin variants.</s> <s>the drawback of this method with respect to max-margin alternatives is its runtime for large graphs, which is high comparatively.</s></p></d>", "label": ["<d><p><s>exponential family graph matching and ranking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>locality information is crucial in datasets where each variable corresponds to a measurement in a manifold (silhouettes, motion trajectories, 2d and 3d images).</s> <s>although these datasets are typically under-sampled and high-dimensional, they often need to be represented with low-complexity statistical models, which are comprised of only the important probabilistic dependencies in the datasets.</s> <s>most methods attempt to reduce model complexity by enforcing structure sparseness.</s> <s>however, sparseness cannot describe inherent regularities in the structure.</s> <s>hence, in this paper we first propose a new class of gaussian graphical models which, together with sparseness, imposes local constancy through ${\\ell}_1$-norm penalization.</s> <s>second, we propose an efficient algorithm which decomposes the strictly convex maximum likelihood estimation into a sequence of problems with closed form solutions.</s> <s>through synthetic experiments, we evaluate the closeness of the recovered models to the ground truth.</s> <s>we also test the generalization performance of our method in a wide range of complex real-world datasets and demonstrate that it can capture useful structures such as the rotation and shrinking of a beating heart, motion correlations between body parts during walking and functional interactions of brain regions.</s> <s>our method outperforms the state-of-the-art structure learning techniques for gaussian graphical models both for small and large datasets.</s></p></d>", "label": ["<d><p><s>sparse and locally constant gaussian graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>speaker comparison, the process of finding the speaker similarity between two speech signals, occupies a central role in a variety of applications---speaker verification, clustering, and identification.</s> <s>speaker comparison can be placed in a geometric framework by casting the problem as a model comparison process.</s> <s>for a given speech signal, feature vectors are produced and used to adapt a gaussian mixture model (gmm).</s> <s>speaker comparison can then be viewed as the process of compensating and finding metrics on the space of adapted models.</s> <s>we propose a framework, inner product discriminant functions (ipdfs), which extends many common techniques for speaker comparison: support vector machines, joint factor analysis, and linear scoring.</s> <s>the framework uses inner products between the parameter vectors of gmm models motivated by several statistical methods.</s> <s>compensation of nuisances is performed via linear transforms on gmm parameter vectors.</s> <s>using the ipdf framework, we show that many current techniques are simple variations of each other.</s> <s>we demonstrate, on a 2006 nist speaker recognition evaluation task, new scoring methods using ipdfs which produce excellent error rates and require significantly less computation than current techniques.</s></p></d>", "label": ["<d><p><s>speaker comparison with inner product discriminant functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we are often interested in casting classification and clustering problems in a regression framework, because it is feasible to achieve some statistical properties in this framework by imposing some penalty criteria.</s> <s>in this paper we illustrate optimal scoring, which was originally proposed for performing fisher linear discriminant analysis by regression, in the application of unsupervised learning.</s> <s>in particular, we devise a novel clustering algorithm that we call optimal discriminant clustering (odc).</s> <s>we associate our algorithm with the existing unsupervised learning algorithms such as spectral clustering, discriminative clustering and sparse principal component analysis.</s> <s>thus, our work shows that optimal scoring  provides a new approach to the implementation of  unsupervised learning.</s> <s>this approach facilitates the development of new unsupervised learning algorithms.</s></p></d>", "label": ["<d><p><s>optimal scoring for unsupervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a multiple incremental decremental algorithm  of support vector machine (svm).</s> <s>conventional single  cremental decremental svm can update the trained model  efficiently when single data point is added to or removed  from the training set.</s> <s>when we add and/or remove multiple  data points, this algorithm is time-consuming because we  need to repeatedly apply it to each data point.</s> <s>the roposed  algorithm is computationally more efficient when multiple  data points are added and/or removed simultaneously.</s> <s>the  single incremental decremental algorithm is built on an  optimization technique called parametric programming.</s> <s>we extend the idea and introduce multi-parametric  programming for developing the proposed algorithm.</s> <s>experimental results on synthetic and real data sets indicate that the proposed algorithm can significantly  reduce the computational cost of multiple incremental  decremental operation.</s> <s>our approach is especially useful  for online svm learning in which we need to remove old  data points and add new data points in a short amount of  time.</s></p></d>", "label": ["<d><p><s>multiple incremental decremental learning of support vector machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a probabilistic latent factor model which can be used for studying spatio-temporal datasets.</s> <s>the spatial and temporal structure is modeled by using gaussian process priors both for the loading matrix and the factors.</s> <s>the posterior distributions are approximated using the variational bayesian framework.</s> <s>high computational cost of gaussian process modeling is reduced by using sparse approximations.</s> <s>the model is used to compute the reconstructions of the global sea surface temperatures from a historical dataset.</s> <s>the results suggest that the proposed model can outperform the state-of-the-art reconstruction systems.</s></p></d>", "label": ["<d><p><s>variational gaussian-process factor analysis for modeling spatio-temporal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the gaussian process regression the observation model is commonly assumed to be gaussian, which is convenient in computational perspective.</s> <s>however, the drawback is that the predictive accuracy of the model can be significantly compromised if the observations are contaminated by outliers.</s> <s>a robust observation model, such as the student-t distribution, reduces the influence of outlying observations and improves the predictions.</s> <s>the problem, however, is the analytically intractable inference.</s> <s>in this work, we discuss the properties of a gaussian process regression model with the student-t likelihood and utilize the laplace approximation for approximate inference.</s> <s>we compare our approach to a variational approximation and a markov chain monte carlo scheme, which utilize the commonly used scale mixture representation of the student-t distribution.</s></p></d>", "label": ["<d><p><s>gaussian process regression with student-t likelihood</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formulate and address the problem of discovering dynamic malicious regions on the internet.</s> <s>we model this problem as one of adaptively pruning a known decision tree, but with additional challenges: (1) severe space requirements, since the underlying decision tree has over 4 billion leaves, and (2) a changing target function, since malicious activity on the internet is dynamic.</s> <s>we present a novel algorithm that addresses this problem, by putting together a number of different ``experts algorithms and online paging algorithms.</s> <s>we prove guarantees on our algorithms performance as a function of the best possible pruning of a similar size, and our experiments show that our algorithm achieves high accuracy on large real-world data sets, with significant improvements over existing approaches.</s></p></d>", "label": ["<d><p><s>tracking dynamic sources of malicious activity at internet scale</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>existing methods for recognition of object instances and categories based on quantized local features can perform poorly when local features exist on transparent surfaces, such as glass or plastic objects.</s> <s>there are characteristic patterns to the local appearance of transparent objects, but they may not be well captured by distances to individual examples or by a local pattern codebook obtained by vector quantization.</s> <s>the appearance of a transparent patch is determined in part by the refraction of a background pattern through a transparent medium: the energy from the background usually dominates the patch appearance.</s> <s>we model transparent local patch appearance using an additive  model of latent factors: background factors due to scene content,  and factors which capture a local edge energy distribution characteristic of the refraction.</s> <s>we implement our method using a novel lda-sift formulation which performs lda prior to any vector quantization step; we discover latent topics which are characteristic of particular transparent patches and quantize the sift space into transparent visual words according to the latent topic dimensions.</s> <s>no knowledge of the background scene is required at test time; we show examples recognizing transparent glasses in a domestic environment.</s></p></d>", "label": ["<d><p><s>an additive latent feature model for transparent object recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the first  temporal-difference  learning algorithms that converge  with smooth value function approximators, such as neural networks.</s> <s>conventional temporal-difference (td) methods, such as td($\\lambda$), q-learning and sarsa have been used successfully with function approximation in many applications.</s> <s>however, it is well known that off-policy sampling, as well as nonlinear function approximation, can cause these algorithms to become unstable (i.e., the parameters of the approximator may diverge).</s> <s>sutton et al (2009a,b) solved the problem of off-policy learning with linear td algorithms by introducing a new objective function, related to the bellman-error, and algorithms that perform stochastic gradient-descent on this function.</s> <s>in this paper, we generalize their work to nonlinear function approximation.</s> <s>we present a bellman error objective function and two gradient-descent td algorithms that optimize it.</s> <s>we prove the  asymptotic almost-sure convergence  of both algorithms for any finite markov decision process and any smooth value function approximator, under usual stochastic approximation conditions.</s> <s>the computational complexity per iteration scales linearly with the number of parameters of the approximator.</s> <s>the algorithms are incremental and are guaranteed to converge to locally optimal solutions.</s></p></d>", "label": ["<d><p><s>convergent temporal-difference learning with arbitrary smooth function approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the question of computing maximum a posteriori (map) assignment in an arbitrary pair-wise markov random field (mrf).</s> <s>we present a randomized iterative algorithm based on simple local updates.</s> <s>the algorithm, starting with an arbitrary initial assignment, updates it in each iteration by first, picking a random node, then selecting an (appropriately chosen) random local neighborhood and optimizing over this local neighborhood.</s> <s>somewhat surprisingly, we show that this algorithm finds a near optimal assignment within $2n\\ln n$ iterations on average and with high probability for {\\em any} $n$ node pair-wise mrf with {\\em geometry} (i.e.</s> <s>mrf graph with polynomial growth) with the approximation error depending on (in a reasonable manner) the geometric growth rate of the graph and the average radius of the local neighborhood -- this allows for a graceful tradeoff between the complexity of the algorithm and the approximation error.</s> <s>through extensive simulations, we show that our algorithm finds extremely good approximate solutions for various kinds of mrfs with geometry.</s></p></d>", "label": ["<d><p><s>local rules for global map: when do they work ?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove that linear projections between distribution families with fixed first and second moments are surjective, regardless of dimension.</s> <s>we further extend this result to families that respect additional constraints, such as symmetry, unimodality and log-concavity.</s> <s>by combining our results with classic univariate inequalities, we provide new worst-case analyses for natural risk criteria arising in different fields.</s> <s>one discovery is that portfolio selection under the worst-case value-at-risk and conditional value-at-risk criteria yields identical portfolios.</s></p></d>", "label": ["<d><p><s>a general projection property for distribution families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting.</s> <s>we make no assumptions about the data, and  our algorithm is very light-weight in terms of memory, and computation.</s> <s>this setting is applicable to unsupervised learning on massive data sets, or resource-constrained devices.</s> <s>the two main ingredients of our theoretical work are:  a derivation of an extremely simple pseudo-approximation batch algorithm for k-means, in which the algorithm is allowed to output more than k centers (based on the recent k-means++\"), and a streaming clustering algorithm in which batch clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner.</s> <s>empirical evaluations on real and simulated data reveal the practical utility of our method.\"</s></p></d>", "label": ["<d><p><s>streaming k-means approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a fundamental objective in reinforcement learning is the maintenance of a proper balance between exploration and exploitation.</s> <s>this problem becomes more challenging when the agent can only partially observe the states of its environment.</s> <s>in this paper we propose a dual-policy method for jointly learning the agent behavior and the balance between exploration exploitation, in partially observable environments.</s> <s>the method subsumes traditional exploration, in which the agent takes actions to gather information about the environment, and active learning, in which the agent queries an oracle for optimal actions (with an associated cost for employing the oracle).</s> <s>the form of the employed exploration is dictated by the specific problem.</s> <s>theoretical guarantees are provided concerning the optimality of the balancing of exploration and exploitation.</s> <s>the effectiveness of the method is demonstrated by experimental results on benchmark problems.</s></p></d>", "label": ["<d><p><s>learning to explore and exploit in pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dependencies among neighbouring labels in a sequence is an important source of information for sequence labeling problems.</s> <s>however, only dependencies between adjacent labels are commonly exploited in practice because of the high computational complexity of typical inference algorithms when longer distance dependencies are taken into account.</s> <s>in this paper, we show that it is possible to design efficient inference algorithms for a conditional random field using features that depend on long consecutive label sequences (high-order features), as long as the number of distinct label sequences in the features used is small.</s> <s>this leads to efficient learning algorithms for these conditional random fields.</s> <s>we show experimentally that exploiting dependencies using high-order features can lead to substantial performance improvements for some problems and discuss conditions under which high-order features can be effective.</s></p></d>", "label": ["<d><p><s>conditional random fields with high-order features for sequence labeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many researchers have suggested that the psychological complexity of a concept is related to the length of its representation in a language of thought.</s> <s>as yet, however, there are few concrete proposals about the nature of this language.</s> <s>this paper makes one such proposal: the language of thought allows first order quantification (quantification over objects) more readily than second-order quantification (quantification over features).</s> <s>to support this proposal we present behavioral results from a concept learning study inspired by the work of shepard, hovland and jenkins.\"</s></p></d>", "label": ["<d><p><s>quantification and the language of thought</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>regularized risk minimization often involves non-smooth optimization, either because of the loss function (e.g., hinge loss) or the regularizer (e.g., $\\ell_1$-regularizer).</s> <s>gradient descent methods, though highly scalable and easy to implement, are known to converge slowly on these problems.</s> <s>in this paper, we develop novel accelerated gradient methods for stochastic optimization while still preserving their computational simplicity and scalability.</s> <s>the proposed algorithm, called sage (stochastic accelerated gradient), exhibits fast convergence rates on stochastic optimization with both convex and strongly convex objectives.</s> <s>experimental results show that sage is faster than recent (sub)gradient methods including folos, smidas and scd.</s> <s>moreover, sage can also be extended for online learning, resulting in a simple but powerful algorithm.</s></p></d>", "label": ["<d><p><s>accelerated gradient methods for stochastic optimization and online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study pool-based active learning in the presence of noise, i.e.</s> <s>the agnostic setting.</s> <s>previous works have shown that the effectiveness of agnostic active learning depends on the learning problem and the hypothesis space.</s> <s>although there are many cases on which active learning is very useful, it is also easy to construct examples that no active learning algorithm can have advantage.</s> <s>in this paper, we propose intuitively reasonable sufficient conditions under which agnostic active learning algorithm is strictly superior to passive supervised learning.</s> <s>we show that under some noise condition, if the classification boundary and the underlying distribution are smooth to a finite order, active learning achieves polynomial improvement in the label complexity; if the boundary and the distribution are infinitely smooth, the improvement is exponential.</s></p></d>", "label": ["<d><p><s>sufficient conditions for agnostic active learnable</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of learning the structure of ising models (pairwise binary markov random fields) from i.i.d.</s> <s>samples.</s> <s>while several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure.</s> <s>by analyzing a number of concrete examples, we show that low-complexity  algorithms systematically fail when the markov random field  develops long-range correlations.</s> <s>more precisely, this phenomenon  appears to be related to the ising model phase transition  (although it does not coincide with it).</s></p></d>", "label": ["<d><p><s>which graphical models are difficult to learn?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an approach for learning stochastic geometric models of object categories from single view images.</s> <s>we focus here on models expressible as a spatially contiguous assemblage of blocks.</s> <s>model topologies are learned across groups of images, and one or more such topologies is linked to an object category (e.g.</s> <s>chairs).</s> <s>fitting learned topologies to an image can be used to identify the object class, as well as detail its geometry.</s> <s>the latter goes beyond labeling objects, as it provides the geometric structure of particular instances.</s> <s>we learn the models using joint statistical inference over structure parameters, camera parameters, and instance parameters.</s> <s>these produce an image likelihood through a statistical imaging model.</s> <s>we use trans-dimensional sampling to explore topology hypotheses, and alternate between metropolis-hastings and stochastic dynamics to explore instance parameters.</s> <s>experiments on images of furniture objects such as tables and chairs suggest that this is an effective approach for learning models that encode simple representations of category geometry and the statistics thereof, and support inferring both category and geometry on held out single view images.</s></p></d>", "label": ["<d><p><s>learning models of object structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that convex kl-regularized objective functions are obtained from a pac-bayes risk bound when using convex loss functions for the stochastic gibbs classifier that upper-bound the standard zero-one loss used for the weighted majority vote.</s> <s>by restricting ourselves to a class of posteriors, that we call quasi uniform, we propose a simple coordinate descent learning algorithm to minimize the proposed kl-regularized cost function.</s> <s>we show that standard ell_p-regularized objective functions currently used, such as ridge regression and ell_p-regularized boosting, are obtained from a relaxation of the kl divergence between the quasi uniform posterior and the uniform prior.</s> <s>we present numerical experiments where the proposed learning algorithm generally outperforms ridge regression and adaboost.</s></p></d>", "label": ["<d><p><s>from pac-bayes bounds to kl regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a convex relaxation of maximum a posteriori estimation of a mixture of regression models.</s> <s>although our relaxation involves a semidefinite matrix variable, we reformulate the problem to eliminate the need for general semidefinite programming.</s> <s>in particular, we provide two reformulations that admit fast algorithms.</s> <s>the first is a max-min spectral reformulation exploiting quasi-newton descent.</s> <s>the second is a min-min reformulation consisting of fast alternating steps of closed-form updates.</s> <s>we evaluate the methods against expectation-maximization in a real problem of motion segmentation from video data.</s></p></d>", "label": ["<d><p><s>convex relaxation of mixture regression with efficient algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a new algorithmic framework for inference in probabilistic models, and apply it to inference for latent dirichlet allocation.</s> <s>our framework adopts the methodology of variational inference, but unlike existing variational methods such as mean field and expectation propagation it is not restricted to tractable classes of approximating distributions.</s> <s>our approach can also be viewed as a sequential monte carlo (smc) method, but unlike existing smc methods there is no need to design the artificial sequence of distributions.</s> <s>notably, our framework offers a principled means to exchange the variance of an importance sampling estimate for the bias incurred through variational approximation.</s> <s>experiments on a challenging inference problem in population genetics demonstrate improvements in stability and accuracy over existing methods, and at a comparable cost.</s></p></d>", "label": ["<d><p><s>a stochastic approximation method for inference in probabilistic graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>solving multi-agent reinforcement learning problems has proven difficult because of the lack of tractable algorithms.</s> <s>we provide the first approximation algorithm which solves stochastic games to within $\\epsilon$ relative error of the optimal game-theoretic solution, in time polynomial in $1/\\epsilon$.</s> <s>our algorithm extends murrays and gordon?s (2007) modified bellman equation which determines the \\emph{set} of all possible achievable utilities; this provides us a truly general framework for multi-agent learning.</s> <s>further, we empirically validate our algorithm and find the computational cost to be orders of magnitude less than what the theory predicts.</s></p></d>", "label": ["<d><p><s>solving stochastic games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel density estimation is the most widely-used practical method for accurate nonparametric density estimation.</s> <s>however, long-standing worst-case theoretical results showing that its performance worsens exponentially with the dimension of the data have quashed its application to modern high-dimensional datasets for decades.</s> <s>in practice, it has been recognized that often such data have a much lower-dimensional intrinsic structure.</s> <s>we propose a small modification to kernel density estimation for estimating probability density functions on riemannian submanifolds of euclidean space.</s> <s>using ideas from riemannian geometry, we prove the consistency of this modified estimator and show that the convergence rate is determined by the intrinsic dimension of the submanifold.</s> <s>we conclude with empirical results demonstrating the behavior predicted by our theory.</s></p></d>", "label": ["<d><p><s>submanifold density estimation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper we make several contributions towards accelerating approximate bayesian structural inference for non-decomposable ggms.</s> <s>our first contribution is to show how to efficiently compute a bic or laplace approximation to the marginal likelihood of non-decomposable graphs using convex methods for precision matrix estimation.</s> <s>this optimization technique can be used as a fast scoring function inside standard stochastic local search (sls) for generating posterior samples.</s> <s>our second contribution is a novel framework for efficiently generating large sets of high-quality graph topologies without performing local search.</s> <s>this graph proposal method, which we call neighborhood fusion\" (nf), samples candidate markov blankets at each node using sparse regression techniques.</s> <s>our final contribution is a hybrid method combining the complementary strengths of nf and sls.</s> <s>experimental results in structural recovery and prediction tasks demonstrate that nf and hybrid nf/sls out-perform state-of-the-art local search methods, on both synthetic and real-world datasets, when realistic computational limits are imposed.\"</s></p></d>", "label": ["<d><p><s>accelerating bayesian structural inference for non-decomposable gaussian graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multiple object tracking is a task commonly used to investigate the architecture of human visual attention.</s> <s>human participants show a distinctive pattern of successes and failures in tracking experiments that is often attributed to limits on an object system, a tracking module, or other specialized cognitive structures.</s> <s>here we use a computational analysis of the task of object tracking to ask which human failures arise from cognitive limitations and which are consequences of inevitable perceptual uncertainty in the tracking task.</s> <s>we find that many human performance phenomena, measured through novel behavioral experiments, are naturally produced by the operation of our ideal observer model (a rao-blackwelized particle filter).</s> <s>the tradeoff between the speed and number of objects being tracked, however, can only arise from the allocation of a flexible cognitive resource, which can be formalized as either memory or attention.</s></p></d>", "label": ["<d><p><s>explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a new algorithm for the linear least squares problem where the unknown variables are constrained to be in a finite set.</s> <s>the factor graph that corresponds to this problem is very loopy; in fact, it is a complete graph.</s> <s>hence, applying the belief propagation (bp) algorithm yields very poor results.</s> <s>the algorithm described here is based on  an optimal  tree approximation of the gaussian density of the unconstrained linear system.</s> <s>it is shown that even though the approximation is not directly applied to the exact discrete distribution, applying the bp algorithm to the modified factor graph outperforms current methods in terms of both performance and complexity.</s> <s>the improved performance of the proposed algorithm is demonstrated  on the problem of mimo detection.</s></p></d>", "label": ["<d><p><s>a gaussian tree approximation for integer least-squares</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>score functions induced by generative models extract fixed-dimension feature vectors from different-length data observations by subsuming the process of data generation, projecting them in highly informative spaces called score spaces.</s> <s>in this way, standard discriminative classifiers are proved to achieve higher performances than a solely generative or discriminative approach.</s> <s>in this paper, we present a novel score space that exploits the free energy associated to a generative model through a score function.</s> <s>this function aims at capturing both the uncertainty of the model learning and ``local compliance  of data observations with respect to the generative process.</s> <s>theoretical justifications and convincing comparative classification results on various generative models prove the goodness of the proposed strategy.</s></p></d>", "label": ["<d><p><s>free energy score space</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>second-order maximum-entropy models have recently gained much interest for describing the statistics of binary spike trains.</s> <s>here, we extend this approach to take continuous stimuli into account as well.</s> <s>by constraining  the joint second-order statistics, we obtain a joint gaussian-boltzmann distribution of continuous stimuli and binary neural firing patterns, for which we also compute marginal and conditional distributions.</s> <s>this model has the same computational complexity as pure binary models and fitting it to data is a convex problem.</s> <s>we show that the model can be seen as an extension to the classical spike-triggered average/covariance analysis and can be used as a non-linear method for extracting features which a neural population is sensitive to.</s> <s>further, by calculating the posterior distribution of stimuli given an observed neural response, the model can be used to decode stimuli and yields a natural spike-train metric.</s> <s>therefore, extending the framework of maximum-entropy models to continuous variables allows us to gain novel insights into the relationship between the firing patterns of neural ensembles and the stimuli they are processing.</s></p></d>", "label": ["<d><p><s>a joint maximum-entropy model for binary neural population patterns and continuous signals</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>large, relational factor graphs with structure defined by   first-order logic or other languages give rise to notoriously   difficult inference problems.</s> <s>because unrolling the structure   necessary to represent distributions over all hypotheses has   exponential blow-up, solutions are often derived from mcmc.</s> <s>however,   because of limitations in the design and parameterization of the   jump function, these sampling-based methods suffer from local   minima|the system must transition through lower-scoring   configurations before arriving at a better map solution.</s> <s>this paper   presents a new method of explicitly selecting fruitful downward   jumps by leveraging reinforcement learning (rl).</s> <s>rather than setting   parameters to maximize the likelihood of the training data,   parameters of the factor graph are treated as a log-linear function   approximator and learned with temporal difference (td); map   inference is performed by executing the resulting policy on held out   test data.</s> <s>our method allows efficient gradient updates since only   factors in the neighborhood of variables affected by an action need   to be computed|we bypass the need to compute marginals entirely.</s> <s>our method provides dramatic empirical success, producing new   state-of-the-art results on a complex joint model of ontology   alignment, with a 48\\% reduction in error over state-of-the-art in   that domain.</s></p></d>", "label": ["<d><p><s>training factor graphs with reinforcement learning for efficient map inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the indian buffet process is a bayesian nonparametric approach that models objects as arising from an infinite number of latent factors.</s> <s>here we extend the latent factor model framework to two or more unbounded layers of latent factors.</s> <s>from a generative perspective, each layer defines a conditional \\emph{factorial} prior distribution over the binary latent variables of the layer below via a noisy-or mechanism.</s> <s>we explore the properties of the model with two empirical studies, one digit recognition task and one music tag data experiment.</s></p></d>", "label": ["<d><p><s>an infinite factor model hierarchy via a noisy-or mechanism</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a new model for natural image statistics.</s> <s>instead of minimizing dependency between components of natural images, we maximize a simple form of dependency in the form of tree-dependency.</s> <s>by learning filters and tree structures which are best suited for natural images we observe that the resulting filters are edge filters, similar to the famous ica on natural images results.</s> <s>calculating the likelihood of the model requires estimating the squared output of pairs of filters connected in the tree.</s> <s>we observe that after learning, these pairs of filters are predominantly of similar orientations but different phases, so their joint energy resembles models of complex cells.</s></p></d>", "label": ["<d><p><s>the 'tree-dependent components' of natural scenes are edge filters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a nonparametric hierarchical bayesian model of document collections that decouples sparsity and smoothness in the component distributions (i.e., the ``topics).</s> <s>in the sparse topic model (stm), each topic is represented by a bank of selector variables that determine which terms appear in the topic.</s> <s>thus each topic is associated with a subset of the vocabulary, and topic smoothness is modeled on this subset.</s> <s>we develop an efficient gibbs sampler for the stm that includes a general-purpose method for sampling from a dirichlet mixture with a combinatorial number of components.</s> <s>we demonstrate the stm on four real-world datasets.</s> <s>compared to traditional approaches, the empirical results show that stms give better predictive performance with simpler inferred models.</s></p></d>", "label": ["<d><p><s>decoupling sparsity and smoothness in the discrete hierarchical dirichlet process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to estimate the changing structure of a varying-coefficient   varying-structure (vcvs) model remains an important and open problem   in dynamic system modelling, which includes learning trajectories of   stock prices, or uncovering the topology of an evolving gene   network.</s> <s>in this paper, we investigate sparsistent learning of a   sub-family of this model --- piecewise constant vcvs models.</s> <s>we   analyze two main issues in this problem: inferring time points where   structural changes occur and estimating model structure (i.e., model   selection) on each of the constant segments.</s> <s>we propose a two-stage   adaptive procedure, which first identifies jump points of structural   changes and then identifies relevant covariates to a response on   each of the segments.</s> <s>we provide an asymptotic analysis of the   procedure, showing that with the increasing sample size, number of   structural changes, and number of variables, the true model can be   consistently selected.</s> <s>we demonstrate the performance of the method   on synthetic data and apply it to the brain computer interface   dataset.</s> <s>we also consider how this applies to structure estimation   of time-varying probabilistic graphical models.</s></p></d>", "label": ["<d><p><s>sparsistent learning of varying-coefficient models with structural changes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the method of common spatio-spectral patterns (cssps) is an extension of common spatial patterns (csps) by utilizing the technique of delay embedding to alleviate the adverse effects of noises and artifacts on the electroencephalogram (eeg) classification.</s> <s>although the cssps method has shown to be more powerful than the csps method in the eeg classification, this method is only suitable for two-class eeg classification problems.</s> <s>in this paper, we generalize the two-class cssps method to multi-class cases.</s> <s>to this end, we first develop a novel theory of multi-class bayes error estimation and then present the multi-class cssps (mcssps) method based on this bayes error theoretical framework.</s> <s>by minimizing the estimated closed-form bayes error, we obtain the optimal spatio-spectral filters of mcssps.</s> <s>to demonstrate the effectiveness of the proposed method, we conduct extensive experiments on the data set of bci competition 2005.</s> <s>the experimental results show that our method significantly outperforms the previous multi-class csps (mcsps) methods in the eeg classification.</s></p></d>", "label": ["<d><p><s>optimizing multi-class spatio-spectral filters via bayes error estimation for eeg classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the purpose of the paper is to explore the connection between multivariate homogeneity tests and $\\auc$ optimization.</s> <s>the latter problem has recently received much attention in the statistical learning literature.</s> <s>from the elementary observation that, in the two-sample problem setup, the null assumption corresponds to the situation where the area under the optimal roc curve is equal to 1/2, we propose a two-stage testing method based on data splitting.</s> <s>a nearly optimal scoring function in the auc sense is first learnt from one of the two half-samples.</s> <s>data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage.</s> <s>the last step amounts to performing a standard mann-whitney wilcoxon  test in the one-dimensional framework.</s> <s>we show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power, provided the ranking produced is accurate enough in the auc sense.</s> <s>the results of a numerical experiment are eventually displayed in order to show the efficiency of the method.</s></p></d>", "label": ["<d><p><s>auc optimization and the two-sample problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the linear correlation coefficient is typically used to characterize and analyze dependencies of neural spike counts.</s> <s>here, we show that the correlation coefficient is in general insufficient to characterize these dependencies.</s> <s>we construct two neuron spike count models with poisson-like marginals and vary their dependence structure using copulas.</s> <s>to this end, we construct a copula that allows to keep the spike counts uncorrelated while varying their dependence strength.</s> <s>moreover, we employ a network of leaky integrate-and-fire neurons to investigate whether weakly correlated spike counts with strong dependencies are likely to occur in real networks.</s> <s>we find that the entropy of uncorrelated but dependent spike count distributions can deviate from the corresponding distribution with independent components by more than 25% and that weakly correlated but strongly dependent spike counts are very likely to occur in biological networks.</s> <s>finally, we introduce a test for deciding whether the dependence structure of distributions with poisson-like marginals is well characterized by the linear correlation coefficient and verify it for different copula-based models.</s></p></d>", "label": ["<d><p><s>correlation coefficients are insufficient for analyzing spike count dependencies</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we investigate how well gaussian process regression can learn functions defined on graphs, using large regular random graphs as a paradigmatic example.</s> <s>random-walk based kernels are shown to have some surprising properties: within the standard approximation of a locally tree-like graph structure, the kernel does not become constant, i.e.neighbouring function values do not become fully correlated, when the lengthscale $\\sigma$ of the kernel is made large.</s> <s>instead the kernel attains a non-trivial limiting form, which we calculate.</s> <s>the fully correlated limit is reached only once loops become relevant, and we estimate where the crossover to this regime occurs.</s> <s>our main subject are learning curves of bayes error versus training set size.</s> <s>we show that these are qualitatively well predicted by a simple approximation using only the spectrum of a large tree as input, and generically scale with $n/v$, the number of training examples per vertex.</s> <s>we also explore how this behaviour changes once kernel lengthscales are large enough for loops to become important.</s></p></d>", "label": ["<d><p><s>kernels and learning curves for gaussian process regression on random graphs</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>synapses exhibit an extraordinary degree of short-term malleability, with release probabilities and effective synaptic strengths changing markedly over multiple timescales.</s> <s>from the perspective of a fixed computational operation in a network, this seems like a most unacceptable degree of added noise.</s> <s>we suggest an alternative theory according to which short term synaptic plasticity plays a normatively-justifiable role.</s> <s>this theory starts from the commonplace observation that the spiking of a neuron is an incomplete, digital, report of the analog quantity that contains all the critical information, namely its membrane potential.</s> <s>we suggest that one key task for a synapse is to solve the inverse problem of estimating the pre-synaptic membrane potential from the spikes it receives and prior  expectations, as in a recursive filter.</s> <s>we show that short-term synaptic depression has canonical dynamics which closely resemble those required for optimal estimation, and that it indeed supports high quality estimation.</s> <s>under this account, the local postsynaptic potential and the level of synaptic resources track the (scaled) mean and variance of the estimated presynaptic membrane potential.</s> <s>we make  experimentally testable predictions for how the statistics of subthreshold membrane potential fluctuations and the form of spiking non-linearity should be related to the properties of short-term plasticity in any particular cell type.</s></p></d>", "label": ["<d><p><s>know thy neighbour: a normative theory of synaptic depression</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives.</s> <s>the primitives are themselves feedback controllers pursuing their own agendas.</s> <s>they are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task.</s> <s>the resulting composite control law is provably optimal when the problem belongs to a certain class.</s> <s>this class is rather general and yet has a number of unique properties - one of which is that the bellman equation can be made linear even for non-linear or discrete dynamics.</s> <s>this gives rise to the compositionality developed here.</s> <s>in the special case of linear dynamics and gaussian noise our framework yields analytical solutions (i.e.</s> <s>non-linear mixtures of linear-quadratic regulators) without requiring the final cost to be quadratic.</s> <s>more generally, a natural set of control primitives can be constructed by applying svd to greens function of the bellman equation.</s> <s>we illustrate the theory in the context of human arm movements.</s> <s>the ideas of optimality and compositionality are both very prominent in the field of motor control, yet they are hard to reconcile.</s> <s>our work makes this possible.</s></p></d>", "label": ["<d><p><s>compositionality of optimal control laws</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we discuss the framework of transductive support vector machine (tsvm) from the perspective of the regularization strength induced by the unlabeled data.</s> <s>in this framework, svm and tsvm can be regarded as a learning machine without regularization and one with full regularization from the unlabeled data, respectively.</s> <s>therefore, to supplement this framework of the regularization strength, it is necessary to introduce data-dependant partial regularization.</s> <s>to this end, we reformulate tsvm into a form with controllable regularization strength, which includes svm and tsvm as special cases.</s> <s>furthermore, we introduce a method of adaptive regularization that is data dependant and is based on the smoothness assumption.</s> <s>experiments on a set of benchmark data sets indicate the promising results of the proposed work compared with state-of-the-art tsvm algorithms.</s></p></d>", "label": ["<d><p><s>adaptive regularization for transductive support vector machine</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the problem of approximating a given probability distribution using a simpler distribution plays an important role in several areas of machine learning, e.g.</s> <s>variational inference and classification.</s> <s>within this context, we consider the task of learning a mixture of tree distributions.</s> <s>although mixtures of trees can be learned by minimizing the kl-divergence using an em algorithm, its success depends heavily on the initialization.</s> <s>we propose an efficient strategy for obtaining a good initial set of trees that attempts to cover the entire observed distribution by minimizing the $\\alpha$-divergence with $\\alpha = \\infty$.</s> <s>we formulate the problem using the fractional covering framework and present a convergent sequential algorithm that only relies on solving a convex program at each iteration.</s> <s>compared to previous methods, our approach results in a significantly smaller mixture of trees that provides similar or better accuracies.</s> <s>we demonstrate the usefulness of our approach by learning pictorial structures for face recognition.</s></p></d>", "label": ["<d><p><s>learning a small mixture of trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of using nearest neighbor methods to provide a conditional probability estimate, p(y|a), when the number of labels y is large and the labels share some underlying structure.</s> <s>we propose a method for learning error-correcting output codes (ecocs) to model the similarity between labels within a nearest neighbor framework.</s> <s>the learned ecocs and nearest neighbor information are used to provide conditional probability estimates.</s> <s>we apply these estimates to the problem of acoustic modeling for speech recognition.</s> <s>we demonstrate an absolute reduction in word error rate (wer) of 0.9% (a 2.5% relative reduction in wer) on a lecture recognition task over a state-of-the-art baseline gmm model.</s></p></d>", "label": ["<d><p><s>learning label embeddings for nearest-neighbor multi-class classification with an application to speech recognition</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>as the availability and importance of relational data -- such as the friendships summarized on a social networking website -- increases, it becomes increasingly important to have good models for such data.</s> <s>the kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited.</s> <s>in particular, the machine learning community has focused on latent class models, adapting nonparametric bayesian methods to jointly infer how many latent classes there are while learning which entities belong to each class.</s> <s>we pursue a similar approach with a richer kind of latent variable -- latent features -- using a nonparametric bayesian technique to simultaneously infer the number of features at the same time we learn which entities have each feature.</s> <s>the greater expressiveness of this approach allows us to improve link prediction on three datasets.</s></p></d>", "label": ["<d><p><s>nonparametric latent feature models for link prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we study the problem of learning a low-dimensional (sparse)  distance matrix.</s> <s>we propose a novel metric learning model which can simultaneously conduct dimension reduction and learn a distance matrix.</s> <s>the sparse representation involves a mixed-norm regularization which is  non-convex.</s> <s>we then show that it can be equivalently formulated  as a convex saddle (min-max) problem.</s> <s>from this saddle representation, we develop an efficient smooth optimization approach for sparse metric learning although the learning model is based on a non-differential loss function.</s> <s>this smooth optimization approach has an optimal convergence rate of $o(1 /\\ell^2)$ for smooth problems where $\\ell$ is the iteration number.</s> <s>finally, we run experiments to validate the effectiveness and efficiency of our sparse metric learning model on various datasets.</s></p></d>", "label": ["<d><p><s>sparse metric learning via smooth optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present arow, a new online learning algorithm that combines several properties of successful : large margin training, confidence weighting, and the capacity to handle non-separable data.</s> <s>arow performs adaptive regularization of the prediction function upon seeing each new instance, allowing it to perform especially well in the presence of label noise.</s> <s>we derive a mistake bound, similar in form to the second order perceptron bound, which does not assume separability.</s> <s>we also relate our algorithm to recent confidence-weighted online learning techniques and empirically show that arow achieves state-of-the-art performance and notable robustness in the case of non-separable data.</s></p></d>", "label": ["<d><p><s>adaptive regularization of weight vectors</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>existing models of categorization typically represent to-be-classified items as points in a multidimensional space.</s> <s>while from a mathematical point of view, an infinite number of basis sets can be used to represent points in this space, the choice of basis set is psychologically crucial.</s> <s>people generally choose the same basis dimensions, and have a strong preference to generalize along the axes of these dimensions, but not diagonally\".</s> <s>what makes some choices of dimension special?</s> <s>we explore the idea that the dimensions used by people echo the natural variation in the environment.</s> <s>specifically, we present a rational model that does not assume dimensions, but learns the same type of dimensional generalizations that people display.</s> <s>this bias is shaped by exposing the model to many categories with a structure hypothesized to be like those which children encounter.</s> <s>our model can be viewed as a type of transformed dirichlet process mixture model, where it is the learning of the base distribution of the dirichlet process which allows dimensional generalization.the learning behaviour of our model captures the developmental shift from roughly \"isotropic\" for children to the axis-aligned generalization that adults show.\"</s></p></d>", "label": ["<d><p><s>hierarchical learning of dimensional biases in human categorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a crucial technique for scaling kernel methods to very large data sets reaching or exceeding millions of instances is based on low-rank approximation of kernel matrices.</s> <s>we introduce a new family of algorithms based on mixtures of nystrom approximations, ensemble nystrom algorithms, that yield more accurate low-rank approximations than the standard nystrom method.</s> <s>we give a detailed study of multiple variants of these algorithms based on simple averaging, an exponential weight method, or regression-based methods.</s> <s>we also present a theoretical analysis of these algorithms, including novel error bounds guaranteeing a better convergence rate than the standard nystrom method.</s> <s>finally, we report the results of extensive experiments with several data sets containing up to 1m points demonstrating the signi?cant performance improvements gained over the standard nystrom approximation.</s></p></d>", "label": ["<d><p><s>ensemble nystrom method</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>non-parametric bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and compressive sensing (cs).</s> <s>the beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dictionary size.</s> <s>the dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image.</s> <s>the proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required.</s> <s>further, the noise variance need not be known, and can be non-stationary.</s> <s>another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images.</s> <s>several example results are presented, using both gibbs and variational bayesian inference, with comparisons to other state-of-the-art approaches.</s></p></d>", "label": ["<d><p><s>non-parametric bayesian dictionary learning for sparse image representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new framework for semi-supervised learning with sparse eigenfunction bases of kernel matrices.</s> <s>it turns out that  when the \\emph{cluster assumption} holds, that is, when the high density regions are sufficiently separated by  low density valleys, each high density area corresponds to a unique representative eigenvector.</s> <s>linear combination of such eigenvectors (or, more precisely, of their nystrom extensions) provide good candidates for good classification functions.</s> <s>by first choosing an appropriate basis of these eigenvectors from unlabeled data and then using labeled data  with lasso to select a classifier in the span of these eigenvectors, we obtain a classifier, which has a very sparse representation in this basis.</s> <s>importantly, the sparsity appears naturally from the  cluster assumption.</s> <s>experimental results on a number  of real-world data-sets show that our method is competitive with the state of the art semi-supervised learning algorithms and outperforms the natural base-line algorithm (lasso in the kernel pca basis).</s></p></d>", "label": ["<d><p><s>semi-supervised learning using sparse eigenfunction bases</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>over recent years dirichlet processes and the associated chinese restaurant process (crp) have found many applications in clustering while the indian buffet process (ibp) is increasingly used to describe latent feature models.</s> <s>in the clustering case, we associate to each data point a latent allocation variable.</s> <s>these latent variables can share the same value and this induces a partition of the data set.</s> <s>the crp is a prior distribution on such partitions.</s> <s>in latent feature models, we associate to each data point a potentially infinite number of binary latent variables indicating the possession of some features and the ibp is a prior distribution on the associated infinite binary matrix.</s> <s>these prior distributions are attractive because they ensure exchangeability (over samples).</s> <s>we propose here extensions of these models to decomposable graphs.</s> <s>these models have appealing properties and can be easily learned using monte carlo techniques.</s></p></d>", "label": ["<d><p><s>bayesian nonparametric models on decomposable graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>implementations of topic models typically use symmetric dirichlet priors with fixed concentration parameters, with the implicit assumption that such smoothing parameters\" have little practical effect.</s> <s>in this paper, we explore several classes of structured priors for topic models.</s> <s>we find that an asymmetric dirichlet prior over the document-topic distributions has substantial advantages over a symmetric prior, while an asymmetric prior over the topic-word distributions provides no real benefit.</s> <s>approximation of this prior structure through simple, efficient hyperparameter optimization steps is sufficient to achieve these performance gains.</s> <s>the prior structure we advocate substantially increases the robustness of topic models to variations in the number of topics and to the highly skewed word frequency distributions common in natural language.</s> <s>since this prior structure can be implemented using efficient algorithms that add negligible cost beyond standard inference techniques, we recommend it as a new standard for topic modeling.\"</s></p></d>", "label": ["<d><p><s>rethinking lda: why priors matter</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>little work has been done to directly combine the outputs of multiple supervised and unsupervised models.</s> <s>however, it can increase the accuracy and applicability of ensemble methods.</s> <s>first, we can boost the diversity of classification ensemble by incorporating multiple clustering outputs, each of which provides grouping constraints for the joint label predictions of a set of related objects.</s> <s>secondly, ensemble of supervised models is limited in applications which have no access to raw data but to the meta-level model outputs.</s> <s>in this paper, we aim at calculating a consolidated classification solution for a set of objects by maximizing the consensus among both supervised predictions and unsupervised grouping constraints.</s> <s>we seek a global optimal label assignment for the target objects, which is different from the result of traditional majority voting and model combination approaches.</s> <s>we cast the problem into an optimization problem on a bipartite graph, where the objective function favors smoothness in the conditional probability estimates over the graph, as well as penalizes deviation from initial labeling of supervised models.</s> <s>we solve the problem through iterative propagation of conditional probability estimates among neighboring nodes, and interpret the method as conducting a constrained embedding in a transformed space, as well as a ranking on the graph.</s> <s>experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives.</s></p></d>", "label": ["<d><p><s>graph-based consensus maximization among multiple supervised and unsupervised models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show how to model documents as bags of words using family of two-layer, undirected graphical models.</s> <s>each member of the family has the same number of binary hidden units but a different number of ``softmax visible units.</s> <s>all of the softmax units in all of the models in the family share the same weights to the binary hidden units.</s> <s>we describe efficient inference and learning procedures for such a family.</s> <s>each member of the family models the probability distribution of documents of a specific length as a product of topic-specific distributions rather than as a mixture and this gives much better generalization than latent dirichlet allocation for modeling the log probabilities of held-out documents.</s> <s>the low-dimensional topic vectors learned by the undirected family are also much better than lda topic vectors for retrieving documents that are similar to a query document.</s> <s>the learned topics are more general than those found by lda because precision is achieved by intersecting many general topics rather than by selecting a single precise topic to generate each word.</s></p></d>", "label": ["<d><p><s>replicated softmax: an undirected topic model</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the worst-case complexity of general decentralized pomdps, which are equivalent to partially observable stochastic games (posgs) is very high, both for the cooperative and competitive cases.</s> <s>some reductions in complexity have been achieved by exploiting independence relations in some models.</s> <s>we show that these results are somewhat limited:  when these independence assumptions are relaxed in very small ways, complexity returns to that of the general case.</s></p></d>", "label": ["<d><p><s>complexity of decentralized control: special cases</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we develop an efficient moments-based permutation test approach to improve the system?s efficiency by approximating the permutation distribution of the test statistic with pearson distribution series.</s> <s>this approach involves the calculation of the first four moments of the permutation distribution.</s> <s>we propose a novel recursive method to derive these moments theoretically and analytically without any permutation.</s> <s>experimental results using different test statistics are demonstrated using simulated data and real data.</s> <s>the proposed strategy takes advantage of nonparametric permutation tests and parametric pearson distribution approximation to achieve both accuracy and efficiency.</s></p></d>", "label": ["<d><p><s>efficient moments-based permutation tests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>continuous-time markov chains are used to model systems in which transitions between states as well as the time the system spends in each state are random.</s> <s>many computational problems related to such chains have been solved, including determining state distributions as a function of time, parameter estimation, and control.</s> <s>however, the problem of inferring most likely trajectories, where a trajectory is a sequence of states as well as the amount of time spent in each state, appears unsolved.</s> <s>we study three versions of this problem: (i) an initial value problem, in which an initial state is given and we seek the most likely trajectory until a given final time, (ii) a boundary value problem, in which initial and final states and times are given, and we seek the most likely trajectory connecting them, and (iii) trajectory inference under partial observability, analogous to finding maximum likelihood trajectories for hidden markov models.</s> <s>we show that maximum likelihood trajectories are not always well-defined, and describe a polynomial time test for well-definedness.</s> <s>when well-definedness holds, we show that each of the three problems can be solved in polynomial time, and we develop efficient dynamic programming algorithms for doing so.</s></p></d>", "label": ["<d><p><s>maximum likelihood trajectories for continuous-time markov chains</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities.</s> <s>when faced with the task of learning a visual model based only on the name of an object, a common approach is to find images on the web that are associated with the object name, and then train a visual classifier from the search result.</s> <s>as words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model.</s> <s>we argue that images associated with an abstract word sense should be excluded when training a visual classifier to learn a model of a physical object.</s> <s>while image clustering can group together visually coherent sets of returned images, it can be difficult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word.</s> <s>we propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses.</s> <s>our model does not require any human supervision, and takes as input only the name of an object category.</s> <s>we show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classifiers trained on concrete-sense images returned by our method for a set of ten common office objects.</s></p></d>", "label": ["<d><p><s>filtering abstract senses from image search results</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a system which constructs a topological map of an environment given a sequence of images.</s> <s>this system includes a novel image similarity score which uses dynamic programming to match images using both the appearance and relative positions of local features simultaneously.</s> <s>additionally an mrf is constructed to model the probability of loop-closures.</s> <s>a locally optimal labeling is found using loopy-bp.</s> <s>finally we outline a method to generate a topological map from loop closure data.</s> <s>results are presented on four urban sequences and one indoor sequence.</s></p></d>", "label": ["<d><p><s>constructing topological maps using markov random fields and loop-closure detection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we visit the following fundamental problem: for a `generic model of consumer choice (namely, distributions over preference lists) and a limited amount of data on how consumers actually make decisions (such as marginal preference information), how may one predict revenues from offering a particular assortment of choices?</s> <s>this problem is central to areas within operations research, marketing and econometrics.</s> <s>we present a framework to answer such questions and design a number of tractable algorithms (from a data and computational standpoint) for the same.</s></p></d>", "label": ["<d><p><s>a data-driven approach to modeling choice</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning probabilistic models for complex relational structures between various types of objects.</s> <s>a model can help us ``understand a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true.</s> <s>often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have better predictive performance on large data sets.</s> <s>we introduce the bayesian clustered tensor factorization (bctf) model, which embeds a factorized representation of relations in a nonparametric bayesian clustering framework.</s> <s>inference is fully bayesian but scales well to large data sets.</s> <s>the model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.</s></p></d>", "label": ["<d><p><s>modelling relational data using bayesian clustered tensor factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the long-standing problem of efficient nearest-neighbor (nn) search has ubiquitous applications ranging from astrophysics to mp3 fingerprinting to bioinformatics to movie recommendations.</s> <s>as the dimensionality of the dataset increases, exact nn search becomes computationally prohibitive; (1+eps)-distance-approximate nn search can provide large speedups but risks losing the meaning of nn search present in the ranks (ordering) of the distances.</s> <s>this paper presents a simple, practical algorithm allowing the user to, for the first time, directly control the true accuracy of nn search (in terms of ranks) while still achieving the large speedups over exact nn.</s> <s>experiments with high-dimensional datasets show that it often achieves faster and more accurate results than the best-known distance-approximate method, with much more stable behavior.</s></p></d>", "label": ["<d><p><s>rank-approximate nearest neighbor search: retaining meaning and speed in high dimensions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we explore the problem of biasing unsupervised models to favor sparsity.</s> <s>we extend the posterior regularization framework [8] to encourage the  model to achieve posterior sparsity on the unlabeled training data.</s> <s>we apply this new method to learn ?rst-order hmms for unsupervised part-of-speech (pos) tagging, and show that hmms learned this way consistently and signi?cantly out-performs both em-trained hmms, and hmms with a sparsity-inducing dirichlet prior trained by variational em.</s> <s>we evaluate these hmms on three languages ?</s> <s>english, bulgarian and portuguese ?</s> <s>under four conditions.</s> <s>we ?nd that our  method always improves performance with respect to both baselines, while variational bayes actually degrades performance in most cases.</s> <s>we increase accuracy with respect to em by 2.5%-8.7% absolute and we see improvements even in a semisupervised condition where a limited dictionary is provided.</s></p></d>", "label": ["<d><p><s>posterior vs parameter sparsity in latent variable models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe probability distributions, dubbed compressible priors, whose independent and identically distributed (iid) realizations result in compressible signals.</s> <s>a signal is compressible when sorted magnitudes of its coefficients exhibit a power-law decay so that the signal can be well-approximated by a sparse signal.</s> <s>since compressible signals live close to sparse signals, their intrinsic information can be stably embedded via simple non-adaptive linear projections into a much lower dimensional space whose dimension grows logarithmically with the ambient signal dimension.</s> <s>by using order statistics, we show that n-sample iid realizations of generalized pareto, student?s t, log-normal, frechet, and log-logistic distributions are compressible, i.e., they have a constant expected decay rate, which is independent of n. in contrast, we show that generalized gaussian distribution with shape parameter q is compressible only in restricted cases since the expected decay rate of its n-sample iid realizations decreases with n as 1/[q log(n/q)].</s> <s>we use compressible priors as a scaffold to build new iterative sparse signal recovery algorithms based on bayesian inference arguments.</s> <s>we show how tuning of these algorithms explicitly depends on the parameters of the compressible prior of the signal, and how to learn the parameters of the signal?s compressible prior on the fly during recovery.</s></p></d>", "label": ["<d><p><s>learning with compressible priors</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we present a novel approach to learn directed acyclic graphs (dag) and factor models within the same framework while also allowing for model comparison between them.</s> <s>for this purpose, we exploit the connection between factor models and dags to propose bayesian hierarchies based on spike and slab priors to promote sparsity, heavy-tailed priors to ensure identifiability and predictive densities to perform the model comparison.</s> <s>we require identifiability to be able to produce variable orderings leading to valid dags and sparsity to learn the structures.</s> <s>the effectiveness of our approach is demonstrated through extensive experiments on artificial and biological data showing that our approach outperform a number of state of the art methods.</s></p></d>", "label": ["<d><p><s>bayesian sparse factor models and dags inference and comparison</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area v1.</s> <s>a single-hidden-layer neural network of this kind of model achieves 1.5% error on mnist.</s> <s>we also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models.</s> <s>this pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells.</s> <s>with this pretraining, the same single-hidden-layer model achieves better generalization error, even though the pretraining sample distribution is very different from the fine-tuning distribution.</s> <s>to implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features.</s></p></d>", "label": ["<d><p><s>slow, decorrelated features for pretraining complex cell-like networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>conditional random fields (crf) are quite successful on sequence labeling tasks such as natural language processing and biological sequence analysis.</s> <s>crf models use linear potential functions to represent the relationship between input features and outputs.</s> <s>however, in many real-world applications such as protein structure prediction and handwriting recognition, the relationship between input features and outputs is highly complex and nonlinear, which cannot be accurately modeled by a linear function.</s> <s>to model the nonlinear relationship between input features and outputs we propose conditional neural fields (cnf), a new conditional probabilistic graphical model for sequence labeling.</s> <s>our cnf model extends crf by adding one (or possibly several) middle layer between input features and outputs.</s> <s>the middle layer consists of a number of hidden parameterized gates, each acting as a local neural network node or feature extractor to capture the nonlinear relationship between input features and outputs.</s> <s>therefore, conceptually this cnf model is much more expressive than the linear crf model.</s> <s>to better control the complexity of the cnf model, we also present a hyperparameter optimization procedure within the evidence framework.</s> <s>experiments on two widely-used benchmarks indicate that this cnf model performs significantly better than a number of popular methods.</s> <s>in particular, our cnf model is the best among about ten machine learning methods for protein secondary tructure prediction and also among a few of the best methods for handwriting recognition.</s></p></d>", "label": ["<d><p><s>conditional neural fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>across a wide range of cognitive tasks, recent experience in?uences behavior.</s> <s>for example, when individuals repeatedly perform a simple two-alternative forced-choice task (2afc), response latencies vary dramatically based on the immediately preceding trial sequence.</s> <s>these sequential effects have been interpreted as adaptation to the statistical structure of an uncertain, changing environment (e.g.</s> <s>jones & sieck, 2003; mozer, kinoshita, & shettel, 2007; yu & cohen, 2008).</s> <s>the dynamic belief model (dbm) (yu & cohen, 2008) explains sequential effects in 2afc tasks as a rational consequence of a dynamic internal representation that tracks second-order statistics of the trial sequence (repetition rates) and predicts whether the upcoming trial will be a repetition or an alternation of the previous trial.</s> <s>experimental results suggest that ?rst-order statistics (base rates) also in?uence sequential effects.</s> <s>we propose a model that learns both ?rst- and second-order sequence properties, each according to the basic principles of the dbm but under a uni?ed inferential framework.</s> <s>this model, the dynamic belief mixture model (dbm2), obtains precise, parsimonious ?ts to data.</s> <s>furthermore, the model predicts dissociations in behavioral (maloney, dal martello, sahm, & spillmann, 2005) and electrophysiological studies (jentzsch & sommer, 2002), supporting the psychological and neurobiological reality of its two components.</s></p></d>", "label": ["<d><p><s>sequential effects reflect parallel learning of multiple environmental regularities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an online decision problem over a discrete space in which the loss function is submodular.</s> <s>we give algorithms which are computationally efficient and are hannan-consistent in both the full information and bandit settings.</s></p></d>", "label": ["<d><p><s>beyond convexity: online submodular minimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a new type of deep belief net and evaluate it on a 3d object recognition task.</s> <s>the top-level model is a third-order boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients.</s> <s>performance is evaluated on the norb database(normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints.</s> <s>our model achieves 6.5% error on the test set, which is close to the best published result for norb (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance.</s> <s>it substantially outperforms shallow models such as svms (11.6%).</s> <s>dbns are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the norb recognition task in which additional unlabeled images are created by applying small translations to the images in the database.</s> <s>with the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error, making it the current best result for norb.</s></p></d>", "label": ["<d><p><s>3d object recognition with deep belief nets</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we adapt a probabilistic latent variable model, namely gap (gamma-poisson), to ad targeting in the contexts of sponsored search (ss) and behaviorally targeted (bt) display advertising.</s> <s>we also approach the important problem of ad positional bias by formulating a one-latent-dimension gap factorization.</s> <s>learning from click-through data is intrinsically large scale, even more so for ads.</s> <s>we scale up the algorithm to terabytes of real-world ss and bt data that contains hundreds of millions of users and hundreds of thousands of features, by leveraging the scalability characteristics of the algorithm and the inherent structure of the problem including data sparsity and locality.</s> <s>specifically, we demonstrate two somewhat orthogonal philosophies of scaling algorithms to large-scale problems, through the ss and bt implementations, respectively.</s> <s>finally, we report the experimental results using yahoos vast datasets, and show that our approach substantially outperform the state-of-the-art methods in prediction accuracy.</s> <s>for bt in particular, the roc area achieved by gap is exceeding 0.95, while one prior approach using poisson regression yielded 0.83.</s> <s>for computational performance, we compare a single-node sparse implementation with a parallel implementation using hadoop mapreduce, the results are counterintuitive yet quite interesting.</s> <s>we therefore provide insights into the underlying principles of large-scale learning.</s></p></d>", "label": ["<d><p><s>factor modeling for advertisement targeting</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in visual recognition, the images are frequently modeled as sets of local features (bags).</s> <s>we show that bag of words, a common method to handle such cases, can be viewed as a special match kernel, which counts 1 if two local features fall into the same regions partitioned by visual words and 0 otherwise.</s> <s>despite its simplicity, this quantization is too coarse.</s> <s>it is, therefore, appealing to design match kernels that more accurately measure the similarity between local features.</s> <s>however, it is impractical to use such kernels on large datasets due to their significant computational cost.</s> <s>to address this problem, we propose an efficient match kernel (emk), which maps local features to a low dimensional feature space, average the resulting feature vectors to form a set-level feature, then apply a linear classifier.</s> <s>the local feature maps are learned so that their inner products preserve, to the best possible, the values of the specified kernel function.</s> <s>emk is linear both in the number of images and in the number of local features.</s> <s>we demonstrate that emk is extremely efficient and achieves the current state of the art performance on three difficult real world datasets: scene-15, caltech-101 and caltech-256.</s></p></d>", "label": ["<d><p><s>efficient match kernel between sets of features for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces a new method for semi-supervised learning on high dimensional nonlinear manifolds, which includes a phase of unsupervised basis learning and a phase of supervised function learning.</s> <s>the learned bases provide a set of anchor points to form a local coordinate system, such that each data point x on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local coordinate coding.</s> <s>we show that a high dimensional nonlinear function can be approximated by a global linear function with respect to this coding scheme, and the approximation quality is ensured by the locality of such coding.</s> <s>the method turns a difficult nonlinear learning problem into a simple global linear learning problem, which overcomes some drawbacks of traditional local learning methods.</s></p></d>", "label": ["<d><p><s>nonlinear learning using local coordinate coding</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a general inference framework for inter-domain gaussian processes (gps), focusing on its usefulness to build sparse gp models.</s> <s>the state-of-the-art sparse gp model introduced by snelson and ghahramani in [1] relies on finding a small, representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well, and then uses it to perform inference.</s> <s>this reduces inference and model selection computation time from o(n^3) to o(m^2n), where m << n. inter-domain gps can be used to find a (possibly more compact) representative set of features lying in a different domain, at the same computational cost.</s> <s>being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions.</s> <s>we will show how previously existing models fit into this framework and will use it to develop two new sparse gp models.</s> <s>tests on large, representative regression data sets suggest that significant improvement can be achieved, while retaining computational efficiency.</s></p></d>", "label": ["<d><p><s>inter-domain gaussian processes for sparse inference using inducing features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>automated recovery from failures is a key component in the management of large data centers.</s> <s>such systems typically employ a hand-made controller created by an expert.</s> <s>while such controllers capture many important aspects of the recovery process, they are often not systematically optimized to reduce costs such as server downtime.</s> <s>in this paper we explain how to use data gathered from the interactions of the hand-made controller with the system, to create an optimized controller.</s> <s>we suggest learning an indefinite horizon partially observable markov decision process, a model for decision making under uncertainty, and solve it using a point-based algorithm.</s> <s>we describe the complete process, starting with data gathering, model learning, model checking procedures, and computing a policy.</s> <s>while our paper focuses on a specific domain, our method is applicable to other systems that use a hand-coded, imperfect controllers.</s></p></d>", "label": ["<d><p><s>improving existing fault recovery policies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables.</s> <s>we show that this problem can be efficiently addressed by using a certain greedy style algorithm.</s> <s>more precisely, we propose the group orthogonal matching pursuit algorithm (group-omp), which extends the standard omp procedure (also referred to as ``forward greedy feature selection algorithm for least squares regression) to perform stage-wise group variable selection.</s> <s>we prove that under certain conditions group-omp can identify the correct (groups of) variables.</s> <s>we also provide an upperbound on the $l_\\infty$ norm of the difference between the estimated regression coefficients and the true coefficients.</s> <s>experimental results on simulated and real world datasets indicate that group-omp compares favorably to group lasso, omp and lasso, both in terms of variable selection and prediction accuracy.</s></p></d>", "label": ["<d><p><s>grouped orthogonal matching pursuit for variable selection and prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with the consistency analysis on listwise ranking methods.</s> <s>among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches.</s> <s>most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important.</s> <s>this paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting.</s> <s>for this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations.</s> <s>this framework can include the permutation-level ranking framework proposed in previous work as a special case.</s> <s>based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions.</s> <s>experimental results show that after the modifications, the methods can work significantly better than their original versions.</s></p></d>", "label": ["<d><p><s>statistical consistency of top-k ranking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated from real world problems, like object categorization, we study a particular mixed-norm regularization for multiple kernel learning (mkl).</s> <s>it is assumed that the given set of kernels are grouped into distinct components where each component is crucial for the learning task at hand.</s> <s>the formulation hence employs $l_\\infty$ regularization for promoting combinations at the component level and $l_1$ regularization for promoting sparsity among kernels in each component.</s> <s>while previous attempts have formulated this as a non-convex problem, the formulation given here is an instance of non-smooth convex optimization problem which admits an efficient mirror-descent (md) based procedure.</s> <s>the md procedure optimizes over product of simplexes, which is not a well-studied case in literature.</s> <s>results on real-world datasets show that the new mkl formulation is well-suited for object categorization tasks and that the md based algorithm outperforms state-of-the-art mkl solvers like \\texttt{simplemkl} in terms of computational effort.</s></p></d>", "label": ["<d><p><s>on the algorithmics and applications of a mixed-norm based kernel learning formulation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>training conditional maximum entropy models on massive data requires significant time and computational resources.</s> <s>in this paper, we investigate three common distributed training strategies: distributed gradient, majority voting ensembles, and parameter mixtures.</s> <s>we analyze the worst-case runtime and resource costs of each and present a theoretical foundation for the convergence of parameters under parameter mixtures, the most efficient strategy.</s> <s>we present large-scale experiments comparing the different strategies and demonstrate that parameter mixtures over independent models use fewer resources and achieve comparable loss as compared to standard approaches.</s></p></d>", "label": ["<d><p><s>efficient large-scale distributed training of conditional maximum entropy models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider regularized stochastic learning and online optimization problems, where the objective function is the sum of two convex terms: one is the loss function of the learning task, and the other is a simple regularization term such as l1-norm for sparsity.</s> <s>we develop a new online algorithm, the regularized dual averaging method, that can explicitly exploit the regularization structure in an online setting.</s> <s>in particular, at each iteration, the learning variables are adjusted by solving a simple optimization problem that involves the running average of all past subgradients of the loss functions and the whole regularization term, not just its subgradient.</s> <s>this method achieves the optimal convergence rate and often enjoys a low complexity per iteration similar as the standard stochastic gradient method.</s> <s>computational experiments are presented for the special case of sparse online learning using l1-regularization.</s></p></d>", "label": ["<d><p><s>dual averaging method for regularized stochastic learning and online optimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of decision-theoretic online learning (dtol).</s> <s>motivated by practical applications, we focus on dtol when the number of actions is very large.</s> <s>previous algorithms for learning in this framework have a tunable learning rate parameter, and a major barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large.</s> <s>in this paper, we offer a clean solution by proposing a novel and completely parameter-free algorithm for dtol.</s> <s>in addition, we introduce a new notion of regret, which is more natural for applications with a large number of actions.</s> <s>we show that our algorithm achieves good performance with respect to this new notion of regret; in addition, it also achieves performance close to that of the best bounds achieved by previous algorithms with optimally-tuned parameters, according to previous notions of regret.</s></p></d>", "label": ["<d><p><s>a parameter-free hedging algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an algorithm for efficient range search when the notion of dissimilarity is given by a bregman divergence.</s> <s>the range search task is to return all points in a potentially large database that are within some specified distance of a query.</s> <s>it arises in many learning algorithms such as locally-weighted regression, kernel density estimation, neighborhood graph-based algorithms, and in tasks like outlier detection and information retrieval.</s> <s>in metric spaces, efficient range search-like algorithms based on spatial data structures have been deployed on a variety of statistical tasks.</s> <s>here we describe the first algorithm for range search for an arbitrary bregman divergence.</s> <s>this broad class of dissimilarity measures includes the relative entropy,  mahalanobis distance, itakura-saito divergence, and a variety of matrix divergences.</s> <s>metric methods cannot be directly applied since bregman divergences do not in general satisfy the triangle inequality.</s> <s>we derive geometric properties of bregman divergences that yield an efficient algorithm for range search based on a recently proposed space decomposition for bregman divergences.</s></p></d>", "label": ["<d><p><s>efficient bregman range search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new sketch recognition framework that combines a rich representation of low level visual appearance with a graphical model for capturing high level relationships between symbols.</s> <s>this joint model of appearance and context allows our framework to be less sensitive to noise and drawing variations, improving accuracy and robustness.</s> <s>the result is a recognizer that is better able to handle the wide range of drawing styles found in messy freehand sketches.</s> <s>we evaluate our work on two real-world domains, molecular diagrams and electrical circuit diagrams, and show that our combined approach significantly improves recognition performance.</s></p></d>", "label": ["<d><p><s>learning from neighboring strokes: combining appearance and context for multi-domain sketch recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the last few decades, model complexity has received a lot of press.</s> <s>while many methods have been proposed that jointly measure a model?s descriptive adequacy and its complexity, few measures exist that measure complexity in itself.</s> <s>moreover, existing measures ignore the parameter prior, which is an inherent part of the model and affects the complexity.</s> <s>this paper presents a stand alone measure for model complexity, that takes the number of parameters, the functional form, the range of the parameters and the parameter prior into account.</s> <s>this prior predictive complexity (ppc) is an intuitive and easy to compute measure.</s> <s>it starts from the observation that model complexity is the property of the model that enables it to fit a wide range of outcomes.</s> <s>the ppc then measures how wide this range exactly is.</s></p></d>", "label": ["<d><p><s>measuring model complexity with the prior predictive</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>images can be segmented by first using a classifier to predict an affinity graph that reflects the degree to which image pixels must be grouped together and then partitioning the graph to yield a segmentation.</s> <s>machine learning has been applied to the affinity classifier to produce affinity graphs that are good in the sense of minimizing edge misclassification rates.</s> <s>however, this error measure is only indirectly related to the quality of segmentations produced by ultimately partitioning the affinity graph.</s> <s>we present the first machine learning algorithm for training a classifier to produce affinity graphs that are good in the sense of producing segmentations that directly minimize the rand index, a well known segmentation performance measure.</s> <s>the rand index measures segmentation performance by quantifying the classification of the connectivity of image pixel pairs after segmentation.</s> <s>by using the simple graph partitioning algorithm of finding the connected components of the thresholded affinity graph, we are able to train an affinity classifier to directly minimize the rand index of segmentations resulting from the graph partitioning.</s> <s>our learning algorithm corresponds to the learning of maximin affinities between image pixel pairs, which are predictive of the pixel-pair connectivity.</s></p></d>", "label": ["<d><p><s>maximin affinity learning of image segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>online learning algorithms have impressive convergence properties   when it comes to risk minimization and convex games on very large   problems.</s> <s>however, they are inherently sequential in their design   which prevents them from taking advantage of modern multi-core   architectures.</s> <s>in this paper we prove that online learning with   delayed updates converges well, thereby facilitating parallel online   learning.</s></p></d>", "label": ["<d><p><s>slow learners are fast</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the forward greedy strategy in sparse nonparametric regression.</s> <s>for additive models, we propose an algorithm called additive forward regression; for general multivariate regression, we propose an algorithm called generalized forward regression.</s> <s>both of them simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem.</s> <s>our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-of-the-art competitors, including the lasso, a nonparametric version of the lasso called the sparse additive model (spam) and a recently proposed adaptive parametric forward-backward algorithm called the foba.</s> <s>some theoretical justifications are also provided.</s></p></d>", "label": ["<d><p><s>nonparametric greedy algorithms for the sparse learning problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the cur decomposition provides an approximation of a matrix x that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of x.</s> <s>in this regard, it appears to be similar to many sparse pca methods.</s> <s>however, cur takes a randomized algorithmic approach whereas most sparse pca methods are framed as convex optimization problems.</s> <s>in this paper, we try to understand cur from a sparse optimization viewpoint.</s> <s>in particular, we show that cur is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse pca method.</s> <s>we observe that the sparsity attained by cur possesses an interesting structure, which leads us to formulate a sparse pca method that achieves a cur-like sparsity.</s></p></d>", "label": ["<d><p><s>cur from a sparse optimization viewpoint</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the commute distance between two vertices in a graph is the expected   time it takes a random walk to travel from the first to the second   vertex and back.</s> <s>we study the behavior of the commute distance as the size of the underlying graph   increases.</s> <s>we prove that the commute distance converges to an   expression that does not take into account the structure of the   graph at all and that is completely meaningless as a distance   function on the graph.</s> <s>consequently, the use of the raw commute   distance for machine learning purposes is strongly discouraged for   large graphs and in high dimensions.</s> <s>as an alternative we introduce   the amplified commute distance that corrects for   the undesired large sample effects.</s></p></d>", "label": ["<d><p><s>getting lost in space: large sample analysis of the resistance distance</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>accurate short-term wind forecasts (stwfs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid.</s> <s>physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing.</s> <s>two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables at geographically distributed sites.</s> <s>in this paper we introduce approaches that address both of these challenges.</s> <s>we describe a new regime-aware approach to stwf that use auto-regressive hidden markov models (ar-hmm), a subclass of conditional linear gaussian (clg) models.</s> <s>although ar-hmms are a natural representation for weather regimes, as with clg models in general, exact inference is np-hard when observations are missing (lerner and parr, 2001).</s> <s>because of this high cost, we introduce a simple approximate inference method for ar-hmms, which we believe has applications to other sequential and temporal problem domains that involve continuous variables.</s> <s>in an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes significantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.</s></p></d>", "label": ["<d><p><s>auto-regressive hmm inference with incomplete data for short-horizon wind forecasting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new family of online learning algorithms based upon   constraining the velocity flow over a distribution of weight   vectors.</s> <s>in particular, we show how to effectively  herd a   gaussian weight vector distribution by trading off velocity   constraints with a loss function.</s> <s>by uniformly bounding this loss   function, we demonstrate how to solve the resulting optimization   analytically.</s> <s>we compare the resulting algorithms on a variety of    real world datasets, and demonstrate how these algorithms achieve   state-of-the-art robust performance, especially with high label   noise in the training data.</s></p></d>", "label": ["<d><p><s>learning via gaussian herding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we establish an excess risk bound of o(h r_n^2 + sqrt{h l*} r_n) for erm with an h-smooth loss function and a hypothesis class with rademacher complexity r_n, where l* is the best risk achievable by the hypothesis class.</s> <s>for typical hypothesis classes where r_n = sqrt{r/n}, this translates to a learning rate of ?</s> <s>o(rh/n) in the separable (l* = 0) case and o(rh/n + sqrt{l* rh/n}) more generally.</s> <s>we also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.</s></p></d>", "label": ["<d><p><s>smoothness, low noise and fast rates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a log-bilinear\" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables.</s> <s>even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables.</s> <s>this makes it possible to get the exact gradient of the log likelihood.</s> <s>the bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions.</s> <s>experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) svms, backpropagation, and deep belief nets.\"</s></p></d>", "label": ["<d><p><s>gated softmax classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the online binary classification problem, where we are given m classifiers.</s> <s>at each stage, the classifiers map the input to the probability that the input belongs to the positive class.</s> <s>an online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classifiers.</s> <s>in this paper, we use sensitivity and  specificity as the performance metrics of the meta-algorithm.</s> <s>in particular, our goal is to design an algorithm which satisfies the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold, and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint, in hindsight.</s> <s>we show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable.</s> <s>hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it.</s> <s>in the case of two classifiers, we show that this algorithm takes a very simple form.</s> <s>to our best knowledge, this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting.</s></p></d>", "label": ["<d><p><s>online classification with specificity constraints</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement.</s> <s>previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own.</s> <s>we consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof.</s> <s>we find that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured.</s></p></d>", "label": ["<d><p><s>probabilistic inference and differential privacy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper explores links between basis construction methods in markov decision processes and power series expansions of value functions.</s> <s>this perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases.</s> <s>krylov and bellman error bases are based on the neumann series expansion.</s> <s>these bases incur very large initial bellman errors, and can converge rather slowly as the discount factor approaches unity.</s> <s>the laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations.</s> <s>the first two terms in the laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the drazin (or group inverse) of a singular matrix derived from the transition matrix.</s> <s>experiments show that drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor.</s> <s>an incremental variant of drazin bases called bellman average-reward bases (barbs) is described, which provides some of the same benefits at lower computational cost.</s></p></d>", "label": ["<d><p><s>basis construction from power series expansions of value functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic graphical models use local factors to represent dependence among sets of variables.</s> <s>for many problem domains, for instance climatology and epidemiology, in addition to local dependencies, we may also wish to model heavy-tailed statistics, where extreme deviations should not be treated as outliers.</s> <s>specifying such distributions using graphical models for probability density functions (pdfs) generally lead to intractable inference and learning.</s> <s>cumulative distribution networks (cdns) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (cdfs).</s> <s>currently, algorithms for inference and learning, which correspond to computing mixed derivatives, are exact only for tree-structured graphs.</s> <s>for graphs of arbitrary topology, an efficient algorithm is needed that takes advantage of the sparse structure of the model, unlike symbolic differentiation programs such as mathematica and d* that do not.</s> <s>we present an algorithm for recursively decomposing the computation of derivatives for cdns of arbitrary topology, where the decomposition is naturally described using junction trees.</s> <s>we compare the performance of the resulting algorithm to mathematica and d*, and we apply our method to learning models for rainfall and h1n1 data, where we show that cdns with cycles are able to provide a significantly better fits to the data as compared to tree-structured and unstructured cdns and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</s></p></d>", "label": ["<d><p><s>exact inference and learning for cumulative distribution functions on loopy graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor.</s> <s>how to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture.</s> <s>specifically, a spatio-temporal model called spikeants is shown to enforce the emergence of synchronized activities in an ant colony.</s> <s>each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network.</s> <s>each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants.</s> <s>interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies.</s> <s>a phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed.</s></p></d>", "label": ["<d><p><s>spikeants, a spiking neuron network modelling the emergence of organization in a complex system</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper discusses the topic of dimensionality reduction for $k$-means clustering.</s> <s>we prove that any set of $n$ points in $d$ dimensions (rows in a matrix $a \\in \\rr^{n \\times d}$) can be projected into $t = \\omega(k / \\eps^2)$ dimensions, for any $\\eps \\in (0,1/3)$, in $o(n d \\lceil \\eps^{-2} k/ \\log(d) \\rceil )$ time, such that with  constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\\eps$.</s> <s>the projection is done by post-multiplying $a$ with a $d \\times t$ random matrix $r$ having entries $+1/\\sqrt{t}$ or $-1/\\sqrt{t}$ with equal probability.</s> <s>a numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</s></p></d>", "label": ["<d><p><s>random projections for </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an online variational bayes (vb) algorithm for latent dirichlet allocation (lda).</s> <s>online lda is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the vb objective function.</s> <s>it can handily analyze massive document collections, including those arriving in a stream.</s> <s>we study the performance of online lda in several ways, including by fitting a 100-topic topic model to 3.3m articles from wikipedia in a single pass.</s> <s>we demonstrate that online lda finds topic models as good or better than those found with batch vb, and in a fraction of the time.</s></p></d>", "label": ["<d><p><s>online learning for latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce cst, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains.</s> <s>cst uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill.</s> <s>the skill chains from each trajectory are then merged to form a skill tree.</s> <s>we demonstrate that cst constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction.</s></p></d>", "label": ["<d><p><s>constructing skill trees for reinforcement learning agents from demonstration trajectories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics.</s> <s>in this paper we propose a simple and fast algorithm svp (singular value projection) for rank minimization under affine constraints armp and show that svp recovers the minimum rank solution for affine constraints that satisfy a restricted isometry property} (rip).</s> <s>our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the rip constants than the existing methods.</s> <s>we also introduce a newton-step for our svp framework to speed-up the convergence with substantial empirical gains.</s> <s>next, we address a practically important application of armp - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey rip, hence the guarantees of svp do not hold.</s> <s>however, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries.</s> <s>we also demonstrate empirically that our algorithms outperform existing methods, such as those of \\cite{caics2008,leeb2009b, keshavanom2009}, for armp and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes.</s> <s>in particular, results show that our svp-newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.</s></p></d>", "label": ["<d><p><s>guaranteed rank minimization via singular value projection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting.</s> <s>while manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied.</s> <s>we perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions.</s> <s>we show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.</s></p></d>", "label": ["<d><p><s>humans learn using manifolds, reluctantly</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking.</s> <s>a popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the luce model and the mallows model.</s> <s>however, these models have their limitations in either poor expressiveness or high computational complexity.</s> <s>to avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a stagewise process.</s> <s>we refer to the new model as coset-permutation distance based stagewise (cps) model.</s> <s>the cps model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance.</s> <s>the complexity of the cps model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances.</s> <s>we apply the cps model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency.</s> <s>experiments on public datasets show that the derived algorithms based on the cps model can achieve state-of-the-art ranking accuracy, and are much more efficient than previous algorithms.</s></p></d>", "label": ["<d><p><s>a new probabilistic model for rank aggregation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data.</s> <s>we introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression; we show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization.</s> <s>while expectation-maximization (em) algorithm is commonly used to learn these models, its optimization usually leads to local minimum because it relies on a non-convex cost function with many such local minima.</s> <s>to avoid this problem, we introduce a local approximation of this cost function, which leads to a quadratic non-convex optimization problem over a product of simplices.</s> <s>in order to minimize such functions, we propose an efficient algorithm based on convex relaxation and low-rank representation of our data, which allows to deal with large instances.</s> <s>experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.</s></p></d>", "label": ["<d><p><s>efficient optimization for discriminative latent class models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we define a data dependent permutation complexity for a hypothesis set \\math{\\hset}, which is similar to a rademacher complexity or maximum discrepancy.</s> <s>the permutation complexity is based like the maximum discrepancy on (dependent) sampling.</s> <s>we prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated.</s></p></d>", "label": ["<d><p><s>permutation complexity bound on out-sample error</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful\" segments in the ensemble.</s> <s>knowledge about any specific objects and surfaces present in the image is not available.</s> <s>the selection of image regions occupied by objects is formalized as the maximum-weight independent set (mwis) problem.</s> <s>mwis is the heaviest subset of mutually non-adjacent nodes of an attributed graph.</s> <s>we construct such a graph from all segments in the ensemble.</s> <s>then, mwis selects maximally distinctive segments that together partition the image.</s> <s>a new mwis algorithm is presented.</s> <s>the algorithm seeks a solution directly in the discrete domain, instead of relaxing mwis to a continuous problem, as common in previous work.</s> <s>it iteratively finds a candidate discrete solution of the taylor series expansion of the original mwis objective function around the previous solution.</s> <s>the algorithm is shown to converge to a maximum.</s> <s>our empirical evaluation on the benchmark berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results.\"</s></p></d>", "label": ["<d><p><s>segmentation as maximum-weight independent set</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes.</s> <s>previous work has assumed that two different mechanisms are involved in processing these two types of motion.</s> <s>in this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception.</s> <s>our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales.</s> <s>we tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research.</s> <s>we demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.</s></p></d>", "label": ["<d><p><s>a unified model of short-range and long-range motion perception</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph.</s> <s>the graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed.</s> <s>the absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix.</s> <s>additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices.</s> <s>we develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features.</s> <s>our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined.</s> <s>we  show experiments with both simulated and real data which reveal the interest of our methodology.</s></p></d>", "label": ["<d><p><s>link discovery using graph feature tracking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories\" (clusters of stimuli) and \"functional units\" (clusters of voxels).</s> <s>we assume that voxels within a unit respond similarly to all stimuli from the same category, and design a nonparametric hierarchical model to capture inter-subject variability among the units.</s> <s>the model explicitly captures the relationship between brain activations and fmri time courses.</s> <s>a variational inference algorithm derived based on the model can learn categories, units, and a set of unit-category activation probabilities from data.</s> <s>when applied to data from an fmri study of object recognition, the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units.\"</s></p></d>", "label": ["<d><p><s>categories and functional units: an infinite hierarchical model for brain activations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we propose an approximated learning framework for large scale graphical models and derive message passing algorithms for learning their parameters efficiently.</s> <s>we first relate crfs and structured svms  and show that in the crf's primal a variant of the log-partition function, known as soft-max, smoothly approximates the hinge loss function of structured svms.</s> <s>we then propose an intuitive approximation for structured prediction problems using fenchel duality based on a local entropy approximation that computes the exact gradients of the approximated problem and is guaranteed to converge.</s> <s>unlike existing approaches, this allow us to learn graphical models with cycles and very large number of parameters efficiently.</s> <s>we demonstrate the effectiveness of our approach  in an image denoising task.</s> <s>this task was previously solved by sharing parameters across cliques.</s> <s>in contrast, our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction.</s></p></d>", "label": ["<d><p><s>a primal-dual message-passing algorithm for approximated large scale structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>support vector machines (svm) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data.</s> <s>moreover, when the kernel is linear, svms can be used to localize spatial patterns of discrimination between two groups of subjects.</s> <s>however, the features' spatial distribution is not taken into account.</s> <s>as a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difficult.</s> <s>this paper introduces a framework to spatially regularize svm for brain image analysis.</s> <s>we show that laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3d brain images.</s> <s>the proposed framework is applied to the classification of mr images based on gray matter concentration maps and cortical thickness measures from 30 patients with alzheimer's disease and 30 elderly controls.</s> <s>the results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.</s></p></d>", "label": ["<d><p><s>spatial and anatomical regularization of svm for brain image analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction.</s> <s>one popular approach to this problem is audio-visual synchrony detection.</s> <s>a candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal.</s> <s>here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all.</s> <s>further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models.</s> <s>the voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g.</s> <s>in the case of a mobile robot).</s> <s>moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection.</s> <s>the work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database.</s></p></d>", "label": ["<d><p><s>an alternative to low-level-sychrony-based methods for speech detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>undirected graphical models encode in a graph $g$ the dependency structure of a random vector $y$.</s> <s>in many applications, it is of interest to model $y$ given another random vector $x$ as input.</s> <s>we refer to the problem of estimating the graph $g(x)$ of $y$ conditioned on $x=x$ as ``graph-valued regression''.</s> <s>in this paper, we propose a semiparametric method for estimating $g(x)$ that builds a tree on the $x$ space just as in cart (classification and regression trees), but at each leaf of the tree estimates a graph.</s> <s>we call the method ``graph-optimized cart'', or go-cart.</s> <s>we study the theoretical properties of go-cart using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency.</s> <s>we also demonstrate the application of go-cart to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data.</s></p></d>", "label": ["<d><p><s>graph-valued regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (stdp) with attenuated and enhanced pre- and postsynaptic contributions to long-term synaptic modifications.</s> <s>in order to investigate the functional consequences of these contribution dynamics (cd) we propose a minimal model formulated in terms of differential equations.</s> <s>we find that our model reproduces a wide range of experimental results with a small number of biophysically interpretable parameters.</s> <s>the model allows to investigate the susceptibility of stdp to arbitrary time courses of pre- and postsynaptic activities, i.e.</s> <s>its nonlinear filter properties.</s> <s>we demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved.</s> <s>it predicts synaptic strengthening for synchronous rate modulations.</s> <s>for low baseline rates modifications are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning.</s> <s>we also find emphasis of low baseline spike rates and suppression for high baseline rates.</s> <s>the latter suggests a mechanism of network activity regulation inherent in stdp.</s> <s>furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the cd of stdp in both spike-based as well as rate-based neuronal network models.</s></p></d>", "label": ["<d><p><s>spike timing-dependent plasticity as dynamic filter</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of inverse reinforcement learning is to find a reward function for a markov decision process, given example traces from its optimal policy.</s> <s>current irl techniques generally rely on user-supplied features that form a concise basis for the reward.</s> <s>we present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy.</s> <s>given example traces, the algorithm returns a reward function as well as the constructed features.</s> <s>the reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.</s></p></d>", "label": ["<d><p><s>feature construction for inverse reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, batch-mode active learning has attracted a lot of attention.</s> <s>in this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural form of mutual information criterion between the labeled and unlabeled instances.</s> <s>by employing a gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem.</s> <s>although the matrix partition is an np-hard combinatorial optimization problem, we show a good local solution can be obtained by exploiting an effective local optimization technique on the relaxed continuous optimization problem.</s> <s>the proposed active learning approach is independent of employed classification models.</s> <s>our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.</s></p></d>", "label": ["<d><p><s>active instance sampling via matrix partition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-label classification is the task of predicting potentially multiple labels for a given instance.</s> <s>this is common in several applications such as image annotation, document classification and gene function prediction.</s> <s>in this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels.</s> <s>by viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised.</s> <s>we optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods, showing excellent performance.</s></p></d>", "label": ["<d><p><s>reverse multi-label learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of learning to predict structured labels is of key importance in many applications.</s> <s>however, for general graph structure both learning and inference in this setting are intractable.</s> <s>here we show that it is possible to circumvent this difficulty when the input distribution is rich enough via a method similar in spirit to pseudo-likelihood.</s> <s>we show how our new method achieves consistency, and illustrate empirically that it indeed performs as well as exact methods when sufficiently large training sets are used.</s></p></d>", "label": ["<d><p><s>more data means less inference: a pseudo-max approach to structured learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems.</s> <s>we describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective.</s> <s>this derivation highlights how likelihood ratio methods under-use past experience by (a) using the past experience to estimate {\\em only} the gradient of the expected return $u(\\theta)$ at the current policy parameterization $\\theta$, rather than to obtain a more complete estimate of $u(\\theta)$, and (b) using past experience under the current policy {\\em only} rather than using all past experience to improve the estimates.</s> <s>we present a new policy search method, which leverages both of these observations as well as generalized baselines---a new technique which generalizes commonly used baseline techniques for policy gradient methods.</s> <s>our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.</s></p></d>", "label": ["<d><p><s>on a connection between importance sampling and the likelihood ratio policy gradient</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>latent variable models are a powerful tool for addressing several tasks in machine learning.</s> <s>however, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum.</s> <s>to alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning.</s> <s>the order of the samples is determined by how easy they are.</s> <s>the main challenge is that often we are not provided with a readily computable measure of the easiness of samples.</s> <s>we address this issue by  proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector.</s> <s>the number of samples selected is governed by a weight that is annealed until the entire training data has been considered.</s> <s>we empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural svm on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.</s></p></d>", "label": ["<d><p><s>self-paced learning for latent variable models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>remarkably easy implementation and guaranteed convergence has made the em algorithm one of the most used algorithms for mixture modeling.</s> <s>on the downside, the e-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data.</s> <s>based on the variational em framework, we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear e-step in sample size, while the algorithm still maintains provable convergence.</s> <s>our approach builds on previous work, but is significantly faster and scales much better in the number of mixture components.</s> <s>we demonstrate this speedup by experiments on large-scale synthetic and real data.</s></p></d>", "label": ["<d><p><s>fast large-scale mixture modeling with component-specific data partitions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence.</s> <s>in general, inference in segmentation models, such as semi-crfs, can be cubic in the length of the sequence.</s> <s>by taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long.</s> <s>furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural svms to optimize the learned mapping for these losses.</s> <s>finally, we present experimental results that demonstrate the advantages of our method against a strong baseline.</s></p></d>", "label": ["<d><p><s>static analysis of binary executables using structural svms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multiple-instance learning has been long known as a hard non-convex problem.</s> <s>in this work, we propose an approach that recasts it as a convex likelihood ratio  estimation problem.</s> <s>firstly, the constraint in multiple-instance learning is reformulated  into a convex constraint on the likelihood ratio.</s> <s>then we show that a joint  estimation of a likelihood ratio function and the likelihood on training instances  can be learned convexly.</s> <s>theoretically, we prove a quantitative relationship between  the risk estimated under the 0-1 classification loss, and under a loss function  for likelihood ratio estimation.</s> <s>it is shown that our likelihood ratio estimation is  generally a good surrogate for the 0-1 loss, and separates positive and negative  instances well.</s> <s>however with the joint estimation it tends to underestimate the  likelihood of an example to be positive.</s> <s>we propose to use these likelihood ratio  estimates as features, and learn a linear combination on them to classify the bags.</s> <s>experiments on synthetic and real datasets show the superiority of the approach.</s></p></d>", "label": ["<d><p><s>convex multiple-instance learning by estimating likelihood ratio</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider markov decision processes where the values of the parameters are uncertain.</s> <s>this uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified.</s> <s>this formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound.</s> <s>we propose a decision criterion based on *distributional robustness*: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information).</s> <s>we show that finding the optimal distributionally robust policy can be reduced to a standard robust mdp where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.</s></p></d>", "label": ["<d><p><s>distributionally robust markov decision processes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we extend logistic regression by using t-exponential families which were introduced recently in statistical physics.</s> <s>this gives rise to a regularized risk minimization problem with a non-convex loss  function.</s> <s>an efficient block coordinate descent optimization  scheme can be derived for estimating the parameters.</s> <s>because of the  nature of the loss function, our algorithm is tolerant to label noise.</s> <s>furthermore, unlike other algorithms which employ non-convex   loss functions, our algorithm is fairly robust to the choice of  initial values.</s> <s>we verify both these observations empirically on a  number of synthetic and real datasets.</s></p></d>", "label": ["<d><p><s>t-logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a principled extension of the traditional single-layer flat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding.</s> <s>the two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner.</s> <s>empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</s></p></d>", "label": ["<d><p><s>deep coding network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images.</s> <s>in this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion.</s> <s>we cast the problem of learning spatio-temporal primitives as a tensor factorization problem  and introduce constraints to learn interpretable primitives.</s> <s>in particular, we use group norms over those tensors, diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion.</s> <s>we demonstrate the effectiveness of our approach to learn interpretable representations  of human motion from motion capture data, and show that our approach outperforms  recently developed matching pursuit and  sparse coding algorithms.</s></p></d>", "label": ["<d><p><s>sparse coding for learning interpretable spatio-temporal primitives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the tree structured group lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features.</s> <s>the structured regularization with a pre-defined tree structure is based on a group-lasso penalty, where one group is defined for each node in the tree.</s> <s>such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features.</s> <s>however, the tree structured group lasso is challenging to solve due to the complex regularization.</s> <s>in this paper, we develop an efficient algorithm for the tree structured group lasso.</s> <s>one of the key steps in the proposed algorithm is to solve the moreau-yosida regularization associated with the grouped tree structure.</s> <s>the main technical contributions of this paper include (1) we show that the associated moreau-yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter.</s> <s>our experimental results on the ar and jaffe face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.</s></p></d>", "label": ["<d><p><s>moreau-yosida regularization for grouped tree structure learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we pose transductive classification as a matrix completion problem.</s> <s>by assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspecified, and iii) missing data, where a large number of features are missing.</s> <s>we obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems.</s> <s>our method allows for different loss functions to apply on the feature and label entries of the matrix.</s> <s>the resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.</s></p></d>", "label": ["<d><p><s>transduction with matrix completion: three birds with one stone</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports.</s> <s>this combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the l1-norm.</s> <s>in this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its lovasz extension, a common tool in submodular analysis.</s> <s>this defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference).</s> <s>by selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.</s></p></d>", "label": ["<d><p><s>structured sparsity-inducing norms through submodular functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although the dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model.</s> <s>the proposed shadow dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation.</s> <s>we describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data.</s></p></d>", "label": ["<d><p><s>shadow dirichlet for restricted probability modeling</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>multi-task learning (mtl) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations.</s> <s>one of the most prominent multi-task learning algorithms is an extension to svms by evgeniou et al.</s> <s>although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes.</s> <s>this paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (lmnn) algorithm to the mtl paradigm.</s> <s>instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning.</s> <s>we evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task knn under several metrics and state-of-the-art mtl classifiers.</s></p></d>", "label": ["<d><p><s>large margin multi-task metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>robust regression and classification are often thought to require non-convex loss functions that prevent scalable, global training.</s> <s>however, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives.</s> <s>a natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold.</s> <s>we demonstrate that a relaxation of this form of ``loss clipping'' can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers.</s> <s>we present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems.</s></p></d>", "label": ["<d><p><s>relaxed clipping: a global training method for robust regression and classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands.</s> <s>the requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology.</s> <s>we formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task.</s> <s>using bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands.</s> <s>our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.</s></p></d>", "label": ["<d><p><s>a rational decision making framework for inhibitory control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression.</s> <s>we propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation.</s> <s>our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the mysterious\" coagulation and fragmentation properties used in the original paper on the sequence memoizer by wood et al.</s> <s>(2009).</s> <s>we present some experimental results supporting our improvements.\"</s></p></d>", "label": ["<d><p><s>improvements to the sequence memoizer</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neuronal connection weights exhibit short-term depression (std).</s> <s>the present study investigates the impact of std on the dynamics of a continuous attractor neural network (cann) and its potential roles in neural information processing.</s> <s>we find that the network with std can generate both static and traveling bumps, and std enhances the performance of the network in tracking external inputs.</s> <s>in particular, we find that std endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of std rather than that of neural signaling.</s> <s>we argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</s></p></d>", "label": ["<d><p><s>attractor dynamics with synaptic depression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a technique for exact simulation of gaussian markov random fields (gmrfs), which can be interpreted as locally injecting noise to each gaussian factor independently, followed by computing the mean/mode of the perturbed gmrf.</s> <s>coupled with standard iterative techniques for the solution of symmetric positive definite systems, this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models.</s> <s>apart from synthesizing data under a gaussian model, the proposed technique directly leads to an efficient unbiased estimator of marginal variances.</s> <s>beyond gaussian models, the proposed algorithm is also very useful for handling highly non-gaussian continuously-valued mrfs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of gaussians with the help of local or distributed latent mixture assignment variables.</s> <s>the bayesian treatment of such models most naturally involves a block gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate gaussian continuous vector and we show that it can directly benefit from the proposed methods.</s></p></d>", "label": ["<d><p><s>gaussian sampling by local perturbations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive.</s> <s>current research mainly focus on avoding the former.</s> <s>we attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter.</s> <s>based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides.</s> <s>we apply the constrained concave-convex procedure to solve the resulted problem.</s> <s>empirical results demonstrate that our approach offers improved generalization performance.</s></p></d>", "label": ["<d><p><s>avoiding false positive in multi-instance learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>continuous markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes.</s> <s>we prove that marginal computation for constrained continuous mrfs is #p-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field.</s> <s>moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency.</s> <s>continuous mrfs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning.</s> <s>on the problem of collective classification, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence.</s></p></d>", "label": ["<d><p><s>computing marginal distributions over continuous markov networks for statistical relational learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian approaches to utility elicitation typically adopt (myopic) expected value of information (evoi) as a natural criterion for selecting queries.</s> <s>however, evoi-optimization is  usually computationally prohibitive.</s> <s>in this paper, we examine evoi optimization using \\emph{choice queries}, queries in which a user  is ask to select her most preferred product from a set.</s> <s>we show that,  under very general assumptions, the optimal choice query w.r.t.\\ evoi coincides with \\emph{optimal recommendation set}, that is, a set maximizing expected utility of the user selection.</s> <s>since recommendation set optimization is a simpler, submodular problem, this can greatly reduce the complexity of both exact and approximate (greedy) computation of optimal choice queries.</s> <s>we  also examine the case where user responses to choice queries are error-prone (using both constant and follow mixed multinomial logit  noise models) and provide worst-case guarantees.</s> <s>finally we present a local search technique that works well with large outcome spaces.</s></p></d>", "label": ["<d><p><s>optimal bayesian recommendation sets and myopically optimal choice query sets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>conventional dynamic bayesian networks (dbns) are based on the homogeneous markov assumption, which is too restrictive in many practical applications.</s> <s>various approaches to relax the homogeneity assumption have therefore been proposed in the last few years.</s> <s>the present paper aims to improve the flexibility of two recent versions of non-homogeneous dbns, which either (i) suffer from the need for data discretization, or (ii) assume a time-invariant network structure.</s> <s>allowing the network structure to be fully flexible leads to the risk of overfitting and inflated inference uncertainty though, especially in the highly topical field of systems biology, where independent measurements tend to be sparse.</s> <s>in the present paper we investigate three conceptually different regularization schemes based on inter-segment information sharing.</s> <s>we assess the performance in a comparative evaluation study based on simulated data.</s> <s>we compare the predicted segmentation of gene expression time series obtained during embryogenesis in drosophila melanogaster with other state-of-the-art techniques.</s> <s>we conclude our evaluation with an application to synthetic biology, where the objective is to predict a known regulatory network of five genes in saccharomyces cerevisiae.</s></p></d>", "label": ["<d><p><s>inter-time segment information sharing for non-homogeneous dynamic bayesian networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a method for computing the rarity of latent fingerprints represented by minutiae is given.</s> <s>it allows determining the probability of finding a match for an evidence print in a database of n known prints.</s> <s>the probability of random correspondence between evidence and database is determined in three procedural steps.</s> <s>in the registration step the latent print is aligned by finding its core point; which is done using a procedure based on a machine learning approach based on gaussian processes.</s> <s>in the evidence probability evaluation step a generative model based on bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence.</s> <s>in the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a specific birthday.</s> <s>the generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints.</s> <s>the probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae.</s></p></d>", "label": ["<d><p><s>evaluation of rarity of fingerprints in forensics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information.</s> <s>the models themselves are novel kinds of adaptor grammars that are an extension of an embedding of topic models into pcfgs.</s> <s>these models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them.</s> <s>we show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own.</s> <s>we argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.</s></p></d>", "label": ["<d><p><s>synergies in learning words and their referents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new variational em algorithm for fitting factor analysis models with mixed continuous and categorical observations.</s> <s>the algorithm is based on a simple quadratic bound to the log-sum-exp function.</s> <s>in the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods.</s> <s>we show that em is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family pca and other related matrix-factorization methods.</s> <s>a further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show.</s> <s>we present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.</s></p></d>", "label": ["<d><p><s>variational bounds for mixed-data factor analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner.</s> <s>the basic goal of our work is to discover experimentally which prior distribution is used.</s> <s>more specifically, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks.</s> <s>we restricted ourselves to priors which combine three terms for motion slowness, first-order smoothness, and second-order smoothness.</s> <s>we focused on two functional forms for prior distributions: l2-norm and l1-norm regularization corresponding to the gaussian and laplace distributions respectively.</s> <s>in our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance.</s> <s>we then measured human performance for motion tasks and found that we obtained better fit for the l1-norm (laplace) than for the l2-norm (gaussian).</s> <s>we note that the l1-norm is also a better fit to the statistics of motion in natural environments.</s> <s>in addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness.</s> <s>to validate our results further, we used the best fit models using the l1-norm to predict human performance in a second session with different experimental setups.</s> <s>our results showed excellent agreement between human performance and model prediction -- ranging from 3\\% to 8\\% for five human subjects over ten experimental conditions -- and give further support that the human visual system uses an l1-norm (laplace) prior.</s></p></d>", "label": ["<d><p><s>functional form of motion priors in human motion perception</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>heavy-tailed distributions naturally occur in many real life problems.</s> <s>unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy tailed distributions.</s> <s>in this work, we propose a novel simple linear graphical model for  independent latent random variables, called linear characteristic model (lcm), defined in the characteristic function domain.</s> <s>using stable distributions, a heavy-tailed family of distributions which is a generalization of cauchy, l\\'evy and gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model.</s> <s>lcms are not limited to only stable distributions, in fact lcms are always defined for any random variables (discrete, continuous or a mixture of both).</s> <s>we provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction.</s> <s>other potential application is iterative decoding of linear channels with non-gaussian noise.</s></p></d>", "label": ["<d><p><s>inference with multivariate heavy-tails in linear models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the diffusion network(dn) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuous-time paths.</s> <s>however, the dynamics of the dn are governed by stochastic differential equations, making the dn unfavourable for simulation in a digital computer.</s> <s>this paper presents the implementation of the dn in analogue very large scale integration, enabling the dn to be simulated in real time.</s> <s>moreover, the log-domain representation is applied to the dn, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes.</s> <s>a vlsi chip containing a dn with two stochastic units has been designed and fabricated.</s> <s>the design of component circuits will be described, so will the simulation of the full system be presented.</s> <s>the simulation results demonstrate that the dn in vlsi is able to regenerate various types of continuous paths in real-time.</s></p></d>", "label": ["<d><p><s>a log-domain implementation of the diffusion network in very large scale integration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other.</s> <s>we study inference and communication in a television game called password, where speakers must convey secret words to hearers by providing one-word clues.</s> <s>our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions.</s> <s>first, we predict that speakers and hearers are both considerate, and that both take the other?s perspective into account.</s> <s>second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other.</s> <s>finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally.</s> <s>we find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure.</s></p></d>", "label": ["<d><p><s>inference and communication in the game of password</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification.</s> <s>in practical applications, reinforcement learning (rl) is complicated by the fact that state is either high-dimensional or partially observable.</s> <s>therefore, rl methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features.</s> <s>by comparison, subspace identification (ssid) methods are designed to select a feature set which preserves as much information as possible about state.</s> <s>in this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation.</s> <s>we introduce a new algorithm for this situation, called predictive state temporal difference (pstd) learning.</s> <s>as in ssid for predictive state representations, pstd finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information.</s> <s>as in rl, pstd then uses a bellman recursion to estimate a value function.</s> <s>we discuss the connection between pstd and prior approaches in rl and ssid.</s> <s>we prove that pstd is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.</s></p></d>", "label": ["<d><p><s>predictive state temporal difference learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities.</s> <s>unfortunately, these approaches involve minimizing non-convex objective functions.</s> <s>in this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques.</s> <s>in particular, we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems.</s> <s>furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow :having latent dimensions shared between any subset of the views instead of between all the views only.</s> <s>we show that our approach outperforms state-of-the-art methods on the task of human pose estimation.</s></p></d>", "label": ["<d><p><s>factorized latent spaces with structured sparsity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (mkl) approaches, which employ the large margin principle to jointly learn both a kernel and an svm classifier.</s> <s>the reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling.</s> <s>we use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning.</s> <s>this formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., l1 or l2) of norm constraints on combination coefficients.</s> <s>we establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning.</s> <s>experiments show that our method significantly outperforms both svm with the uniform combination of basis kernels and other state-of-art mkl approaches.</s></p></d>", "label": ["<d><p><s>learning kernels with radiuses of minimum enclosing balls</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of segmenting specific white matter structures of interest from diffusion tensor (dt-mr) images of the human brain.</s> <s>this is an important requirement in many neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images.</s> <s>typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today.</s> <s>to address this problem, we endow an image segmentation algorithm with 'advice' encoding some global characteristics of the region(s) we want to extract.</s> <s>this is accomplished by constructing (using expert-segmented images) an epitome of a specific region - as a histogram over a bag of 'words' (e.g.,suitable feature descriptors).</s> <s>now, given such a representation, the problem reduces to segmenting new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features.</s> <s>we present combinatorial approximation algorithms to incorporate such domain specific constraints for markov random field (mrf) segmentation.</s> <s>making use of recent results on image co-segmentation, we derive effective solution strategies for our problem.</s> <s>we provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in neuroscience can be extracted reliably from 3-d brain image volumes using our algorithm.</s></p></d>", "label": ["<d><p><s>epitome driven 3-d diffusion tensor image segmentation: on extracting specific structures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the copula bayesian network model for representing multivariate continuous distributions.</s> <s>our approach builds on a novel copula-based parameterization of a conditional density that, joined with a graph that encodes independencies, offers great flexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals.</s> <s>we demonstrate the advantage of our framework for generalization over standard bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature.</s></p></d>", "label": ["<d><p><s>copula bayesian networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>network models are widely used to capture interactions among component of complex systems, such as social and biological.</s> <s>to understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems.</s> <s>therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components.</s> <s>we propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks.</s> <s>the proposed method offers an efficient dimension reduction strategy using laplacian eigenmaps with neumann boundary conditions, and provides a flexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs.</s> <s>asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings.</s> <s>the performance of the proposed methodology is illustrated using simulated and real data examples from biology.</s></p></d>", "label": ["<d><p><s>penalized principal component regression on graphs for analysis of subnetworks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed manifold learning.</s> <s>in this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error.</s> <s>given upper bounds on the dimension, volume, and curvature, we show that empirical risk minimization can produce a nearly optimal manifold using a number of random samples that is {\\it independent} of the ambient dimension of the space in which data lie.</s> <s>we obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume.</s> <s>for constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable.</s> <s>whether the known lower bound of $o(\\frac{k}{\\eps^2} + \\frac{\\log \\frac{1}{\\de}}{\\eps^2})$ for the sample complexity of empirical risk minimization on $k-$means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 \\cite{bart2}.</s> <s>here $\\eps$ is the desired bound on the error and $\\de$ is a bound on the probability of failure.</s> <s>we improve the best currently known upper bound \\cite{pontil} of $o(\\frac{k^2}{\\eps^2} + \\frac{\\log \\frac{1}{\\de}}{\\eps^2})$ to $o\\left(\\frac{k}{\\eps^2}\\left(\\min\\left(k, \\frac{\\log^4 \\frac{k}{\\eps}}{\\eps^2}\\right)\\right) + \\frac{\\log \\frac{1}{\\de}}{\\eps^2}\\right)$.</s> <s>based on these results, we devise a simple algorithm for $k-$means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to  high dimensional data, where the sample complexity is independent of the ambient dimension.</s></p></d>", "label": ["<d><p><s>sample complexity of testing the manifold hypothesis</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop a deterministic single-pass algorithm for latent dirichlet allocation (lda) in order to process received documents one at a time and then discard them in an excess text stream.</s> <s>our algorithm does not need to store old statistics for all data.</s> <s>the proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</s></p></d>", "label": ["<d><p><s>deterministic single-pass algorithm for lda</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in learning using privileged information (lupi) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space.</s> <s>the goal of the learner is to find a classifier with a low generalization error in the decision space.</s> <s>we consider a new version of  empirical risk minimization algorithm, called privileged erm, that takes into account the privileged information in order to find a good function in the decision space.</s> <s>we outline the conditions on the correcting space that, if satisfied, allow privileged erm to have much faster learning rate in the decision space than the one of the regular empirical risk minimization.</s></p></d>", "label": ["<d><p><s>on the theory of learnining with privileged information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations.</s> <s>recent developments have shown ways to use multiple data sets, provided they originate from identical experiments.</s> <s>we present the mci-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments.</s> <s>it is fast, reliable and produces very clear and easily interpretable output.</s> <s>it is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models.</s> <s>we test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output.</s> <s>the method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.</s></p></d>", "label": ["<d><p><s>causal discovery in multiple models from different experiments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider bandit problems, motivated by applications in online advertising and news story selection, in which the learner must repeatedly select a slate, that is, a subset of size s from k possible actions, and then receives rewards for just the selected actions.</s> <s>the goal is to minimize the regret with respect to total reward of the best slate computed in hindsight.</s> <s>we consider unordered and ordered versions of the problem, and give efficient algorithms which have regret o(sqrt(t)), where the constant depends on the specific nature of the problem.</s> <s>we also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round, and give algorithms with o(sqrt(t)) regret for competing with the best such policy as well.</s> <s>we make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.</s></p></d>", "label": ["<d><p><s>non-stochastic bandit slate problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text.</s> <s>concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search.</s> <s>the proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents.</s> <s>we describe a generative model that is based on a stick-breaking process for graphs, and a markov chain monte carlo inference procedure.</s> <s>experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes.</s> <s>we also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (such as hpam and hlda) on real-world text data sets.</s> <s>finally, we illustrate the application of the model to the problem of updating wikipedia category graphs.</s></p></d>", "label": ["<d><p><s>learning concept graphs from text with stick-breaking priors</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in some stochastic environments the well-known reinforcement learning algorithm q-learning performs very poorly.</s> <s>this poor performance is caused by large overestimations of action values.</s> <s>these overestimations result from a positive bias that is introduced because q-learning uses the maximum action value as an approximation for the maximum expected action value.</s> <s>we introduce an alternative way to approximate the maximum expected value for any set of random variables.</s> <s>the obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value.</s> <s>we apply the double estimator to q-learning to construct double q-learning, a new off-policy reinforcement learning algorithm.</s> <s>we show the new algorithm converges to the optimal policy and that it performs well in some settings in which q-learning performs poorly due to its overestimation.</s></p></d>", "label": ["<d><p><s>double q-learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $\\ell_\\infty$-norms over groups of variables.</s> <s>whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups.</s> <s>to this end, we show that the corresponding optimization problem is related to network flow optimization.</s> <s>more precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem.</s> <s>we propose an efficient procedure which computes its solution exactly in polynomial time.</s> <s>our algorithm scales up to millions of groups and variables, and opens up a whole new range of applications for structured sparse models.</s> <s>we present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.</s></p></d>", "label": ["<d><p><s>network flow algorithms for structured sparsity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way.</s> <s>the standard techniques include $k$-fold cross-validation ($k$-cv), akaike information criterion (aic), and bayesian information criterion (bic).</s> <s>though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings.</s> <s>in this paper, we present stars: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs.</s> <s>the method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling.</s> <s>this interpretation requires essentially no conditions.</s> <s>under mild conditions, we show that stars is partially sparsistent in terms of graph estimation: i.e.</s> <s>with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size.</s> <s>empirically, the performance of stars is compared with the state-of-the-art model selection procedures, including $k$-cv, aic, and bic, on both synthetic data and a real microarray dataset.</s> <s>stars outperforms all competing procedures.</s></p></d>", "label": ["<d><p><s>stability approach to regularization selection (stars) for high dimensional graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>generalized linear models (glms) are an increasingly popular framework for modeling neural spike trains.</s> <s>they have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory, e.g.</s> <s>the time-rescaling theorem.</s> <s>however, high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection.</s> <s>here, we show how goodness-of-fit tests from point-process theory can still be applied to glms by constructing equivalent surrogate point processes out of time-series observations.</s> <s>furthermore, two additional tests based on thinning and complementing point processes are introduced.</s> <s>they augment the instruments available for checking model adequacy of point processes as well as discretized models.</s></p></d>", "label": ["<d><p><s>rescaling, thinning or complementing? on goodness-of-fit procedures for point process models and generalized linear models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a discriminative latent model for annotating images with unaligned object-level textual annotations.</s> <s>instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information.</s> <s>in particular, we model the mapping that translates image regions to annotations.</s> <s>this mapping allows us to relate image regions to their corresponding annotation terms.</s> <s>we also model the overall scene label as latent information.</s> <s>this allows us to cluster test images.</s> <s>our training data consist of images and their associated annotations.</s> <s>but we do not have access to the ground-truth region-to-annotation mapping or the overall scene label.</s> <s>we develop a novel variant of the latent svm framework to model them as latent variables.</s> <s>our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</s></p></d>", "label": ["<d><p><s>a discriminative latent model of image region and object tag correspondence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel probabilistic model for distributions over sets of structures -- for example, sets of sequences, trees, or graphs.</s> <s>the critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely.</s> <s>our model is a marriage of structured probabilistic models, like markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions.</s> <s>we extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts.</s> <s>we show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling.</s> <s>our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities.</s> <s>we illustrate the advantages of the model on tracking and articulated pose estimation problems.</s></p></d>", "label": ["<d><p><s>structured determinantal point processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sodium entry during an action potential determines the energy efficiency of a neuron.</s> <s>the classic hodgkin-huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization.</s> <s>yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic hodgkin-huxley model during the action potential.</s> <s>nevertheless, the original hodgkin-huxley model is still widely used and rarely to model the squid giant axon from which it was extracted.</s> <s>here, we introduce a novel family of hodgkin-huxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons.</s> <s>we speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to purkinje cells, yielding a very economical framework to model a wide range of different central neurons.</s> <s>the present paper demonstrates the performances and discuss the properties of this new family of models.</s></p></d>", "label": ["<d><p><s>sodium entry efficiency during action potentials: a novel single-parameter family of hodgkin-huxley models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>partially observable markov decision processes (pomdps) model sequential decision-making problems under uncertainty and partial observability.</s> <s>unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state.</s> <s>to that end, we introduce rho-pomdps, an extension of pomdps where the reward function rho depends on the belief state.</s> <s>we show that, under the common assumption that rho is convex, the value function is also convex, what makes it possible to (1) approximate rho arbitrarily well with a piecewise linear and convex (pwlc) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.</s></p></d>", "label": ["<d><p><s>a pomdp extension with belief-dependent rewards</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying gibbs distribution.</s> <s>in the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies.</s></p></d>", "label": ["<d><p><s>random walk approach to regret minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider least-squares regression using a randomly generated subspace g_p\\subset f of finite dimension p, where f is a function space of infinite dimension, e.g.~l_2([0,1]^d).</s> <s>g_p is defined as the span of p random features  that are linear combinations of the basis functions of f weighted by random gaussian i.i.d.~coefficients.</s> <s>in particular, we consider multi-resolution random combinations at all scales of a given mother function,  such as a hat function or a wavelet.</s> <s>in this latter case, the resulting gaussian objects are called {\\em scrambled wavelets} and we show that they enable to approximate functions in sobolev spaces h^s([0,1]^d).</s> <s>as a result, given n data, the least-squares estimate \\hat g built from p scrambled wavelets has excess risk ||f^* - \\hat g||_\\p^2 = o(||f^*||^2_{h^s([0,1]^d)}(\\log n)/p + p(\\log n )/n) for target functions f^*\\in h^s([0,1]^d) of smoothness order s>d/2.</s> <s>an interesting aspect of the resulting bounds is that they do not depend on the distribution \\p from which the data are generated, which is important in a statistical regression setting considered here.</s> <s>randomization enables to adapt to any possible distribution.</s> <s>we conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity \\tilde o(2^d n^{3/2}\\log n + n^2), where d is the dimension of the input space.</s></p></d>", "label": ["<d><p><s>scrambled objects for least-squares regression</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we deal with the problem of variable selection when  variables must be selected group-wise, with possibly overlapping groups defined a priori.</s> <s>in particular we propose a new optimization procedure  for solving the regularized algorithm presented in jacob et al.</s> <s>09, where the group lasso  penalty is generalized to overlapping groups of variables.</s> <s>while in jacob et al.</s> <s>09 the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and constrained newton method in a reduced dual space, corresponding to the active groups.</s> <s>this procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing  to reduce the  dimensionality of the data.</s> <s>the computational advantages of our scheme with respect to state-of-the-art algorithms  using data duplication are shown empirically with numerical simulations.</s></p></d>", "label": ["<d><p><s>a primal-dual algorithm for group sparse regularization with overlapping groups</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>probabilistic grammars are generative statistical models that are  useful for compositional and sequential structures.</s> <s>we present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss.</s> <s>we derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting.</s></p></d>", "label": ["<d><p><s>empirical risk minimization with approximations of probabilistic grammars</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies.</s> <s>the framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables.</s> <s>the method is motivated by the use of genotypes as ``instruments'' to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods.</s> <s>the method may be used for an effective screening of potentially interesting genotype phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early stage clinical trials.</s> <s>where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (qtls) detected in genetic linkage studies.</s> <s>the method is applied for examining effects of gene transcript levels in the liver on plasma hdl cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with $\\sim 10^5$ genetic instruments and $\\sim 47 \\times 10^3$ gene transcripts.</s></p></d>", "label": ["<d><p><s>sparse instrumental variables (spiv) for genome-wide studies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a sound and consistent foundation for the use of \\emph{nonrandom} exploration data in ``contextual bandit'' or ``partially labeled'' settings where only the value of a chosen action is learned.</s> <s>the primary challenge in a variety of settings is that the exploration policy, in which ``offline'' data is logged, is not explicitly known.</s> <s>prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner.</s> <s>the techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged.</s> <s>we empirically verify our solution on two reasonably sized sets of real-world data obtained from an internet %online advertising company.</s></p></d>", "label": ["<d><p><s>learning from logged implicit exploration data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for markov random fields (mrf).</s> <s>we start proving a bound   independent of the mrf structure and parameters.</s> <s>afterwards, we show how this bound can be improved for mrfs with particular structures such as bipartite graphs or grids.</s> <s>our results provide interesting insight into the behavior of max-product.</s> <s>for example, we prove that max-product provides very good results (at least 90% of the optimal) on mrfs  with large variable-disjoint cycles (mrfs in which all cycles are variable-disjoint, namely that they do not share any edge and in which each cycle contains at least 20 variables).</s></p></d>", "label": ["<d><p><s>worst-case bounds on the quality of max-product fixed-points</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the euclidean embedding of large sets of categorical data based on co-occurrence statistics.</s> <s>we use the code model of globerson et al.</s> <s>but constrain the embedding to lie on a high-dimensional unit sphere.</s> <s>this constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality.</s> <s>using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results.</s> <s>we analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks.</s></p></d>", "label": ["<d><p><s>sphere embedding: an application to part-of-speech induction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state.</s> <s>such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics.</s> <s>prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network.</s> <s>however a more ethologically relevant scenario is that of sparse input sequences.</s> <s>in this scenario, we show how linear neural networks can essentially perform compressed sensing (cs) of past inputs, thereby attaining a memory capacity that {\\it exceeds} the number of neurons.</s> <s>this enhanced capacity is achieved by a class of ``orthogonal recurrent networks and not by feedforward networks or generic recurrent networks.</s> <s>we exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time.</s> <s>alternately, viewed purely from the perspective of cs, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.\"</s></p></d>", "label": ["<d><p><s>short-term memory in neuronal networks through dynamical compressed sensing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study learning curves for gaussian process regression which characterise performance in terms of the bayes error averaged over datasets of a given size.</s> <s>whilst learning curves are in general very difficult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained.</s> <s>these should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a finite number of others.</s> <s>the method is based on translating the appropriate belief propagation equations to the graph ensemble.</s> <s>we demonstrate the accuracy of the predictions for poisson (erdos-renyi) and regular random graphs, and discuss when and why previous approximations to the learning curve fail.</s></p></d>", "label": ["<d><p><s>exact learning curves for gaussian process regression on large random graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the determination of dominant orientation at a given image location is formulated as a decision-theoretic question.</s> <s>this leads to a novel measure for the dominance of a given orientation $\\theta$, which is similar to that used by sift.</s> <s>it is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of v1.</s> <s>the measure can thus be seen as a biologically plausible version of sift, and is denoted as biosift.</s> <s>the network units are shown to exhibit trademark properties of v1 neurons, such as cross-orientation suppression, sparseness and independence.</s> <s>the connection between sift and biological vision provides a justification for the success of sift-like features and reinforces the importance of contrast normalization in computer vision.</s> <s>we illustrate this by replacing the gabor units of an hmax network with the new biosift units.</s> <s>this is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems.</s></p></d>", "label": ["<d><p><s>a biologically plausible network for the computation of orientation dominance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation.</s> <s>here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels.</s> <s>a simple standard thresholding spiking neuron suffices to carry out such an approximation, given a suitable refractory response.</s> <s>empirically, we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components, like long-memory self-similar signals.</s> <s>for such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar snr as compared to sums of similar but exponentially decaying kernels.</s> <s>as power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel.</s></p></d>", "label": ["<d><p><s>fractionally predictive spiking neurons</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many statistical $m$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.</s> <s>we analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.</s> <s>this high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis.</s> <s>we define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models.</s> <s>under these conditions, our theory guarantees that nesterov's first-order method~\\cite{nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical euclidean distance between the true unknown parameter $\\theta^*$ and the optimal solution $\\widehat{\\theta}$.</s> <s>this globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.</s> <s>our analysis applies to a wide range of $m$-estimators and statistical models, including sparse linear regression using lasso ($\\ell_1$-regularized regression), group lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization.</s> <s>overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.</s></p></d>", "label": ["<d><p><s>fast global convergence rates of gradient methods for high-dimensional statistical recovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our objective is to train $p$-norm multiple kernel learning (mkl) and, more generally, linear mkl regularised by the bregman divergence, using the sequential minimal optimization (smo) algorithm.</s> <s>the smo algorithm is simple, easy to implement and adapt, and efficiently scales to large problems.</s> <s>as a result, it has gained widespread acceptance and svms are routinely trained using smo in diverse real world applications.</s> <s>training using smo has been a long standing goal in mkl for the very same reasons.</s> <s>unfortunately, the standard mkl dual is not differentiable, and therefore can not be optimised using smo style co-ordinate ascent.</s> <s>in this paper, we demonstrate that linear mkl regularised with the $p$-norm squared, or with certain bregman divergences, can indeed be trained using smo.</s> <s>the resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised $p$-norm mkl solvers.</s> <s>we show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core.</s></p></d>", "label": ["<d><p><s>multiple kernel learning and the smo algorithm</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics.</s> <s>our algorithm solves an integer linear program in an online fashion.</s> <s>it exploits total unimodularity of the constraint matrix and a lagrangian relaxation to solve the problem as a convex online game.</s> <s>the algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows.</s> <s>we apply the algorithm to optimize tier arrangement of over 80 million web pages on a layered set of caches to serve an incoming query stream optimally.</s> <s>we provide an empirical demonstration of the effectiveness of our method on real query-pages data.</s></p></d>", "label": ["<d><p><s>optimal web-scale tiering as a flow problem</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper, we propose an efficient algorithm for estimating the natural policy gradient with parameter-based exploration; this algorithm samples directly in the parameter space.</s> <s>unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact fisher information matrix.</s> <s>the computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost.</s> <s>experimental results show that the proposed method outperforms several policy gradient methods.</s></p></d>", "label": ["<d><p><s>natural policy gradient methods with parameter-based exploration for control tasks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>feature selection is an important component of many machine learning applications.</s> <s>especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones.</s> <s>in this paper, we propose a new robust feature selection method with emphasizing joint ?2,1-norm minimization on both loss function and regularization.</s> <s>the ?2,1-norm based loss function is robust to outliers in data points and the ?2,1-norm regularization selects features across all data points with joint sparsity.</s> <s>an efficient algorithm is introduced with proved convergence.</s> <s>our regression based objective makes the feature selection process more efficient.</s> <s>our method has been applied into both genomic and proteomic biomarkers discovery.</s> <s>extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.</s></p></d>", "label": ["<d><p><s>efficient and robust feature selection via joint ?2,1-norms minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>applications of brain-machine-interfaces typically estimate user intent based on biological signals that are under voluntary control.</s> <s>for example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity.</s> <s>to solve such problems it is necessary to integrate obtained information over time.</s> <s>to do so, state of the art approaches typically use a probabilistic model of how the state, e.g.</s> <s>position and velocity of the arm, evolves over time ?</s> <s>a so-called trajectory model.</s> <s>we wanted to further develop this approach using two intuitive insights: (1) at any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user.</s> <s>(2) the user may want to produce movements at varying speeds.</s> <s>we thus use a generative model with a trajectory model incorporating these insights.</s> <s>approximate inference on that generative model is implemented using a mixture of extended kalman filters.</s> <s>we find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.</s></p></d>", "label": ["<d><p><s>mixture of time-warped trajectory models for movement decoding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available.</s> <s>this is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle.</s> <s>our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms.</s> <s>our proposed approach has a direct application for data integration with different label spaces for the purpose of classification, such as integrating yahoo!</s> <s>and dmoz web directories.</s></p></d>", "label": ["<d><p><s>multitask learning without label correspondences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the sample complexity of active learning under the realizability assumption has been well-studied.</s> <s>the realizability assumption, however, rarely holds in practice.</s> <s>in this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting.</s> <s>we prove that, with unbounded tsybakov noise, the sample complexity of multi-view active learning can be $\\widetilde{o}(\\log \\frac{1}{\\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement.</s> <s>we also prove that in general multi-view setting the sample complexity of active learning with unbounded tsybakov noise is $\\widetilde{o}(\\frac{1}{\\epsilon})$, where the order of $1/\\epsilon$ is independent of the parameter in tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\\epsilon$ is related to the parameter in tsybakov noise.</s></p></d>", "label": ["<d><p><s>multi-view active learning in the non-realizable case</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations.</s> <s>our nonparametric bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories.</s> <s>we introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian policy priors for reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider multivariate regression problems involving high-dimensional predictor and response spaces.</s> <s>to efficiently address such problems, we propose a variable selection method, multivariate group orthogonal matching pursuit, which extends the standard orthogonal matching pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs.</s> <s>we illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables.</s> <s>when applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to pagerank.</s> <s>theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.</s></p></d>", "label": ["<d><p><s>block variable selection in multivariate regression and high-dimensional causal inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples.</s> <s>in particular, we study the least-squares temporal difference (lstd) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space.</s> <s>we provide a thorough theoretical analysis of the lstd with random projections and derive performance bounds for the resulting algorithm.</s> <s>we also show how the error of lstd with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (lspi) algorithm.</s></p></d>", "label": ["<d><p><s>lstd with random projections</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model.</s> <s>firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip.</s> <s>the neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities.</s> <s>as the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.</s></p></d>", "label": ["<d><p><s>a vlsi implementation of the adaptive exponential integrate-and-fire neuron model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space.</s> <s>often, however, we are confronted with ``outliers'' in input space, which are isolated observations in sparsely populated regions.</s> <s>we show that heavy-tailed process priors (which we construct from gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions.</s> <s>we carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails.</s> <s>the analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.</s></p></d>", "label": ["<d><p><s>heavy-tailed process priors for selective shrinkage</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a class of sparse coding models that utilizes a laplacian scale mixture (lsm) prior to model dependencies among coefficients.</s> <s>each coefficient is modeled as a laplacian distribution with a variable scale parameter, with a gamma distribution prior over the scale parameter.</s> <s>we show that, due to the conjugacy of the gamma prior, it is possible to derive efficient inference procedures for both the coefficients and the scale parameter.</s> <s>when the scale parameters of a group of coefficients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients, which have been shown to constitute a large fraction of the redundancy in natural images.</s> <s>we show that, as a consequence of this group sparse coding, the resulting inference of the coefficients follows a divisive normalization rule, and that this may be efficiently implemented a network architecture similar to that which has been proposed to occur in primary visual cortex.</s> <s>we also demonstrate improvements in image coding and compressive sensing recovery using the lsm model.</s></p></d>", "label": ["<d><p><s>group sparse coding with a laplacian scale mixture prior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved.</s> <s>in this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis~(lda) model and note that the formulation is based on the average-case view.</s> <s>based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~(wlda) by defining new between-class and within-class scatter measures.</s> <s>this new model adopts the worst-case view which arguably is more suitable for applications such as classification.</s> <s>when the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem.</s> <s>otherwise, we take a greedy approach by finding one direction of the transformation at a time.</s> <s>moreover, we also analyze a special case of wlda to show its relationship with conventional lda.</s> <s>experiments conducted on several benchmark datasets demonstrate the effectiveness of wlda when compared with some related dimensionality reduction methods.</s></p></d>", "label": ["<d><p><s>worst-case linear discriminant analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of estimating the f-measure of a given model as accurately as possible on a fixed labeling budget.</s> <s>this problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution.</s> <s>in this case, new test instances have to be drawn and labeled at a cost.</s> <s>an active estimation procedure selects instances according to an instrumental sampling distribution.</s> <s>an analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance.</s> <s>we explore conditions under which active estimates of f-measures are more accurate than estimates based on instances sampled from the test distribution.</s></p></d>", "label": ["<d><p><s>active estimation of f-measures</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>regularization technique has become a principle tool for statistics and machine learning research and practice.</s> <s>however, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data.</s> <s>in this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions.</s> <s>we show that various regularization terms are essentially corresponding to different distortions to the original data matrix.</s> <s>this minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases.</s> <s>within this minimax framework, we further gave mathematically exact definition for a novel representation called sparse grouping representation (sgr), and proved sufficient conditions for generating such group level sparsity.</s> <s>under these sufficient conditions, a large set of consistent regularization terms can be designed.</s> <s>this sgr is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise.</s> <s>we also gave out some generalization bounds in a classification setting.</s></p></d>", "label": ["<d><p><s>sufficient conditions for generating group level sparsity in a robust minimax framework</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function.</s> <s>each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a gaussian process prior.</s> <s>in this paper we consider employing the latent force model framework for the problem of determining robot motor primitives.</s> <s>to deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems.</s> <s>this creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics.</s> <s>we give illustrative examples on both synthetic data and for striking movements recorded using a barrett wam robot as haptic input device.</s> <s>our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.</s></p></d>", "label": ["<d><p><s>switched latent force models for movement segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence.</s> <s>our approach retains the advantages of tractable discriminative models, namely efficient exact inference and exact parameter learning.</s> <s>at the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures.</s> <s>on real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup</s></p></d>", "label": ["<d><p><s>evidence-specific structures for rich tractable crfs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs.</s> <s>each of these tasks is often notoriously hard, and state-of-the-art classifiers already exist for many sub-tasks.</s> <s>it is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classifier.</s> <s>we propose feedback enabled cascaded classification models (fe-ccm), that maximizes the joint likelihood of the sub-tasks, while requiring only a ?black-box?</s> <s>interface to the original classifier for each sub-task.</s> <s>we use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input.</s> <s>our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about what error modes to focus on.</s> <s>we show that our method significantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classification.</s></p></d>", "label": ["<d><p><s>towards holistic scene understanding: feedback enabled cascaded classification models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms.</s> <s>it is shown that both algorithms can be viewed as an application of the perceptron cycling theorem.</s> <s>this connection strengthens some herding results and suggests new (supervised) herding algorithms that, like crfs or discriminative rbms, make predictions by conditioning on the input attributes.</s> <s>we develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative rbm.</s></p></d>", "label": ["<d><p><s>on herding and the perceptron cycling theorem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>singular value decomposition (and principal component analysis)  is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers.</s> <s>recent work has considered the setting where each point has a few arbitrarily corrupted components.</s> <s>yet, in applications of svd or pca such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted.</s> <s>we present an efficient convex optimization-based  algorithm we call outlier pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in pca problems) recovers the *exact* optimal low-dimensional subspace, and identifies the corrupted points.</s> <s>such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond.</s> <s>our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct *column space* of the uncorrupted matrix, rather than the exact matrix itself.</s></p></d>", "label": ["<d><p><s>robust pca via outlier pursuit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>with the increase in available data parallel machine learning has   become an increasingly pressing problem.</s> <s>in this paper we present   the first parallel stochastic gradient descent algorithm including a   detailed analysis and experimental evidence.</s> <s>unlike prior work on   parallel optimization algorithms our   variant comes with parallel acceleration guarantees and it poses no   overly tight latency constraints, which might only be available in   the multicore setting.</s> <s>our analysis introduces a novel proof   technique --- contractive mappings to quantify the   speed of convergence of parameter distributions to their asymptotic   limits.</s> <s>as a side effect this answers the question of how quickly   stochastic gradient descent algorithms reach the asymptotically   normal regime.</s></p></d>", "label": ["<d><p><s>parallelized stochastic gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera.</s> <s>in order to overcome such limitations we contribute threefold: (i) we introduce a taxonomy of camera shakes, (ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from cho and lee to introduce a method for blind deconvolution for space-variant blur.</s> <s>and (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur.</s> <s>finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.</s></p></d>", "label": ["<d><p><s>space-variant single-image blind deconvolution for removing camera shake</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings.</s> <s>for high level visual tasks, such low-level image representations are potentially not enough.</s> <s>in this paper, we propose a high-level image representation, called the object bank, where an image is represented as a scale invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task.</s> <s>leveraging on the object bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear svm.</s> <s>sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</s></p></d>", "label": ["<d><p><s>object bank: a high-level image representation for scene classification & semantic feature sparsification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a co-regularization based approach to semi-supervised domain adaptation.</s> <s>our proposed approach (ea++) builds on the notion of augmented space (introduced in easyadapt (ea) [1]) and harnesses unlabeled data in target domain to further enable the transfer of information from source to target.</s> <s>this semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner.</s> <s>our theoretical analysis (in terms of rademacher complexity) of ea and ea++ show that the hypothesis class of ea++ has lower complexity (compared to ea) and hence results in tighter generalization bounds.</s> <s>experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to ea as well as a few other baseline approaches.</s></p></d>", "label": ["<d><p><s>co-regularization based semi-supervised domain adaptation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present an algorithm for learning high-treewidth markov networks where inference is still tractable.</s> <s>this is made possible by exploiting context specific independence and determinism in the domain.</s> <s>the class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed form weight learning, etc., but is much broader.</s> <s>our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature or its negation) and recurses on each subspace/subset of variables until no useful new features can be found.</s> <s>we provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is k (the treewidth can be much larger) and dependences are of bounded strength.</s> <s>we also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient.experiments on a variety of domains show that our approach compares favorably with thin junction trees and other markov network structure learners.</s></p></d>", "label": ["<d><p><s>learning efficient markov networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>arithmetic circuits (acs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth.</s> <s>in this paper, we introduce the first ever approximate inference methods using acs, for domains where exact inference remains intractable.</s> <s>we propose and evaluate a variety of techniques based on exact compilation, forward sampling, ac structure learning, markov network parameter learning, variational inference, and gibbs sampling.</s> <s>in experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (ac2-f) is faster and usually more accurate than loopy belief propagation, mean field, and gibbs sampling; another (ac2-g) has a running time similar to gibbs sampling but is consistently more accurate than all baselines.</s></p></d>", "label": ["<d><p><s>approximate inference by compilation to arithmetic circuits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents an approach to the visual recognition of human actions using only single images as input.</s> <s>the task is easy for humans but difficult for current approaches to object recognition, because action instances may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized.</s> <s>the proposed approach applies a two-stage interpretation procedure to each training and test image.</s> <s>the first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action.</s> <s>the second stage extracts features that are ?anchored?</s> <s>to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action.</s> <s>the body anchored priors we propose apply to a large range of human actions.</s> <s>these priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance.</s></p></d>", "label": ["<d><p><s>using body-anchored priors for identifying actions in single images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present policy gradient results within the framework of linearly-solvable mdps.</s> <s>for the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional mdps.</s> <s>we also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.</s></p></d>", "label": ["<d><p><s>policy gradients in linearly-solvable mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present and analyze an agnostic active learning algorithm that works without keeping a version space.</s> <s>this is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned.</s> <s>by avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.</s></p></d>", "label": ["<d><p><s>agnostic active learning without constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the following sparse signal recovery (or feature selection) problem: given a design matrix $x\\in \\mathbb{r}^{n\\times m}$ $(m\\gg n)$ and a noisy observation vector $y\\in \\mathbb{r}^{n}$ satisfying $y=x\\beta^*+\\epsilon$ where $\\epsilon$ is the noise vector following a gaussian distribution $n(0,\\sigma^2i)$, how to recover the signal (or parameter vector) $\\beta^*$ when the signal is sparse?</s> <s>the dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees.</s> <s>in this paper, we propose a multi-stage dantzig selector method, which iteratively refines the target signal $\\beta^*$.</s> <s>we show that if $x$ obeys a certain condition, then with a large probability the difference between the solution $\\hat\\beta$ estimated by the proposed method and the true solution $\\beta^*$ measured in terms of the $l_p$ norm ($p\\geq 1$) is bounded as \\begin{equation*} \\|\\hat\\beta-\\beta^*\\|_p\\leq \\left(c(s-n)^{1/p}\\sqrt{\\log m}+\\delta\\right)\\sigma, \\end{equation*} $c$ is a constant, $s$ is the number of nonzero entries in $\\beta^*$, $\\delta$ is independent of $m$ and is much smaller than the first term, and $n$ is the number of entries of $\\beta^*$ larger than a certain value in the order of $\\mathcal{o}(\\sigma\\sqrt{\\log m})$.</s> <s>the proposed method improves the estimation bound of the standard dantzig selector approximately from $cs^{1/p}\\sqrt{\\log m}\\sigma$ to $c(s-n)^{1/p}\\sqrt{\\log m}\\sigma$ where the value $n$ depends on the number of large entries in $\\beta^*$.</s> <s>when $n=s$, the proposed algorithm achieves the oracle solution with a high probability.</s> <s>in addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the dantzig selector.</s></p></d>", "label": ["<d><p><s>multi-stage dantzig selector</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>functional segregation and integration are fundamental characteristics of the human brain.</s> <s>studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest.</s> <s>a very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or rois (regions of interests) for individuals.</s> <s>essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the rois.</s> <s>this paper presents a novel methodology to optimize the locations of an individual's rois in the working memory system.</s> <s>our strategy is to formulate the individual roi optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints.</s> <s>the optimization problem is solved via the simulated annealing approach.</s> <s>our experimental results show that the optimized rois have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles.</s></p></d>", "label": ["<d><p><s>individualized roi optimization via maximization of group-wise consistency of structural and functional profiles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting.</s> <s>this framework allows to design and prove   relative mistake bounds for any generic loss function.</s> <s>the mistake   bounds can be specialized for the hinge loss, allowing to recover   and improve the bounds of known online classification   algorithms.</s> <s>by optimizing the general bound we derive a new online   classification algorithm, called narow, that hybridly uses adaptive- and fixed- second order   information.</s> <s>we analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.</s></p></d>", "label": ["<d><p><s>new adaptive algorithms for online classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems.</s> <s>however, they generally require the temporal parameters (for e.g.</s> <s>the movement duration or the time point of reaching an intermediate goal) to be specified \\textit{a priori}.</s> <s>here, we present a methodology that is capable of jointly optimising the temporal parameters in addition to the control command profiles.</s> <s>the presented approach is based on a bayesian canonical time formulation of the optimal control problem, with the temporal mapping from canonical to real time parametrised by an additional control variable.</s> <s>an approximate em algorithm is derived that efficiently optimises both the movement duration and control commands offering, for the first time, a practical approach to tackling generic via point problems in a systematic way under the optimal control framework.</s> <s>the proposed approach is evaluated on simulations of a redundant robotic plant.</s></p></d>", "label": ["<d><p><s>an approximate inference approach to temporal optimization in optimal control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with the generalization analysis on learning to rank for information retrieval (ir).</s> <s>in ir, data are hierarchically organized, i.e., consisting of queries and documents per query.</s> <s>previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of algorithms.</s> <s>in this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d.</s> <s>sampling of queries and the conditional i.i.d sampling of documents per query.</s> <s>such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms.</s> <s>however, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent if represented by features extracted from the matching between document and query.</s> <s>to tackle the challenge, we decompose the generalization error according to the two layers, and make use of the new concept of two-layer rademacher average.</s> <s>the generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performance of ranking algorithms.</s></p></d>", "label": ["<d><p><s>two-layer generalization analysis for ranking using rademacher average</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy.</s> <s>cortical neurons are then connected by a sparse network of lateral synapses.</s> <s>here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept.</s> <s>we demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity.</s> <s>we term this choice of connectivity receptive field recombination (refire) networks.</s> <s>the sparse refire network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit.</s></p></d>", "label": ["<d><p><s>over-complete representations on recurrent neural networks can support persistent percepts</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider a convex relaxation $\\hat f$ of a pseudo-boolean function $f$.</s> <s>we say that the relaxation is {\\em totally half-integral} if $\\hat f(\\bx)$ is a polyhedral function with half-integral extreme points $\\bx$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\\gamma$ where $\\gamma\\in\\{0,1,\\frac{1}{2}\\}$ is a constant.</s> <s>a well-known example is the {\\em roof duality} relaxation for quadratic pseudo-boolean functions $f$.</s> <s>we argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions.</s> <s>our contributions are as follows.</s> <s>first, we provide a complete characterization of totally half-integral relaxations $\\hat f$ by establishing a one-to-one correspondence with {\\em bisubmodular functions}.</s> <s>second, we give a new characterization of bisubmodular functions.</s> <s>finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality.</s></p></d>", "label": ["<d><p><s>generalized roof duality and bisubmodular functions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>when software developers modify one or more files in a large code base, they must also identify and update other related files.</s> <s>many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows.</s> <s>from data of this form, we show how to detect dependent files by solving a problem in binary matrix completion.</s> <s>we explore different latent variable models (lvms) for this problem, including bernoulli mixture models, exponential family pca, restricted boltzmann machines, and fully bayesian approaches.</s> <s>we evaluate these models on the development histories of three large, open-source software systems: mozilla firefox, eclipse subversive, and gimp.</s> <s>in all of these applications, we find that lvms improve the performance of related file prediction over current leading methods.</s></p></d>", "label": ["<d><p><s>latent variable models for predicting file dependencies in large-scale software development</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel approach to inference in conditionally gaussian continuous time stochastic processes, where the latent process is a markovian jump process.</s> <s>we first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points.</s> <s>we derive partial differential equations for exact inference and present a very efficient mean field approximation.</s> <s>by introducing a novel lower bound on the free energy, we then generalise our approach to gaussian processes with arbitrary covariance, such as the non-markovian rbf covariance.</s> <s>we present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</s></p></d>", "label": ["<d><p><s>approximate inference in continuous time gaussian-jump processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of controlling the margin of a classifier is studied.</s> <s>a detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties.</s> <s>it is shown that for a class of risks, denoted canonical risks, asymptotic bayes consistency is compatible with simple analytical relationships between these functions.</s> <s>these enable a precise characterization of the loss for a popular class of link functions.</s> <s>it is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter.</s> <s>novel families of bayes consistent loss functions, of variable margin, are derived.</s> <s>these families are then used to design boosting style algorithms with explicit control of the classification margin.</s> <s>the new algorithms generalize well established approaches, such as logitboost.</s> <s>experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms.</s> <s>finally, it is shown that best performance can be achieved by cross-validating the margin parameter.</s></p></d>", "label": ["<d><p><s>variable margin losses for classifier design</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a setting in which poisson processes generate sequences of decision-making events.</s> <s>the optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions.</s> <s>we model the problem as a poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently.</s> <s>this problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events.</s> <s>we report on experiments on abuse detection for an email service.</s></p></d>", "label": ["<d><p><s>throttling poisson processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel algorithm, random conic pursuit, that solves semidefinite programs (sdps) via repeated optimization over randomly selected two-dimensional subcones of the psd cone.</s> <s>this scheme is simple, easily implemented, applicable to very general sdps, scalable, and theoretically interesting.</s> <s>its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained.</s> <s>this property renders random conic pursuit of particular interest for machine learning applications, in which the relevant sdps are generally based upon random data and so exact minima are often not a priority.</s> <s>indeed, we present empirical results to this effect for various sdps encountered in machine learning; these experiments demonstrate the potential practical usefulness of random conic pursuit.</s> <s>we also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm.</s></p></d>", "label": ["<d><p><s>random conic pursuit for semidefinite programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible.</s> <s>this problem can be alleviated by imposing (or learning) a structure over the set of classes.</s> <s>we propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods.</s> <s>we also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches.</s> <s>finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including one-vs-rest while being orders of magnitude faster.</s></p></d>", "label": ["<d><p><s>label embedding trees for large multi-class tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function.</s> <s>submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time.</s> <s>unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for practical problems.</s> <s>in this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable.</s> <s>decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions.</s> <s>we develop an algorithm, slg, that can efficiently minimize decomposable submodular functions with tens of thousands of variables.</s> <s>our algorithm exploits recent results in smoothed convex minimization.</s> <s>we apply slg to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.</s></p></d>", "label": ["<d><p><s>efficient minimization of decomposable submodular functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes.</s> <s>it is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making.</s> <s>choice behavior has been empirically found to follow herrnstein?s matching law.</s> <s>loewenstein &amp; seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities.</s> <s>however, their proof did not take into account the change in entire synaptic distributions.</s> <s>in this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons.</s> <s>this is caused by the increasing variance in the input potential due to the diffusion of synaptic weights.</s> <s>this effect causes an undermatching phenomenon, which has been observed in many behavioral experiments.</s> <s>we suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</s></p></d>", "label": ["<d><p><s>effects of synaptic weight diffusion on learning in decision making networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other.</s> <s>for image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods.</s> <s>we present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches.</s> <s>in particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation.</s> <s>additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an mrf with a robust spatial prior; the resulting model allows roughness in layers.</s> <s>finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation.</s> <s>the method achieves state-of-the-art results on the middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.</s></p></d>", "label": ["<d><p><s>layered image motion with explicit occlusions, temporal consistency, and depth ordering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces a monte-carlo algorithm for online planning in large pomdps.</s> <s>the algorithm combines a monte-carlo update of the agent's belief state with a monte-carlo tree search from the current belief state.</s> <s>the new algorithm, pomcp, has two important properties.</s> <s>first, monte-carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning.</s> <s>second, only a black box simulator of the pomdp is required, rather than explicit probability distributions.</s> <s>these properties enable pomcp to plan effectively in significantly larger pomdps than has previously been possible.</s> <s>we demonstrate its effectiveness in three large pomdps.</s> <s>we scale up a well-known benchmark problem, rocksample, by several orders of magnitude.</s> <s>we also introduce two challenging new pomdps: 10x10 battleship and partially observable pacman, with approximately 10^18 and 10^56 states respectively.</s> <s>our monte-carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search.</s> <s>pomcp is the first general purpose planner to achieve high performance in such large and unfactored pomdps.</s></p></d>", "label": ["<d><p><s>monte-carlo planning in large pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with l2 regularization: we introduce the gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the gamma-adapted-dimension of the source distribution.</s> <s>we conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification.</s> <s>the bounds hold for a rich family of sub-gaussian distributions.</s></p></d>", "label": ["<d><p><s>tight sample complexity of large-margin learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of optimal and automatic design of a detector cascade is considered.</s> <s>a novel mathematical model is introduced for a cascaded detector.</s> <s>this model is analytically tractable, leads to recursive computation, and accounts for both classification and complexity.</s> <s>a boosting algorithm, fcboost, is proposed for fully automated cascade design.</s> <s>it exploits the new cascade model, minimizes a lagrangian cost that accounts for both classification risk and complexity.</s> <s>it searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning.</s> <s>experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.</s></p></d>", "label": ["<d><p><s>boosting classifier cascades</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>as increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances.</s> <s>though the differential privacy model  provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting.</s> <s>in this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier  using classifiers trained locally by separate mutually untrusting parties.</s> <s>the protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier.</s> <s>we also present a detailed theoretical analysis containing a proof of differential privacy  of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation.</s> <s>we verify the bound with an experimental evaluation on a real dataset.</s></p></d>", "label": ["<d><p><s>multiparty differential privacy via aggregation of locally trained classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of semi-supervised learning in an adversarial setting.</s> <s>instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples.</s> <s>we present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information.</s> <s>motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efficient algorithm, and analyze its convergence.</s> <s>we provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines.</s></p></d>", "label": ["<d><p><s>semi-supervised learning with adversarially missing label information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>since the discovery of sophisticated fully polynomial randomized algorithms for a range of #p problems (karzanov et al., 1991; jerrum et al., 2001; wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on markov chain monte carlo methods.</s> <s>despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.</s> <s>because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (siepel et al., 2004).</s> <s>variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments.</s> <s>we propose a new framework that extends variational inference to a wide range of combinatorial spaces.</s> <s>our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples.</s> <s>simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.</s> <s>we also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the balibase dataset (thompson et al., 1999).</s></p></d>", "label": ["<d><p><s>variational inference over combinatorial spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new way of converting a reversible finite markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the mcmc estimator based on the non-reversible chain is reduced.</s> <s>the method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph.</s> <s>our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving mcmc.</s></p></d>", "label": ["<d><p><s>improving the asymptotic performance of markov chain monte-carlo by inserting vortices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to understand the relationship between genomic variations among population and complex diseases, it is essential to detect eqtls which are associated with phenotypic effects.</s> <s>however, detecting eqtls remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples.</s> <s>thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.</s> <s>in this paper, we propose a novel regularized regression approach for detecting eqtls which takes into account related traits simultaneously while incorporating many regulatory features.</s> <s>we first present a bayesian network for a multi-task learning problem that includes priors on snps, making it possible to estimate the significance of each covariate adaptively.</s> <s>then we find the maximum a posteriori (map) estimation of regression coefficients and estimate weights of covariates jointly.</s> <s>this optimization procedure is efficient since it can be achieved by using convex optimization and a coordinate descent procedure iteratively.</s> <s>experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eqtls.</s></p></d>", "label": ["<d><p><s>adaptive multi-task lasso: with application to eqtl detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the random projection tree (rptree) structures proposed in [dasgupta-freund-stoc-08] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data.</s> <s>we prove new results for both the rptree-max and the rptree-mean data structures.</s> <s>our result for rptree-max gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s >= 2.</s> <s>we also prove a packing lemma for this data structure.</s> <s>our final result shows that low-dimensional manifolds possess bounded local covariance dimension.</s> <s>as a consequence we show that rptree-mean adapts to manifold dimension as well.</s></p></d>", "label": ["<d><p><s>random projection trees revisited</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning a local metric to enhance the performance of nearest neighbor classification.</s> <s>conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models.</s> <s>we focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models.</s> <s>as a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches.</s> <s>empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models.</s></p></d>", "label": ["<d><p><s>generative local metric learning for nearest neighbor classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels.</s> <s>this is the case, e.g., when learning visual classifiers from images downloaded from the web, using just their text captions or tags as learning oracles.</s> <s>in general, these problems can be very difficult.</s> <s>however most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed.</s> <s>in this paper, we propose a semi-supervised framework to model this kind of problems.</s> <s>each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors.</s> <s>each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct.</s> <s>the use of the labeling vectors provides a principled way not to exclude any information.</s> <s>we propose a large margin discriminative formulation, and an efficient algorithm to solve it.</s> <s>experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to svm trained with the ground-truth labels, and outperforms other baselines.</s></p></d>", "label": ["<d><p><s>learning from candidate labeling sets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>steinwart was the ?rst to prove universal consistency of support vector machine  classi?cation.</s> <s>his proof analyzed the ?standard?</s> <s>support vector machine classi?er,  which is restricted to binary classi?cation problems.</s> <s>in contrast, recent analysis  has resulted in the common belief that several extensions of svm classi?cation to  more than two classes are inconsistent.</s> <s>countering this belief, we proof the universal consistency of the multi-class support vector machine by crammer and singer.</s> <s>our proof extends steinwart?s techniques to the multi-class case.</s></p></d>", "label": ["<d><p><s>universal consistency of multi-class support vector classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames.</s> <s>we focus on the practically-attractive case when the training images are annotated with dots (one dot per object).</s> <s>our goal is to accurately estimate the count.</s> <s>however, we evade the hard task of learning to detect and localize individual object instances.</s> <s>instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region.</s> <s>learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function.</s> <s>we introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm.</s> <s>the learning can then be posed as a convex quadratic program solvable with cutting-plane optimization.</s> <s>the proposed framework is very flexible as it can accept any domain-specific visual features.</s> <s>once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.</s></p></d>", "label": ["<d><p><s>learning to count objects in images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when working with network datasets, the theoretical framework of detection theory for euclidean vector spaces no longer applies.</s> <s>nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties.</s> <s>casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a detection theory\" for graph-valued data.</s> <s>its focus is the detection of anomalies in unweighted, undirected graphs through l1 properties of the eigenvectors of the graph?s so-called modularity matrix.</s> <s>this metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph.</s> <s>an analysis of subgraphs in real network datasets confirms the efficacy of this approach.\"</s></p></d>", "label": ["<d><p><s>subgraph detection using eigenvector l1 norms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations.</s> <s>the average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function.</s> <s>we present an efficient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data.</s> <s>our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods.</s> <s>our method also provides a unified solution to clustering from k-ary affinity relations with k ?</s> <s>2, that is, it applies to both graph-based and hypergraph-based clustering problems.</s> <s>both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</s></p></d>", "label": ["<d><p><s>robust clustering as ensembles of affinity relations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in the neural-network parameter space,  an attractive field is likely to be induced by singularities.</s> <s>in such a singularity region, first-order gradient learning typically causes a long plateau with very little change  in the objective function value e (hence, a flat region).</s> <s>therefore, it may be confused with ``attractive'' local minima.</s> <s>our analysis shows that the hessian matrix of e tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus.</s> <s>for numerical evidence, we limit the scope to small examples (some of which are found in journal papers)  that allow us to confirm singularities and the eigenvalues of the hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau.</s> <s>even for those small problems, no efficient methods have been previously developed that avoided plateaus.</s></p></d>", "label": ["<d><p><s>an analysis on negative curvature induced by singularity in multi-layer neural-network learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new learning strategy for classification problems in which train and/or test data suffer from missing features.</s> <s>in previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace.</s> <s>in contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case.</s> <s>building onto this framework, we propose a classification strategy for sets.</s> <s>our proposal maps (feature,value) pairs into an embedding space and then non-linearly combines the set of embedded vectors.</s> <s>the embedding and the combination parameters are learned jointly on the final classification objective.</s> <s>this simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets.</s></p></d>", "label": ["<d><p><s>feature set embedding for incomplete data</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider online learning in finite stochastic markovian environments where in each time step a new reward function is chosen by an oblivious adversary.</s> <s>the goal of the learning agent is to compete with the best stationary policy in terms of the total reward received.</s> <s>in each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs.</s> <s>the agent is assumed to know the transition probabilities.</s> <s>the state of the art result for this setting is a no-regret algorithm.</s> <s>in this paper we propose a new learning algorithm and assuming that stationary policies mix uniformly fast, we show that after t time steps, the expected regret of the new algorithm is o(t^{2/3} (ln t)^{1/3}), giving the first rigorously proved convergence rate result for the problem.</s></p></d>", "label": ["<d><p><s>online markov decision processes under bandit feedback</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>identifying the features of objects becomes a challenge when those features can change in their appearance.</s> <s>we introduce the transformed indian buffet process (tibp), and use it to define a nonparametric bayesian model that infers features that can transform across instantiations.</s> <s>we show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning.</s> <s>however, allowing features to transform adds new kinds of ambiguity: are two parts of an object the same feature with different transformations or two unique features?</s> <s>what transformations can features undergo?</s> <s>we present two new experiments in which we explore how people resolve these questions, showing that the tibp model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features.</s></p></d>", "label": ["<d><p><s>learning invariant features using the transformed indian buffet process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context.</s> <s>this suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron.</s> <s>in this paper we implemented an l1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from v1 with a 96-electrode utah\" array.</s> <s>we found that the spikes of surrounding neurons indeed provide strong predictions of a neuron's response, in addition to the neuron's receptive field transfer function.</s> <s>we also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states.</s> <s>this work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions.\"</s></p></d>", "label": ["<d><p><s>accounting for network effects in neuronal responses using l1 regularized point process models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>how are the spatial patterns of spontaneous and evoked population responses related?</s> <s>we study the impact of connectivity on the spatial pattern of fluctuations in the input-generated response of a neural network, by comparing the distribution of evoked and intrinsically generated activity across the different units.</s> <s>we develop a complementary approach to principal component analysis in which separate high-variance directions are typically derived for each input condition.</s> <s>we analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity.</s> <s>in addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g.</s> <s>from voltage- or calcium-sensitive optical imaging experiments).</s> <s>we conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses.</s></p></d>", "label": ["<d><p><s>inferring stimulus selectivity from the spatial structure of neural network dynamics</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study several classes of interactive assistants from the points of view of decision theory and computational complexity.</s> <s>we first introduce a class of pomdps called hidden-goal mdps (hgmdps), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable.</s> <s>in spite of its restricted nature, we show that optimal action selection in finite horizon hgmdps is pspace-complete even in domains with deterministic dynamics.</s> <s>we then introduce a more restricted model called helper action mdps (hamdps), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise.</s> <s>we show classes of hamdps that are complete for pspace and np along with a polynomial time class.</s> <s>furthermore, we show that for general hamdps a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution.</s> <s>a variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.</s></p></d>", "label": ["<d><p><s>a computational decision theory for interactive assistants</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>experts (human or computer) are often required to assess the probability of uncertain events.</s> <s>when a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability.</s> <s>such an assessment is termed incoherent.</s> <s>in this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.</s></p></d>", "label": ["<d><p><s>probabilistic belief revision with structural constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances.</s> <s>studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data.</s> <s>thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation.</s> <s>in this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem.</s> <s>in particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance.</s> <s>we show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.</s></p></d>", "label": ["<d><p><s>energy disaggregation via discriminative sparse coding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider linear models for stochastic dynamics.</s> <s>any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics.</s> <s>we tackle the problem of learning such a network from observation of the system trajectory over a time interval t. we analyse the l1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high.</s> <s>this result substantiates the notion of a well defined ?time complexity?</s> <s>for the network inference problem.</s></p></d>", "label": ["<d><p><s>learning networks of stochastic differential equations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach.</s> <s>in order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the hidden markov model technology that is at the core of all present commercial systems.</s> <s>in this paper, it is shown that the recently introduced concept of reservoir computing might form the basis of such a methodology.</s> <s>in a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built.</s> <s>the system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.</s></p></d>", "label": ["<d><p><s>phoneme recognition with large hierarchical reservoirs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>functional magnetic resonance imaging (fmri) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level.</s> <s>most analyses of functional resting state networks (rsn) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain.</s> <s>while these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact.</s> <s>in this paper we take a different view on the analysis of functional resting state networks.</s> <s>starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information.</s> <s>we use the infinite relational model (irm) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects.</s></p></d>", "label": ["<d><p><s>infinite relational modeling of functional connectivity in resting state fmri</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>figure/ground assignment, in which the visual image is divided into nearer (figural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood.</s> <s>figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership.</s> <s>in this paper we model figure/ground estimation in a bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and t-junctions) interact with more global cues to yield a figure/ground assignment.</s> <s>our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure ``draws'' border ownership so that borders are owned by their interiors.</s> <s>we also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a t-junction).</s> <s>both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours.</s></p></d>", "label": ["<d><p><s>a bayesian framework for figure-ground interpretation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent.</s> <s>it is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems.</s> <s>in such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions.</s> <s>rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain.</s> <s>we first embed each brain into a functional map that reflects connectivity patterns during a fmri experiment.</s> <s>the resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains.</s> <s>in application to a language fmri experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects.</s> <s>this advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.</s></p></d>", "label": ["<d><p><s>functional geometry alignment and localization of brain areas</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>score matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable.</s> <s>it has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters.</s> <s>we show how this differentiation can be automated with an extended version of the double-backpropagation algorithm.</s> <s>in addition, we introduce a regularization term for the score matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values.</s> <s>results are reported for image denoising and super-resolution.</s></p></d>", "label": ["<d><p><s>regularized estimation of image statistics by score matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep networks can potentially express a learning problem more efficiently than local learning machines.</s> <s>while deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure.</s> <s>we present an analysis based on gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input.</s> <s>we use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.</s></p></d>", "label": ["<d><p><s>layer-wise analysis of deep networks with gaussian kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we consider the problem of learning from data  the  support of a probability distribution when  the distribution {\\em does not} have a density (with respect to some reference measure).</s> <s>we  propose a  new class of regularized spectral estimators based on a new notion of reproducing kernel hilbert space,  which we call {\\em   ``completely regular''}.</s> <s>completely regular kernels   allow to capture the relevant  geometric and topological properties  of an arbitrary probability space.</s> <s>in particular, they are the key ingredient to prove the  universal consistency  of the spectral  estimators and in this respect they are the analogue of  universal kernels for supervised problems.</s> <s>numerical  experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for  density support  estimation.</s></p></d>", "label": ["<d><p><s>spectral regularization for support estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian methods of matrix factorization (mf) have been actively explored recently as promising alternatives to classical singular value decomposition.</s> <s>in this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational bayesian (vb) mf can be computed analytically by solving a quartic equation.</s> <s>this is highly advantageous over a popular vbmf algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations.</s> <s>we further show that the global optimal solution of empirical vbmf (hyperparameters are also learned from data) can also be analytically computed.</s> <s>we illustrate the usefulness of our results through experiments.</s></p></d>", "label": ["<d><p><s>global analytic solution for variational bayesian matrix factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models.</s> <s>the time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes.</s> <s>in order to address this shortcoming, in recent years several authors have proposed to learn object classifiers from weakly-labeled internet images, such as photos retrieved by keyword-based image search engines.</s> <s>while this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to web data.</s> <s>in this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled web photos retrieved using keyword-based image search.</s> <s>we cast this as a domain adaptation problem: given a few strongly-labeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled web photos), learn classifiers yielding small generalization error on the target domain.</s> <s>our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces significant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</s></p></d>", "label": ["<d><p><s>exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>divisive normalization (dn) has been advocated as an effective nonlinear {\\em efficient coding} transform for natural sensory signals with applications in biology and engineering.</s> <s>in this work, we aim to establish a connection between the dn transform and the statistical properties of natural sensory signals.</s> <s>our analysis is based on the use of multivariate {\\em t} model to capture some important statistical properties of natural sensory signals.</s> <s>the multivariate {\\em t} model justifies dn as an approximation to the transform that completely eliminates its statistical dependency.</s> <s>furthermore, using the multivariate {\\em t} model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the dn transform.</s> <s>we compare this with the actual performance of the dn transform in reducing statistical dependencies of natural sensory signals.</s> <s>our theoretical analysis and quantitative evaluations confirm dn as an effective efficient coding transform for natural sensory signals.</s> <s>on the other hand, we also observe a previously unreported phenomenon that dn may increase statistical dependencies when the size of pooling is small.</s></p></d>", "label": ["<d><p><s>divisive normalization: justification and effectiveness as efficient coding transform</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many studies have explored the impact of response variability on the quality of sensory codes.</s> <s>the source of this variability is almost always assumed to be intrinsic to the brain.</s> <s>however, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise.</s> <s>here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference.</s> <s>we characterize the response distribution for the binocular energy model in response to random dot stereograms and find it to be very different from the poisson-like noise usually assumed.</s> <s>we then compute the fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them.</s> <s>then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response.</s> <s>we find that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts.</s> <s>furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions.</s></p></d>", "label": ["<d><p><s>evaluating neuronal codes for inference using fisher information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lifted inference algorithms for representations that combine first-order logic and probabilistic graphical models have been the focus of much recent research.</s> <s>all lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.)</s> <s>and improve its efficiency by exploiting repeated structure in the first-order model.</s> <s>in this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference.</s> <s>in particular, we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible.</s> <s>we show that our rules yield several new tractable classes that cannot be solved efficiently by any of the existing techniques.</s></p></d>", "label": ["<d><p><s>lifted inference seen from the other side : the tractable features</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>for a density f on r^d, a high-density cluster is any connected component of {x: f(x) >= c}, for some c > 0.</s> <s>the set of all high-density clusters form a hierarchy called the cluster tree of f. we present a procedure for estimating the cluster tree given samples from f. we give finite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem.</s></p></d>", "label": ["<d><p><s>rates of convergence for the cluster tree</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss.</s> <s>in binary classification one typically tries to minimizes the error rate.</s> <s>but in structured prediction each task often has its own measure of performance such as the bleu score in machine translation or the intersection-over-union score in pascal segmentation.</s> <s>the most common approaches to structured prediction, structural svms and crfs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss.</s> <s>the main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss.</s> <s>we give empirical results on phonetic alignment of a standard test set from the timit corpus, which surpasses all previously reported results on this problem.</s></p></d>", "label": ["<d><p><s>direct loss minimization for structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system.</s> <s>here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model.</s> <s>the input to the circuit is an analog signal that belongs to the space of bandlimited functions.</s> <s>the output is a time sequence associated with the spike train.</s> <s>we derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision.</s></p></d>", "label": ["<d><p><s>identifying dendritic processing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks.</s> <s>our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters.</s> <s>we extend the infinite hidden markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously.</s> <s>we show the usefulness of the model through experiments with synthetic and real-world data sets.</s></p></d>", "label": ["<d><p><s>dynamic infinite relational model for time-varying relational data analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present simple and computationally efficient nonparametric estimators of r\\'enyi entropy and mutual information based on an i.i.d.</s> <s>sample drawn from an unknown, absolutely continuous distribution over $\\r^d$.</s> <s>the estimators are calculated as the sum of $p$-th powers of the euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively.</s> <s>for the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is lipschitz continuous.</s> <s>experiments demonstrate their usefulness in independent subspace analysis.</s></p></d>", "label": ["<d><p><s>estimation of r?nyi entropy and mutual information based on generalized nearest-neighbor graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we tackle the fundamental problem of bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution.</s> <s>in the case of noise-free observations, a greedy algorithm called generalized binary search (gbs) is known to perform near-optimally.</s> <s>we show that if the observations are noisy, perhaps surprisingly, gbs can perform very poorly.</s> <s>we develop ec2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for bayesian active learning with noisy observations.</s> <s>our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies.</s> <s>our results hold even if the tests have non?uniform cost and their noise is correlated.</s> <s>we also propose effecxtive, a particularly fast approximation of ec2, and evaluate it on a bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.</s></p></d>", "label": ["<d><p><s>near-optimal bayesian active learning with noisy observations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of identifying an activation pattern in a complex, large-scale network that is embedded in very noisy measurements.</s> <s>this problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the internet.</s> <s>extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node.</s> <s>however, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns.</s> <s>in this paper, we analyze an estimator based on the graph laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (gaussian or ising) model based on an arbitrary graph structure.</s> <s>we consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size.</s></p></d>", "label": ["<d><p><s>identifying graph-structured activation patterns in networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>how much information does a neural population convey about a stimulus?</s> <s>answers to this question are known to strongly depend on the correlation of response variability in neural populations.</s> <s>these noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size.</s> <s>here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix.</s> <s>our basic assumption is that noise correlations arise due to common inputs between neurons.</s> <s>on average, noise correlations will therefore reflect signal correlations, which can be measured in neural populations.</s> <s>we suggest an explicit parametric dependency between signal and noise correlations.</s> <s>we show how this dependency can be used to fill the gaps\" in noise correlations matrices using an iterative application of the wishart distribution over positive definitive matrices.</s> <s>we apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task.</s> <s>we compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations.\"</s></p></d>", "label": ["<d><p><s>linear readout from a neural population with partial correlation data</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping.</s> <s>this method is directly related to kernel partial least squares, a regression method that combines supervised dimensionality reduction with least squares projection.</s> <s>the rates depend on two key quantities: first, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space.</s> <s>lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if  the true regression function belongs to the reproducing kernel hilbert space.</s> <s>if the latter assumption is not fulfilled, we obtain similar convergence rates provided additional unlabeled data are available.</s> <s>the order of the learning rates in these two situations match state-of-the-art results that were recently obtained for the least squares support vector machine and for linear regularization operators.</s></p></d>", "label": ["<d><p><s>optimal learning rates for kernel conjugate gradient regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year.</s> <s>despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy.</s> <s>in this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification.</s> <s>the key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity.</s> <s>we hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity.</s> <s>we describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks.</s> <s>we first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ecg) signals.</s> <s>this algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals.</s> <s>we then show how this measure can be used with each of a one-class svm, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification.</s> <s>we evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data.</s> <s>in a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days.</s> <s>in a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.</s></p></d>", "label": ["<d><p><s>identifying patients at risk of major adverse cardiovascular events using symbolic mismatch</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a new algorithm for isotonic regression is presented based on recursively partitioning the solution space.</s> <s>we develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem, and prove that this sequence of partitions converges to the global solution.</s> <s>these network flow problems can further be decomposed in order to solve very large problems.</s> <s>success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2x10^5 variables and 10^7 constraints.</s></p></d>", "label": ["<d><p><s>decomposing isotonic regression for efficiently solving large problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies.</s> <s>an important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs.</s> <s>however, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data.</s> <s>learning such models entails two main challenges:  i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and  ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging.</s> <s>we describe subject-level brain functional connectivity structure as a multivariate gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population.</s> <s>we show that individual models learned from functional magnetic resonance imaging (fmri) data using this population prior generalize better to unseen data than models based on alternative regularization schemes.</s> <s>to our knowledge, this is the first report of a cross-validated model of spontaneous brain activity.</s> <s>finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.</s></p></d>", "label": ["<d><p><s>brain covariance selection: better individual functional connectivity models using population prior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present original empirical bernstein inequalities for u-statistics with bounded symmetric kernels q.</s> <s>they are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the bernstein-type inequality for u-statistics derived by arcones [2].</s> <s>our result subsumes other existing empirical bernstein inequalities, as it reduces to them when u-statistics of order 1 are considered.</s> <s>in addition, it is based on a rather direct argument using two applications of the same (non-empirical) bernstein inequality for u-statistics.</s> <s>we discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions.</s> <s>in the process, we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument.</s> <s>we also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.</s></p></d>", "label": ["<d><p><s>empirical bernstein inequalities for u-statistics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions.</s> <s>as an example, we develop a stochastic volatility model, gaussian copula process volatility (gcpv), to predict the latent standard deviations of a sequence of random variables.</s> <s>to make predictions we use bayesian inference, with the laplace approximation, and with markov chain monte carlo as an alternative.</s> <s>we find our model can outperform garch on simulated and financial data.</s> <s>and unlike garch, gcpv can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.</s></p></d>", "label": ["<d><p><s>copula processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate.</s> <s>typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation.</s> <s>there are a number of effective sequential policies for selecting the individual inputs.</s> <s>in many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once.</s> <s>in this paper, we propose a novel approach to batch bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible.</s> <s>the key idea is to exploit the availability of high-quality and efficient sequential policies, by using monte-carlo simulation to select input batches that closely match their expected behavior.</s> <s>to the best of our knowledge, this is the first batch selection policy for bayesian optimization.</s> <s>our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.</s></p></d>", "label": ["<d><p><s>batch bayesian optimization via simulation matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when learning models that are represented in matrix forms, enforcing   a low-rank constraint can dramatically improve the memory and run   time complexity, while providing a natural regularization of the   model.</s> <s>however, naive approaches for minimizing functions over the   set of low-rank matrices are either prohibitively time   consuming (repeated singular value decomposition of the matrix) or   numerically unstable (optimizing a factored representation of the   low rank matrix).</s> <s>we build on recent advances in optimization over   manifolds, and describe an iterative online learning procedure, consisting of a   gradient step, followed by a second-order retraction back to the   manifold.</s> <s>while the ideal retraction is hard to compute, and so is   the projection operator that approximates it, we describe another   second-order retraction that can be computed efficiently, with run   time and memory complexity of o((n+m)k) for a rank-k   matrix of dimension m x n, given rank one gradients.</s> <s>we use   this algorithm, loreta, to learn a matrix-form similarity measure over pairs of   documents represented as high dimensional vectors.</s> <s>loreta   improves the mean average precision over a passive-   aggressive approach in a factorized model, and also improves over   a full model trained over pre-selected features using the same   memory requirements.</s> <s>loreta also showed consistent improvement over   standard methods in a large (1600 classes) multi-label image classification task.</s></p></d>", "label": ["<d><p><s>online learning in the manifold of low-rank matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>languages vary widely in many ways, including their canonical word order.</s> <s>a basic aspect of the observed variation is the fact that some word orders are much more common than others.</s> <s>although this regularity has been recognized for some time, it has not been well-explained.</s> <s>in this paper we offer an information-theoretic explanation for the observed word-order distribution across languages, based on the concept of uniform information density (uid).</s> <s>we suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of uid.</s> <s>we support our theoretical analysis with data from child-directed speech and experimental work.</s></p></d>", "label": ["<d><p><s>why are some word orders more common than others? a uniform information density account</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work in reinforcement learning has emphasized the power of l1 regularization to perform feature selection and prevent overfitting.</s> <s>we propose formulating the l1 regularized linear fixed point problem as a linear complementarity problem (lcp).</s> <s>this formulation offers several advantages over the lars-inspired formulation, lars-td.</s> <s>the lcp formulation allows the use of efficient off-the-shelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts).</s> <s>we demonstrate that warm starts, as well as the efficiency of lcp solvers, can speed up policy iteration.</s> <s>moreover, warm starts permit a form of modified policy iteration that can be used to approximate a greedy\" homotopy path, a generalization of the lars-td homotopy path that combines policy evaluation and optimization.\"</s></p></d>", "label": ["<d><p><s>linear complementarity for regularized policy evaluation and improvement</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications.</s> <s>for the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique.</s> <s>in this paper we establish the asymptotic consistency of an extended bayesian information criterion for gaussian graphical models in a scenario where both the number of variables p and the sample size n grow.</s> <s>compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs.</s> <s>we demonstrate the performance of this criterion on simulated data when used in conjuction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary bayesian information criterion when p and the number of non-zero parameters q both scale with n.</s></p></d>", "label": ["<d><p><s>extended bayesian information criteria for gaussian graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of retrieving the database points nearest to a given {\\em hyperplane} query without exhaustively scanning the database.</s> <s>we propose two hashing-based solutions.</s> <s>our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point.</s> <s>our second approach embeds the data into a vector space where the euclidean norm reflects the desired distance between the original points and hyperplane query.</s> <s>both use hashing to retrieve near points in sub-linear time.</s> <s>our first method's preprocessing stage is more efficient, while the second has stronger accuracy guarantees.</s> <s>we apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion.</s> <s>we empirically demonstrate our methods' tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.</s></p></d>", "label": ["<d><p><s>hashing hyperplane queries to near points with applications to large-scale active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a model based on a boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations.</s> <s>the model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the glimpse\" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object.</s> <s>we evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.\"</s></p></d>", "label": ["<d><p><s>learning to combine foveal glimpses with a third-order boltzmann machine</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the reinforcement learning community has explored many approaches to obtain- ing value estimates and models to guide decision making; these approaches, how- ever, do not usually provide a measure of confidence in the estimate.</s> <s>accurate estimates of an agent?s confidence are useful for many applications, such as bi- asing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning.</s> <s>computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent- environment interaction rarely satisfies traditional assumptions.</s> <s>samples of value- estimates are dependent, likely non-normally distributed and often limited, partic- ularly in early learning when confidence estimates are pivotal.</s> <s>in this work, we investigate how to compute robust confidences for value estimates in continuous markov decision processes.</s> <s>we illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions.</s> <s>we demonstrate the applica- bility of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.</s></p></d>", "label": ["<d><p><s>interval estimation for reinforcement-learning algorithms in continuous-state domains</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while clinicians can accurately identify different types of heartbeats in electrocardiograms (ecgs) from different patients, researchers have had limited success in applying supervised machine learning to the same task.</s> <s>the problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients.</s> <s>we address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification.</s> <s>when tested on a benchmark database of cardiologist annotated ecg recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the association for the advancement of medical instrumentation.</s> <s>additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.</s></p></d>", "label": ["<d><p><s>active learning applied to patient-adaptive heartbeat classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds (www.research.microsoft.com/~jojic/aihs).</s> <s>the resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects.</s> <s>our first analysis goal is to create a visual summary of the subject?s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions.</s> <s>direct application of existing algorithms, such as panoramic stitching (e.g.</s> <s>photosynth) or appearance-based clustering models (e.g.</s> <s>the epitome), is impractical due to either the large dataset size or the dramatic variation in the lighting conditions.</s> <s>as a remedy to these problems, we introduce a novel image representation, the ?stel epitome,?</s> <s>and an associated efficient learning algorithm.</s> <s>in our model, each image or image patch is characterized by a hidden mapping t, which, as in previous epitome models, defines a mapping between the image-coordinates and the coordinates in the large all-i-have-seen\" epitome matrix.</s> <s>the limited epitome real-estate forces the mappings of different images to overlap, with this overlap indicating image similarity.</s> <s>however, in our model the image similarity does not depend on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models.</s> <s>as a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination, that tend to uniformly affect pixels belonging to a single scene or object part.\"</s></p></d>", "label": ["<d><p><s>structural epitome: a way to summarize one?s visual experience</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes.</s> <s>we verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling.</s> <s>further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations.</s> <s>the new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.</s></p></d>", "label": ["<d><p><s>deciphering subsampled data: adaptive compressive sampling as a principle of brain communication</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we extend latent dirichlet allocation (lda) by explicitly allowing for the encoding of side information in the distribution over words.</s> <s>this results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages.</s> <s>we present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics.</s> <s>results indicate that our model substantially improves topic cohesion when compared to the standard lda model.</s></p></d>", "label": ["<d><p><s>word features for latent dirichlet allocation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks.</s> <s>learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively.</s> <s>following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation.</s> <s>several recently proposed methods are variants of the special cases of this formulation.</s> <s>to address the overfitting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via l-1 penalties on task and feature inverse covariances.</s> <s>we empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple fields and recognizing faces between different subjects.</s> <s>experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks.</s></p></d>", "label": ["<d><p><s>learning multiple tasks with a sparse matrix-normal penalty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning a coefficient vector x0 from noisy linear observation y=ax0+w.</s> <s>in many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator.</s> <s>in this case, a popular approach consists in solving an l1-penalized least squares problem known as the lasso or bpdn.</s> <s>for sequences of matrices a of increasing dimensions, with iid gaussian entries, we prove that the normalized risk of the lasso converges to a limit, and we obtain an explicit expression for this limit.</s> <s>our result is the first rigorous derivation of an explicit formula for the asymptotic risk of the lasso for random instances.</s> <s>the proof technique is based on the analysis of amp, a recently developed efficient algorithm, that is inspired from graphical models ideas.</s> <s>through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.</s></p></d>", "label": ["<d><p><s>the lasso risk: asymptotic results and real world examples</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the charles bonnet syndrome (cbs) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology.</s> <s>we present a deep boltzmann machine model of cbs, exploring two core hypotheses: first, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery.</s> <s>and second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking.</s> <s>we reproduce a variety of qualitative findings in cbs.</s> <s>we also introduce a modification to the dbm that allows us to model a possible role of acetylcholine in cbs as mediating the balance of feed-forward and feed-back processing.</s> <s>our model might provide new insights into cbs and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception.</s></p></d>", "label": ["<d><p><s>hallucinations in charles bonnet syndrome induced by homeostasis: a deep boltzmann machine model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made.</s> <s>there are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function.</s> <s>currently, there is no general purpose algorithm to solve this class of problems.</s> <s>we use nonparametric density estimation for the joint distribution of state-outcome pairs to create weights for previous observations.</s> <s>the weights effectively group similar states.</s> <s>those similar to the current state are used to create a convex, deterministic approximation of the objective function.</s> <s>we propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization.</s> <s>we offer two weighting schemes, kernel based weights and dirichlet process based weights, for use with the solution methods.</s> <s>the weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem.</s> <s>our results show dirichlet process weights can offer substantial benefits over kernel based weights and, more generally, that nonparametric estimation methods provide good solutions to otherwise intractable problems.</s></p></d>", "label": ["<d><p><s>nonparametric density estimation for stochastic optimization with an observable state variable</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gaussian graphical models are of great interest in statistical learning.</s> <s>because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an $\\ell_1$-regularization term.</s> <s>in this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions.</s> <s>moreover, our algorithm obtains an $\\epsilon$-optimal solution in $o(1/\\epsilon)$ iterations.</s> <s>numerical experiments on both synthetic  and real data from gene association networks show that a practical version of  this algorithm outperforms other competitive algorithms.</s></p></d>", "label": ["<d><p><s>sparse inverse covariance selection via alternating linearization methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the international monitoring system (ims) is a global network of sensors whose purpose is to identify potential violations of the comprehensive nuclear-test-ban treaty (ctbt), primarily through detection and localization of seismic events.</s> <s>we report on the first stage of a project to improve on the current automated software system with a bayesian inference system that computes the most likely global event history given the record of local sensor data.</s> <s>the new system, visa (vertically integrated seismological analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection.</s> <s>visa exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the ims output.</s></p></d>", "label": ["<d><p><s>global seismic monitoring as probabilistic inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering is a basic data mining task with a wide variety of applications.</s> <s>not surprisingly, there exist many clustering algorithms.</s> <s>however, clustering is an ill defined problem - given a data set, it is not clear what a ?correct?</s> <s>clustering for that set is.</s> <s>indeed, different algorithms may yield dramatically different outputs for the same input sets.</s> <s>faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm.</s> <s>currently, such decisions are often made in a very ad hoc, if not completely random, manner.</s> <s>given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable.</s> <s>in this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data.</s> <s>this is, of course, a very ambitious endeavor, and in this paper, we make some first steps towards this goal.</s> <s>we propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms.</s> <s>in this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms.</s> <s>on top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering.</s> <s>we also study relationships between the properties, independent of any particular algorithm.</s> <s>in particular, we strengthen kleinbergs famous impossibility result, while providing a simpler proof.</s></p></d>", "label": ["<d><p><s>towards property-based classification of clustering paradigms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling.</s> <s>we show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled netflix dataset.</s></p></d>", "label": ["<d><p><s>collaborative filtering in a non-uniform world: learning with the weighted trace norm</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>generalized binary search (gbs) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes\" or \"no\" questions posed about that object, and arises in problems such as active learning and active diagnosis.</s> <s>here, we provide a coding-theoretic interpretation for gbs and show that gbs can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object.</s> <s>this interpretation is then used to extend gbs in two ways.</s> <s>first, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs.</s> <s>then, we consider the case where the cost of identifying an object grows exponentially in the number of queries.</s> <s>in each case, we present an exact formula for the objective function involving shannon or renyi entropy, and develop a greedy algorithm for minimizing it.\"</s></p></d>", "label": ["<d><p><s>extensions of generalized binary search to group identification and exponential costs</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-d sensor the quantity of training data may be severly limited.</s> <s>in this paper, we show how a crucial aspect of 3-d information?object and feature absolute size?can be added to models learned from commonly available online imagery, without use of any 3-d sensing or re- construction at training time.</s> <s>such models can be utilized at test time together with explicit 3-d sensing to perform robust search.</s> <s>our model uses a ?2.1d?</s> <s>local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window.</s> <s>we show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics.</s> <s>we develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-d size constraints as part of an optimal search for a set of features which indicate the presence of a category.</s> <s>experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated exif metadata.</s></p></d>", "label": ["<d><p><s>size matters: metric visual search constraints from monocular metadata</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (roi) specifying each instance of the object in the training images.</s> <s>in this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated.</s> <s>to this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance.</s> <s>the method is demonstrated on the benchmark inria pedestrian detection dataset of dalal and triggs and the pascal voc dataset, and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results.</s></p></d>", "label": ["<d><p><s>simultaneous object detection and ranking with weak supervision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a number of objective functions in clustering problems can be described with submodular functions.</s> <s>in this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions.</s> <s>the proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points.</s> <s>the minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total.</s> <s>additionally, we evaluate the performance of the proposed algorithm through computational experiments.</s></p></d>", "label": ["<d><p><s>minimum average cost clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models.</s> <s>given an arbitrary loss function, defined on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters.</s> <s>this method can be used with approximate inference, with a loss function over approximate marginals.</s> <s>convenient choices of loss functions make it practical to fit graphical models with hidden variables, high treewidth and/or model misspecification.</s></p></d>", "label": ["<d><p><s>implicit differentiation by perturbation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many data are naturally modeled by an unobserved hierarchical structure.</s> <s>in this paper we propose a flexible nonparametric prior over unknown data hierarchies.</s> <s>the approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable.</s> <s>one can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree.</s> <s>by using a stick-breaking approach, we can apply markov chain monte carlo methods based on slice sampling to perform bayesian inference and simulate from the posterior distribution on trees.</s> <s>we apply our method to hierarchical clustering of images and topic modeling of text data.</s></p></d>", "label": ["<d><p><s>tree-structured stick breaking for hierarchical data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many time-series such as human movement data consist of a sequence of basic actions, e.g., forehands and backhands in tennis.</s> <s>automatically extracting and characterizing such actions is an important problem for a variety of different applications.</s> <s>in this paper, we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions.</s> <s>each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement, with possible time re-scaling.</s> <s>we analyze three different approximation methods for dealing with model intractability, and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device.</s></p></d>", "label": ["<d><p><s>movement extraction by detecting dynamics switches and repetitions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many problems in machine learning and statistics can be formulated as (generalized) eigenproblems.</s> <s>in terms of the associated optimization problem, computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints.</s> <s>in this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems.</s> <s>we derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector.</s> <s>we apply the inverse power method to 1-spectral clustering and sparse pca which can naturally be formulated as nonlinear eigenproblems.</s> <s>in both applications we achieve state-of-the-art results in terms of solution quality and runtime.</s> <s>moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems.</s></p></d>", "label": ["<d><p><s>an inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (sfm), non-rigid sfm and photometric stereo.</s> <s>we formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (lrsdp) with the advantage that: $1)$ an efficient quasi-newton implementation of the lrsdp enables us to solve large-scale factorization problems, and $2)$ additional constraints such as ortho-normality, required in orthographic sfm, can be directly incorporated in the new formulation.</s> <s>our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms.</s> <s>we further demonstrate the effectiveness of the proposed algorithm in solving the affine sfm problem, non-rigid sfm and photometric stereo problems.</s></p></d>", "label": ["<d><p><s>large-scale matrix factorization with missing data under additional constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention.</s> <s>if each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks.</s> <s>we analyze feature transitions associated with saccadic search and find out that size, color, and orientation are not alike in dynamic attribute processing over time.</s> <s>the markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation.</s></p></d>", "label": ["<d><p><s>feature transitions with saccadic search: size, color, and orientation are not alike</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in many real-world scenarios, it is nearly impossible to collect explicit social network data.</s> <s>in such cases, whole networks must be inferred from underlying observations.</s> <s>here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data.</s> <s>we consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them.</s> <s>given such node infection times, we then identify the optimal network that best explains the observed data.</s> <s>we present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity.</s> <s>experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model.</s> <s>moreover, our approach scales well as it can infer optimal networks on thousands of nodes in a matter of minutes.</s></p></d>", "label": ["<d><p><s>on the convexity of latent social network inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the gaussian process (gp) is a popular way to specify dependencies between random variables in a probabilistic model.</s> <s>in the bayesian framework the covariance structure can be specified using unknown hyperparameters.</s> <s>integrating over these hyperparameters considers different possible explanations for the data when making predictions.</s> <s>this integration is often performed using markov chain monte carlo (mcmc) sampling.</s> <s>however, with non-gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly.</s> <s>in this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.</s></p></d>", "label": ["<d><p><s>slice sampling covariance hyperparameters of latent gaussian models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority  of work in this direction has focused on unsupervised clustering.</s> <s>we study a recently proposed framework for supervised clustering where there is access to a teacher.</s> <s>we give an improved generic algorithm to cluster any concept class  in that model.</s> <s>our algorithm is query-efficient in the sense that it involves only a small amount  of interaction with the teacher.</s> <s>we also present and study two natural generalizations of the model.</s> <s>the model assumes that the teacher response to the algorithm is perfect.</s> <s>we eliminate  this limitation by proposing a noisy model and give an algorithm for  clustering the class of intervals in this noisy model.</s> <s>we also propose a dynamic model where the teacher sees  a random subset of the points.</s> <s>finally, for datasets  satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class  of clustering functions containing single-linkage will find the target clustering under the strongest  property.</s></p></d>", "label": ["<d><p><s>supervised clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a theory of online learning by defining several complexity measures.</s> <s>among them are analogues of rademacher complexity, covering numbers and fat-shattering dimension from statistical learning theory.</s> <s>relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided.</s> <s>we apply these results to various learning problems.</s> <s>we provide a complete characterization of online learnability in the supervised setting.</s></p></d>", "label": ["<d><p><s>online learning: random averages, combinatorial parameters, and learnability</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper introduces the first set of pac-bayesian bounds for the batch reinforcement learning problem in finite state spaces.</s> <s>these bounds hold regardless of the correctness of the prior distribution.</s> <s>we demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment, or on the value of actions.</s> <s>our empirical results confirm that pac-bayesian model-selection is able to leverage prior distributions when they are informative and, unlike standard bayesian rl approaches, ignores them when they are misleading.</s></p></d>", "label": ["<d><p><s>pac-bayesian model selection for reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we tackle the problem of simultaneously detecting occlusions and estimating optical flow.</s> <s>we show that, under standard assumptions of lambertian reflection and static illumination, the task can be posed as a convex minimization problem.</s> <s>therefore, the solution, computed using efficient algorithms, is guaranteed to be globally optimal, for any number of independently moving objects, and any number of occlusion layers.</s> <s>we test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance.</s></p></d>", "label": ["<d><p><s>occlusion detection and motion estimation with convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing two-way and multi-way set similarities is a fundamental problem.</s> <s>this study focuses on estimating 3-way resemblance (jaccard similarity) using b-bit minwise hashing.</s> <s>while traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b>= 2 for 3-way).</s> <s>the extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial.</s> <s>we develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data.</s> <s>our analysis shows that $b$-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.</s></p></d>", "label": ["<d><p><s>b-bit minwise hashing for estimating three-way similarities</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>there has been a recent push in extraction of 3d spatial layout of scenes.</s> <s>however, none of these approaches model the 3d interaction between objects and the spatial layout.</s> <s>in this paper, we argue for a parametric representation of objects in 3d, which allows us to incorporate volumetric constraints of the physical world.</s> <s>we show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art.</s></p></d>", "label": ["<d><p><s>estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several motor related brain computer interfaces (bcis) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices.</s> <s>many recent studies have also talked about the importance of ipsilateral activity in planning of motor movements.</s> <s>for successful upper limb bcis, it is important to decode finger movements from brain activity.</s> <s>this study uses ipsilateral cortical signals from humans (using ecog) to decode finger movements.</s> <s>we demonstrate, for the first time, successful finger movement detection using machine learning algorithms.</s> <s>our results show high decoding accuracies in all cases which are always above chance.</s> <s>we also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features also make sense physiologically.</s> <s>the results of this study have a great potential in the emerging world of motor neuroprosthetics and other bcis.</s></p></d>", "label": ["<d><p><s>decoding ipsilateral finger movements from ecog signals in humans</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction.</s> <s>in this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses.</s> <s>we extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same.</s> <s>our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data.</s> <s>furthermore, when used in conjunction with supervised learners for classification, our methods lead to lower classification errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.</s></p></d>", "label": ["<d><p><s>unsupervised kernel dimension reduction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study repeated zero-sum games against an adversary on a budget.</s> <s>given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player's best mixed strategy with knowledge of this budget.</s> <s>we show that, for a general class of normal-form games, the minimax strategy is indeed efficiently computable and relies on a random playout\" technique.</s> <s>we give three diverse applications of this algorithmic template: a cost-sensitive \"hedge\" setting, a particular problem in metrical task systems, and the design of combinatorial prediction markets.\"</s></p></d>", "label": ["<d><p><s>repeated games against budgeted adversaries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the max-norm was proposed as a convex matrix regularizer by srebro et al (2004) and was shown to be empirically superior to the trace-norm for collaborative filtering problems.</s> <s>although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm.</s> <s>the present work uses a factorization technique of burer and monteiro (2003) to devise scalable first-order algorithms for convex programs involving the max-norm.</s> <s>these algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems.</s> <s>empirically, the new methods outperform mature techniques from all three areas.</s></p></d>", "label": ["<d><p><s>practical large-scale optimization for max-norm regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the multiple linear regression problem, in a setting where some of the set of relevant features could be shared across the tasks.</s> <s>a lot of recent research has studied the use of $\\ell_1/\\ell_q$ norm block-regularizations with $q > 1$  for such (possibly) block-structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations.</s> <s>however, these papers also caution that the performance of such block-regularized methods are very dependent on the {\\em extent} to which the features are shared across tasks.</s> <s>indeed they show~\\citep{nwjoint} that if the extent of overlap is less than a threshold, or even if parameter {\\em values} in the shared features are highly uneven, then block $\\ell_1/\\ell_q$ regularization could actually perform {\\em worse} than simple separate elementwise $\\ell_1$ regularization.</s> <s>we are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well.</s> <s>here, we ask the question: can we leverage support and parameter overlap when it exists, but not pay a penalty when it does not?</s> <s>indeed, this falls under a more general question of whether we can model such \\emph{dirty data} which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on).</s> <s>here, we take a first step, focusing on developing a dirty model for the multiple regression problem.</s> <s>our method uses a very simple idea: we decompose  the parameters into two components and {\\em regularize these differently.}</s> <s>we show both theoretically and empirically, our method strictly and noticeably outperforms both $\\ell_1$ and $\\ell_1/\\ell_q$ methods, over the entire range of possible overlaps.</s> <s>we also provide theoretical guarantees that the method performs well under high-dimensional scaling.</s></p></d>", "label": ["<d><p><s>a dirty model for multi-task learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>hypothesis testing on point processes has several applications such as model fitting, plasticity detection, and non-stationarity detection.</s> <s>standard tools for hypothesis testing include tests on mean firing rate and time varying rate function.</s> <s>however, these statistics do not fully describe a point process and thus the tests can be misleading.</s> <s>in this paper, we introduce a family of non-parametric divergence measures for hypothesis testing.</s> <s>we extend the traditional kolmogorov--smirnov and cramer--von-mises tests for point process via stratification.</s> <s>the proposed divergence measures compare the underlying probability structure and, thus, is zero if and only if the point processes are the same.</s> <s>this leads to a more robust test of hypothesis.</s> <s>we prove consistency and show that these measures can be efficiently estimated from data.</s> <s>we demonstrate an application of using the proposed divergence as a cost function to find optimally matched spike trains.</s></p></d>", "label": ["<d><p><s>a novel family of non-parametric cumulative based divergences for point processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (hrtfs).</s> <s>these hrtfs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired.</s> <s>since hrtfs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds.</s> <s>we present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of hrtfs.</s> <s>after learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.</s></p></d>", "label": ["<d><p><s>learning to localise sounds with spiking neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning from multi-view data is important in many applications, such as image classification and annotation.</s> <s>in this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views.</s> <s>our approach is based on an undirected latent space markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables.</s> <s>we provide efficient inference and parameter estimation methods for the latent subspace model.</s> <s>finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification, annotation and retrieval.</s></p></d>", "label": ["<d><p><s>predictive subspace learning for multi-view data: a large margin approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to cope with concept drift, we placed a probability distribution over the location of the most-recent drift point.</s> <s>we used bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability.</s> <s>we compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection.</s> <s>in our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.</s></p></d>", "label": ["<d><p><s>a bayesian approach to concept drift</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>optimal coding provides a guiding principle for understanding the representation of sensory variables in neural populations.</s> <s>here we consider the influence of a prior probability distribution over sensory variables on the optimal allocation of cells and spikes in a neural population.</s> <s>we model the spikes of each cell as samples from an independent poisson process with rate governed by an associated tuning curve.</s> <s>for this response model, we approximate the fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density.</s> <s>we consider a family of objective functions based on the expected value, over the sensory prior, of a functional of the fisher information.</s> <s>this family includes lower bounds on mutual information and perceptual discriminability as special cases.</s> <s>in all cases, we find a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior.</s> <s>this also implies a power law relationship between the prior and perceptual discriminability.</s> <s>we show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and firing rates), and psychophysically measured discrimination thresholds.</s></p></d>", "label": ["<d><p><s>implicit encoding of prior probabilities in optimal neural populations</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper we consider the problem of learning an n x n kernel matrix from m similarity matrices under general convex loss.</s> <s>past research have extensively studied the m =1 case and have derived several algorithms which require sophisticated techniques like accp, socp, etc.</s> <s>the existing algorithms do not apply if one uses arbitrary losses and often can not handle m > 1 case.</s> <s>we present several provably convergent iterative algorithms, where each iteration requires either an svm or a multiple kernel learning (mkl) solver for m > 1 case.</s> <s>one of the major contributions of the paper is to extend the well known mirror descent(md) framework to handle cartesian product of psd matrices.</s> <s>this novel extension leads to an algorithm, called emkl, which solves the problem in o(m^2 log n) iterations; in each iteration one solves an mkl involving m kernels and m eigen-decomposition of n x n matrices.</s> <s>by suitably defining a restriction on the objective function, a faster version of emkl is proposed, called rekl, which avoids the eigen-decomposition.</s> <s>an alternative to both emkl and rekl is also suggested which requires only an svm solver.</s> <s>experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms.</s></p></d>", "label": ["<d><p><s>efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the application of a strongly non-linear generative model to image patches.</s> <s>as in standard approaches such as sparse coding or independent component analysis, the model assumes a sparse prior with independent hidden variables.</s> <s>however, in the place where standard approaches use the sum to combine basis functions we use the maximum.</s> <s>to derive tractable approximations for parameter estimation we apply a novel approach based on variational expectation maximization.</s> <s>the derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables.</s> <s>furthermore, we can infer all model parameters including observation noise and the degree of sparseness.</s> <s>in applications to image patches we find that gabor-like basis functions are obtained.</s> <s>gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition.</s> <s>quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions.</s> <s>the distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches.</s> <s>in the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging.</s> <s>the presented algorithm represents the first large-scale application of such an approach.</s></p></d>", "label": ["<d><p><s>the maximal causes of natural scenes are edge filters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an unsupervised method for learning multi-stage   hierarchies of sparse convolutional features.</s> <s>while sparse coding   has become an increasingly popular method for learning visual   features, it is most often trained at the patch level.</s> <s>applying the   resulting filters convolutionally results in highly redundant codes   because overlapping patches are encoded in isolation.</s> <s>by training   convolutionally over large image windows, our method reduces the   redudancy between feature vectors at neighboring locations and   improves the efficiency of the overall representation.</s> <s>in addition   to a linear decoder that reconstructs the image from sparse   features, our method trains an efficient feed-forward encoder that   predicts quasi-sparse features from the input.</s> <s>while patch-based   training rarely produces anything but oriented edge detectors, we   show that convolutional training produces highly diverse filters,   including center-surround filters, corner detectors, cross   detectors, and oriented grating detectors.</s> <s>we show that using these   filters in multi-stage convolutional network architecture improves   performance on a number of visual recognition and detection tasks.</s></p></d>", "label": ["<d><p><s>learning convolutional feature hierarchies for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees.</s> <s>in this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade.</s> <s>we focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks.</s> <s>unlike other variational methods, our ensembles do not enforce agreement between sub-models, but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model.</s> <s>our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model.</s> <s>we provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos.</s> <s>we find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem.</s></p></d>", "label": ["<d><p><s>sidestepping intractable inference with structured ensemble cascades</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>convolutional neural networks (cnns) have been successfully applied to many tasks such as digit and object recognition.</s> <s>using convolutional (tied) weights signi?cantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture.</s> <s>in this paper, we consider the problem of learning invariances, rather than relying on hard-coding.</s> <s>we propose tiled convolution neural networks (tiled cnns), which use a regular ?tiled?</s> <s>pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights.</s> <s>by pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance.</s> <s>further, it also enjoys much of cnns?</s> <s>advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability).</s> <s>we provide an efficient learning algorithm for tiled cnns based on topographic ica, and show that learning complex invariant features allows us to achieve highly competitive results for both the norb and cifar-10 datasets.</s></p></d>", "label": ["<d><p><s>tiled convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern.</s> <s>we present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients.</s> <s>this family subsumes the $\\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance.</s> <s>we establish some important properties of these functions and discuss some examples where they can be computed explicitly.</s> <s>moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions.</s> <s>numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the lasso and other related methods.</s></p></d>", "label": ["<d><p><s>a family of penalty functions for structured sparsity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting.</s> <s>a more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images.</s> <s>this method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection.</s> <s>we investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before.</s> <s>the overall model can be interpreted as a gated mrf where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables.</s> <s>finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated mrf learns more efficient internal representations, as demonstrated in several recognition tasks.</s></p></d>", "label": ["<d><p><s>generating more realistic images using gated mrf's</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper outlines a hierarchical bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied.</s> <s>the model is fit to multiple data sets, and provides a parsimonious method for describing how humans learn context specific conceptual representations.</s></p></d>", "label": ["<d><p><s>learning the context of a category</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we combine random forest (rf) and conditional random field (crf) into a new computational framework, called random forest random field (rf)^2.</s> <s>inference of (rf)^2 uses the swendsen-wang cut algorithm, characterized by metropolis-hastings jumps.</s> <s>a jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states.</s> <s>prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio.</s> <s>our key idea is to instead directly estimate these ratios using rf.</s> <s>rf collects in leaf nodes of each decision tree the class histograms of training examples.</s> <s>we use these class histograms for a non-parametric estimation of the distribution ratios.</s> <s>we derive the theoretical error bounds of a two-class (rf)^2.</s> <s>(rf)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions.</s> <s>in our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3d layout of surfaces in the scene.</s> <s>nevertheless, (rf)^2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</s></p></d>", "label": ["<d><p><s>(rf)^2 -- random forest random field</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian approaches to preference elicitation (pe) are particularly attractive due to their ability to explicitly model uncertainty in users' latent utility functions.</s> <s>however, previous approaches to bayesian pe have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users.</s> <s>in this paper, we address this deficiency by introducing a gaussian process (gp) prior over users' latent utility functions on the joint space of user and item features.</s> <s>we learn the hyper-parameters of this gp on a set of preferences of previous users and use it to aid in the elicitation process for a new user.</s> <s>this approach provides a flexible model of a multi-user utility function, facilitates an efficient value of information (voi) heuristic query selection strategy, and provides a principled way to incorporate the elicitations of multiple users back into the model.</s> <s>we show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types.</s></p></d>", "label": ["<d><p><s>gaussian process preference elicitation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we discuss an online learning framework in which the agent is allowed to say ``i don't know'' as well as making incorrect predictions on given examples.</s> <s>we analyze the trade off between saying ``i don't know'' and making mistakes.</s> <s>if the number of don't know predictions is forced to be zero, the model reduces to the well-known mistake-bound model introduced by littlestone [lit88].</s> <s>on the other hand, if no mistakes are allowed, the model reduces to kwik framework introduced by li et.</s> <s>al.</s> <s>[llw08].</s> <s>we propose a general, though inefficient, algorithm for general finite concept classes that minimizes the number of don't-know predictions if a certain number of mistakes are allowed.</s> <s>we then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators.</s></p></d>", "label": ["<d><p><s>trading off mistakes and don't-know predictions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes.</s> <s>we achieve this with a novel method for learning a nonlinear embedding based on several extensions to the neighborhood component analysis (nca) framework.</s> <s>our method is convolutional, enabling it to scale to realistically-sized images.</s> <s>by cheaply labeling the head and hands in large video databases through amazon mechanical turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose.</s> <s>we apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose.</s> <s>we evaluate our method quantitatively against other embedding methods.</s> <s>we also demonstrate that real-world performance can be improved through the use of synthetic data.</s></p></d>", "label": ["<d><p><s>pose-sensitive embedding by nonlinear nca regression</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in a recent paper joachims (2006) presented svm-perf, a cutting plane method (cpm) for training linear support vector machines (svms) which converges to an $\\epsilon$ accurate solution in $o(1/\\epsilon^{2})$ iterations.</s> <s>by tightening the analysis, teo et al.</s> <s>(2010) showed that $o(1/\\epsilon)$ iterations suffice.</s> <s>given the impressive convergence speed of cpm on a number of practical problems, it was conjectured that these rates could be further improved.</s> <s>in this paper we disprove this conjecture.</s> <s>we present counter examples which are not only applicable for training linear svms with hinge loss, but also hold for support vector methods which optimize a \\emph{multivariate} performance score.</s> <s>however, surprisingly, these problems are not inherently hard.</s> <s>by exploiting the structure of the objective function we can devise an algorithm that converges in $o(1/\\sqrt{\\epsilon})$ iterations.</s></p></d>", "label": ["<d><p><s>lower bounds on rate of convergence of cutting plane methods</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>predicting the execution time of computer programs is an important but challenging problem in the community of computer systems.</s> <s>existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features.</s> <s>we recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge.</s> <s>in this paper we study the construction of predictive models for this problem.</s> <s>we propose the spore (sparse polynomial regression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs.</s> <s>our two spore algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable.</s> <s>the compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program?s behavior.</s> <s>our evaluation on three widely used computer programs shows that spore methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples.</s> <s>in addition, we compare spore algorithms to state-of-the-art sparse regression algorithms, and show that spore methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.</s></p></d>", "label": ["<d><p><s>predicting execution time of computer programs using sparse polynomial regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's.</s> <s>this gives rise to the optimization problem of designing the artificial agent's goals---in the rl framework, designing the agent's reward function.</s> <s>existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure.</s> <s>in this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime.</s> <s>we show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.</s></p></d>", "label": ["<d><p><s>reward design via online gradient ascent</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the design of low-level image features is critical for computer vision algorithms.</s> <s>orientation histograms, such as those in sift~\\cite{lowe2004distinctive} and hog~\\cite{dalal2005histograms}, are the most successful and popular features for visual object and scene recognition.</s> <s>we highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches.</s> <s>this novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes (gradient, color, local binary pattern, \\etc) into compact patch-level features.</s> <s>in particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (kpca)~\\cite{scholkopf1998nonlinear}.</s> <s>kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features.</s> <s>they outperform carefully tuned and sophisticated features including sift and deep belief networks.</s> <s>we report superior performance on standard image classification benchmarks: scene-15, caltech-101, cifar10 and cifar10-imagenet.</s></p></d>", "label": ["<d><p><s>kernel descriptors for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers.</s> <s>the detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers, and concentrates the computation on difficult parts of the image which can not be trivially rejected.</s> <s>we introduce a novel algorithm to construct jointly the classifiers of such a cascade.</s> <s>we interpret the response of a classifier as a probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive.</s> <s>from this noisy-and model, we derive a consistent loss and a boosting procedure to optimize that global probability on the training set.</s> <s>such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade.</s> <s>we demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines.</s></p></d>", "label": ["<d><p><s>joint cascade optimization using a product of boosted classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>upstream supervised topic models have been widely used for complicated scene understanding.</s> <s>however, existing maximum likelihood estimation (mle) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification.</s> <s>this paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced.</s> <s>the optimization problem is efficiently solved with a variational em procedure, which iteratively solves an online loss-augmented svm.</s> <s>we demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class mit indoor scene dataset for scene categorization.</s></p></d>", "label": ["<d><p><s>large margin learning of upstream scene understanding models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, some variants of the $l_1$ norm, particularly matrix norms such as the $l_{1,2}$ and $l_{1,\\infty}$ norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization.</s> <s>in this paper, we unify the $l_{1,2}$ and $l_{1,\\infty}$ norms by considering a family of $l_{1,q}$ norms for $1 < q\\le\\infty$ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection.</s> <s>using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the $l_{1,q}$ norm.</s> <s>based on this probabilistic interpretation, we develop a probabilistic model using the noninformative jeffreys prior.</s> <s>we also extend the model to learn and exploit more general types of pairwise relationships between tasks.</s> <s>for both versions of the model, we devise expectation-maximization~(em) algorithms to learn all model parameters, including $q$, automatically.</s> <s>experiments have been conducted on two cancer classification applications using microarray gene expression data.</s></p></d>", "label": ["<d><p><s>probabilistic multi-task feature selection</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix).</s> <s>an objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix.</s> <s>in addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes.</s> <s>by jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation).</s> <s>the research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling.</s> <s>the analysis is performed from a bayesian perspective, with efficient inference constituted via gibbs sampling.</s> <s>the framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the united states senate and house of representatives.</s></p></d>", "label": ["<d><p><s>joint analysis of time-evolving binary matrices and associated documents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent studies compare gene expression data across species to identify core and species specific genes in biological systems.</s> <s>to perform such comparisons researchers need to match genes across species.</s> <s>this is a challenging task since the correct matches (orthologs) are not known for most genes.</s> <s>previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation.</s> <s>here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species.</s> <s>our method uses a dirichlet process mixture model which includes a latent data matching variable.</s> <s>we present learning and inference algorithms based on variational methods for this model.</s> <s>applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes.</s></p></d>", "label": ["<d><p><s>cross species expression analysis using a dirichlet process mixture model with latent matchings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set?</s> <s>we present a framework that simultaneously clusters the data and trains a discriminative classifier.</s> <s>we call it regularized information maximization (rim).</s> <s>rim optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classifier complexity.</s> <s>the approach can flexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning.</s> <s>in particular, we instantiate the framework to unsupervised, multi-class kernelized logistic regression.</s> <s>our empirical evaluation indicates that rim outperforms existing methods on several real data sets, and demonstrates that rim is an effective model selection method.</s></p></d>", "label": ["<d><p><s>discriminative clustering by regularized information maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>from a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon.</s> <s>we demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone.</s> <s>we begin by posing the problem in a classification based framework.</s> <s>we then derive a novel kernel for an srm0 model that is based on psp and ahp like functions.</s> <s>with the kernel we demonstrate how the learning problem can be posed as a quadratic program.</s> <s>experimental results demonstrate the strength of our approach.</s></p></d>", "label": ["<d><p><s>a novel kernel for learning a neuron model from spike train data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results.</s> <s>we point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique.</s> <s>we then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the renyi divergence of the training and test distributions.</s> <s>these results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest.</s> <s>we use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits.</s> <s>finally, we analyze the properties of normalized importance weights which are also commonly used.</s></p></d>", "label": ["<d><p><s>learning bounds for importance weighting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals.</s> <s>we give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.</s></p></d>", "label": ["<d><p><s>fast detection of multiple change-points shared by many signals using group lars</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a discriminative model for recognizing group activities.</s> <s>our model jointly captures the group activity, the individual person actions, and the interactions among them.</s> <s>two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework.</s> <s>different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g.</s> <s>a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference.</s> <s>our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.</s></p></d>", "label": ["<d><p><s>beyond actions: discriminative models for contextual group activities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we consider the fundamental problem of semi-supervised kernel function learning.</s> <s>we propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem.</s> <s>our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points.</s> <s>furthermore, our result gives a constructive method for kernelizing most existing mahalanobis metric learning formulations.</s> <s>to make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process.</s> <s>we also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting.</s> <s>we introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer.</s> <s>we empirically demonstrate that our framework learns useful kernel functions, improving the $k$-nn classification accuracy significantly in a variety of domains.</s> <s>furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.</s></p></d>", "label": ["<d><p><s>inductive regularized learning of kernel functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space.</s> <s>inverse reinforcement learning (irl) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features.</s> <s>most irl algorithms use a simple monte carlo estimation to approximate the expected feature counts under the expert's policy.</s> <s>in this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts.</s> <s>to reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known.</s> <s>empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations.</s></p></d>", "label": ["<d><p><s>bootstrapping apprenticeship learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel bayesian nonparametric approach to learning with probabilistic deterministic finite automata (pdfa).</s> <s>we define and develop and sampler for a pdfa with an infinite number of states which we call the probabilistic deterministic infinite automata (pdia).</s> <s>posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple pdfas of varying structure, where each pdfa is biased towards having few states.</s> <s>we suggest that our method for averaging over pdfas is a novel approach to predictive distribution smoothing.</s> <s>we test pdia inference both on pdfa structure learning and on both natural language and dna data prediction tasks.</s> <s>the results suggest that the pdia presents an attractive compromise between the computational cost of hidden markov models and the storage requirements of hierarchically smoothed markov models.</s></p></d>", "label": ["<d><p><s>probabilistic deterministic infinite automata</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales.</s> <s>when individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience.</s> <s>this relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms.</s> <s>fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner.</s> <s>we explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments.</s> <s>in our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies.</s> <s>we propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment.</s> <s>our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task.</s> <s>our decontamination techniques yield an over 20% reduction in the error of human judgments.</s></p></d>", "label": ["<d><p><s>improving human judgments by decontaminating sequential dependencies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel method for multitask learning (mtl) based on {\\it manifold regularization}: assume that all task parameters lie on a manifold.</s> <s>this is the generalization of a common assumption made in the existing literature: task parameters share a common {\\it linear} subspace.</s> <s>one proposed method uses the projection distance from the manifold to regularize the task parameters.</s> <s>the manifold structure and the task parameters are learned using an alternating optimization framework.</s> <s>when the manifold structure is fixed, our method decomposes across tasks which can be learnt independently.</s> <s>an approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed mtl framework efficient and easy to implement.</s> <s>we show the efficacy of our method on several datasets.</s></p></d>", "label": ["<d><p><s>learning multiple tasks using manifold regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication.</s> <s>we develop and analyze distributed algorithms based on dual averaging of subgradients, and we provide sharp bounds on their convergence rates as a function of the network size and topology.</s> <s>our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure.</s> <s>we show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network.</s> <s>the sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.</s></p></d>", "label": ["<d><p><s>distributed dual averaging in networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing a {\\em maximum a posteriori} (map) assignment in graphical models is a crucial inference problem for many practical applications.</s> <s>several provably convergent approaches have been successfully developed using linear programming (lp) relaxation of the map problem.</s> <s>we present an alternative approach, which transforms the map problem into that of inference in a finite mixture of simple bayes nets.</s> <s>we then derive the expectation maximization (em) algorithm for this mixture that also monotonically increases a lower bound on the map assignment until convergence.</s> <s>the update equations for the em algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation.</s> <s>we experiment on the real-world protein design dataset and show that em's convergence rate is significantly higher than the previous lp relaxation based approach mplp.</s> <s>em achieves a solution quality within $95$\\% of optimal for most instances and is often an order-of-magnitude faster than mplp.</s></p></d>", "label": ["<d><p><s>map estimation for graphical models by likelihood maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite.</s> <s>we pro- pose a new optimistic, ucb-like, algorithm for non-linearly parameterized bandit problems using the generalized linear model (glm) framework.</s> <s>we analyze the regret of the proposed algorithm, termed glm-ucb, obtaining results similar to those recently proved in the literature for the linear regression case.</s> <s>the analysis also highlights a key difficulty of the non-linear case which is solved in glm-ucb by focusing on the reward space rather than on the parameter space.</s> <s>moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence.</s> <s>simulation studies on real data sets illustrate the performance and the robustness of the proposed glm-ucb approach.</s></p></d>", "label": ["<d><p><s>parametric bandits: the generalized linear case</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>optimal control entails combining probabilities and utilities.</s> <s>however, for most practical problems probability densities can be represented only approximately.</s> <s>choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it.</s> <s>we propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints.</s> <s>the essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility.</s> <s>this theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields.</s></p></d>", "label": ["<d><p><s>the neural costs of optimal control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>during the last years support vector machines (svms) have been successfully applied even in situations where the input space $x$ is not necessarily a subset of $r^d$.</s> <s>examples include svms using probability measures to analyse e.g.</s> <s>histograms or coloured images, svms for text classification and web mining, and svms for applications from computational biology using, e.g., kernels for trees and graphs.</s> <s>moreover, svms are known to be consistent to the bayes risk, if either the input space is a complete separable metric space and the reproducing kernel hilbert space (rkhs) $h\\subset l_p(p_x)$ is dense, or if the svm is based on a universal kernel $k$.</s> <s>so far, however, there are no rkhss of practical interest known that satisfy these assumptions on $\\ch$ or $k$ if $x \\not\\subset r^d$.</s> <s>we close this gap by providing a general technique based on taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of $r^d$.</s> <s>we apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on fourier transforms, and universal kernels for signal processing.</s></p></d>", "label": ["<d><p><s>universal kernels on non-standard input spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>straightforward application of deep belief nets (dbns) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent timit phone recognition task.</s> <s>however, the first-layer gaussian-bernoulli restricted boltzmann machine (grbm) has an important limitation, shared with mixtures of diagonal-covariance gaussians: grbms treat different components of the acoustic input vector as conditionally independent given the hidden state.</s> <s>the mean-covariance restricted boltzmann machine (mcrbm), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data.</s> <s>every configuration of the precision units of the mcrbm specifies a different precision matrix for the conditional distribution over the acoustic space.</s> <s>in this work, we use the mcrbm to learn features of speech data that serve as input into a standard dbn.</s> <s>the mcrbm features combined with dbns allow us to achieve a phone error rate of 20.5\\%, which is superior to all published results on speaker-independent timit to date.</s></p></d>", "label": ["<d><p><s>phone recognition with the mean-covariance restricted boltzmann machine</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>games of incomplete information, or bayesian games, are an important game-theoretic model and have many applications in economics.</s> <s>we propose bayesian action-graph games (baggs), a novel graphical representation for bayesian games.</s> <s>baggs can represent arbitrary bayesian games, and furthermore can compactly express bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-specific utility independence, and probabilistic independence of type distributions.</s> <s>we provide an algorithm for computing expected utility in baggs, and discuss conditions under which the algorithm runs in polynomial time.</s> <s>bayes-nash equilibria of baggs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm.</s> <s>we show both theoretically and empirically that our approaches improve significantly on the state of the art.</s></p></d>", "label": ["<d><p><s>bayesian action-graph games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary firing rates using a switching state space model (sssm).</s> <s>this model enables us to detect state transitions based not only on the discontinuous changes of mean firing rates but also on discontinuous changes in temporal profiles of firing rates, e.g., temporal correlation.</s> <s>we derive a variational bayes algorithm for a non-gaussian sssm whose non-gaussian property is caused by binary spike events.</s> <s>synthetic data analysis reveals the high performance of our algorithm in estimating state transitions, the number of neural states, and nonstationary firing rates compared to previous methods.</s> <s>we also analyze neural data recorded from the medial temporal area.</s> <s>the statistically detected neural states probably coincide with transient and sustained states, which have been detected heuristically.</s> <s>estimated parameters suggest that our algorithm detects the state transition based on discontinuous change in the temporal correlation of firing rates, which transitions previous methods cannot detect.</s> <s>this result suggests the advantage of our algorithm in real-data analysis.</s></p></d>", "label": ["<d><p><s>switching state space model for simultaneously estimating state transitions and nonstationary firing rates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel method for inferring whether x causes y or vice versa from joint observations of x and y.</s> <s>the basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise.</s> <s>to this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive).</s> <s>an important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause.</s> <s>the causal direction can then be inferred by using standard bayesian model selection.</s> <s>we evaluate our approach on synthetic data and real-world data and report encouraging results.</s></p></d>", "label": ["<d><p><s>probabilistic latent variable models for distinguishing between cause and effect</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new bayesian nonparametric approach to identification of sparse dynamic linear systems.</s> <s>the impulse responses are modeled as gaussian processes whose autocovariances encode the bibo stability constraint, as defined by the recently introduced ?stable spline kernel?.</s> <s>sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels.</s> <s>numerical experiments regarding estimation of armax models show that this technique provides a definite advantage over a group lar algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.</s></p></d>", "label": ["<d><p><s>learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>markov networks (mns) can incorporate arbitrarily complex features in modeling relational data.</s> <s>however, this flexibility comes at a sharp price of training an exponentially complex model.</s> <s>to address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational mns (rmns) called relation tree-based rmn (treermn), and an efficient hidden variable detection algorithm called contrastive variable induction (cvi).</s> <s>on one hand, the restricted treermn only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the cvi algorithm efficiently detects hidden variables which can capture long range dependencies.</s> <s>therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power.</s> <s>empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treermns.</s></p></d>", "label": ["<d><p><s>efficient relational learning with hidden variable detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most active learning approaches select either informative or representative unlabeled instances to query their labels.</s> <s>although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative.</s> <s>we address this challenge by a principled approach, termed quire, based on the min-max view of active learning.</s> <s>the proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance.</s> <s>extensive experimental results show that the proposed quire approach outperforms several state-of -the-art active learning approaches.</s></p></d>", "label": ["<d><p><s>active learning by querying informative and representative examples</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity  of kernel learning in computer vision problems.</s> <s>in this work, we develop an efficient algorithm for multi-label multiple kernel learning (ml-mkl).</s> <s>we assume that all the classes under  consideration  share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes.</s> <s>although several algorithms have been developed for ml-mkl, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge  frequently  encountered in visual object recognition.</s> <s>we address this computational challenge by developing a framework for ml-mkl that combines the worst-case analysis with stochastic approximation.</s> <s>our analysis shows that the complexity of our algorithm is $o(m^{1/3}\\sqrt{ln m})$, where $m$ is the number of classes.</s> <s>empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ml-mkl.</s></p></d>", "label": ["<d><p><s>multi-label multiple kernel learning by stochastic approximation: application to visual object recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new nonparametric learning method based on multivariate dyadic regression trees (mdrts).</s> <s>unlike traditional dyadic decision trees (ddts) or classification and regression trees (carts), mdrts are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty.</s> <s>theoretically, we show that mdrts can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of $(\\alpha, c)$-smooth functions.</s> <s>empirically, mdrts can simultaneously conduct function estimation and variable selection in high dimensions.</s> <s>to make mdrts applicable  for large-scale learning problems, we propose a greedy heuristics.</s> <s>the superior performance of mdrts are demonstrated on both synthetic and real datasets.</s></p></d>", "label": ["<d><p><s>multivariate dyadic regression trees for sparse learning problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimating 3d pose from monocular images is a highly ambiguous problem.</s> <s>physical constraints can be exploited to restrict the space of feasible configurations.</s> <s>in this paper we propose an approach to constraining the prediction of a discriminative predictor.</s> <s>we first show that the mean prediction of a gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples.</s> <s>we then show how, by performing a change of variables, a gp can be forced to satisfy quadratic constraints.</s> <s>as evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.</s></p></d>", "label": ["<d><p><s>implicitly constrained gaussian process regression for monocular non-rigid pose estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert.</s> <s>we study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior.</s> <s>although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis.</s> <s>we prove that, if the learned classifier has error rate $\\eps$, the difference between the value of the apprentice's policy and the expert's policy is $o(\\sqrt{\\eps})$.</s> <s>further, we prove that this difference is only $o(\\eps)$ when the expert's policy is close to optimal.</s> <s>this latter result has an important practical consequence: not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert.</s> <s>this suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.</s></p></d>", "label": ["<d><p><s>a reduction from apprenticeship learning to classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the question of how the approximation error/bellman residual at each iteration of the approximate policy/value iteration algorithms influences the quality of the resulted policy.</s> <s>we quantify the performance loss as the lp norm of the approximation error/bellman residual at each iteration.</s> <s>moreover, we show that the performance loss depends on the expectation of the squared radon-nikodym derivative of a certain distribution rather than its supremum -- as opposed to what has been suggested by the previous results.</s> <s>also our results indicate that the contribution of the approximation/bellman error to the performance loss is more prominent in the later iterations of api/avi, and the effect of an error term in the earlier iterations decays exponentially fast.</s></p></d>", "label": ["<d><p><s>error propagation for approximate policy and value iteration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the problem of policy evaluation for continuous-state systems.</s> <s>we present a non-parametric approach to policy evaluation, which uses kernel density estimation to represent the system.</s> <s>the true form of the value function for this model can be determined, and can be computed using galerkin's method.</s> <s>furthermore, we also present a unified view of several well-known policy evaluation methods.</s> <s>in particular, we show that the same galerkin method can be used to derive least-squares temporal difference learning, kernelized temporal difference learning, and a discrete-state dynamic programming solution, as well as our proposed method.</s> <s>in a numerical evaluation of these algorithms, the proposed approach performed better than the other methods.</s></p></d>", "label": ["<d><p><s>a non-parametric approach to dynamic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many applications in computer vision measure the similarity between images or image patches based on some statistics such as oriented gradients.</s> <s>these are often modeled implicitly or explicitly with a gaussian noise assumption, leading to the use of the euclidean distance when comparing image descriptors.</s> <s>in this paper, we show that the statistics of gradient based image descriptors often follow a heavy-tailed distribution, which undermines any principled motivation for the use of euclidean distances.</s> <s>we advocate for the use of a distance measure based on the likelihood ratio test with appropriate probabilistic models that fit the empirical data distribution.</s> <s>we instantiate this similarity measure with the gamma-compound-laplace distribution, and show significant improvement over existing distance measures in the application of sift feature matching, at relatively low computational cost.</s></p></d>", "label": ["<d><p><s>heavy-tailed distances for gradient based image descriptors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study multi-label prediction for structured output spaces, a problem that occurs, for example, in object detection in images, secondary structure prediction in computational biology, and graph matching with symmetries.</s> <s>conventional multi-label classification techniques are typically not applicable in this situation, because they require explicit enumeration of the label space, which is infeasible in case of structured outputs.</s> <s>relying on techniques originally designed for single- label structured prediction, in particular structured support vector machines, results in reduced prediction accuracy, or leads to infeasible optimization problems.</s> <s>in this work we derive a maximum-margin training formulation for multi-label structured prediction that remains computationally tractable while achieving high prediction accuracy.</s> <s>it also shares most beneficial properties with single-label maximum-margin approaches, in particular a formulation as a convex optimization problem, efficient working set training, and pac-bayesian generalization bounds.</s></p></d>", "label": ["<d><p><s>maximum margin multi-label structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the family of p-resistances on graphs for p ?</s> <s>1.</s> <s>this family generalizes the standard resistance distance.</s> <s>we prove that for any fixed graph, for p=1, the p-resistance coincides with the shortest path distance, for p=2 it coincides with the standard resistance distance, and for p ?</s> <s>?</s> <s>it converges to the inverse of the minimal s-t-cut in the graph.</s> <s>secondly, we consider the special case of random geometric graphs (such as k-nearest neighbor graphs) when the number n of vertices in the graph tends to infinity.</s> <s>we prove that an interesting phase-transition takes place.</s> <s>there exist two critical thresholds p^* and p^** such that if p < p^*, then the p-resistance depends on meaningful global properties of the graph, whereas if p > p^**, it only depends on trivial local quantities and does not convey any useful information.</s> <s>we can explicitly compute the critical values: p^* = 1 + 1/(d-1) and p^** = 1 + 1/(d-2) where d is the dimension of the underlying space (we believe that the fact that there is a small gap between p^* and p^** is an artifact of our proofs.</s> <s>we also relate our findings to laplacian regularization and suggest to use q-laplacians as regularizers, where q satisfies 1/p^* + 1/q = 1.</s></p></d>", "label": ["<d><p><s>phase transition in the family of p-resistances</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose maximum covariance unfolding (mcu), a manifold learning algorithm for simultaneous dimensionality reduction of data from different input modalities.</s> <s>given high dimensional inputs from two different but naturally aligned sources, mcu computes a common low dimensional embedding that maximizes the cross-modal (inter-source) correlations while preserving the local (intra-source) distances.</s> <s>in this paper, we explore two applications of mcu.</s> <s>first we use mcu to analyze eeg-fmri data, where an important goal is to visualize the fmri voxels that are most strongly correlated with changes in eeg traces.</s> <s>to perform this visualization, we augment mcu with an additional step for metric learning in the high dimensional voxel space.</s> <s>second, we use mcu to perform cross-modal retrieval of matched image and text samples from wikipedia.</s> <s>to manage large applications of mcu, we develop a fast implementation based on ideas from spectral graph theory.</s> <s>these ideas transform the original problem for mcu, one of semidefinite programming, into a simpler problem in semidefinite quadratic linear programming.</s></p></d>", "label": ["<d><p><s>maximum covariance unfolding : manifold learning for bimodal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>is it possible to crowdsource categorization?</s> <s>amongst the challenges: (a) each annotator has only a partial view of the data, (b) different annotators may have different clustering criteria and may produce different numbers of categories, (c) the underlying category structure may be hierarchical.</s> <s>we propose a bayesian model of how annotators may approach clustering and show how one may infer clusters/categories, as well as annotator parameters, using this model.</s> <s>our experiments, carried out on large collections of images, suggest that bayesian crowdclustering works well and may be superior to single-expert annotations.</s></p></d>", "label": ["<d><p><s>crowdclustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new algorithm for exactly solving decision-making problems represented as an influence diagram.</s> <s>we do not require the usual assumptions of no forgetting and regularity, which allows us to solve problems with limited information.</s> <s>the algorithm, which implements a sophisticated variable elimination procedure, is empirically shown to outperform a state-of-the-art algorithm in randomly generated problems of up to 150 variables and $10^{64}$ strategies.</s></p></d>", "label": ["<d><p><s>solving decision problems with limited information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel generative model that is able to reason jointly about the 3d scene layout as well as the 3d location and orientation of objects in the scene.</s> <s>in particular, we infer the scene topology, geometry as well as traffic activities from a short video sequence acquired with a single camera mounted on a moving car.</s> <s>our generative model takes advantage of dynamic information in the form of vehicle tracklets as well as static information coming from semantic labels and geometry  (i.e., vanishing points).</s> <s>experiments show that our approach outperforms a discriminative baseline based on multiple kernel learning (mkl) which has access to the same image information.</s> <s>furthermore, as we reason about objects in 3d, we are able to significantly increase the performance of state-of-the-art object detectors in their ability to estimate  object orientation.</s></p></d>", "label": ["<d><p><s>joint 3d estimation of objects and scene layout</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the problem of active learning in a stream-based setting, allowing the distribution of the examples to change over time.</s> <s>we prove upper bounds on the number of prediction mistakes and number of label requests for established disagreement-based active learning algorithms, both in the realizable case and under tsybakov noise.</s> <s>we further prove minimax lower bounds for this problem.</s></p></d>", "label": ["<d><p><s>active learning with a drifting distribution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most methods for decision-theoretic online learning are based on the hedge algorithm, which takes a parameter called the learning rate.</s> <s>in most previous analyses the learning rate was carefully tuned to obtain optimal worst-case performance, leading to suboptimal performance on easy instances, for example when there exists an action that is significantly better than all others.</s> <s>we propose a new way of setting the learning rate, which adapts to the difficulty of the learning problem: in the worst case our procedure still guarantees optimal performance, but on easy instances it achieves much smaller regret.</s> <s>in particular, our adaptive method achieves constant regret in a probabilistic setting, when there exists an action that on average obtains strictly smaller loss than all other actions.</s> <s>we also provide a simulation study comparing our approach to existing methods.</s></p></d>", "label": ["<d><p><s>adaptive hedge</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in matrix completion, we are given a matrix where the values of only some of the entries are present, and we want to reconstruct the missing ones.</s> <s>much work has focused on the assumption that the data matrix has low rank.</s> <s>we propose a more general assumption based on denoising, so that we expect that the value of a missing entry can be predicted from the values of neighboring points.</s> <s>we propose a nonparametric version of denoising based on local, iterated averaging with mean-shift, possibly constrained to preserve local low-rank manifold structure.</s> <s>the few user parameters required (the denoising scale, number of neighbors and local dimensionality) and the number of iterations can be estimated by cross-validating the reconstruction error.</s> <s>using our algorithms as a postprocessing step on an initial reconstruction (provided by e.g.</s> <s>a low-rank method), we show consistent improvements with synthetic, image and motion-capture data.</s></p></d>", "label": ["<d><p><s>a denoising view of matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, there has been substantial interest in using large amounts of unlabeled data to learn word representations which can then be used as features in supervised classifiers for nlp tasks.</s> <s>however, most current approaches are slow to train, do not model context of the word, and lack theoretical grounding.</s> <s>in this paper, we present a new learning method, low rank multi-view learning (lr-mvl) which uses a fast spectral method to estimate  low dimensional context-specific word representations from unlabeled data.</s> <s>these representation features can then be used with any supervised learner.</s> <s>lr-mvl is extremely fast, gives guaranteed convergence to a global optimum, is theoretically elegant, and achieves state-of-the-art performance on named entity recognition (ner) and chunking problems.</s></p></d>", "label": ["<d><p><s>multi-view learning of word embeddings via cca</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a novel regularization-based multitask learning (mtl) formulation  for structured output (so) prediction for the case of hierarchical task relations.</s> <s>structured output learning often results in dif?cult inference problems and   requires large amounts of training data to obtain accurate models.</s> <s>we propose to  use mtl to exploit information available for related structured output learning  tasks by means of hierarchical regularization.</s> <s>due to the combination of example   sets, the cost of training models for structured output prediction can easily  become infeasible for real world applications.</s> <s>we thus propose an ef?cient   algorithm based on bundle methods to solve the optimization problems resulting from  mtl structured output learning.</s> <s>we demonstrate the performance of our approach  on gene ?nding problems from the application domain of computational biology.</s> <s>we show that 1) our proposed solver achieves much faster convergence than previous   methods and 2) that the hierarchical so-mtl approach clearly outperforms  considered non-mtl methods.</s></p></d>", "label": ["<d><p><s>hierarchical multitask structured output learning for large-scale sequence segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discriminative learning when training and test data belong to different distributions is a challenging and complex task.</s> <s>often times we have very few or no labeled data from the test or target distribution but may have plenty of labeled data from multiple related sources with different distributions.</s> <s>the difference in distributions may be in both marginal and conditional probabilities.</s> <s>most of the existing domain adaptation work focuses on the marginal  probability distribution difference between the domains, assuming that the conditional probabilities are similar.</s> <s>however in many real world applications, conditional probability distribution differences are as commonplace as marginal probability differences.</s> <s>in this paper we propose a two-stage domain adaptation methodology which combines weighted data from multiple sources based on marginal probability differences (first stage) as well as conditional probability differences (second stage), with the target domain data.</s> <s>the weights for minimizing the marginal probability differences are estimated independently, while the weights for minimizing conditional  probability differences are computed simultaneously by exploiting the potential interaction among multiple sources.</s> <s>we also provide a theoretical analysis on the generalization performance of the proposed multi-source domain adaptation formulation using the weighted rademacher complexity measure.</s> <s>empirical comparisons with existing state-of-the-art domain adaptation methods using three real-world datasets demonstrate the effectiveness of the proposed approach.</s></p></d>", "label": ["<d><p><s>a two-stage weighting framework for multi-source domain adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>principal components analysis~(pca) is often used as a feature extraction procedure.</s> <s>given a matrix $x \\in \\mathbb{r}^{n \\times d}$, whose rows represent $n$ data points with respect to $d$ features, the top $k$ right singular vectors of $x$ (the so-called \\textit{eigenfeatures}), are arbitrary linear combinations of all available features.</s> <s>the eigenfeatures are very useful in data analysis, including the regularization of linear regression.</s> <s>enforcing sparsity on the eigenfeatures, i.e., forcing them to be linear combinations of only a \\textit{small} number of actual features (as opposed to all available features), can promote better generalization error and improve the interpretability of the eigenfeatures.</s> <s>we present deterministic and randomized algorithms that construct such sparse eigenfeatures while \\emph{provably} achieving in-sample performance comparable to regularized linear regression.</s> <s>our algorithms are relatively simple and practically efficient, and we demonstrate their performance on several data sets.</s></p></d>", "label": ["<d><p><s>sparse features for pca-like linear regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning rules from natural language text  sources.</s> <s>these sources, such as news articles and web texts, are  created by a writer to communicate information to a reader, where the  writer and reader share substantial domain knowledge.</s> <s>consequently,  the texts tend to be concise and mention the minimum information  necessary for the reader to draw the correct conclusions.</s> <s>we study  the problem of learning domain knowledge from such concise texts,  which is an instance of the general problem of learning in the  presence of missing data.</s> <s>however, unlike standard approaches to  missing data, in this setting we know that facts are more likely to be  missing from the text in cases where the reader can infer them from  the facts that are mentioned combined with the domain knowledge.</s> <s>hence, we can explicitly model this \"missingness\" process and invert  it via probabilistic inference to learn the underlying domain  knowledge.</s> <s>this paper introduces a mention model that models  the probability of facts being mentioned in the text based on what  other facts have already been mentioned and domain knowledge in the  form of horn clause rules.</s> <s>learning must simultaneously search the  space of rules and learn the parameters of the mention model.</s> <s>we  accomplish this via an application of expectation maximization within  a markov logic framework.</s> <s>an experimental evaluation on synthetic and  natural text data shows that the method can learn accurate rules and  apply them to new texts to make correct inferences.</s> <s>experiments also  show that the method out-performs the standard em approach that  assumes mentions are missing at random.</s></p></d>", "label": ["<d><p><s>inverting grice's maxims to learn rules from natural language extractions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years semidefinite optimization has become a tool of major importance in various optimization and machine learning problems.</s> <s>in many of these problems the amount of data in practice is so large that there is a constant need for faster algorithms.</s> <s>in this work we present the first  sublinear time approximation algorithm for semidefinite programs which we believe may be useful for such problems in which the size of data may cause even linear time algorithms to have prohibitive running times in practice.</s> <s>we present the algorithm and its analysis alongside with some theoretical lower bounds and an improved algorithm for the special problem of supervised learning of a distance metric.</s></p></d>", "label": ["<d><p><s>approximating semidefinite programs in sublinear time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>knowledge-based support vector machines (kbsvms) incorporate advice from domain experts, which can improve generalization significantly.</s> <s>a major limitation that has not been fully addressed occurs when the expert advice is imperfect, which can lead to poorer models.</s> <s>we propose a model that extends kbsvms and is able to not only learn from data and advice, but also simultaneously improve the advice.</s> <s>the proposed approach is particularly effective for knowledge discovery in domains with few labeled examples.</s> <s>the proposed model contains bilinear constraints, and is solved using two iterative approaches: successive linear programming and a constrained concave-convex approach.</s> <s>experimental results demonstrate that these algorithms yield useful refinements to expert advice, as well as improve the performance of the learning algorithm overall.</s></p></d>", "label": ["<d><p><s>advice refinement in knowledge-based svms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we give a new generalization error bound of multiple kernel learning  (mkl) for a general class of regularizations.</s> <s>our main target in this paper is  dense type regularizations including ?p-mkl that imposes ?p-mixed-norm regularization  instead of ?1-mixed-norm regularization.</s> <s>according to the recent numerical  experiments, the sparse regularization does not necessarily show a good  performance compared with dense type regularizations.</s> <s>motivated by this fact,  this paper gives a general theoretical tool to derive fast learning rates that is applicable  to arbitrary monotone norm-type regularizations in a unifying manner.</s> <s>as  a by-product of our general result, we show a fast learning rate of ?p-mkl that  is tightest among existing bounds.</s> <s>we also show that our general learning rate  achieves the minimax lower bound.</s> <s>finally, we show that, when the complexities  of candidate reproducing kernel hilbert spaces are inhomogeneous, dense type  regularization shows better learning rate compared with sparse ?1 regularization.</s></p></d>", "label": ["<d><p><s>unifying framework for fast learning rate of non-sparse multiple kernel learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>graph cut optimization is one of the standard workhorses of image segmentation since for binary random field representations of the  image, it gives globally optimal results and there are efficient  polynomial time implementations.</s> <s>often, the random field is applied  over a flat partitioning of the image into non-intersecting elements,  such as pixels or super-pixels.</s> <s>in the paper we show that if, instead of a flat partitioning, the image is represented by a hierarchical segmentation tree, then the resulting energy combining unary and boundary terms can still be optimized using graph cut (with all the corresponding benefits of global optimality and efficiency).</s> <s>as a result of such inference, the image gets partitioned into a set of segments that may come from different layers of the tree.</s> <s>we apply this formulation, which we call the pylon model, to the task of semantic segmentation where the goal is to separate an image into areas belonging to different semantic classes.</s> <s>the experiments highlight the advantage of inference on a segmentation tree (over a flat partitioning) and demonstrate that the optimization in the pylon model is able to flexibly choose the level of segmentation across the image.</s> <s>overall, the proposed system has superior segmentation accuracy on several datasets (graz-02, stanford background) compared to previously suggested approaches.</s></p></d>", "label": ["<d><p><s>pylon model for semantic segmentation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>many clustering techniques aim at optimizing empirical criteria that are of the  form of a u-statistic of degree two.</s> <s>given a measure of dissimilarity between  pairs of observations, the goal is to minimize the within cluster point scatter over  a class of partitions of the feature space.</s> <s>it is the purpose of this paper to define  a general statistical framework, relying on the theory of u-processes, for studying  the performance of such clustering methods.</s> <s>in this setup, under adequate  assumptions on the complexity of the subsets forming the partition candidates, the  excess of clustering risk is proved to be of the order o(1/\\sqrt{n}).</s> <s>based on recent  results related to the tail behavior of degenerate u-processes, it is also shown how  to establish tighter rate bounds.</s> <s>model selection issues, related to the number of  clusters forming the data partition in particular, are also considered.</s></p></d>", "label": ["<d><p><s>on u-processes and clustering performance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper considers the problem of combining multiple models to achieve a prediction accuracy not much worse than that of the best single model for least squares regression.</s> <s>it is known that if the models are mis-specified, model averaging is superior to model selection.</s> <s>specifically, let $n$ be the sample size, then the worst case regret of the former decays at the rate of $o(1/n)$   while the worst case regret of the latter decays at the rate of $o(1/\\sqrt{n})$.</s> <s>in the literature, the most important and widely studied model averaging method that achieves the optimal $o(1/n)$ average regret   is the exponential weighted model averaging (ewma) algorithm.</s> <s>however this method suffers from several limitations.</s> <s>the purpose of this paper is to present a new greedy model averaging procedure that improves ewma.</s> <s>we prove strong theoretical guarantees for the new procedure and illustrate our theoretical results with empirical examples.</s></p></d>", "label": ["<d><p><s>greedy model averaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many species show avoidance reactions in response to looming object approaches.</s> <s>in locusts, the corresponding escape behavior correlates with the activity  of the lobula giant movement detector (lgmd) neuron.</s> <s>during an object approach,  its firing rate was reported to gradually increase until a peak is reached,  and then it declines quickly.</s> <s>the $\\eta$-function predicts that the lgmd activity  is a product between an exponential function of angular size $\\exp(-\\theta)$ and  angular velocity $\\dot{\\theta}$, and that peak activity is reached before time-to-contact  (ttc).</s> <s>the $\\eta$-function has become the prevailing lgmd model because it  reproduces many experimental observations, and even experimental evidence for  the multiplicative operation was reported.</s> <s>several inconsistencies remain  unresolved, though.</s> <s>here we address these issues with a new model ($\\psi$-model),  which explicitly connects $\\theta$ and $\\dot{\\theta}$ to biophysical quantities.</s> <s>the $\\psi$-model avoids biophysical problems associated with implementing  $\\exp(\\cdot)$, implements the multiplicative operation of $\\eta$ via divisive  inhibition, and explains why activity peaks could occur after ttc.</s> <s>it consistently  predicts response features of the lgmd, and provides excellent fits to published  experimental data, with goodness of fit measures comparable to corresponding  fits with the $\\eta$-function.</s></p></d>", "label": ["<d><p><s>emergence of multiplication in a biophysical model of a wide-field visual neuron for computing object approaches: dynamics, peaks, & fits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents an approach that predicts the effectiveness of hiv combination therapies by simultaneously addressing several problems affecting the available hiv clinical data sets: the different treatment backgrounds of the samples, the uneven representation of the levels of therapy experience, the missing treatment history information, the uneven therapy representation and the unbalanced therapy outcome representation.</s> <s>the computational validation on clinical data shows that, compared to the most commonly used approach that does not account for the issues mentioned above, our model has significantly higher predictive power.</s> <s>this is especially true for samples stemming from patients with longer treatment history and samples associated with rare therapies.</s> <s>furthermore, our approach is at least as powerful for the remaining samples.</s></p></d>", "label": ["<d><p><s>history distribution matching method for predicting effectiveness of hiv combination therapies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a novel boosting algorithm called vadaboost which is motivated by recent empirical bernstein bounds.</s> <s>vadaboost iteratively minimizes a cost function that balances the sample mean and the sample variance of the exponential loss.</s> <s>each step of the proposed algorithm minimizes the cost efficiently by providing weighted data to a weak learner rather than requiring a brute force evaluation of all possible weak learners.</s> <s>thus, the proposed algorithm solves a key limitation of previous empirical bernstein boosting methods which required brute force enumeration of all possible weak learners.</s> <s>experimental results confirm that the new algorithm achieves the performance improvements of ebboost yet goes beyond decision stumps to handle any weak learner.</s> <s>significant performance gains are obtained over adaboost for arbitrary weak learners including decision trees (cart).</s></p></d>", "label": ["<d><p><s>variance penalizing adaboost</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work considers the problem of learning the structure of multivariate linear tree models, which include a variety of directed tree graphical models with continuous, discrete, and mixed latent variables such as linear-gaussian models, hidden markov models, gaussian mixture models, and markov evolutionary trees.</s> <s>the setting is one where we only have samples from certain observed variables in the tree, and our goal is to estimate the tree structure (i.e., the graph of how the underlying hidden variables are connected to each other and to the observed variables).</s> <s>we propose the spectral recursive grouping algorithm, an efficient and simple bottom-up procedure for recovering the tree structure from independent samples of the observed variables.</s> <s>our finite sample size bounds for exact recovery of the tree structure  reveal certain natural dependencies on underlying statistical and structural properties of the underlying joint distribution.</s> <s>furthermore, our sample complexity guarantees have no explicit dependence on the dimensionality of the observed variables, making the algorithm applicable to many high-dimensional settings.</s> <s>at the heart of our algorithm is a spectral quartet test for determining the relative topology of a quartet of variables from second-order statistics.</s></p></d>", "label": ["<d><p><s>spectral methods for learning multivariate latent tree structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in discrete undirected graphical models, the conditional independence of node labels y is specified by the graph structure.</s> <s>we study the case where there is another input random vector x (e.g.</s> <s>observed features) such that the distribution p (y | x) is determined by functions of x that characterize the (higher-order) interactions among the y ?s.</s> <s>the main contribution of this paper is to learn the graph structure and the functions conditioned on x at the same time.</s> <s>we prove that discrete undirected graphical models with feature x are equivalent to mul- tivariate discrete models.</s> <s>the reparameterization of the potential functions in graphical models by conditional log odds ratios of the latter offers advantages in representation of the conditional independence structure.</s> <s>the functional spaces can be flexibly determined by kernels.</s> <s>additionally, we impose a structure lasso (slasso) penalty on groups of functions to learn the graph structure.</s> <s>these groups with overlaps are designed to enforce hierarchical function selection.</s> <s>in this way, we are able to shrink higher order interactions to obtain a sparse graph structure.</s></p></d>", "label": ["<d><p><s>learning higher-order graph structure with features by structure penalty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an accurate model of patient survival time can help in the treatment and care of cancer patients.</s> <s>the common practice of providing survival time estimates based only on population averages for the site and stage of cancer ignores many important individual differences among patients.</s> <s>in this paper, we propose a local regression method for learning patient-specific survival time distribution based on patient attributes such as blood tests and clinical assessments.</s> <s>when tested on a cohort of more than 2000 cancer patients, our method gives survival time predictions that are much more accurate than popular survival analysis models such as the cox and aalen regression models.</s> <s>our results also show that using patient-specific attributes can reduce the prediction error on survival time by as much as 20% when compared to using cancer site and stage only.</s></p></d>", "label": ["<d><p><s>learning patient-specific cancer survival distributions as a sequence of dependent regressors</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a novel class of bayesian nonparametric models for sequential data called fragmentation-coagulation processes (fcps).</s> <s>fcps model a set of sequences using  a partition-valued markov process which evolves by splitting and merging clusters.</s> <s>an fcp is exchangeable, projective, stationary and reversible, and its equilibrium distributions are given by the chinese restaurant process.</s> <s>as opposed to hidden markov models, fcps allow for flexible modelling of the number of clusters, and they avoid label switching non-identifiability problems.</s> <s>we develop an efficient gibbs sampler for fcps which uses uniformization and the forward-backward algorithm.</s> <s>our development of fcps is motivated by applications in population genetics, and we demonstrate the utility of fcps on problems of genotype imputation with phased and unphased snp data.</s></p></d>", "label": ["<d><p><s>modelling genetic variations using fragmentation-coagulation processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel approach to efficiently learn a label tree for large scale classification with many classes.</s> <s>the key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree.</s> <s>this approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees.</s> <s>experiments are performed on large scale image classification with 10184 classes and 9 million images.</s> <s>we demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by bengio et al.</s></p></d>", "label": ["<d><p><s>fast and balanced: efficient label tree learning for large scale object recognition</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>multiclass prediction is the problem of classifying an object into a    relevant target class.</s> <s>we consider the problem of learning a    multiclass predictor that uses only few features, and in particular,    the number of used features should increase sub-linearly with the    number of possible classes.</s> <s>this implies that features should be    shared by several classes.</s> <s>we describe and analyze the shareboost    algorithm for learning a multiclass predictor that uses few shared    features.</s> <s>we prove that shareboost efficiently finds a predictor    that uses few shared features (if such a predictor exists) and that    it has a small generalization error.</s> <s>we also describe how to use    shareboost for learning a non-linear predictor that has a fast    evaluation time.</s> <s>in a series of experiments with natural data sets    we demonstrate the benefits of shareboost and evaluate its success    relatively to other state-of-the-art approaches.</s></p></d>", "label": ["<d><p><s>shareboost: efficient multiclass learning with feature sharing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>state-of-the-art statistical methods in neuroscience have enabled us to fit mathematical models to experimental data and subsequently to infer the dynamics of hidden parameters underlying the observable phenomena.</s> <s>here, we develop a bayesian method for inferring the time-varying mean and variance of the synaptic input, along with the dynamics of each ion channel from a single voltage trace of a neuron.</s> <s>an estimation problem may be formulated on the basis of the state-space model with prior distributions that penalize large fluctuations in these parameters.</s> <s>after optimizing the hyperparameters by maximizing the marginal likelihood, the state-space model provides the time-varying parameters of the input signals and the ion channel states.</s> <s>the proposed method is tested not only on the simulated data from the hodgkin-huxley type models but also on experimental data obtained from a cortical slice in vitro.</s></p></d>", "label": ["<d><p><s>estimating time-varying input signals and ion channel states from a single voltage trace of a neuron</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many experiments, the data points collected live in high-dimensional observation spaces, yet can be assigned a set of labels or parameters.</s> <s>in electrophysiological recordings, for instance, the responses of populations of neurons generally depend on mixtures of experimentally controlled parameters.</s> <s>the heterogeneity and diversity of these parameter dependencies can make visualization and interpretation of such data extremely difficult.</s> <s>standard dimensionality reduction techniques such as principal component analysis (pca) can provide a succinct and complete description of the data, but the description is constructed independent of the relevant task variables and is often hard to interpret.</s> <s>here, we start with the assumption that a particularly informative description is one that reveals the dependency of the high-dimensional data on the individual parameters.</s> <s>we show how to modify the loss function of pca so that the principal components seek to capture both the maximum amount of variance about the data, while also depending on a minimum number of parameters.</s> <s>we call this method demixed principal component analysis (dpca) as the principal components here segregate the parameter dependencies.</s> <s>we phrase the problem as a probabilistic graphical model, and present a fast expectation-maximization (em) algorithm.</s> <s>we demonstrate the use of this algorithm for electrophysiological data and show that it serves to demix the parameter-dependence of a neural population response.</s></p></d>", "label": ["<d><p><s>demixed principal component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove a new oracle inequality for support vector machines with gaussian rbf kernels solving the regularized least squares regression problem.</s> <s>to this end, we apply the modulus of smoothness.</s> <s>with the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method.</s> <s>finally, it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions.</s></p></d>", "label": ["<d><p><s>optimal learning rates for least squares svms using gaussian kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel-based reinforcement-learning (kbrl) is a method for learning a decision policy from a set of sample transitions which stands out for its strong theoretical guarantees.</s> <s>however, the size of the approximator grows with the number of transitions, which makes the approach impractical for large problems.</s> <s>in this paper we introduce a novel algorithm to improve the scalability of kbrl.</s> <s>we resort to a special decomposition of a transition matrix, called stochastic factorization, to fix the size of the approximator while at the same time incorporating all the information contained in the data.</s> <s>the resulting algorithm, kernel-based stochastic factorization (kbsf), is much faster but still converges to a unique solution.</s> <s>we derive a theoretical upper bound for the distance between the value functions computed by kbrl and kbsf.</s> <s>the effectiveness of our method is illustrated with computational experiments on four reinforcement-learning problems, including a difficult task in which the goal is to learn a neurostimulation policy to suppress the occurrence of seizures in epileptic rat brains.</s> <s>we empirically demonstrate that the proposed approach is able to compress the information contained in kbrl's model.</s> <s>also, on the tasks studied, kbsf outperforms two of the most prominent reinforcement-learning algorithms, namely least-squares policy iteration and fitted q-iteration.</s></p></d>", "label": ["<d><p><s>reinforcement learning using kernel-based stochastic factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of identifying a sparse set of relevant columns and rows in a large data matrix with highly corrupted entries.</s> <s>this problem of identifying groups from a collection of bipartite variables such as proteins and drugs, biological species and gene sequences, malware and signatures, etc is commonly referred to as biclustering or co-clustering.</s> <s>despite its great practical relevance, and although several ad-hoc methods are available for biclustering, theoretical analysis of the problem is largely non-existent.</s> <s>the problem we consider is also closely related to structured multiple hypothesis testing, an area of statistics that has recently witnessed a flurry of activity.</s> <s>we make the following contributions: i) we prove lower bounds on the minimum signal strength needed for successful recovery of a bicluster as a function of the noise variance, size of the matrix and bicluster of interest.</s> <s>ii) we show that a combinatorial procedure based on the scan statistic achieves this optimal limit.</s> <s>iii) we characterize the snr required by several computationally tractable procedures for biclustering including element-wise thresholding, column/row average thresholding and a convex relaxation approach to sparse singular vector decomposition.</s></p></d>", "label": ["<d><p><s>minimax localization of structural information in large noisy matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we derive a method to refine a bayes network diagnostic model by exploiting constraints implied by expert decisions on test ordering.</s> <s>at each step, the expert executes an evidence gathering test, which suggests the test's relative diagnostic value.</s> <s>we demonstrate that consistency with an expert's test selection leads to non-convex constraints on the model parameters.</s> <s>we incorporate these constraints by augmenting the network with nodes that represent the constraint likelihoods.</s> <s>gibbs sampling, stochastic hill climbing and greedy search algorithms are proposed to find a map estimate that takes into account test ordering constraints and any data available.</s> <s>we demonstrate our approach on diagnostic sessions from a manufacturing scenario.</s></p></d>", "label": ["<d><p><s>automated refinement of bayes networks' parameters based on test ordering constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information (counts or low-dimensional contingency tables).</s> <s>this paper introduces collective graphical models---a framework for modeling and probabilistic inference that operates directly on the sufficient statistics of the individual model.</s> <s>we derive a highly-efficient gibbs sampling algorithm for sampling from the posterior distribution of the sufficient statistics conditioned on noisy aggregate observations, prove its correctness, and demonstrate its effectiveness experimentally.</s></p></d>", "label": ["<d><p><s>collective graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a gaussian process model of functions which are additive.</s> <s>an additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables.</s> <s>additive gps generalize both generalized additive models, and the standard gp models which use squared-exponential kernels.</s> <s>hyperparameter learning in this model can be seen as bayesian hierarchical kernel learning (hkl).</s> <s>we introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension.</s> <s>the additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.</s></p></d>", "label": ["<d><p><s>additive gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of reconstructing an unknown matrix m of rank r and dimension d using o(rd polylog d) pauli measurements.</s> <s>this has applications in quantum state tomography, and is a non-commutative analogue of a well-known problem in compressed sensing:  recovering a sparse vector from a few of its fourier coefficients.</s> <s>we show that almost all sets of o(rd log^6 d) pauli measurements satisfy the rank-r restricted isometry property (rip).</s> <s>this implies that m can be recovered from a fixed (\"universal\") set of pauli measurements, using nuclear-norm minimization (e.g., the matrix lasso), with nearly-optimal bounds on the error.</s> <s>a similar result holds for any class of measurements that use an orthonormal operator basis whose elements have small operator norm.</s> <s>our proof uses dudley's inequality for gaussian processes, together with bounds on covering numbers obtained via entropy duality.</s></p></d>", "label": ["<d><p><s>universal low-rank matrix recovery from pauli measurements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a multi-armed bandit problem where there are two phases.</s> <s>the first phase is an experimentation phase where the decision maker is free to explore multiple options.</s> <s>in the second phase the decision maker has to commit to one of the arms and stick with it.</s> <s>cost is incurred during both phases with a higher cost during the experimentation phase.</s> <s>we analyze the regret in this setup, and both propose algorithms and provide upper and lower bounds that depend on the ratio of the duration of the experimentation phase to the duration of the commitment phase.</s> <s>our analysis reveals that if given the choice, it is optimal to experiment $\\theta(\\ln t)$ steps and then commit, where $t$ is the time horizon.</s></p></d>", "label": ["<d><p><s>committing bandits</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images.</s> <s>we build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks.</s> <s>we introduce three principal contributions.</s> <s>first, we replace the standard quantized local hog/sift features with stronger discriminatively trained body part and object detectors.</s> <s>second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects.</s> <s>third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (svm) with a sparsity inducing regularizer.</s> <s>learning of action-specific body part and object interactions bypasses the difficult problem of estimating the complete human body pose configuration.</s> <s>benefits of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline.</s></p></d>", "label": ["<d><p><s>learning person-object interactions for action recognition in still images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of stratified sampling for monte-carlo integration.</s> <s>we model this problem in a multi-armed bandit setting, where the arms represent the strata, and the goal is to estimate a weighted average of the mean values of the arms.</s> <s>we propose a strategy that samples the arms according to an upper bound on their standard deviations and compare its estimation quality to an ideal allocation that would know the standard deviations of the arms.</s> <s>we provide two regret analyses: a distribution-dependent bound o(n^{-3/2}) that depends on a measure of the disparity of the arms, and a distribution-free bound o(n^{-4/3}) that does not.</s> <s>to the best of our knowledge, such a finite-time analysis is new for this problem.</s></p></d>", "label": ["<d><p><s>finite time analysis of stratified sampling for monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inexpensive rgb-d cameras that give an rgb image together with depth data have become widely available.</s> <s>in this paper, we use this data to build 3d point clouds of full indoor scenes such as an office and address the task of semantic labeling of these 3d point clouds.</s> <s>we propose a graphical model that captures various features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships.</s> <s>with a large number of object classes and relations, the model?s parsimony becomes important and we address that by using multiple types of edge potentials.</s> <s>the model admits efficient approximate inference, and we train it using a maximum-margin learning approach.</s> <s>in our experiments over a total of 52 3d scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06% in labeling 17 object classes for offices, and 73.38% in labeling 17 object classes for home scenes.</s> <s>finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms.</s></p></d>", "label": ["<d><p><s>semantic labeling of 3d point clouds for indoor scenes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits).</s> <s>the scaling of our regret bound with the number of states (contexts) $n$ goes as $\\sqrt{n i_{\\rho_t}(s;a)}$, where $i_{\\rho_t}(s;a)$ is the mutual information between states and actions (the side information) used by the algorithm at round $t$.</s> <s>if the algorithm uses all the side information, the regret bound scales as $\\sqrt{n \\ln k}$, where $k$ is the number of actions (arms).</s> <s>however, if the side information $i_{\\rho_t}(s;a)$ is not fully used, the regret bound is significantly tighter.</s> <s>in the extreme case, when $i_{\\rho_t}(s;a) = 0$, the dependence on the number of states reduces from linear to logarithmic.</s> <s>our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto.</s> <s>we also present an algorithm for multiarmed bandits with side information with computational complexity that is a linear in the number of actions.</s></p></d>", "label": ["<d><p><s>pac-bayesian analysis of contextual bandits</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a joint image segmentation and labeling model (jsl) which, given a bag of figure-ground segment hypotheses extracted at multiple image locations and scales, constructs a joint probability distribution over both the compatible image interpretations (tilings or image segmentations) composed from those segments, and over their labeling into categories.</s> <s>the process of drawing samples from the joint distribution can be interpreted as first sampling tilings, modeled as maximal cliques, from a graph connecting spatially non-overlapping segments in the bag, followed by sampling labels for those segments, conditioned on the choice of a particular tiling.</s> <s>we learn the segmentation and labeling parameters jointly, based on maximum likelihood with a novel incremental saddle point estimation procedure.</s> <s>the partition function over tilings and labelings is increasingly more accurately approximated by including incorrect configurations that a not-yet-competent model rates probable during learning.</s> <s>we show that the proposed methodology matches the current state of the art in the stanford dataset, as well as in voc2010, where 41.7% accuracy on the test set is achieved.</s></p></d>", "label": ["<d><p><s>probabilistic joint image segmentation and labeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the 'precis' problem of sampling k representative yet diverse data points from a large dataset.</s> <s>this problem arises frequently in applications such as video and document summarization, exploratory data analysis, and pre-filtering.</s> <s>we formulate a general theory which encompasses not just traditional techniques devised for vector spaces, but also non-euclidean manifolds, thereby enabling these techniques to shapes, human activities, textures and many other image and video based datasets.</s> <s>we propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power, and their diversity.</s> <s>we then propose efficient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm.</s> <s>the proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples.</s> <s>experimental results show the strength and generality of the proposed approach.</s></p></d>", "label": ["<d><p><s>manifold precis: an annealing technique for diverse sampling of manifolds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent variable models are frequently used to identify structure in dichotomous network data, in part because they give rise to a bernoulli product likelihood that is both well understood and consistent with the notion of exchangeable random graphs.</s> <s>in this article we propose conservative confidence sets that hold with respect to these underlying bernoulli parameters as a function of any given partition of network nodes, enabling us to assess estimates of \\emph{residual} network structure, that is, structure that cannot be explained by known covariates and thus cannot be easily verified by manual inspection.</s> <s>we demonstrate the proposed methodology by analyzing student friendship networks from the national longitudinal survey of adolescent health that include race, gender, and school year as covariates.</s> <s>we employ a stochastic expectation-maximization algorithm to fit a logistic regression model that includes these explanatory variables as well as a latent stochastic blockmodel component and additional node-specific effects.</s> <s>although maximum-likelihood estimates do not appear consistent in this context, we are able to evaluate confidence sets as a function of different blockmodel partitions, which enables us to qualitatively assess the significance of estimated residual network structure relative to a baseline, which models covariates but lacks block structure.</s></p></d>", "label": ["<d><p><s>confidence sets for network structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>non-negative data are commonly encountered in numerous fields, making non-negative least squares regression (nnls) a frequently used tool.</s> <s>at least relative to its simplicity, it often performs rather well in practice.</s> <s>serious doubts about its usefulness arise for modern  high-dimensional linear models.</s> <s>even in this setting - unlike first intuition may suggest - we show that for a broad class of designs, nnls is resistant to overfitting and works excellently for sparse recovery when combined with thresholding, experimentally even outperforming l1-regularization.</s> <s>since nnls also circumvents the delicate choice of a regularization parameter, our findings suggest that nnls may be the method of choice.</s></p></d>", "label": ["<d><p><s>sparse recovery by thresholded non-negative least squares</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the computational complexity of probabilistic inference in latent dirichlet allocation (lda).</s> <s>first, we study the problem of finding the maximum a posteriori (map) assignment of topics to words, where the document's topic distribution is integrated out.</s> <s>we show that, when the effective number of topics per document is small, exact inference takes polynomial time.</s> <s>in contrast, we show that, when a document has a large number of topics, finding the map assignment of topics to words in lda is np-hard.</s> <s>next, we consider the problem of finding the map topic distribution for a document, where the topic-word assignments are integrated out.</s> <s>we show that this problem is also np-hard.</s> <s>finally, we briefly discuss the problem of sampling from the posterior, showing that this is np-hard in one restricted setting, but leaving open the general question.</s></p></d>", "label": ["<d><p><s>complexity of inference in latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel active learning framework for video annotation.</s> <s>by  judiciously choosing which frames a user should annotate, we can obtain highly  accurate tracks with minimal user effort.</s> <s>we cast this problem as one of  active learning, and show that we can obtain excellent performance by querying  frames that, if annotated, would produce a large expected change in the  estimated object track.</s> <s>we implement a constrained tracker and compute the  expected change for putative annotations with efficient dynamic programming  algorithms.</s> <s>we demonstrate our framework on four datasets, including two  benchmark datasets constructed with key frame annotations obtained by amazon  mechanical turk.</s> <s>our results indicate that we could obtain equivalent labels  for a small fraction of the original cost.</s></p></d>", "label": ["<d><p><s>video annotation and tracking with active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive here new generalization bounds, based on rademacher complexity theory, for model selection and error estimation   of linear (kernel) classifiers, which exploit  the availability of unlabeled samples.</s> <s>in particular, two results are obtained: the first one shows that, using the unlabeled samples, the  confidence term of the conventional bound can be reduced by a factor of three; the second one  shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions  of the hypothesis class containing the optimal classifier.</s></p></d>", "label": ["<d><p><s>the impact of unlabeled patterns in rademacher complexity theory for kernel classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we consider general rank minimization problems with rank appearing in either objective function or constraint.</s> <s>we first show that a class of matrix optimization problems can be solved as lower dimensional vector optimization problems.</s> <s>as a consequence, we establish that a class of rank minimization problems have closed form solutions.</s> <s>using this result, we then propose penalty decomposition methods for general rank minimization problems.</s> <s>the convergence results of the pd methods have been shown in the longer version of the paper.</s> <s>finally, we test the performance of our methods by applying them to matrix completion and nearest low-rank correlation matrix problems.</s> <s>the computational results demonstrate that our methods generally outperform the existing methods in terms of solution quality and/or speed.</s></p></d>", "label": ["<d><p><s>penalty decomposition methods for rank minimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper proposes a parsing algorithm for scene understanding which includes four aspects: computing 3d scene layout, detecting 3d objects (e.g.</s> <s>furniture), detecting 2d faces (windows, doors etc.</s> <s>), and segmenting background.</s> <s>in contrast to previous scene labeling work that applied discriminative classifiers to pixels (or super-pixels), we use a generative stochastic scene grammar (ssg).</s> <s>this grammar represents the compositional structures of visual entities from scene categories, 3d foreground/background, 2d faces, to 1d lines.</s> <s>the grammar includes three types of production rules and two types of contextual relations.</s> <s>production rules: (i) and rules represent the decomposition of an entity into sub-parts; (ii) or rules represent the switching among sub-types of an entity; (iii) set rules rep- resent an ensemble of visual entities.</s> <s>contextual relations: (i) cooperative ?+?</s> <s>relations represent positive links between binding entities, such as hinged faces of a object or aligned boxes; (ii) competitive ?-?</s> <s>relations represents negative links between competing entities, such as mutually exclusive boxes.</s> <s>we design an efficient mcmc inference algorithm, namely hierarchical cluster sampling, to search in the large solution space of scene configurations.</s> <s>the algorithm has two stages: (i) clustering: it forms all possible higher-level structures (clusters) from lower-level entities by production rules and contextual relations.</s> <s>(ii) sampling: it jumps between alternative structures (clusters) in each layer of the hierarchy to find the most probable configuration (represented by a parse tree).</s> <s>in our experiment, we demonstrate the superiority of our algorithm over existing methods on public dataset.</s> <s>in addition, our approach achieves richer structures in the parse tree.</s></p></d>", "label": ["<d><p><s>image parsing with stochastic scene grammar</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>traditional approaches to probabilistic inference such as loopy belief propagation and gibbs sampling typically compute marginals for it all the unobserved variables in a graphical model.</s> <s>however, in many real-world applications the user's interests are focused on a subset of the variables, specified by a query.</s> <s>in this case it would be wasteful to uniformly sample, say, one million variables when the query concerns only ten.</s> <s>in this paper we propose a query-specific approach to mcmc that accounts for the query variables and their generalized mutual information with neighboring variables in order to achieve higher computational efficiency.</s> <s>surprisingly there has been almost no previous work on query-aware mcmc.</s> <s>we demonstrate the success of our approach with positive experimental results on a wide range of graphical models.</s></p></d>", "label": ["<d><p><s>query-aware mcmc</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>skill discovery algorithms in reinforcement learning typically identify single states or regions in state space that correspond to task-specific subgoals.</s> <s>however, such methods do not directly address the question of how many distinct skills are appropriate for solving the tasks that the agent faces.</s> <s>this can be highly inefficient when many identified subgoals correspond to the same underlying skill, but are all used individually as skill goals.</s> <s>furthermore, skills created in this manner are often only transferable to tasks that share identical state spaces, since corresponding subgoals across tasks are not merged into a single skill goal.</s> <s>we show that these problems can be overcome by clustering subgoal data defined in an agent-space and using the resulting clusters as templates for skill termination conditions.</s> <s>clustering via a dirichlet process mixture model is used to discover a minimal, sufficient collection of portable skills.</s></p></d>", "label": ["<d><p><s>clustering via dirichlet process mixture models for portable skill discovery</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we present an algorithm to learn a multi-label classifier which attempts at directly optimising the f-score.</s> <s>the key novelty of our formulation is that we explicitly allow for assortative (submodular) pairwise label interactions, i.e., we can leverage the co-ocurrence of pairs of labels in order to improve the quality of prediction.</s> <s>prediction in this model consists of minimising a particular submodular set function, what can be accomplished exactly and efficiently via graph-cuts.</s> <s>learning however is substantially more involved and requires the solution of an intractable combinatorial optimisation problem.</s> <s>we present an approximate algorithm for this problem and prove that it is sound in the sense that it never predicts incorrect labels.</s> <s>we also present a nontrivial test of a sufficient condition for our algorithm to have found an optimal solution.</s> <s>we present experiments on benchmark multi-label datasets, which attest the value of our proposed technique.</s> <s>we also make available source code that enables the reproduction of our experiments.</s></p></d>", "label": ["<d><p><s>submodular multi-label learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present theoretical and empirical results for a framework  that combines the benefits of apprenticeship and autonomous reinforcement learning.</s> <s>our approach modifies an existing apprenticeship learning framework that relies on teacher demonstrations and does not necessarily explore the environment.</s> <s>the first change is replacing previously used mistake bound model learners with a recently proposed framework that melds the  kwik and mistake bound supervised learning protocols.</s> <s>the second change is introducing a communication of expected utility from the student to the teacher.</s> <s>the resulting system only uses teacher traces when the agent needs to learn concepts it cannot efficiently learn on its own.</s></p></d>", "label": ["<d><p><s>blending autonomous exploration and apprenticeship learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-class gaussian process classifiers (mgpcs) are often affected by over-fitting problems when labeling errors occur far from the decision boundaries.</s> <s>to prevent this, we investigate a robust mgpc (rmgpc) which considers labeling errors independently of their distance to the decision boundaries.</s> <s>expectation propagation is used for approximate inference.</s> <s>experiments with several datasets in which noise is injected in the class labels illustrate the benefits of rmgpc.</s> <s>this method performs better than other gaussian process alternatives based on considering latent gaussian noise or heavy-tailed processes.</s> <s>when no noise is injected in the labels, rmgpc still performs equal or better than the other methods.</s> <s>finally, we show how rmgpc can be used for successfully identifying data instances which are difficult to classify accurately in practice.</s></p></d>", "label": ["<d><p><s>robust multi-class gaussian process classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new sparse bayesian model for multi-task regression and classification.</s> <s>the model is able to capture correlations between tasks, or more specifically a low-rank approximation of the covariance matrix, while being sparse in the features.</s> <s>we introduce a general family of group sparsity inducing priors based on matrix-variate gaussian scale mixtures.</s> <s>we show the amount of sparsity can be learnt from the data by combining an approximate inference approach with type ii maximum likelihood estimation of the hyperparameters.</s> <s>empirical evaluations on data sets from biology and vision demonstrate the applicability of the model, where on both regression and classification tasks it achieves competitive predictive performance compared to previously proposed methods.</s></p></d>", "label": ["<d><p><s>sparse bayesian multi-task learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there is much evidence that humans and other animals utilize a combination of model-based and model-free rl methods.</s> <s>although it has been proposed that these systems may dominate according to their relative statistical efficiency in different circumstances, there is little specific evidence -- especially in humans -- as to the details of this trade-off.</s> <s>accordingly, we examine the relative performance of different rl approaches under situations in which the statistics of reward are differentially noisy and volatile.</s> <s>using theory and simulation, we show that model-free td learning is relatively most disadvantaged in cases of high volatility and low noise.</s> <s>we present data from a decision-making experiment manipulating these parameters, showing that humans shift learning strategies in accord with these predictions.</s> <s>the statistical circumstances favoring model-based rl are also those that promote a high learning rate, which helps explain why, in psychology, the distinction between these strategies is traditionally conceived in terms of rule-based vs. incremental learning.</s></p></d>", "label": ["<d><p><s>environmental statistics and the trade-off between model-based and td learning in humans</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>off-policy learning, the ability for an agent to learn about a policy other than the one it is following, is a key element of reinforcement learning, and in recent years there has been much work on developing temporal different (td) algorithms that are guaranteed to converge under off-policy sampling.</s> <s>it has remained an open question, however, whether anything can be said a priori about the quality of the td solution when off-policy sampling is employed with function approximation.</s> <s>in general the answer is no: for arbitrary off-policy sampling the error of the td solution can be unboundedly large, even when the approximator can represent the true value function well.</s> <s>in this paper we propose a novel approach to address this problem: we show that by considering a certain convex subset of off-policy distributions we can indeed provide guarantees as to the solution quality similar to the on-policy case.</s> <s>furthermore, we show that we can efficiently project on to this convex set using only samples generated from the system.</s> <s>the end result is a novel td algorithm that has approximation guarantees even in the case of off-policy sampling and which empirically outperforms existing td methods.</s></p></d>", "label": ["<d><p><s>the fixed points of off-policy td</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an efficient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting.</s> <s>we measure its regret with respect to the log-loss defined in \\cite{abernethyr09}, which is parameterized by a scalar \\(\\alpha\\).</s> <s>we prove that the regret of \\newtron is \\(o(\\log t)\\) when \\(\\alpha\\) is a constant that does not vary with horizon \\(t\\), and at most \\(o(t^{2/3})\\) if \\(\\alpha\\) is allowed to increase to infinity with \\(t\\).</s> <s>for \\(\\alpha\\) = \\(o(\\log t)\\), the regret is bounded by \\(o(\\sqrt{t})\\), thus solving the open problem of \\cite{kst08, abernethyr09}.</s> <s>our algorithm is based on a novel application of the online newton method \\cite{hak07}.</s> <s>we test our algorithm and show it to perform well in experiments, even when \\(\\alpha\\) is a small constant.</s></p></d>", "label": ["<d><p><s>newtron: an efficient bandit algorithm for online multiclass prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an algorithm called sparse manifold clustering and embedding (smce) for simultaneous clustering and dimensionality reduction of data lying in multiple nonlinear manifolds.</s> <s>similar to most dimensionality reduction methods, smce finds a small neighborhood around each data point and connects each point to its neighbors with appropriate weights.</s> <s>the key difference is that smce finds both the neighbors and the weights automatically.</s> <s>this is done by solving a sparse optimization problem, which encourages selecting nearby points that lie in the same manifold and approximately span a low-dimensional affine subspace.</s> <s>the optimal solution encodes information that can be used for clustering and dimensionality reduction using spectral clustering and embedding.</s> <s>moreover, the size of the optimal neighborhood of a data point, which can be different for different points, provides an estimate of the dimension of the manifold to which the point belongs.</s> <s>experiments demonstrate that our method can effectively handle multiple manifolds that are very close to each other, manifolds with non-uniform sampling and holes, as well as estimate the intrinsic dimensions of the manifolds.</s></p></d>", "label": ["<d><p><s>sparse manifold clustering and embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the convergence of gradient-based optimization algorithms whose updates depend on delayed stochastic gradient information.</s> <s>the  main application of our results is to the development of distributed  minimization algorithms where a master node performs parameter updates while worker nodes compute stochastic gradients based on local information in parallel, which may give rise to delays due to  asynchrony.</s> <s>our main contribution is to show that for smooth stochastic problems, the delays are asymptotically negligible.</s> <s>in  application to distributed optimization, we show $n$-node architectures whose optimization error in stochastic problems---in spite of asynchronous delays---scales asymptotically as $\\order(1 / \\sqrt{nt})$, which is known to be optimal even in the absence of delays.</s></p></d>", "label": ["<d><p><s>distributed delayed stochastic optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider loss functions for multiclass prediction problems.</s> <s>we   show when a  multiclass loss can be expressed as a ``proper   composite loss'', which is the composition of a proper loss and a link   function.</s> <s>we extend existing results for binary losses to   multiclass losses.</s> <s>we determine the stationarity condition,   bregman representation, order-sensitivity, existence and uniqueness   of the composite representation for multiclass losses.</s> <s>we also   show that the integral representation  for binary proper losses can   not be extended to  multiclass losses.</s> <s>we subsume existing results   on ``classification calibration'' by relating it to properness.</s> <s>we   draw conclusions concerning the design of multiclass losses.</s></p></d>", "label": ["<d><p><s>composite multiclass losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an agglomerative clustering algorithm merges the most similar pair of clusters at every iteration.</s> <s>the function that evaluates similarity is traditionally hand- designed, but there has been recent interest in supervised or semisupervised settings in which ground-truth clustered data is available for training.</s> <s>here we show how to train a similarity function by regarding it as the action-value function of a reinforcement learning problem.</s> <s>we apply this general method to segment images by clustering superpixels, an application that we call learning to agglomerate superpixel hierarchies (lash).</s> <s>when applied to a challenging dataset of brain images from serial electron microscopy, lash dramatically improved segmentation accuracy when clustering supervoxels generated by state of the boundary detection algorithms.</s> <s>the naive strategy of directly training only supervoxel similarities and applying single linkage clustering produced less improvement.</s></p></d>", "label": ["<d><p><s>learning to agglomerate superpixel hierarchies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce an approach to learn discriminative visual representations while exploiting external semantic knowledge about object category relationships.</s> <s>given a hierarchical taxonomy that captures semantic similarity between the objects, we learn a corresponding tree of metrics (tom).</s> <s>in this tree, we have one metric for each non-leaf node of the object hierarchy, and each metric is responsible for discriminating among its immediate subcategory children.</s> <s>specifically, a mahalanobis metric learned for a given node must satisfy the appropriate (dis)similarity constraints generated only among its subtree members' training instances.</s> <s>to further exploit the semantics, we introduce a novel regularizer coupling the metrics that prefers a sparse disjoint set of features to be selected for each metric relative to its ancestor supercategory nodes' metrics.</s> <s>intuitively, this reflects that visual cues most useful to distinguish the generic classes (e.g., feline vs. canine) should be different than those cues most useful to distinguish their component fine-grained classes (e.g., persian cat vs. siamese cat).</s> <s>we validate our approach with multiple image datasets using the wordnet taxonomy, show its advantages over alternative metric learning approaches, and analyze the meaning of attribute features selected by our algorithm.</s></p></d>", "label": ["<d><p><s>learning a tree of metrics with disjoint visual features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new convergent variant of q-learning, called speedy q-learning, to address the problem of slow convergence in the standard form of the q-learning algorithm.</s> <s>we prove a pac bound on the performance of sql, which shows that for an mdp with n state-action pairs  and the discount factor \\gamma only t=o\\big(\\log(n)/(\\epsilon^{2}(1-\\gamma)^{4})\\big) steps are required for the sql algorithm to converge to an \\epsilon-optimal action-value function with high probability.</s> <s>this bound has a better dependency on 1/\\epsilon and 1/(1-\\gamma), and thus, is tighter than the best available result for q-learning.</s> <s>our bound is also superior to the existing results for both model-free and model-based instances of batch q-value iteration that are considered to be more efficient than the incremental methods like q-learning.</s></p></d>", "label": ["<d><p><s>speedy q-learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose the first exact algorithm for minimizing the difference of two submodular functions (d.s.</s> <s>), i.e., the discrete version of the d.c. programming problem.</s> <s>the developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity.</s> <s>the d.s.</s> <s>programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions.</s> <s>we empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.</s></p></d>", "label": ["<d><p><s>prismatic algorithm for discrete d.c. programming problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while signal estimation under random amplitudes, phase shifts, and additive noise is studied frequently, the problem of estimating a deterministic signal under random time-warpings has been relatively unexplored.</s> <s>we present a novel framework for estimating the unknown signal that utilizes the action of the warping group to form an equivalence relation between signals.</s> <s>first, we derive an estimator for the equivalence class of the unknown signal using the notion of karcher mean on the quotient space of equivalence classes.</s> <s>this step requires the use of fisher-rao riemannian metric  and a square-root representation of signals to enable computations of distances and means under this metric.</s> <s>then, we define a notion of the center of a class and show that the center of the estimated class is a consistent estimator of the underlying unknown signal.</s> <s>this estimation algorithm has many applications: (1)registration/alignment of functional data, (2) separation of phase/amplitude components of functional data, (3) joint demodulation and carrier estimation, and (4) sparse modeling of functional data.</s> <s>here we demonstrate only (1) and (2):  given signals are temporally aligned using nonlinear warpings and, thus, separated into their phase and amplitude components.</s> <s>the proposed method for signal alignment is shown to have state of the art performance using berkeley growth, handwritten signatures, and neuroscience spike train data.</s></p></d>", "label": ["<d><p><s>signal estimation under random time-warpings and nonlinear signal alignment</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection, transfer learning, and two-sample homogeneity test.</s> <s>however, since density-ratio functions often possess high fluctuation, divergence estimation is still a challenging task in practice.</s> <s>in this paper, we propose to use relative divergences for distribution comparison, which involves approximation of relative density-ratios.</s> <s>since relative density-ratios are always smoother than corresponding ordinary density-ratios, our proposed method is favorable in terms of the non-parametric convergence speed.</s> <s>furthermore, we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup, implying that the proposed estimator hardly overfits even with complex models.</s> <s>through experiments, we demonstrate the usefulness of the proposed approach.</s></p></d>", "label": ["<d><p><s>relative density-ratio estimation for robust distribution comparison</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new le ?vy process prior is proposed for an uncountable collection of covariate- dependent feature-learning measures; the model is called the kernel beta process (kbp).</s> <s>available covariates are handled efficiently via the kernel construction, with covariates assumed observed with each data sample (?customer?</s> <s>), and latent covariates learned for each feature (?dish?).</s> <s>each customer selects dishes from an infinite buffet, in a manner analogous to the beta process, with the added constraint that a customer first decides probabilistically whether to ?consider?</s> <s>a dish, based on the distance in covariate space between the customer and dish.</s> <s>if a customer does consider a particular dish, that dish is then selected probabilistically as in the beta process.</s> <s>the beta process is recovered as a limiting case of the kbp.</s> <s>an efficient gibbs sampler is developed for computations, and state-of-the-art results are presented for image processing and music analysis tasks.</s></p></d>", "label": ["<d><p><s>the kernel beta process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the challenging task of decoupling material properties from lighting properties given a single image.</s> <s>in the last two decades virtually all works have concentrated on exploiting edge information to address this problem.</s> <s>we take a different route by introducing a new prior on reflectance, that models reflectance values as being drawn from a sparse set of basis colors.</s> <s>this results in a random field model with global, latent variables (basis colors) and pixel-accurate output reflectance values.</s> <s>we show that without edge information high-quality results can be achieved, that are on par with methods exploiting this source of information.</s> <s>finally, we present competitive results by integrating an additional edge model.</s> <s>we believe that our approach is a solid starting point for future development in this domain.</s></p></d>", "label": ["<d><p><s>recovering intrinsic images with a global sparsity prior on reflectance</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>simultaneous recordings of many neurons embedded within a  recurrently-connected cortical network may provide concurrent views into the dynamical processes of that network, and thus its computational function.</s> <s>in principle, these dynamics might be identified by purely unsupervised, statistical means.</s> <s>here, we show that a hidden switching linear dynamical systems (hslds) model---in which multiple linear dynamical laws approximate a nonlinear and potentially non-stationary dynamical process---is able to distinguish different dynamical regimes within single-trial motor cortical activity associated with the preparation and initiation of hand movements.</s> <s>the regimes are identified without reference to behavioural or experimental epochs, but nonetheless transitions between them correlate strongly with external events whose timing may vary from trial to trial.</s> <s>the hslds model also performs better than recent comparable models in predicting the firing rate of an isolated neuron based on the firing rates of others, suggesting that it  captures more of the \"shared variance\" of the data.</s> <s>thus, the method is able to trace the dynamical processes underlying the coordinated evolution of network activity in a way that appears to reflect its computational role.</s></p></d>", "label": ["<d><p><s>dynamical segmentation of single trials from population neural data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>topic models are learned via a statistical model of variation within document collections, but designed to extract meaningful semantic structure.</s> <s>desirable traits include the ability to incorporate annotations or metadata associated with documents; the discovery of correlated patterns of topic usage; and the avoidance of parametric assumptions, such as manual specification of the number of topics.</s> <s>we propose a doubly correlated nonparametric topic (dcnt) model, the first model to simultaneously capture all three of these properties.</s> <s>the dcnt models metadata via a flexible, gaussian regression on arbitrary input features; correlations via a scalable square-root covariance representation; and nonparametric selection from an unbounded series of potential topics via a stick-breaking construction.</s> <s>we validate the semantic structure and predictive performance of the dcnt using a corpus of nips documents annotated by various metadata.</s></p></d>", "label": ["<d><p><s>the doubly correlated nonparametric topic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive an upper bound on the local rademacher complexity of lp-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches.</s> <s>previous local approaches analyzed the case p=1 only while our analysis covers all cases $1\\leq p\\leq\\infty$, assuming the different feature mappings corresponding to the different kernels to be uncorrelated.</s> <s>we also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates of the order $o(n^{-\\frac{\\alpha}{1+\\alpha}})$, where $\\alpha$ is the minimum eigenvalue decay rate of the individual kernels.</s></p></d>", "label": ["<d><p><s>the local rademacher complexity of lp-norm multiple kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. our contribution is a new test statistic for the two-sample test of means that integrates a random projection with the classical hotelling t squared statistic.</s> <s>working within a high- dimensional framework that allows (p,n) to tend to infinity, we first derive an asymptotic power function for our test, and then provide sufficient conditions for it to achieve greater power than other  state-of-the-art tests.</s> <s>using roc curves generated from simulated data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results.</s> <s>lastly, we illustrate an advantage of our procedure with comparisons on a high-dimensional gene expression dataset involving the discrimination of different types of cancer.</s></p></d>", "label": ["<d><p><s>a more powerful two-sample test in high dimensions using random projection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>spectral clustering is based on the spectral relaxation of the normalized/ratio graph cut criterion.</s> <s>while the  spectral relaxation is known to be loose, it has been shown recently that a non-linear  eigenproblem yields a tight relaxation of the cheeger cut.</s> <s>in this paper, we extend this result considerably by providing a   characterization of all balanced graph cuts which allow for a tight relaxation.</s> <s>although the resulting optimization problems are non-convex and non-smooth, we provide   an efficient first-order scheme which scales to large graphs.</s> <s>moreover, our approach  comes with the quality guarantee that given any partition as initialization the   algorithm either outputs a better partition or it stops immediately.</s></p></d>", "label": ["<d><p><s>beyond spectral clustering - tight relaxations of balanced graph cuts</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d.</s> <s>from a fixed distribution, and the adversarial scenario whereby at every time step the worst instance is revealed to the player.</s> <s>it can be argued that in the real world neither of these assumptions is reasonable.</s> <s>we define the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data.</s> <s>building on the sequential symmetrization approach, we define a notion of distribution-dependent rademacher complexity for the spectrum of problems ranging from i.i.d.</s> <s>to worst-case.</s> <s>the bounds let us immediately deduce variation-type bounds.</s> <s>we study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite littlestone dimension learnable.</s></p></d>", "label": ["<d><p><s>online learning: stochastic, constrained, and smoothed adversaries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data.</s> <s>here we extend the indian buffet process (ibp), a nonparametric bayesian model, to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks.</s> <s>we present an application of this method to study how micrornas regulate mrnas in cells.</s> <s>analysis of synthetic and real data indicates that the method improves upon prior methods, correctly recovers interactions and clusters, and provides accurate biological predictions.</s></p></d>", "label": ["<d><p><s>inferring interaction networks using the ibp applied to microrna target prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>policy gradient is a useful model-free reinforcement learning approach,  but it tends to suffer from instability of gradient estimates.</s> <s>in this paper, we analyze and improve the stability of policy gradient methods.</s> <s>we first prove that the variance of gradient estimates in the pgpe(policy gradients with parameter-based exploration) method is smaller than that of the classical reinforce method under a mild assumption.</s> <s>we then derive the optimal baseline for pgpe, which contributes to further reducing the variance.</s> <s>we also theoretically show that pgpe with the optimal baseline is more preferable than reinforce with the optimal baseline in terms of the variance of gradient estimates.</s> <s>finally, we demonstrate the usefulness of the improved pgpe method through experiments.</s></p></d>", "label": ["<d><p><s>analysis and improvement of policy gradient estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider regularized risk minimization in a large dictionary of reproducing  kernel hilbert spaces (rkhss) over which the target function has a sparse representation.</s> <s>this setting, commonly referred to as sparse multiple kernel learning  (mkl), may be viewed as the non-parametric extension of group sparsity in linear  models.</s> <s>while the two dominant algorithmic strands of sparse learning, namely  convex relaxations using l1 norm (e.g., lasso) and greedy methods (e.g., omp),  have both been rigorously extended for group sparsity, the sparse mkl literature  has so farmainly adopted the former withmild empirical success.</s> <s>in this paper, we  close this gap by proposing a group-omp based framework for sparse multiple  kernel learning.</s> <s>unlike l1-mkl, our approach decouples the sparsity regularizer  (via a direct l0 constraint) from the smoothness regularizer (via rkhs norms)  which leads to better empirical performance as well as a simpler optimization  procedure that only requires a black-box single-kernel solver.</s> <s>the algorithmic  development and empirical studies are complemented by theoretical analyses in  terms of rademacher generalization bounds and sparse recovery conditions analogous  to those for omp [27] and group-omp [16].</s></p></d>", "label": ["<d><p><s>non-parametric group orthogonal matching pursuit for sparse learning with multiple kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the l_1 regularized gaussian maximum likelihood estimator has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix, or alternatively the underlying graph structure of a gaussian markov random field, from very limited samples.</s> <s>we propose a novel algorithm for solving the resulting optimization problem which is a regularized log-determinant program.</s> <s>in contrast to other state-of-the-art methods that largely use first order gradient information, our algorithm is based on newton's method and employs a quadratic approximation, but with some modifications that leverage the structure of the sparse gaussian mle problem.</s> <s>we show that our method is superlinearly convergent, and also present experimental results using synthetic and real application data that demonstrate the considerable improvements in performance of our method when  compared to other state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>sparse inverse covariance matrix estimation using quadratic approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces two new frameworks for learning action models for planning.</s> <s>in the mistake-bounded planning framework, the learner has access to a planner for the given model representation, a simulator, and a planning problem generator, and aims to learn a model with at most a polynomial number of faulty plans.</s> <s>in the planned exploration framework, the learner does not have access to a problem generator and must instead design its own problems, plan for them, and converge with at most a polynomial number of planning attempts.</s> <s>the paper reduces learning in these frameworks to concept learning with one-sided error and provides algorithms for successful learning in both frameworks.</s> <s>a specific family of hypothesis spaces is shown to be efficiently learnable in both the frameworks.</s></p></d>", "label": ["<d><p><s>autonomous learning of action models for planning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider latent structural versions of probit loss and ramp loss.</s> <s>we show that  these surrogate loss functions are consistent in the strong sense that for any feature map  (finite or infinite dimensional) they yield predictors approaching the infimum  task loss achievable by any linear predictor over the given features.</s> <s>we also give  finite sample generalization bounds (convergence rates) for these loss functions.</s> <s>these bounds suggest that probit loss converges more rapidly.</s> <s>however, ramp loss is more easily optimized and may ultimately  be more practical.</s></p></d>", "label": ["<d><p><s>generalization bounds and consistency for latent structural probit and ramp loss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how do people determine which elements of a set are most representative of that set?</s> <s>we extend an existing bayesian measure of representativeness, which indicates the representativeness of a sample from a distribution, to define a measure of the representativeness of an item to a set.</s> <s>we show that this measure is formally related to a machine learning method known as bayesian sets.</s> <s>building on this connection, we derive an analytic expression for the representativeness of objects described by a sparse vector of binary features.</s> <s>we then apply this measure to a large database of images, using it to determine which images are the most representative members of different sets.</s> <s>comparing the resulting predictions to human judgments of representativeness provides a test of this measure with naturalistic stimuli, and illustrates how databases that are more commonly used in computer vision and machine learning can be used to evaluate psychological theories.</s></p></d>", "label": ["<d><p><s>testing a bayesian measure of representativeness using a large image database</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we discuss new methods for the recovery of signals with block-sparse structure, based on l1-minimization.</s> <s>our emphasis is on the efficiently computable error bounds for the recovery routines.</s> <s>we optimize these bounds with respect to the method parameters to construct the estimators with improved statistical properties.</s> <s>we justify the proposed approach with an oracle inequality which links the properties of the recovery algorithms and the best estimation performance.</s></p></d>", "label": ["<d><p><s>on the accuracy of l1-filtering of signals with block-sparse structure</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this work describes a conceptually simple method for structured sparse coding and dictionary design.</s> <s>supposing a dictionary with k atoms, we introduce a structure as a set of penalties or interactions between every pair of atoms.</s> <s>we describe modifications of standard sparse coding algorithms for inference in this setting, and describe experiments showing that these algorithms are efficient.</s> <s>we show that interesting dictionaries can be learned for interactions that encode tree structures or locally connected structures.</s> <s>finally, we show that our framework allows us to learn the values of the interactions from the data, rather than having them pre-specified.</s></p></d>", "label": ["<d><p><s>structured sparse coding via lateral inhibition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, mahoney and orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph laplacian exactly solve certain regularized semi-definite programs (sdps).</s> <s>in this paper, we extend that result by providing a statistical interpretation of their approximation procedure.</s> <s>our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2 regression (often called ridge regression and lasso regression, respectively) can be interpreted in terms of a gaussian prior or a laplace prior, respectively, on the coefficient vector of the regression problem.</s> <s>our framework will imply that the solutions to the mahoney-orecchia regularized sdp can be interpreted as regularized estimates of the pseudoinverse of the graph laplacian.</s> <s>conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based pagerank procedure for computing an approximation to the first nontrivial eigenvector of the graph laplacian.</s> <s>empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.</s></p></d>", "label": ["<d><p><s>regularized laplacian estimation and fast eigenvector approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we describe a maximum likelihood likelihood approach for dictionary learning in the multiplicative exponential noise model.</s> <s>this model is prevalent in audio signal processing where it underlies a generative composite model of the power spectrogram.</s> <s>maximum joint likelihood estimation of the dictionary and expansion coefficients leads to a nonnegative matrix factorization problem where the itakura-saito divergence is used.</s> <s>the optimality of this approach is in question because the number of parameters (which include the expansion coefficients) grows with the number of observations.</s> <s>in this paper we describe a variational procedure for optimization of the marginal likelihood, i.e., the likelihood of the dictionary where the activation coefficients have been integrated out (given a specific prior).</s> <s>we compare the output of both maximum joint likelihood estimation (i.e., standard itakura-saito nmf) and maximum marginal likelihood estimation (mmle) on real and synthetical datasets.</s> <s>the mmle approach is shown to embed automatic model order selection, akin to automatic relevance determination.</s></p></d>", "label": ["<d><p><s>nonnegative dictionary learning in the exponential noise model for adaptive music signal representation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a majority of approximate dynamic programming approaches to the reinforcement learning problem can be categorized into greedy value function methods and value-based policy gradient methods.</s> <s>the former approach, although fast, is well known to be susceptible to the policy oscillation phenomenon.</s> <s>we take a fresh view to this phenomenon by casting a considerable subset of the former approach as a limiting special case of the latter.</s> <s>we explain the phenomenon in terms of this view and illustrate the underlying mechanism with artificial examples.</s> <s>we also use it to derive the constrained natural actor-critic algorithm that can interpolate between the aforementioned approaches.</s> <s>in addition, it has been suggested in the literature that the oscillation phenomenon might be subtly connected to the grossly suboptimal performance in the tetris benchmark problem of all attempted approximate dynamic programming methods.</s> <s>we report empirical evidence against such a connection and in favor of an alternative explanation.</s> <s>finally, we report scores in the tetris problem that improve on existing dynamic programming based results.</s></p></d>", "label": ["<d><p><s>a reinterpretation of the policy oscillation phenomenon in approximate policy iteration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the group lasso is an extension of the lasso for feature selection on (predefined) non-overlapping groups of features.</s> <s>the non-overlapping group structure limits its applicability in practice.</s> <s>there have been several recent attempts to study a more general formulation, where groups of features are given, potentially with overlaps between the groups.</s> <s>the resulting optimization is, however, much more challenging to solve due to the group overlaps.</s> <s>in this paper, we consider the efficient optimization of the overlapping group lasso penalized problem.</s> <s>we reveal several key properties of the proximal operator associated with the overlapping group lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization.</s> <s>we have performed empirical evaluations using both synthetic and the breast cancer gene expression data set, which consists of 8,141 genes organized into (overlapping) gene sets.</s> <s>experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>efficient methods for overlapping group lasso</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motor prostheses aim to restore function to disabled patients.</s> <s>despite compelling proof of concept systems, barriers to clinical translation remain.</s> <s>one challenge is to develop a low-power, fully-implantable system that dissipates only minimal power so as not to damage tissue.</s> <s>to this end, we implemented a kalman-filter based decoder via a spiking neural network (snn) and tested it in brain-machine interface (bmi) experiments with a rhesus monkey.</s> <s>the kalman filter was trained to predict the arm?s velocity and mapped on to the snn using the neural engineer- ing framework (nef).</s> <s>a 2,000-neuron embedded matlab snn implementation runs in real-time and its closed-loop performance is quite comparable to that of the standard kalman filter.</s> <s>the success of this closed-loop decoder holds promise for hardware snn implementations of statistical signal processing algorithms on neuromorphic chips, which may offer power savings necessary to overcome a major obstacle to the successful clinical translation of neural motor prostheses.</s></p></d>", "label": ["<d><p><s>a brain-machine interface operating with a real-time spiking neural network control algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>using the $\\ell_1$-norm to regularize the estimation of  the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated.</s> <s>in this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation.</s> <s>this norm, called the trace lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity.</s> <s>we analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net.</s></p></d>", "label": ["<d><p><s>trace lasso: a trace norm regularization for correlated designs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of this paper is to investigate the advantages and disadvantages of learning in banach spaces over hilbert spaces.</s> <s>while many works have been carried out in generalizing hilbert methods to banach spaces, in this paper, we consider the simple problem of learning a parzen window classifier in a reproducing kernel banach space (rkbs)---which is closely related to the notion of embedding probability measures into an rkbs---in order to carefully understand its pros and cons over the hilbert space classifier.</s> <s>we show that while this generalization yields richer distance measures on probabilities compared to its hilbert space counterpart, it however suffers from serious computational drawback limiting its practical applicability, which therefore demonstrates the need for developing efficient learning algorithms in banach spaces.</s></p></d>", "label": ["<d><p><s>learning in hilbert vs. banach spaces: a measure embedding viewpoint</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show an application of a tree structure for approximate inference in graphical models using the expectation propagation algorithm.</s> <s>these approximations are typically used over graphs with short-range cycles.</s> <s>we demonstrate that these approximations also help in sparse graphs with long-range loops, as the ones used in coding theory to approach channel capacity.</s> <s>for asymptotically large sparse graph, the expectation propagation algorithm together with the tree structure yields a completely disconnected approximation to the graphical model but, for for finite-length practical sparse graphs, the tree structure approximation to the code graph provides accurate estimates for the marginal of each variable.</s></p></d>", "label": ["<d><p><s>an application of tree-structured expectation propagation for channel decoding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper considers the problem of embedding directed graphs in euclidean space while retaining directional information.</s> <s>we model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field.</s> <s>the algorithm is motivated by our analysis of laplacian-type operators and their continuous limit as generators of diffusions on a manifold.</s> <s>we illustrate the recovery algorithm on both artificially constructed and real data.</s></p></d>", "label": ["<d><p><s>directed graph embedding: an algorithm based on continuous limits of laplacian-type operators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>approximate inference is an important technique for dealing with large, intractable graphical models based on the exponential family of distributions.</s> <s>we extend the idea of approximate inference to the t-exponential family by defining a new t-divergence.</s> <s>this divergence measure is obtained via convex duality between the log-partition function of the t-exponential family and a new t-entropy.</s> <s>we illustrate our approach on the bayes point machine with a student's t-prior.</s></p></d>", "label": ["<d><p><s>t-divergence based approximate inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a number of recent scientific and engineering problems require signals to be decomposed into a product of a slowly varying positive envelope and a quickly varying carrier whose instantaneous frequency also varies slowly over time.</s> <s>although signal processing provides algorithms for so-called amplitude- and frequency-demodulation (afd), there are well known problems with all of the existing methods.</s> <s>motivated by the fact that afd is ill-posed, we approach the problem using probabilistic inference.</s> <s>the new approach, called probabilistic amplitude and frequency demodulation (pafd), models instantaneous frequency using an auto-regressive generalization of the von mises distribution, and the envelopes using gaussian auto-regressive dynamics with a positivity constraint.</s> <s>a novel form of expectation propagation is used for inference.</s> <s>we demonstrate that although pafd is computationally demanding, it outperforms previous approaches on synthetic and real signals in clean, noisy and missing data settings.</s></p></d>", "label": ["<d><p><s>probabilistic amplitude and frequency demodulation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider a general inference setting for discrete probabilistic graphical models where we seek maximum a posteriori (map) estimates for a subset of the random variables (max nodes), marginalizing over the rest (sum nodes).</s> <s>we present a hybrid message-passing algorithm to accomplish this.</s> <s>the hybrid algorithm passes a mix of sum and max messages depending on the type of source node (sum or max).</s> <s>we derive our algorithm by showing that it falls out as the solution of a particular relaxation of a variational framework.</s> <s>we further show that the expectation maximization algorithm can be seen as an approximation to our algorithm.</s> <s>experimental results on synthetic and real-world datasets, against several baselines, demonstrate the efficacy of our proposed algorithm.</s></p></d>", "label": ["<d><p><s>message-passing for approximate map inference with latent variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of bayesian inference for continuous time multi-stable stochastic systems which can change both their diffusion and drift parameters at discrete times.</s> <s>we propose exact inference and sampling methodologies for two specific cases where the discontinuous dynamics is given by a poisson process and a two-state markovian switch.</s> <s>we test the methodology on simulated data, and apply it to two real data sets in finance and systems biology.</s> <s>our experimental results show that the approach leads to valid inferences and non-trivial insights.</s></p></d>", "label": ["<d><p><s>inference in continuous-time change-point models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection.</s> <s>several approaches to learning minimum volume sets have been proposed in the literature, including the k-point nearest neighbor graph (k-knng) algorithm based on the geometric entropy minimization (gem) principle [4].</s> <s>the k-knng detector, while possessing several desirable characteristics, suffers from high computation complexity, and in [4] a simpler heuristic approximation, the leave-one-out knng (l1o-knng) was proposed.</s> <s>in this paper, we propose a novel bipartite k-nearest neighbor graph (bp-knng) anomaly detection scheme for estimating minimum volume sets.</s> <s>our bipartite estimator retains all the desirable theoretical properties of the k-knng, while being computationally simpler than the k-knng and the surrogate l1o-knng detectors.</s> <s>we show that bp-knng is asymptotically consistent in recovering the p-value of each test point.</s> <s>experimental results are given that illustrate the superior performance of bp-knng as compared to the l1o-knng and other state of the art anomaly detection schemes.</s></p></d>", "label": ["<d><p><s>efficient anomaly detection using bipartite k-nn graphs</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>monte-carlo tree search (mcts) has proven to be a powerful, generic planning technique for decision-making in single-agent and adversarial environments.</s> <s>the stochastic nature of the monte-carlo simulations introduces errors in the value estimates, both in terms of bias and variance.</s> <s>whilst reducing bias (typically through the addition of domain knowledge) has been studied in the mcts literature, comparatively little effort has focused on reducing variance.</s> <s>this is somewhat surprising, since variance reduction techniques are a well-studied area in classical statistics.</s> <s>in this paper, we examine the application of some standard techniques for variance reduction in mcts, including common random numbers, antithetic variates and control variates.</s> <s>we demonstrate how these techniques can be applied to mcts and explore their efficacy on three different stochastic, single-agent settings: pig, can't stop and dominion.</s></p></d>", "label": ["<d><p><s>variance reduction in monte-carlo tree search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neurons in the neocortex code and compute as part of a locally interconnected population.</s> <s>large-scale multi-electrode recording makes it possible to access these population processes empirically by fitting statistical models to unaveraged data.</s> <s>what statistical structure best describes the concurrent spiking of cells within  a local network?</s> <s>we argue that in the cortex, where firing exhibits extensive correlations in both time and space and where a typical sample of neurons still reflects only a very small fraction of the local population, the most appropriate model captures shared variability by a low-dimensional latent process evolving with smooth dynamics, rather than by putative direct coupling.</s> <s>we test this claim by comparing  a latent dynamical model with realistic spiking observations to coupled generalised  linear spike-response models (glms) using cortical recordings.</s> <s>we find that the latent dynamical approach outperforms the glm in terms of goodness-of-fit, and reproduces the temporal correlations in the data more accurately.</s> <s>we also compare models whose observations models are either derived from a gaussian or point-process models, finding that the non-gaussian model provides slightly  better goodness-of-fit and more realistic population spike counts.</s></p></d>", "label": ["<d><p><s>empirical models of spiking in neural populations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting.</s> <s>our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models.</s> <s>as a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation.</s> <s>as a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability.</s> <s>our result guarantees graph selection for samples scaling as n = omega(d log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of omega(d^2 log(p)).</s> <s>further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions.</s> <s>we corroborate these results using numerical simulations at the end.</s></p></d>", "label": ["<d><p><s>on learning discrete graphical models using greedy methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents.</s> <s>when learned topics are coherent and interpretable, they can be valuable for faceted browsing, results set diversity analysis, and document retrieval.</s> <s>however, when dealing with small collections or noisy text (e.g.</s> <s>web search result snippets or blog posts), learned topics can be less coherent, less interpretable, and less useful.</s> <s>to overcome this, we propose two methods to regularize the learning of topic models.</s> <s>our regularizers work by creating a structured prior over words that reflect broad patterns in the external data.</s> <s>using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest.</s> <s>overall, this work makes topic models more useful across a broader range of text data.</s></p></d>", "label": ["<d><p><s>improving topic coherence with regularized topic models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>multi-task learning (mtl) learns multiple related tasks simultaneously to improve generalization performance.</s> <s>alternating structure optimization (aso) is a popular mtl method that learns a shared low-dimensional predictive structure on hypothesis spaces from multiple related tasks.</s> <s>it has been applied successfully in many real world applications.</s> <s>as an alternative mtl approach, clustered multi-task learning (cmtl) assumes that multiple tasks follow a clustered structure, i.e., tasks are partitioned into a set of groups where tasks in the same group are similar to each other, and that such a clustered structure is unknown a priori.</s> <s>the objectives in aso and cmtl differ in how multiple tasks are related.</s> <s>interestingly, we show in this paper the equivalence relationship between aso and cmtl, providing significant new insights into aso and cmtl as well as their inherent relationship.</s> <s>the cmtl formulation is non-convex, and we adopt a convex relaxation to the cmtl formulation.</s> <s>we further establish the equivalence relationship between the proposed convex relaxation of cmtl and an existing convex relaxation of aso, and show that the proposed convex cmtl formulation is significantly more efficient especially for high-dimensional data.</s> <s>in addition, we present three algorithms for solving the convex cmtl formulation.</s> <s>we report experimental results on benchmark datasets to demonstrate the efficiency of the proposed algorithms.</s></p></d>", "label": ["<d><p><s>clustered multi-task learning via alternating structure optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.</s> <s>unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded \"local receptive fields\" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality).</s> <s>in this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods.</s> <s>specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric.</s> <s>this approach  allows us to harness the advantages of local receptive fields (such  as improved scalability, and reduced data requirements) when we do  not know how to specify such receptive fields by hand or where our  unsupervised training algorithm has no obvious generalization to a  topographic setting.</s> <s>we produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on cifar and stl datasets: 82.0% and 60.1% accuracy, respectively.</s></p></d>", "label": ["<d><p><s>selecting receptive fields in deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in standard gaussian process regression input locations are assumed to be noise free.</s> <s>we present a simple yet effective gp model for training on input points corrupted by i.i.d.</s> <s>gaussian noise.</s> <s>to make computations tractable we use a local linear expansion about each input point.</s> <s>this allows the input noise to be recast as output noise proportional to the squared gradient of the gp posterior mean.</s> <s>the input noise variances are inferred from the data as extra hyperparameters.</s> <s>they are trained alongside other hyperparameters by the usual method of maximisation of the marginal likelihood.</s> <s>training uses an iterative scheme, which alternates between optimising the hyperparameters and calculating the posterior gradient.</s> <s>analytic predictive moments can then be found for gaussian distributed test points.</s> <s>we compare our model to others over a range of different regression problems and show that it improves over current methods.</s></p></d>", "label": ["<d><p><s>gaussian process training with input noise</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>applications such as robot control and wireless communication require planning under uncertainty.</s> <s>partially observable markov decision processes (pomdps) plan policies for single agents under uncertainty and their decentralized versions (dec-pomdps) find a policy for multiple agents.</s> <s>the policy in infinite-horizon pomdp and dec-pomdp problems has been represented as finite state controllers (fscs).</s> <s>we introduce a novel class of periodic fscs, composed of layers connected only to the previous and next layer.</s> <s>our periodic fsc method finds a deterministic finite-horizon policy and converts it to an initial periodic infinite-horizon policy.</s> <s>this policy is optimized by a new infinite-horizon algorithm to yield deterministic periodic policies, and by a new expectation maximization algorithm to yield stochastic periodic policies.</s> <s>our method yields better results than earlier planning methods and can compute larger solutions than with regular fscs.</s></p></d>", "label": ["<d><p><s>periodic finite state controllers for efficient pomdp and dec-pomdp planning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fitted value iteration (fvi) with ordinary least squares regression is known to diverge.</s> <s>we present a new method, \"expansion-constrained ordinary least squares\" (ecols), that produces a linear approximation but also guarantees convergence when used with fvi.</s> <s>to ensure convergence, we constrain the least squares regression operator to be a non-expansion in the infinity-norm.</s> <s>we show that the space of function approximators that satisfy this constraint is more rich than the space of \"averagers,\" we prove a minimax property of the ecols residual error, and we give an efficient algorithm for computing the coefficients of ecols based on constraint generation.</s> <s>we illustrate the algorithmic convergence of fvi with ecols in a suite of experiments, and discuss its properties.</s></p></d>", "label": ["<d><p><s>convergent fitted value iteration with linear function approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important task in exploring and analyzing real-world data sets is to detect unusual and interesting phenomena.</s> <s>in this paper, we study the group anomaly detection problem.</s> <s>unlike traditional anomaly detection research that focuses on data points, our goal is to discover anomalous aggregated behaviors of groups of points.</s> <s>for this purpose, we propose the flexible genre model (fgm).</s> <s>fgm is designed to characterize data groups at both the point level and the group level so as to detect various types of group anomalies.</s> <s>we evaluate the effectiveness of fgm on both synthetic and real data sets including images and turbulence data, and show that it is superior to existing approaches in detecting group anomalies.</s></p></d>", "label": ["<d><p><s>group anomaly detection using flexible genre models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>budgeted optimization involves optimizing an unknown function that is costly to evaluate by requesting a limited number of function evaluations at intelligently selected inputs.</s> <s>typical problem formulations assume that experiments are selected one at a time with a limited total number of experiments, which fail to capture important aspects of many real-world problems.</s> <s>this paper defines a novel problem formulation with the following important extensions: 1) allowing for concurrent experiments; 2) allowing for stochastic experiment durations; and 3) placing constraints on both the total number of experiments and the total experimental time.</s> <s>we develop both offline and online algorithms for selecting concurrent experiments in this new setting and provide experimental results on a number of optimization benchmarks.</s> <s>the results show that our algorithms produce highly effective schedules compared to natural baselines.</s></p></d>", "label": ["<d><p><s>budgeted optimization with concurrent stochastic-duration experiments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>focusing on short term trend prediction in a financial context, we consider the problem of selective prediction whereby the predictor can abstain from prediction in order to improve performance.</s> <s>we examine two types of selective mechanisms for hmm predictors.</s> <s>the first is a rejection in the spirit of chow?s well-known ambiguity principle.</s> <s>the second is a specialized mechanism for hmms that identifies low quality hmm states and abstain from prediction in those states.</s> <s>we call this model selective hmm (shmm).</s> <s>in both approaches we can trade-off prediction coverage to gain better accuracy in a controlled manner.</s> <s>we compare performance of the ambiguity-based rejection technique with that of the shmm approach.</s> <s>our results indicate that both methods are effective, and that the shmm  model is superior.</s></p></d>", "label": ["<d><p><s>selective prediction of financial trends with hidden markov models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>motivated by applications in electronic games as well as teaching systems, we investigate the problem of dynamic difficulty adjustment.</s> <s>the task here is to repeatedly find a game difficulty setting that is neither `too easy' and bores the player, nor `too difficult' and overburdens the player.</s> <s>the contributions of this paper are ($i$) formulation of difficulty adjustment as an online learning problem on partially ordered sets, ($ii$) an exponential update algorithm for dynamic difficulty adjustment, ($iii$) a bound on the number of wrong difficulty settings relative to the best static setting chosen in hindsight, and ($iv$) an empirical investigation of the algorithm when playing against adversaries.</s></p></d>", "label": ["<d><p><s>predicting dynamic difficulty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions.</s> <s>we show that the standard weighted-trace norm might fail when the sampling distribution is not a product distribution (i.e.</s> <s>when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice.</s> <s>we provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.</s></p></d>", "label": ["<d><p><s>learning with the weighted trace-norm under arbitrary sampling distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a variational bayesian inference algorithm which can be widely applied to sparse linear models.</s> <s>the algorithm is based on the spike and slab prior which, from a bayesian perspective, is the golden standard for sparse inference.</s> <s>we apply the method to a general multi-task and multiple kernel learning model in which a common set of gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks.</s> <s>this model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases.</s> <s>we demonstrate our approach in multi-output gaussian process regression, multi-class classification, image processing applications and collaborative filtering.</s></p></d>", "label": ["<d><p><s>spike and slab variational inference for multi-task and multiple kernel learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of classification using similarity/distance functions over data.</s> <s>specifically, we propose a framework for defining the goodness of a (dis)similarity function with respect to a given learning task and propose algorithms that have guaranteed generalization properties when working with such good functions.</s> <s>our framework unifies and generalizes the frameworks proposed by (balcan-blum 2006) and (wang et al 2007).</s> <s>an attractive feature of our framework is its adaptability to data - we do not promote a fixed notion of goodness but rather let data dictate it.</s> <s>we show, by giving theoretical guarantees that the goodness criterion best suited to a problem can itself be learned which makes our approach applicable to a variety of domains and problems.</s> <s>we propose a landmarking-based approach to obtaining a classifier from such learned goodness criteria.</s> <s>we then provide a novel diversity based heuristic to perform task-driven selection of landmark points instead of random selection.</s> <s>we demonstrate the effectiveness of our goodness criteria learning method as well as the landmark selection heuristic on a variety of similarity-based learning datasets and benchmark uci datasets on which our method consistently outperforms existing approaches by a significant margin.</s></p></d>", "label": ["<d><p><s>similarity-based learning via data driven embeddings</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>compositional models provide an elegant formalism for representing the  visual appearance of highly variable objects.</s> <s>while such models are  appealing from a theoretical point of view, it has been difficult to  demonstrate that they lead to performance advantages on challenging  datasets.</s> <s>here we develop a grammar model for person detection  and show that it outperforms previous high-performance systems on the  pascal benchmark.</s> <s>our model represents people using a hierarchy of  deformable parts, variable structure and an explicit model of  occlusion for partially visible objects.</s> <s>to train the model, we  introduce a new discriminative framework for learning structured  prediction models from weakly-labeled data.</s></p></d>", "label": ["<d><p><s>object detection with grammar models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning problems such as logistic regression are typically formulated as pure  optimization problems defined on some loss function.</s> <s>we argue that this view  ignores the fact that the loss function depends on stochastically generated data  which in turn determines an intrinsic scale of precision for statistical estimation.</s> <s>by considering the statistical properties of the update variables used during  the optimization (e.g.</s> <s>gradients), we can construct frequentist hypothesis tests  to determine the reliability of these updates.</s> <s>we utilize subsets of the data  for computing updates, and use the hypothesis tests for determining when the  batch-size needs to be increased.</s> <s>this provides computational benefits and avoids  overfitting by stopping when the batch-size has become equal to size of the full  dataset.</s> <s>moreover, the proposed algorithms depend on a single interpretable  parameter ?</s> <s>the probability for an update to be in the wrong direction ?</s> <s>which is  set to a single value across all algorithms and datasets.</s> <s>in this paper, we illustrate  these ideas on three l1 regularized coordinate algorithms: l1 -regularized l2 -loss  svms, l1 -regularized logistic regression, and the lasso, but we emphasize that  the underlying methods are much more generally applicable.</s></p></d>", "label": ["<d><p><s>statistical tests for optimization efficiency</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic programming languages allow modelers to specify a stochastic    process using syntax that resembles modern programming languages.</s> <s>because    the program is in machine-readable format, a variety of techniques from    compiler design and program analysis can be used to examine the structure of    the distribution represented by the probabilistic program.</s> <s>we show how    nonstandard interpretations of probabilistic programs can be used to    craft efficient inference algorithms: information about the structure of a    distribution (such as gradients or dependencies) is generated as a monad-like side    computation while executing the program.</s> <s>these interpretations can be easily    coded using special-purpose objects and operator overloading.</s> <s>we implement    two examples of nonstandard interpretations in two different languages, and    use them as building blocks to construct inference algorithms: automatic    differentiation, which enables gradient based methods, and provenance    tracking, which enables efficient construction of global proposals.</s></p></d>", "label": ["<d><p><s>nonstandard interpretations of probabilistic programs for efficient inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>classical boosting algorithms, such as adaboost, build a strong classifier without concern about the computational cost.</s> <s>some applications, in particular in computer vision, may involve up to millions of training examples and features.</s> <s>in such contexts, the training time may become prohibitive.</s> <s>several methods exist to accelerate training, typically either by sampling the features, or the examples, used to train the weak learners.</s> <s>even if those methods can precisely quantify the speed improvement they deliver, they offer no guarantee of being more efficient than any other, given the same amount of time.</s> <s>this paper aims at shading some light on this problem, i.e.</s> <s>given a fixed amount of time, for a particular problem, which strategy is optimal in order to reduce the training loss the most.</s> <s>we apply this analysis to the design of new algorithms which estimate on the fly at every iteration the optimal trade-off between the number of samples and the number of features to look at in order to maximize the expected loss reduction.</s> <s>experiments in object recognition with two standard computer vision data-sets show that the adaptive methods we propose outperform basic sampling and state-of-the-art bandit methods.</s></p></d>", "label": ["<d><p><s>boosting with maximum adaptive sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>biased labelers are a systemic problem in crowdsourcing, and a  comprehensive toolbox for handling their responses is still being  developed.</s> <s>a typical crowdsourcing application can be divided into  three steps: data collection, data curation, and learning.</s> <s>at present  these steps are often treated separately.</s> <s>we present bayesian bias  mitigation for crowdsourcing (bbmc), a bayesian model to unify all  three.</s> <s>most data curation methods account for the {\\it effects} of  labeler bias by modeling all labels as coming from a single latent  truth.</s> <s>our model captures the {\\it sources} of bias by describing  labelers as influenced by shared random effects.</s> <s>this approach can  account for more complex bias patterns that arise in ambiguous or hard  labeling tasks and allows us to merge data curation and learning into  a single computation.</s> <s>active learning integrates data collection with  learning, but is commonly considered infeasible with gibbs sampling  inference.</s> <s>we propose a general approximation strategy for markov  chains to efficiently quantify the effect of a perturbation on the  stationary distribution and specialize this approach to active  learning.</s> <s>experiments show bbmc to outperform many common heuristics.</s></p></d>", "label": ["<d><p><s>bayesian bias mitigation for crowdsourcing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of assigning class labels to an unlabeled test  data set, given several labeled training data sets drawn from similar  distributions.</s> <s>this problem arises in several applications where data  distributions fluctuate because of biological, technical, or other sources  of variation.</s> <s>we develop a distribution-free, kernel-based approach to the  problem.</s> <s>this approach involves identifying an appropriate reproducing     kernel hilbert space and optimizing a regularized empirical risk over the  space.</s> <s>we present generalization error analysis, describe universal  kernels, and establish universal consistency of the proposed methodology.</s> <s>experimental results on flow cytometry data are presented.</s></p></d>", "label": ["<d><p><s>generalizing from several related classification tasks to a new unlabeled sample</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce hierarchically supervised latent dirichlet allocation (hslda), a model for hierarchically and multiply labeled bag-of-word data.</s> <s>examples of such data include web pages and their placement in directories, product descriptions and associated categories from product hierarchies, and free-text clinical records and their assigned diagnosis codes.</s> <s>out-of-sample label prediction is the primary goal of this work, but improved lower-dimensional representations of the bag-of-word data are also of interest.</s> <s>we demonstrate hslda on large-scale data from clinical document labeling and retail product categorization tasks.</s> <s>we show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not.</s></p></d>", "label": ["<d><p><s>hierarchically supervised latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>speech conveys different yet mixed information ranging from linguistic to speaker-specific components, and each of them should be exclusively used in a specific task.</s> <s>however, it is extremely difficult to extract a specific information component given the fact that nearly all existing acoustic representations carry all types of speech information.</s> <s>thus, the use of the same representation in both speech and speaker recognition hinders a system from producing better performance due to interference of irrelevant information.</s> <s>in this paper, we present a deep neural architecture to extract speaker-specific information from mfccs.</s> <s>as a result, a multi-objective loss function is proposed for learning speaker-specific characteristics and regularization via normalizing interference of non-speaker related information and avoiding information loss.</s> <s>with ldc benchmark corpora and a chinese speech corpus, we demonstrate that a resultant speaker-specific representation is insensitive to text/languages spoken and environmental mismatches and hence outperforms mfccs and other state-of-the-art techniques in speaker recognition.</s> <s>we discuss relevant issues and relate our approach to previous work.</s></p></d>", "label": ["<d><p><s>extracting speaker-specific information with a regularized siamese deep network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking inputs.</s> <s>conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing.</s> <s>here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree.</s> <s>our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs.</s> <s>this approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity.</s></p></d>", "label": ["<d><p><s>active dendrites: adaptation to spike-based communication</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the minimization of a convex objective function defined on a hilbert space, which is only available through unbiased estimates of  its gradients.</s> <s>this problem includes  standard machine learning algorithms such as kernel logistic regression and least-squares regression, and is commonly referred to as a stochastic approximation problem in the operations research community.</s> <s>we provide a non-asymptotic analysis of the  convergence of two well-known algorithms, stochastic gradient descent (a.k.a.~robbins-monro algorithm) as well as a simple modification where iterates are averaged (a.k.a.~polyak-ruppert averaging).</s> <s>our analysis suggests that a learning rate proportional to the inverse of the number of iterations, while leading to the optimal convergence rate in the strongly convex case, is not robust to the lack of strong convexity or the setting of the proportionality constant.</s> <s>this situation is remedied when using slower decays together with averaging, robustly leading to the optimal rate of convergence.</s> <s>we illustrate our theoretical results with simulations on synthetic and standard datasets.</s></p></d>", "label": ["<d><p><s>non-asymptotic analysis of stochastic approximation algorithms for machine learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many functional descriptions of spiking neurons assume a cascade structure where inputs are passed through an initial linear filtering stage that produces a low-dimensional signal that drives subsequent nonlinear stages.</s> <s>this paper presents a novel and systematic parameter estimation procedure for such models and applies the method to two neural estimation problems: (i) compressed-sensing based neural mapping from multi-neuron excitation, and (ii) estimation of neural receptive yields in sensory neurons.</s> <s>the proposed estimation algorithm models the neurons via a graphical model and then estimates the parameters in the model using a recently-developed generalized approximate message passing (gamp) method.</s> <s>the gamp method is based on gaussian approximations of loopy belief propagation.</s> <s>in the neural connectivity problem, the gamp-based   method is shown to be computational efficient, provides a more exact modeling of the sparsity, can incorporate nonlinearities in the output and significantly outperforms previous compressed-sensing methods.</s> <s>for the receptive field estimation, the gamp method can also exploit inherent structured sparsity in the linear weights.</s> <s>the method is validated on estimation of linear nonlinear poisson (lnp) cascade models for receptive fields of salamander retinal ganglion cells.</s></p></d>", "label": ["<d><p><s>neural reconstruction with approximate message passing (neuramp)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many studies have uncovered evidence that visual cortex contains specialized regions involved in processing faces but not other object classes.</s> <s>recent electrophysiology studies of cells in several of these specialized regions revealed that at least some of these regions are organized in a hierarchical manner with viewpoint-specific cells projecting to downstream viewpoint-invariant identity-specific cells (freiwald and tsao 2010).</s> <s>a separate computational line of reasoning leads to the claim that some transformations of visual inputs that preserve viewed object identity are class-specific.</s> <s>in particular, the 2d images evoked by a face undergoing a 3d rotation are not produced by the same image transformation (2d) that would produce the images evoked by an object of another class undergoing the same 3d rotation.</s> <s>however, within the class of faces, knowledge of the image transformation evoked by 3d rotation can be reliably transferred from previously viewed faces to help identify a novel face at a new viewpoint.</s> <s>we show, through computational simulations, that an architecture which applies this method of gaining invariance to class-specific transformations is effective when restricted to faces and fails spectacularly when applied across object classes.</s> <s>we argue here that in order to accomplish viewpoint-invariant face identification from a single example view, visual cortex must separate the circuitry involved in discounting 3d rotations of faces from the generic circuitry involved in processing other objects.</s> <s>the resulting model of the ventral stream of visual cortex is consistent with the recent physiology results showing the hierarchical organization of the face processing network.</s></p></d>", "label": ["<d><p><s>why the brain separates face recognition from object recognition</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce picodes: a very compact image descriptor which nevertheless allows high performance on object category recognition.</s> <s>in particular, we address novel-category recognition: the task of defining indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built.</s> <s>instead, the training images defining the category are supplied at query time.</s> <s>we explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance.</s> <s>in contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes.</s> <s>in contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classification performance.</s> <s>optimization directly for binary features is difficult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice.</s> <s>picodes of 256 bytes match the accuracy of the current best known classifier for the caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude.</s></p></d>", "label": ["<d><p><s>picodes: learning a compact code for novel-category recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>extensive evidence suggests that items are not encoded independently in visual short-term memory (vstm).</s> <s>however, previous research has not quantitatively considered how the encoding of an item influences the encoding of other items.</s> <s>here, we model the dependencies among vstm representations using a multivariate gaussian distribution with a stimulus-dependent mean and covariance matrix.</s> <s>we report the results of an experiment designed to determine the specific form of the stimulus-dependence of the mean and the covariance matrix.</s> <s>we find that the magnitude of the covariance between the representations of two items is a monotonically decreasing function of the difference between the items' feature values, similar to a gaussian process with a distance-dependent, stationary kernel function.</s> <s>we further show that this type of covariance function can be explained as a natural consequence of encoding multiple stimuli in a population of neurons with correlated responses.</s></p></d>", "label": ["<d><p><s>probabilistic modeling of dependencies among visual short-term memory representations</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly not very popular in the literature.</s> <s>we present here some empirical results using thompson sampling on simulated and real data, and show that it is highly competitive.</s> <s>and since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.</s></p></d>", "label": ["<d><p><s>an empirical evaluation of thompson sampling</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a sizable literature has focused on the problem of estimating a low-dimensional feature space capturing a neuron's stimulus sensitivity.</s> <s>however, comparatively little work has addressed the problem of estimating the nonlinear function from feature space to a neuron's output spike rate.</s> <s>here, we use a gaussian process (gp) prior over the infinite-dimensional space of nonlinear functions to obtain bayesian estimates of the \"nonlinearity\" in the linear-nonlinear-poisson (lnp) encoding model.</s> <s>this offers flexibility, robustness, and computational tractability compared to traditional methods (e.g., parametric forms, histograms, cubic splines).</s> <s>most importantly, we develop a framework for optimal experimental design based on uncertainty sampling.</s> <s>this involves adaptively selecting stimuli to characterize the nonlinearity with as little experimental data as possible, and relies on a method for rapidly updating hyperparameters using the laplace approximation.</s> <s>we apply these methods to data from color-tuned neurons in macaque v1.</s> <s>we estimate nonlinearities in the 3d space of cone contrasts, which reveal that v1 combines cone inputs in a highly nonlinear manner.</s> <s>with simulated experiments, we show that optimal design substantially reduces the amount of data required to estimate this nonlinear combination rule.</s></p></d>", "label": ["<d><p><s>active learning of neural response functions with gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a simple algorithm that runs in time  poly(n,1/gamma,1/eps) and learns an unknown n-dimensional  gamma-margin halfspace to accuracy 1-eps in the presence of  malicious noise, when the noise rate is allowed to be as high as  theta(eps gamma sqrt(log(1/gamma))).</s> <s>previous efficient  algorithms could only learn to accuracy eps in the presence of  malicious noise of rate at most theta(eps gamma).</s> <s>our algorithm does not work by optimizing a convex loss function.</s> <s>we  show that no algorithm for learning gamma-margin halfspaces that  minimizes a convex proxy for misclassification error can tolerate  malicious noise at a rate greater than theta(eps gamma); this may  partially explain why previous algorithms could not achieve the higher  noise tolerance of our new algorithm.</s></p></d>", "label": ["<d><p><s>learning large-margin halfspaces with more malicious noise</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a robust filtering approach based on semi-supervised and multiple instance learning (mil).</s> <s>we assume that the posterior density would be unimodal if not for the effect of outliers that we do not wish to explicitly model.</s> <s>therefore, we seek for a point estimate at the outset, rather than a generic approximation of the entire posterior.</s> <s>our approach can be thought of as a combination of standard finite-dimensional filtering (extended kalman filter, or unscented filter) with multiple instance learning, whereby the initial condition comes with a putative set of inlier measurements.</s> <s>we show how both the state (regression) and the inlier set (classification) can be estimated iteratively and causally by processing only the current measurement.</s> <s>we illustrate our approach on visual tracking problems whereby the object of interest (target) moves and evolves as a result of occlusions and deformations, and partial knowledge of the target is given in the form of a bounding box (training set).</s></p></d>", "label": ["<d><p><s>multiple instance filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop unified information-theoretic machinery for deriving lower bounds for passive and active learning schemes.</s> <s>our bounds involve the so-called alexander's capacity function.</s> <s>the supremum of this function has been recently rediscovered by hanneke in the context of active learning under the name of \"disagreement coefficient.\"</s> <s>for passive learning, our lower bounds match the upper bounds of gine and koltchinskii up to constants and generalize analogous results of massart and nedelec.</s> <s>for active learning, we provide first known lower bounds based on the capacity function rather than the disagreement coefficient.</s></p></d>", "label": ["<d><p><s>lower bounds for passive and active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>local coordinate coding (lcc) [18] is a method for modeling functions of data lying on non-linear manifolds.</s> <s>it provides a set of anchor points which form a local coordinate system, such that each data point on the manifold can be approximated by a linear combination of its anchor points, and the linear weights become the local coordinate coding.</s> <s>in this paper we propose encoding data using orthogonal  anchor planes, rather than anchor points.</s> <s>our method needs only a few orthogonal anchor planes for coding, and it can linearize any (\\alpha,\\beta,p)-lipschitz smooth nonlinear  function with a fixed expected value of the upper-bound approximation error on any high dimensional data.</s> <s>in practice, the orthogonal coordinate system can be easily learned by minimizing this upper bound using singular value decomposition (svd).</s> <s>we apply our method to model the coordinates locally in linear svms for classification tasks, and our experiment on mnist shows that using only 50 anchor planes our method achieves 1.72% error rate, while lcc achieves 1.90% error rate using 4096 anchor points.</s></p></d>", "label": ["<d><p><s>learning anchor planes for classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most action potentials in the nervous system take on the form of strong, rapid, and brief voltage deflections known as spikes, in stark contrast to other action potentials, such as in the heart, that are characterized by broad voltage plateaus.</s> <s>we derive the shape of the neuronal action potential from first principles, by postulating that action potential generation is strongly constrained by the brain's need to minimize energy expenditure.</s> <s>for a given height of an action potential, the least energy is consumed when the underlying currents obey the bang-bang principle: the currents giving rise to the spike should be intense, yet short-lived, yielding  spikes with sharp onsets and offsets.</s> <s>energy optimality predicts features in the biophysics that are not per se required for producing the characteristic neuronal action potential:  sodium currents should be extraordinarily powerful and inactivate with voltage; both potassium and sodium currents should have kinetics that have a bell-shaped voltage-dependence; and the cooperative action of multiple `gates'  should start the flow of current.</s></p></d>", "label": ["<d><p><s>energetically optimal action potentials</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational methods have been previously explored as a tractable approximation to bayesian inference for neural networks.</s> <s>however the approaches proposed so far have only been applicable to a few simple network architectures.</s> <s>this paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks.</s> <s>along the way it revisits several common regularisers from a variational perspective.</s> <s>it also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation.</s> <s>experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the timit speech corpus.</s></p></d>", "label": ["<d><p><s>practical variational inference for neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>high dimensional time series are endemic in applications of machine learning  such as robotics (sensor data), computational biology (gene expression data), vision   (video sequences) and graphics (motion capture data).</s> <s>practical nonlinear  probabilistic approaches to this data are required.</s> <s>in this paper we introduce  the variational gaussian process dynamical system.</s> <s>our work builds on recent  variational approximations for gaussian process latent variable models to allow  for nonlinear dimensionality reduction simultaneously with learning a dynamical  prior in the latent space.</s> <s>the approach also allows for the appropriate dimensionality   of the latent space to be automatically determined.</s> <s>we demonstrate the  model on a human motion capture data set and a series of high resolution video  sequences.</s></p></d>", "label": ["<d><p><s>variational gaussian process dynamical systems</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the efficient coding hypothesis holds that neural receptive fields are adapted to the statistics of the environment, but is agnostic to the timescale of this adaptation, which occurs on both evolutionary and developmental timescales.</s> <s>in this work we focus on that component of adaptation which occurs during an organism's lifetime, and show that a number of unsupervised feature learning algorithms can account for features of normal receptive field properties across multiple primary sensory cortices.</s> <s>furthermore, we show that the same algorithms account for altered receptive field properties in response to experimentally altered environmental statistics.</s> <s>based on these modeling results we propose these models as phenomenological models of receptive field plasticity during an organism's lifetime.</s> <s>finally, due to the success of the same models in multiple sensory areas, we suggest that these algorithms may provide a constructive realization of the theory, first proposed by mountcastle (1978), that a qualitatively similar learning algorithm acts throughout primary sensory cortices.</s></p></d>", "label": ["<d><p><s>unsupervised learning models of primary cortical receptive fields and receptive field plasticity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the multi-armed bandit (mab) setting is a useful abstraction of many online learning tasks which focuses on the trade-off between exploration and exploitation.</s> <s>in this setting, an online algorithm has a fixed set of alternatives (\"arms\"), and in each round it selects one arm and then observes the corresponding reward.</s> <s>while the case of small number of arms is by now well-understood, a lot of recent work has focused on multi-armed bandits with (infinitely) many arms, where one needs to assume extra structure in order to make the problem tractable.</s> <s>in particular, in the lipschitz mab problem there is an underlying similarity metric space, known to the algorithm, such that any two arms that are close in this metric space have similar payoffs.</s> <s>in this paper we consider the more realistic scenario in which the metric space is *implicit* -- it is defined by the available structure but not revealed to the algorithm directly.</s> <s>specifically, we assume that an algorithm is given a tree-based classification of arms.</s> <s>for any given problem instance such a classification implicitly defines a similarity metric space, but the numerical similarity information is not available to the algorithm.</s> <s>we provide an algorithm for this setting, whose performance guarantees (almost) match the best known guarantees for the corresponding instance of the lipschitz mab problem.</s></p></d>", "label": ["<d><p><s>multi-armed bandits on implicit metric spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an asymptotic analysis of viterbi training (vt) and contrast it with a more conventional maximum likelihood (ml) approach to parameter estimation in hidden markov models.</s> <s>while ml estimator works by (locally) maximizing the likelihood of the observed data, vt seeks to maximize the probability of the most likely hidden state sequence.</s> <s>we develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of hmm with one unambiguous symbol.</s> <s>for this particular model the ml objective function is continuously degenerate.</s> <s>vt objective, in contrast, is shown to have only finite degeneracy.</s> <s>furthermore, vt converges faster and results in sparser (simpler) models, thus realizing an automatic occam's razor for hmm learning.</s> <s>for more general scenario vt can be worse compared to ml but still capable of correctly recovering most of the parameters.</s></p></d>", "label": ["<d><p><s>comparative analysis of viterbi training and maximum likelihood estimation for hmms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classification.</s> <s>however, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning.</s> <s>in this work, we present sparse filtering, a simple new algorithm which is efficient and only has one hyperparameter, the number of features to learn.</s> <s>in contrast to most other feature learning methods, sparse filtering does not explicitly attempt to construct a model of the data distribution.</s> <s>instead, it optimizes a simple cost function -- the sparsity of l2-normalized features -- which can easily be implemented in a few lines of matlab code.</s> <s>sparse filtering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking.</s> <s>we evaluate sparse filtering on natural images, object classification (stl-10), and phone classification (timit), and show that our method works well on a range of different modalities.</s></p></d>", "label": ["<d><p><s>sparse filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the vast majority of recent work on sparse estimation algorithms, performance has been evaluated using ideal or quasi-ideal dictionaries (e.g., random gaussian or fourier) characterized by unit $\\ell_2$ norm, incoherent columns or features.</s> <s>but in reality, these types of dictionaries represent only a subset of the dictionaries that are actually used in practice (largely restricted to idealized compressive sensing applications).</s> <s>in contrast, herein sparse estimation is considered in the context of structured dictionaries possibly exhibiting high coherence between arbitrary groups of columns and/or rows.</s> <s>sparse penalized regression models are analyzed with the purpose of finding, to the extent possible, regimes of dictionary invariant performance.</s> <s>in particular, a type ii bayesian estimator with a dictionary-dependent sparsity penalty is shown to have a number of desirable invariance properties leading to provable advantages over more conventional penalties such as the $\\ell_1$ norm, especially in areas where existing theoretical recovery guarantees no longer hold.</s> <s>this can translate into improved performance in applications such as model selection with correlated features, source localization, and compressive sensing with constrained measurement directions.</s></p></d>", "label": ["<d><p><s>sparse estimation with structured dictionaries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an approach for linear unsupervised dimensionality reduction, based on the sparse linear model that has been used to probabilistically interpret sparse coding.</s> <s>we formulate an optimization problem for learning a linear projection from the original signal domain to a lower-dimensional one in a way that approximately preserves, in expectation, pairwise inner products in the sparse domain.</s> <s>we derive solutions to the problem, present nonlinear extensions, and discuss relations to compressed sensing.</s> <s>our experiments using facial images, texture patches, and images of object categories suggest that the approach can improve our ability to recover meaningful structure in many classes of signals.</s></p></d>", "label": ["<d><p><s>dimensionality reduction using the sparse linear model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse pca provides a linear combination of small number of features that maximizes variance across data.</s> <s>although sparse pca has apparent advantages compared to pca, such as better interpretability, it is generally thought to be computationally much more expensive.</s> <s>in this paper, we demonstrate the surprising fact that sparse pca can be easier than pca in practice, and that it can be reliably applied to very large data sets.</s> <s>this comes from a rigorous feature elimination  pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated.</s> <s>we introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones.</s> <s>we provide  experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features.</s> <s>these results illustrate how sparse pca can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.</s></p></d>", "label": ["<d><p><s>large-scale sparse principal component analysis with application to text data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we use branch-and-bound (bb) to efficiently detect objects with deformable part models.</s> <s>instead of evaluating the classifier score exhaustively over image locations and scales, we use bb to focus on promising image  locations.</s> <s>the core problem is to compute bounds that accommodate part deformations; for this we adapt the dual trees data structure  to our problem.</s> <s>we evaluate our approach using  mixture-of-deformable part models.</s> <s>we obtain exactly the same results but are 10-20 times faster on average.</s> <s>we also develop a multiple-object detection  variation of the system, where  hypotheses for 20 categories are inserted in a common   priority queue.</s> <s>for the problem of finding the strongest category in an image this results in up to a 100-fold speedup.</s></p></d>", "label": ["<d><p><s>rapid deformable object detection using dual-tree branch-and-bound</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for a learning problem whose associated excess loss class is $(\\beta,b)$-bernstein, we show that it is theoretically possible to track the same classification performance  of the best (unknown) hypothesis in our class, provided that we are free to abstain from prediction in some region of our choice.</s> <s>the (probabilistic) volume of this rejected region of the domain is shown to be diminishing at rate $o(b\\theta (\\sqrt{1/m}))^\\beta)$, where $\\theta$ is hanneke's disagreement coefficient.</s> <s>the strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting.</s> <s>nevertheless, we heuristically approximate this strategy and develop a novel selective classification algorithm  using constrained svms.</s> <s>we show empirically that the resulting algorithm consistently outperforms  the traditional rejection mechanism based on distance from decision boundary.</s></p></d>", "label": ["<d><p><s>agnostic selective classification</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>modern classification tasks usually involve many class labels and can be informed by a broad range of features.</s> <s>many of these tasks are tackled by constructing a set of classifiers, which are then applied at test time and then pieced together in a fixed procedure determined in advance or at training time.</s> <s>we present an active classification process at the test time, where each classifier in a large ensemble is viewed as a potential observation that might inform our classification process.</s> <s>observations are then selected dynamically based on previous observations, using a value-theoretic computation that balances an estimate of the expected classification gain from each observation as well as its computational cost.</s> <s>the expected classification gain is computed using a probabilistic model that uses the outcome from previous observations.</s> <s>this active classification process is applied at test time for each individual test instance, resulting in an efficient instance-specific decision path.</s> <s>we demonstrate the benefit of the active scheme on various real-world datasets, and show that it can achieve comparable or even higher classification accuracy at a fraction of the computational costs of traditional methods.</s></p></d>", "label": ["<d><p><s>active classification based on value of classifier</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a bayesian approach to partitioning distance matrices is presented.</s> <s>it is inspired by the 'translation-invariant wishart-dirichlet' process (tiwd) in (vogt et al., 2010) and shares a number of advantageous properties like the fully probabilistic nature of the inference model, automatic selection of the number of clusters and applicability in semi-supervised settings.</s> <s>in addition, our method (which we call 'fasttiwd') overcomes the main shortcoming of the original tiwd, namely its high computational costs.</s> <s>the fasttiwd reduces the workload in each iteration of a gibbs sampler from o(n^3) in the tiwd to o(n^2).</s> <s>our experiments show that this cost reduction does not compromise the quality of the inferred partitions.</s> <s>with this new method it is now possible to 'mine' large relational datasets with a probabilistic model, thereby automatically detecting new and potentially interesting clusters.</s></p></d>", "label": ["<d><p><s>bayesian partitioning of large-scale distance data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although spectral clustering has enjoyed considerable empirical success in machine learning, its theoretical properties are not yet fully developed.</s> <s>we analyze the performance of a spectral algorithm for hierarchical clustering and show that on a class of hierarchically structured similarity matrices, this algorithm can tolerate noise that grows with the number of data points while still perfectly recovering the hierarchical clusters with high probability.</s> <s>we  additionally improve upon previous results for k-way spectral clustering to derive conditions under which spectral clustering makes no mistakes.</s> <s>further, using minimax analysis, we derive tight upper and lower bounds for the  clustering problem and compare the performance of spectral clustering to these  information theoretic limits.</s> <s>we also present experiments on simulated and real  world data illustrating our results.</s></p></d>", "label": ["<d><p><s>noise thresholds for spectral clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this manuscript considers the convergence rate of boosting under a large class of losses, including the exponential and logistic losses, where the best previous rate of convergence was o(exp(1/??)).</s> <s>first, it is established that the setting of weak learnability aids the entire class, granting a rate o(ln(1/?)).</s> <s>next, the (disjoint) conditions under which the infimal empirical risk is attainable are characterized in terms of the sample and weak learning class, and a new proof is given for the known rate o(ln(1/?)).</s> <s>finally, it is established that any instance can be decomposed into two smaller instances resembling the two preceding special cases, yielding a rate o(1/?</s> <s>), with a matching lower bound for the logistic loss.</s> <s>the principal technical hurdle throughout this work is the potential unattainability of the infimal empirical risk; the technique for overcoming this barrier may be of general interest.</s></p></d>", "label": ["<d><p><s>the fast convergence of boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational bayesian matrix factorization (vbmf) efficiently   approximates the posterior distribution of factorized matrices   by assuming matrix-wise independence of the two factors.</s> <s>a recent study on fully-observed vbmf  showed that, under a stronger assumption that the two factorized matrices are  column-wise independent,  the global optimal solution can be analytically computed.</s> <s>however, it was not clear how restrictive  the column-wise independence assumption is.</s> <s>in this paper, we prove that the global solution under matrix-wise  independence is actually column-wise independent,  implying that the column-wise independence assumption is harmless.</s> <s>a practical consequence of our theoretical finding is that  the global solution under matrix-wise independence (which is a standard setup)  can be obtained analytically in a computationally very efficient way  without any iterative algorithms.</s> <s>we experimentally illustrate advantages of using our analytic solution  in probabilistic principal component analysis.</s></p></d>", "label": ["<d><p><s>global solution of fully-observed variational bayesian matrix factorization is column-wise independent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a family of global optimization procedures that automatically decompose optimization problems into smaller loosely coupled problems, then combine the solutions of these with message passing algorithms.</s> <s>we show empirically that these methods excel in avoiding local minima and produce better solutions with fewer function evaluations than existing global optimization methods.</s> <s>to develop these methods, we introduce a notion of coupling between variables of optimization that generalizes the notion of coupling that arises from factoring functions into terms that involve small subsets of the variables.</s> <s>it therefore subsumes the notion of independence between random variables in statistics, sparseness of the hessian in nonlinear optimization, and the generalized distributive law.</s> <s>despite being more general, this notion of coupling is easier to verify empirically -- making structure estimation easy -- yet it allows us to migrate well-established inference methods on graphical models to the setting of global optimization.</s></p></d>", "label": ["<d><p><s>structure learning for optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment.</s> <s>one outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution.</s> <s>two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior.</s> <s>we show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations.</s> <s>neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of markov chain monte carlo (mcmc) to approximate the posterior over the relevant states.</s> <s>we demonstrate the effectiveness and efficiency of this approach on a sparse coding model.</s> <s>in numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact em, variational state space selection alone, mcmc alone, and the combined select and sample approach.</s> <s>the select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks.</s> <s>for sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.</s></p></d>", "label": ["<d><p><s>select and sample - a model of efficient neural inference and learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most previous research on image categorization has focused on medium-scale data sets, while large-scale image categorization with millions of images from thousands of categories remains a challenge.</s> <s>with the emergence of structured large-scale dataset such as the imagenet, rich information about the conceptual relationships between images, such as a tree hierarchy among various image categories, become available.</s> <s>as human cognition of complex visual world benefits from underlying semantic relationships between object classes, we believe a machine learning system can and should leverage such information as well for better performance.</s> <s>in this paper, we employ such semantic relatedness among image categories for large-scale image categorization.</s> <s>specifically, a category hierarchy is utilized to properly define loss function and select common set of features for  related categories.</s> <s>an efficient optimization method based on proximal approximation and accelerated parallel gradient method is introduced.</s> <s>experimental results on a subset of imagenet containing 1.2 million images from 1000 categories demonstrate the effectiveness and promise of our proposed approach.</s></p></d>", "label": ["<d><p><s>large-scale category structure aware image categorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we are motivated by an application to extract a representative subset of machine learning training data and by the poor empirical performance we observe of the popular minimum norm algorithm.</s> <s>in fact, for our application, minimum norm can have a running time of about o(n^7 ) (o(n^5 ) oracle calls).</s> <s>we therefore propose a fast approximate method to minimize arbitrary submodular functions.</s> <s>for a large sub-class of submodular functions, the algorithm is exact.</s> <s>other submodular functions are iteratively approximated by tight submodular upper bounds, and then repeatedly optimized.</s> <s>we show theoretical properties, and empirical results suggest significant speedups over minimum norm while retaining higher accuracies.</s></p></d>", "label": ["<d><p><s>on fast approximate submodular minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of recovering the parameter alpha in r^k of a sparse function f, i.e.</s> <s>the number of non-zero entries of alpha is small compared to the number k of features, given noisy evaluations of f at a set of well-chosen sampling points.</s> <s>we introduce an additional randomisation process, called brownian sensing, based on the computation of stochastic integrals, which produces a gaussian sensing matrix, for which good recovery properties are proven independently on the number of sampling points n, even when the features are arbitrarily non-orthogonal.</s> <s>under the assumption that f is h?lder continuous with exponent at least 1/2, we provide an estimate a of the parameter such that ||\\alpha - a||_2 = o(||eta||_2\\sqrt{n}), where eta is the observation noise.</s> <s>the method uses a set of sampling points uniformly distributed along a one-dimensional curve selected according to the features.</s> <s>we report numerical experiments illustrating our method.</s></p></d>", "label": ["<d><p><s>sparse recovery with brownian sensing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we investigate the representational power of sum-product networks  (computation networks analogous to neural networks,  but whose individual units compute either products  or weighted sums), through a theoretical analysis that compares  deep (multiple hidden layers) vs. shallow (one hidden layer) architectures.</s> <s>we prove there exist families of functions that can be represented  much more efficiently with a deep network than with a shallow one, i.e.</s> <s>with substantially fewer hidden units.</s> <s>such results were not available until now, and  contribute to motivate recent research involving learning of deep  sum-product networks, and more generally motivate research in deep  learning.</s></p></d>", "label": ["<d><p><s>shallow vs. deep sum-product networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov random fields (mrfs) have proven very powerful both as density estimators and feature extractors for classification.</s> <s>however, their use is often limited by an inability to estimate the partition function $z$.</s> <s>in this paper, we exploit the gradient descent training procedure of restricted boltzmann machines (a type of mrf) to {\\bf track} the log partition function during learning.</s> <s>our method relies on two distinct sources of information: (1) estimating the change $\\delta z$ incurred by each gradient update, (2) estimating the difference in $z$ over a small set of tempered distributions using bridge sampling.</s> <s>the two sources of information are then combined using an inference procedure similar to kalman filtering.</s> <s>learning mrfs through tempered stochastic maximum likelihood, we can estimate $z$ using no more temperatures than are required for learning.</s> <s>comparing to both exact values and estimates using annealed importance sampling (ais), we show on several datasets that our method is able to accurately track the log partition function.</s> <s>in contrast to ais, our method provides this estimate at each time-step, at a computational cost similar to that required for training alone.</s></p></d>", "label": ["<d><p><s>on tracking the partition function</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a common approach for handling the complexity and inherent ambiguities of 3d human pose estimation is to use pose priors learned from  training data.</s> <s>existing approaches however, are either too simplistic (linear), too complex to learn, or  can only learn latent spaces from \"simple data\", i.e., single activities such as walking or running.</s> <s>in this paper, we present an efficient stochastic gradient descent algorithm that is able to learn probabilistic non-linear latent  spaces composed of multiple  activities.</s> <s>furthermore, we derive an incremental algorithm for the online setting which can update the latent space without extensive relearning.</s> <s>we demonstrate the  effectiveness of our approach on the  task of monocular and multi-view tracking and show that our approach  outperforms the state-of-the-art.</s></p></d>", "label": ["<d><p><s>learning probabilistic non-linear latent variable models for tracking complex activities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a set v of n vectors in d-dimensional space, we provide an efficient method  for computing quality upper and lower bounds of the euclidean distances between  a pair of the vectors in v .</s> <s>for this purpose, we define a distance measure, called  the ms-distance, by using the mean and the standard deviation values of vectors in  v .</s> <s>once we compute the mean and the standard deviation values of vectors in v in  o(dn) time, the ms-distance between them provides upper and lower bounds of  euclidean distance between a pair of vectors in v in constant time.</s> <s>furthermore,  these bounds can be refined further such that they converge monotonically to the  exact euclidean distance within d refinement steps.</s> <s>we also provide an analysis on  a random sequence of refinement steps which can justify why ms-distance should  be refined to provide very tight bounds in a few steps of a typical sequence.</s> <s>the  ms-distance can be used to various problems where the euclidean distance is used  to measure the proximity or similarity between objects.</s> <s>we provide experimental  results on the nearest and the farthest neighbor searches.</s></p></d>", "label": ["<d><p><s>convergent bounds on the euclidean distance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the object people perceive in an image can depend on its orientation relative to the scene it is in (its reference frame).</s> <s>for example, the images of the symbols $\\times$ and $+$ differ by a 45 degree rotation.</s> <s>although real scenes have multiple images and reference frames, psychologists have focused on scenes with only one reference frame.</s> <s>we propose an ideal observer model based on nonparametric bayesian statistics for inferring the number of reference frames in a scene and their parameters.</s> <s>when an ambiguous image could be assigned to two conflicting reference frames, the model predicts two factors should influence the reference frame inferred for the image: the image should be more likely to share the reference frame of the closer object ({\\em proximity}) and it should be more likely to share the reference frame containing the most objects ({\\em alignment}).</s> <s>we confirm people use both cues using a novel methodology that allows for easy testing of human reference frame inference.</s></p></d>", "label": ["<d><p><s>an ideal observer model for identifying the reference frame of objects</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variability in single neuron models is typically implemented either by a stochastic leaky-integrate-and-fire model or by a model of the generalized linear model (glm) family.</s> <s>we use analytical and numerical methods to relate state-of-the-art models from both schools of thought.</s> <s>first we find the analytical expressions relating the subthreshold voltage from the adaptive exponential integrate-and-fire model (adex) to the spike-response model with escape noise (srm as an example of a glm).</s> <s>then we calculate numerically the link-function that provides the firing probability given a deterministic membrane potential.</s> <s>we find a mathematical expression for this link-function and test the ability of the glm to predict the firing probability of a neuron receiving complex stimulation.</s> <s>comparing the prediction performance of various link-functions, we find that a glm with an exponential link-function provides an excellent approximation to the adaptive exponential integrate-and-fire with colored-noise input.</s> <s>these results help to understand the relationship between the different approaches to stochastic neuron models.</s></p></d>", "label": ["<d><p><s>from stochastic nonlinear integrate-and-fire to generalized linear models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry.</s> <s>previous approaches are not high-throughput, are not generalizable or scalable, or lack sufficient data to be effective.</s> <s>we describe single mechanistic reactions as concerted electron movements from an electron orbital source to an electron orbital sink.</s> <s>we use an existing rule-based expert system to derive a dataset consisting of 2,989 productive mechanistic steps and 6.14 million non-productive mechanistic steps.</s> <s>we then pose identifying productive mechanistic steps as a ranking problem: rank potential orbital interactions such that the top ranked interactions yield the major products.</s> <s>the machine learning implementation follows a two-stage approach, in which we first train atom level reactivity filters to prune 94.0% of non-productive reactions with less than a 0.1% false negative rate.</s> <s>then, we train an ensemble of ranking models on pairs of interacting orbitals to learn a relative productivity function over single mechanistic reactions in a given system.</s> <s>without the use of explicit transformation patterns, the ensemble perfectly ranks the productive mechanisms at the top 89.1% of the time, rising to 99.9% of the time when top ranked lists with at most four non-productive reactions are considered.</s> <s>the final system allows multi-step reaction prediction.</s> <s>furthermore, it is generalizable, making reasonable predictions over reactants and conditions which the rule-based expert system does not handle.</s></p></d>", "label": ["<d><p><s>a machine learning approach to predict chemical reactions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>maximum entropy models have become popular statistical models in neuroscience and other areas in biology, and can be useful tools for obtaining estimates of mu- tual information in biological systems.</s> <s>however, maximum entropy models fit to small data sets can be subject to sampling bias; i.e.</s> <s>the true entropy of the data can be severely underestimated.</s> <s>here we study the sampling properties of estimates of the entropy obtained from maximum entropy models.</s> <s>we show that if the data is generated by a distribution that lies in the model class, the bias is equal to the number of parameters divided by twice the number of observations.</s> <s>however, in practice, the true distribution is usually outside the model class, and we show here that this misspecification can lead to much larger bias.</s> <s>we provide a perturba- tive approximation of the maximally expected bias when the true model is out of model class, and we illustrate our results using numerical simulations of an ising model; i.e.</s> <s>the second-order maximum entropy distribution on binary data.</s></p></d>", "label": ["<d><p><s>how biased are maximum entropy models?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>renewal processes are generalizations of the poisson process on the real line, whose intervals are drawn i.i.d.</s> <s>from some distribution.</s> <s>modulated renewal processes allow these distributions to vary with time, allowing the introduction nonstationarity.</s> <s>in this work, we take a nonparametric bayesian approach, modeling this nonstationarity with a gaussian process.</s> <s>our approach is based on the idea of uniformization, allowing us to draw exact samples from an otherwise intractable distribution.</s> <s>we develop a novel and efficient mcmc sampler for posterior inference.</s> <s>in our experiments, we test these on a number of synthetic and real datasets.</s></p></d>", "label": ["<d><p><s>gaussian process modulated renewal processes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present an optimization approach for linear svms based on a stochastic primal-dual approach, where the primal step is akin to an importance-weighted sgd, and the dual step is a stochastic update on the importance weights.</s> <s>this yields an optimization method with a sublinear dependence on the training set size, and the first method for learning linear svms with runtime less then the size of the training set required for learning!</s></p></d>", "label": ["<d><p><s>beating sgd: learning svms in sublinear time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many clustering problems, we have access to multiple views of the data each  of which could be individually used for clustering.</s> <s>exploiting information from  multiple views, one can hope to find a clustering that is more accurate than the  ones obtained using the individual views.</s> <s>since the true clustering would assign  a point to the same cluster irrespective of the view, we can approach this problem  by looking for clusterings that are consistent across the views, i.e., corresponding  data points in each view should have same cluster membership.</s> <s>we propose a  spectral clustering framework that achieves this goal by co-regularizing the clustering  hypotheses, and propose two co-regularization schemes to accomplish this.</s> <s>experimental comparisons with a number of baselines on two synthetic and three  real-world datasets establish the efficacy of our proposed approaches.</s></p></d>", "label": ["<d><p><s>co-regularized multi-view spectral clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the distance dependent chinese restaurant process (ddcrp) was recently introduced to accommodate random partitions of non-exchangeable data.</s> <s>the ddcrp clusters data in a biased way: each data point is more likely to be clustered with other data that are near it in an external sense.</s> <s>this paper examines the ddcrp in a spatial setting with the goal of natural image segmentation.</s> <s>we explore the biases of the spatial ddcrp model and propose a novel hierarchical extension better suited for producing \"human-like\" segmentations.</s> <s>we then study the sensitivity of the models to various distance and appearance hyperparameters, and provide the first rigorous comparison of nonparametric bayesian models in the image segmentation domain.</s> <s>on unsupervised image segmentation, we demonstrate that similar performance to existing nonparametric bayesian models is possible with substantially simpler models and algorithms.</s></p></d>", "label": ["<d><p><s>spatial distance dependent chinese restaurant processes for image segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how can we train a statistical mixture model on a massive data set?</s> <s>in this paper, we show how to construct coresets for mixtures of gaussians and natural generalizations.</s> <s>a coreset is a weighted subset of the data, which guarantees that models fitting the coreset will also provide a good fit for the original data set.</s> <s>we show that, perhaps surprisingly, gaussian mixtures admit coresets of size independent of the size of the data set.</s> <s>more precisely, we prove that a weighted set of $o(dk^3/\\eps^2)$ data points suffices for computing a $(1+\\eps)$-approximation for the optimal model on the original $n$ data points.</s> <s>moreover, such coresets can be efficiently constructed in a map-reduce style computation, as well as in a streaming setting.</s> <s>our results rely on a novel reduction of statistical estimation to problems in computational geometry, as well as new complexity results about mixtures of gaussians.</s> <s>we empirically evaluate our algorithms on several real data sets, including a density estimation problem in the context of earthquake detection using accelerometers in  mobile phones.</s></p></d>", "label": ["<d><p><s>scalable training of mixture models via coresets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items.</s> <s>knowing the age of a pattern thus becomes critical for recalling it faithfully.</s> <s>this implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit.</s> <s>using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition.</s> <s>this finding provides a new window onto actively contentious psychological and neural aspects of recognition memory.</s></p></d>", "label": ["<d><p><s>two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unlike existing nonparametric bayesian models, which rely solely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations, we study nonparametric bayesian inference with regularization on the desired posterior distributions.</s> <s>while priors can indirectly affect posterior distributions through bayes' theorem, imposing posterior regularization is arguably more direct and in some cases can be much easier.</s> <s>we particularly focus on developing infinite latent support vector machines (ilsvm) and multi-task infinite latent support vector machines (mt-ilsvm), which explore the large-margin idea in combination with a nonparametric bayesian model for discovering predictive latent features for classification and multi-task learning, respectively.</s> <s>we present efficient inference methods and report empirical studies on several benchmark datasets.</s> <s>our results appear to demonstrate the merits inherited from both large-margin learning and bayesian nonparametrics.</s></p></d>", "label": ["<d><p><s>infinite latent svm for classification and multi-task learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game.</s> <s>in addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions.</s> <s>the observation structure is encoded as a graph, where node i is linked to  node j if sampling i provides information on the reward of j.</s> <s>this setting naturally interpolates between the well-known ``experts'' setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action.</s> <s>we develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure.</s> <s>we also provide partially-matching lower bounds.</s></p></d>", "label": ["<d><p><s>from bandits to experts: on the value of side-observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory, necessitating the use of abstraction to reduce the game size.</s> <s>typically, strategies from abstract games perform better in the real game as the granularity of abstraction is increased.</s> <s>this paper investigates two techniques for stitching a base strategy in a coarse abstraction of the full game tree, to expert strategies in fine abstractions of smaller subtrees.</s> <s>we provide a general framework for creating static experts, an approach that generalizes some previous strategy stitching efforts.</s> <s>in addition, we show that static experts can create strong agents for both 2-player and 3-player leduc and limit texas hold'em poker, and that a specific class of static experts can be preferred among a number of alternatives.</s> <s>furthermore, we describe a poker agent that used static experts and won the 3-player events of the 2010 annual computer poker competition.</s></p></d>", "label": ["<d><p><s>on strategy stitching in large extensive form multiplayer games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a type of temporal restricted boltzmann machine that defines a probability distribution over an output sequence conditional on an input sequence.</s> <s>it shares the desirable properties of rbms: efficient exact inference, an exponentially more expressive latent state than hmms, and the ability to model nonlinear structure and dynamics.</s> <s>we apply our model to a challenging real-world graphics problem: facial expression transfer.</s> <s>our results demonstrate improved performance over several baselines modeling high-dimensional 2d and 3d data.</s></p></d>", "label": ["<d><p><s>facial expression transfer with input-output temporal restricted boltzmann machines</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>psychologists have long been struck by individuals' limitations in expressing their internal sensations, impressions, and evaluations via rating scales.</s> <s>instead of using an absolute scale, individuals rely on reference points from recent experience.</s> <s>this _relativity of judgment_ limits  the informativeness of responses on surveys, questionnaires, and evaluation forms.</s> <s>fortunately, the cognitive processes that map stimuli to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner.</s> <s>we explore techniques to remove sequential dependencies, and thereby  _decontaminate_ a series of ratings to obtain more meaningful human  judgments.</s> <s>in our formulation, the problem is to infer latent (subjective) impressions from a sequence of stimulus labels (e.g., movie names) and responses.</s> <s>we describe an unsupervised approach that simultaneously recovers  the impressions and parameters of a contamination model that predicts how  recent judgments affect the current response.</s> <s>we test our _iterated  impression inference_, or i^3, algorithm in three domains:  rating the gap between dots, the desirability of a movie based on an advertisement, and the morality of an action.</s> <s>we demonstrate significant objective improvements  in the quality of the recovered impressions.</s></p></d>", "label": ["<d><p><s>an unsupervised decontamination procedure for improving the reliability of human judgments</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of ising and gaussian graphical model selection given n i.i.d.</s> <s>samples from the model.</s> <s>we propose an efficient threshold-based algorithm   for structure estimation based known as  conditional mutual information test.</s> <s>this simple local algorithm    requires only low-order statistics of the data and decides    whether  two nodes   are neighbors in the unknown graph.</s> <s>under some transparent assumptions, we establish that the proposed algorithm is structurally consistent (or sparsistent)  when the number of samples scales as n= omega(j_{min}^{-4} log p), where p is the number of nodes and j_{min} is the minimum edge potential.</s> <s>we also prove novel non-asymptotic necessary conditions for graphical model selection.</s></p></d>", "label": ["<d><p><s>high-dimensional graphical model selection: tractable graph families and necessary conditions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many fundamental questions in theoretical neuroscience involve optimal  decoding and the computation of shannon information rates in  populations of spiking neurons.</s> <s>in this paper, we apply methods from  the asymptotic theory of statistical inference to obtain a clearer  analytical understanding of these quantities.</s> <s>we find that for large  neural populations carrying a finite total amount of information, the  full spiking population response is asymptotically as informative as a  single observation from a gaussian process whose mean and covariance  can be characterized explicitly in terms of network and single neuron  properties.</s> <s>the gaussian form of this asymptotic sufficient  statistic allows us in certain cases to perform optimal bayesian  decoding by simple linear transformations, and to obtain closed-form  expressions of the shannon information carried by the network.</s> <s>one  technical advantage of the theory is that it may be applied easily  even to non-poisson point process network models; for example, we find  that under some conditions, neural populations with strong  history-dependent (non-poisson) effects carry exactly the same  information as do simpler equivalent populations of non-interacting  poisson neurons with matched firing rates.</s> <s>we argue that our findings  help to clarify some results from the recent literature on neural  decoding and neuroprosthetic design.</s></p></d>", "label": ["<d><p><s>information rates and optimal decoding in large neural populations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider feature selection and weighting for nearest neighbor classifiers.</s> <s>a technical challenge in this scenario is how to cope with the discrete update of nearest neighbors when the feature space metric is changed during the learning process.</s> <s>this issue, called the target neighbor change, was not properly addressed in the existing feature weighting and metric learning literature.</s> <s>in this paper, we propose a novel feature weighting algorithm that can exactly and efficiently keep track of the correct target neighbors via sequential quadratic programming.</s> <s>to the best of our knowledge, this is the first algorithm that guarantees the consistency between target neighbors and the feature space metric.</s> <s>we further show that the proposed algorithm can be naturally combined with regularization path tracking, allowing computationally efficient selection of the regularization parameter.</s> <s>we demonstrate the effectiveness of the proposed algorithm through experiments.</s></p></d>", "label": ["<d><p><s>target neighbor consistent feature weighting for nearest neighbor classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic logics are receiving a lot of attention today because of their expressive power for knowledge representation and learning.</s> <s>however, this expressivity is detrimental to the tractability of inference, when done at the propositional level.</s> <s>to solve this problem, various lifted inference algorithms have been proposed that reason at the first-order level, about groups of objects as a whole.</s> <s>despite the existence of various lifted inference approaches, there are currently no completeness results about these algorithms.</s> <s>the key contribution of this paper is that we introduce a formal definition of lifted inference that allows us to reason about the completeness of lifted inference algorithms relative to a particular class of probabilistic models.</s> <s>we then show how to obtain a completeness result using a first-order knowledge compilation approach for theories of formulae containing up to two logical variables.</s></p></d>", "label": ["<d><p><s>on the completeness of first-order knowledge compilation for lifted probabilistic inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recovering hidden structure from complex and noisy non-linear data is one of the most fundamental problems in machine learning and statistical inference.</s> <s>while such data is often high-dimensional, it is of interest to approximate it with a low-dimensional or even one-dimensional space, since  many important aspects of  data are often intrinsically low-dimensional.</s> <s>furthermore, there are many scenarios where the underlying structure is graph-like, e.g, river/road networks or various trajectories.</s> <s>in this paper, we develop a framework to extract, as well as to simplify, a one-dimensional \"skeleton\" from unorganized data using the reeb graph.</s> <s>our algorithm is very simple, does not require complex optimizations and can be easily applied to unorganized high-dimensional data such as point clouds or  proximity graphs.</s> <s>it can also represent arbitrary graph structures in the data.</s> <s>we also give  theoretical results to justify our method.</s> <s>we  provide a number of experiments to demonstrate the effectiveness and generality of our algorithm, including comparisons to existing methods, such as principal curves.</s> <s>we believe that the simplicity and practicality of our algorithm will help to promote skeleton graphs as a data analysis tool for a broad range of applications.</s></p></d>", "label": ["<d><p><s>data skeletonization via reeb graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies privacy preserving m-estimators using perturbed histograms.</s> <s>the proposed approach allows the release of a  wide class of m-estimators with both differential privacy and statistical utility without knowing a priori the particular inference procedure.</s> <s>the performance of the proposed method is demonstrated through a careful study of the convergence rates.</s> <s>a practical algorithm is given and applied on a real world data set containing both continuous and categorical  variables.</s></p></d>", "label": ["<d><p><s>differentially private m-estimators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we extend the classical problem of predicting a sequence  of outcomes from a finite alphabet to the matrix domain.</s> <s>in this extension, the alphabet of $n$ outcomes is replaced by the set of all dyads, i.e.</s> <s>outer products $\\u\\u^\\top$ where $\\u$ is a vector in $\\r^n$ of unit length.</s> <s>whereas in the classical case the goal is to learn   (i.e.</s> <s>sequentially predict as well as) the best multinomial distribution,  in the matrix case we desire to learn the density matrix  that best explains the observed sequence of dyads.</s> <s>we show  how popular online algorithms for learning a multinomial  distribution can be extended to learn density matrices.</s> <s>intuitively, learning the $n^2$ parameters of a density matrix is much harder than learning the $n$ parameters of a multinomial distribution.</s> <s>completely surprisingly, we prove that the worst-case regrets of   certain classical algorithms and   their matrix generalizations are identical.</s> <s>the reason is that the worst-case sequence of dyads share a common   eigensystem, i.e.</s> <s>the worst case regret is achieved in the  classical case.</s> <s>so these matrix algorithms learn the eigenvectors  without any regret.</s></p></d>", "label": ["<d><p><s>learning eigenvectors for free</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many real-world applications, we often need to select correlated variables---such as genetic variations and imaging features associated with alzheimer's disease---in a high dimensional space.</s> <s>the correlation between variables presents a challenge to classical variable selection methods.</s> <s>to address this challenge, the elastic net has been developed and successfully applied to many applications.</s> <s>despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables.</s> <s>to overcome this limitation, we present a novel hybrid model, eigennet, that uses the eigenstructures of data to guide variable selection.</s> <s>specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled bayesian framework.</s> <s>we develop an efficient active-set algorithm to estimate the model via evidence maximization.</s> <s>experiments on synthetic data and imaging genetics data demonstrated the superior predictive performance of the eigennet over the lasso, the elastic net, and the automatic relevance determination.</s></p></d>", "label": ["<d><p><s>eigennet: a bayesian hybrid of generative and conditional models for sparse learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by the spread of on-line information in general and   on-line petitions in particular, recent research has raised the following   combinatorial estimation problem.</s> <s>there is a tree t that we cannot observe directly (representing  the structure along which the information has spread), and certain  nodes randomly decide to make their copy of the information public.</s> <s>in the case of a petition, the list of names on each public copy   of the petition also reveals a path leading back to the root of the tree.</s> <s>what can we conclude about the properties of the tree we observe  from these revealed paths,  and can we use the structure of the observed tree  to estimate the size of the full unobserved tree t?</s> <s>here we provide the first algorithm for this size estimation task,  together with provable guarantees on its performance.</s> <s>we also establish structural properties of the observed tree, providing the  first rigorous explanation for some of the unusual structural  phenomena present in the spread of real chain-letter petitions  on the internet.</s></p></d>", "label": ["<d><p><s>reconstructing patterns of information diffusion from incomplete observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present explicit classes of probability distributions that can be learned by restricted boltzmann machines (rbms) depending on the number of units that they contain, and which are representative for the expressive power of the model.</s> <s>we use this to show that the maximal kullback-leibler divergence to the rbm model with n visible and m hidden units is bounded from above by (n-1)-log(m+1).</s> <s>in this way we can specify the number of hidden units that guarantees a sufficiently rich model containing different classes of distributions and respecting a given error tolerance.</s></p></d>", "label": ["<d><p><s>expressive power and approximation errors of restricted boltzmann machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of finding the nearest neighbor (or    one of the $r$-nearest neighbors) of a query object $q$ in a    database of $n$ objects, when we can only use a comparison    oracle.</s> <s>the comparison oracle, given two reference objects and a    query object, returns the reference object most similar to the query    object.</s> <s>the main problem we study is how to search the database for    the nearest neighbor (nn) of a query, while minimizing the    questions.</s> <s>the difficulty of this problem depends on properties of    the underlying database.</s> <s>we show the importance of a    characterization: \\emph{combinatorial disorder} $d$ which defines    approximate triangle inequalities on ranks.</s> <s>we present a lower bound    of $\\omega(d\\log \\frac{n}{d}+d^2)$ average number of questions in    the search phase for any randomized algorithm, which demonstrates    the fundamental role of $d$ for worst case behavior.</s> <s>we develop a    randomized scheme for nn retrieval in $o(d^3\\log^2 n+ d\\log^2 n    \\log\\log n^{d^3})$ questions.</s> <s>the learning requires asking $o(n    d^3\\log^2 n+ d \\log^2 n \\log\\log n^{d^3})$ questions and    $o(n\\log^2n/\\log(2d))$ bits to store.</s></p></d>", "label": ["<d><p><s>randomized algorithms for comparison-based search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>machine learning competitions such as the netflix prize have proven reasonably successful as a method of ?crowdsourcing?</s> <s>prediction tasks.</s> <s>but these compe- titions have a number of weaknesses, particularly in the incentive structure they create for the participants.</s> <s>we propose a new approach, called a crowdsourced learning mechanism, in which participants collaboratively ?learn?</s> <s>a hypothesis for a given prediction task.</s> <s>the approach draws heavily from the concept of a prediction market, where traders bet on the likelihood of a future event.</s> <s>in our framework, the mechanism continues to publish the current hypothesis, and par- ticipants can modify this hypothesis by wagering on an update.</s> <s>the critical in- centive property is that a participant will profit an amount that scales according to how much her update improves performance on a released test set.</s></p></d>", "label": ["<d><p><s>a collaborative mechanism for crowdsourcing prediction problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a statistical framework in which recurrent networks of spiking neurons learn to generate spatio-temporal spike patterns.</s> <s>given biologically realistic stochastic neuronal dynamics we derive a tractable learning rule for the synaptic weights towards hidden and visible neurons that leads to optimal recall of the training sequences.</s> <s>we show that learning synaptic weights towards hidden neurons significantly improves the storing capacity of the network.</s> <s>furthermore, we derive an approximate online learning rule and show that our learning rule is consistent with spike-timing dependent plasticity in that if a presynaptic spike shortly precedes a postynaptic spike, potentiation is induced and otherwise depression is elicited.</s></p></d>", "label": ["<d><p><s>sequence learning with hidden units in spiking neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>factored decentralized partially observable markov decision processes (dec-pomdps) form a powerful framework for multiagent planning under uncertainty, but optimal solutions require a rigid history-based policy representation.</s> <s>in this paper we allow inter-agent communication which turns the problem in a centralized multiagent pomdp (mpomdp).</s> <s>we map belief distributions over state factors to an agent's local actions by exploiting structure in the joint mpomdp policy.</s> <s>the key point is that when sparse dependencies between the agents' decisions exist, often the belief over its local state factors is sufficient for an agent to unequivocally identify the optimal action, and communication can be avoided.</s> <s>we formalize these notions by casting the problem into convex optimization form, and present experimental results illustrating the savings in communication that we can obtain.</s></p></d>", "label": ["<d><p><s>efficient offline communication policies for factored multiagent pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the problem of accurately recovering a sparse vector $\\beta^{\\star}$ from highly corrupted linear measurements $y = x \\beta^{\\star} + e^{\\star} + w$ where $e^{\\star}$ is a sparse error vector whose nonzero entries may be unbounded and $w$ is a bounded noise.</s> <s>we propose a so-called extended lasso optimization which takes into consideration sparse prior information of both $\\beta^{\\star}$ and $e^{\\star}$.</s> <s>our first result shows that the extended lasso can faithfully recover both the regression and the corruption vectors.</s> <s>our analysis is relied on a notion of extended restricted eigenvalue for the design matrix $x$.</s> <s>our second set of results applies to a general class of gaussian design matrix $x$ with i.i.d rows $\\oper n(0, \\sigma)$, for which we provide a surprising phenomenon: the extended lasso can recover exact signed supports of both $\\beta^{\\star}$ and $e^{\\star}$ from only $\\omega(k \\log p \\log n)$ observations, even the fraction of corruption is arbitrarily close to one.</s> <s>our analysis also shows that this amount of observations required to achieve exact signed support is optimal.</s></p></d>", "label": ["<d><p><s>robust lasso with missing and grossly corrupted observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive a plausible learning rule updating the synaptic efficacies   for feedforward, feedback and lateral connections between observed and latent neurons.</s> <s>operating in the context of a generative model for distributions of spike sequences,    the learning mechanism is derived from variational inference principles.</s> <s>the synaptic plasticity rules found   are interesting in that they are strongly reminiscent of experimentally found results on spike time    dependent plasticity, and in that they differ for excitatory and inhibitory neurons.</s> <s>a simulation   confirms the method's applicability to learning both stationary and temporal spike patterns.</s></p></d>", "label": ["<d><p><s>variational learning for recurrent spiking networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider a sequence of bits where we are trying to predict the next bit from the previous bits.</s> <s>assume we are allowed to say `predict 0' or `predict 1', and our payoff is $+1$ if the prediction is correct and $-1$ otherwise.</s> <s>we will say that at each point in time the loss of an algorithm is the number of wrong predictions minus the number of right predictions so far.</s> <s>in this paper we are interested in algorithms that have essentially zero (expected) loss over any string at any point in time and yet have small regret with respect to always predicting $0$ or always predicting $1$.</s> <s>for a sequence of length $t$ our algorithm has  regret $14\\epsilon t $ and loss   $2\\sqrt{t}e^{-\\epsilon^2 t} $ in expectation for all strings.</s> <s>we show that the tradeoff between loss and regret is optimal up to constant factors.</s> <s>our techniques extend to the general setting of $n$ experts, where the related problem of trading off regret to the best expert for regret to the 'special' expert has been studied by even-dar et al.</s> <s>(colt'07).</s> <s>we obtain essentially zero loss with respect to the special expert and optimal loss/regret tradeoff,  improving upon the results of even-dar et al (colt'07) and settling the main question left open in their paper.</s> <s>the strong loss bounds of the algorithm have some surprising consequences.</s> <s>first, we obtain a parameter free algorithm for the experts problem that has optimal regret bounds with respect to $k$-shifting optima, i.e.</s> <s>bounds with respect to the optimum that is allowed to change arms multiple times.</s> <s>moreover, for {\\em any window of size $n$} the regret of our algorithm to any expert never exceeds $o(\\sqrt{n(\\log n+\\log t)})$, where $n$ is the number of experts and $t$ is the time horizon, while maintaining the essentially zero loss property.</s></p></d>", "label": ["<d><p><s>prediction strategies without loss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the f-measure, originally introduced in information retrieval, is nowadays routinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction.</s> <s>optimizing this measure remains a statistically and computationally challenging problem, since no closed-form maximizer exists.</s> <s>current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables.</s> <s>in this paper, we present an algorithm which is not only computationally efficient but also exact, regardless of the underlying distribution.</s> <s>the algorithm requires only a quadratic number of parameters of the joint distribution  (with respect to the number of binary responses).</s> <s>we illustrate its practical performance by means of experimental results for multi-label classification.</s></p></d>", "label": ["<d><p><s>an exact algorithm for f-measure maximization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>stochastic gradient descent (sgd) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine  learning tasks.</s> <s>several researchers have recently proposed schemes  to parallelize sgd, but all require  performance-destroying memory locking and synchronization.</s> <s>this work aims to show using novel theoretical analysis, algorithms,  and implementation that sgd can be implemented *without any locking*.</s> <s>we present an update scheme called hogwild which allows processors access to shared memory with the possibility of overwriting each other's work.</s> <s>we show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then hogwild achieves a nearly optimal rate of convergence.</s> <s>we demonstrate experimentally that hogwild outperforms alternative schemes that use locking by an order of magnitude.</s></p></d>", "label": ["<d><p><s>hogwild: a lock-free approach to parallelizing stochastic gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>loopy belief propagation performs approximate inference on graphical models with loops.</s> <s>one might hope to compensate for the approximation by adjusting model parameters.</s> <s>learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model.</s> <s>on the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm.</s> <s>we call such marginals `unbelievable.'</s> <s>this problem occurs whenever the hessian of the bethe free energy is not positive-definite at the target marginals.</s> <s>all learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation.</s> <s>we then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.</s></p></d>", "label": ["<d><p><s>learning unbelievable probabilities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many real-world networks are described by both connectivity information and features for every node.</s> <s>to better model and understand these networks, we present structure preserving metric learning (spml), an algorithm for learning a mahalanobis distance metric from a network such that the learned distances are tied to the inherent connectivity structure of the network.</s> <s>like the graph embedding algorithm structure preserving embedding, spml learns a metric which is structure preserving, meaning a connectivity algorithm such as k-nearest neighbors will yield the correct connectivity when applied using the distances from the learned metric.</s> <s>we show a variety of synthetic and real-world experiments where spml predicts link patterns from node features more accurately than standard techniques.</s> <s>we further demonstrate a method for optimizing spml based on stochastic gradient descent which removes the running-time dependency on the size of the network and allows the method to easily scale to networks of thousands of nodes and millions of edges.</s></p></d>", "label": ["<d><p><s>learning a distance metric from a network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a model of human visual search is proposed.</s> <s>it predicts both response time (rt) and error rates (rt) as a function of image parameters such as target contrast and clutter.</s> <s>the model is an ideal observer, in that it optimizes the bayes ratio of tar- get present vs target absent.</s> <s>the ratio is computed on the firing pattern of v1/v2 neurons, modeled by poisson distributions.</s> <s>the optimal mechanism for integrat- ing information over time is shown to be a ?soft max?</s> <s>of diffusions, computed over the visual field by ?hypercolumns?</s> <s>of neurons that share the same receptive field and have different response properties to image features.</s> <s>an approximation of the optimal bayesian observer, based on integrating local decisions, rather than diffusions, is also derived; it is shown experimentally to produce very similar pre- dictions.</s> <s>a psychophyisics experiment is proposed that may discriminate between which mechanism is used in the human brain.</s></p></d>", "label": ["<d><p><s>predicting response time and error rates in visual search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent tree graphical models are natural tools for expressing long range and hierarchical dependencies among many variables which are common in computer vision, bioinformatics and natural language processing problems.</s> <s>however, existing models are largely restricted to discrete and gaussian variables due to computational constraints; furthermore, algorithms for estimating the latent tree structure and learning the model parameters are largely restricted to heuristic local search.</s> <s>we present a method based on kernel embeddings of distributions for latent tree graphical models with continuous and non-gaussian variables.</s> <s>our method can recover the latent tree structures with provable guarantees and perform local-minimum free parameter learning and efficient inference.</s> <s>experiments on simulated and real data show the advantage of our proposed approach.</s></p></d>", "label": ["<d><p><s>kernel embeddings of latent tree graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the piecewise-constant conditional intensity model, a model for learning temporal dependencies in event streams.</s> <s>we describe a closed-form bayesian approach to learning these models, and describe an importance sampling algorithm for forecasting future events using these models, using a proposal distribution based on poisson superposition.</s> <s>we then use synthetic data, supercomputer event logs, and web search query logs to illustrate that our learning algorithm can efficiently learn nonlinear temporal dependencies, and that our importance sampling algorithm can effectively forecast future events.</s></p></d>", "label": ["<d><p><s>a model for temporal dependencies in event streams</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>crowdsourcing systems, in which tasks are electronically distributed to numerous ``information piece-workers'', have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading.</s> <s>because these low-paid workers can be unreliable, nearly all crowdsourcers must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in some way such as majority voting.</s> <s>in this paper, we consider a general model of such  rowdsourcing tasks, and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability.</s> <s>we give new algorithms for deciding which tasks to assign to which workers and for inferring correct answers from the workers?</s> <s>answers.</s> <s>we show that our algorithm significantly outperforms majority voting and, in fact, are asymptotically optimal through comparison to an oracle that knows the reliability of every worker.</s></p></d>", "label": ["<d><p><s>iterative learning for reliable crowdsourcing systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cancer has complex patterns of progression that include converging as  well as diverging progressional pathways.</s> <s>vogelstein's path model of  colon cancer was a pioneering contribution to cancer research.</s> <s>since  then, several attempts have been made at obtaining mathematical models  of cancer progression, devising learning algorithms, and applying  these to cross-sectional data.</s> <s>beerenwinkel {\\em et al.}</s> <s>provided,  what they coined, em-like algorithms for oncogenetic trees (ots) and  mixtures of such.</s> <s>given the small size of current and future data  sets, it is important to minimize the number of parameters of a  model.</s> <s>for this reason, we too focus on tree-based models and  introduce hidden-variable oncogenetic trees (hots).</s> <s>in contrast to  ots, hots allow for errors in the data and thereby provide more  realistic modeling.</s> <s>we also design global structural em algorithms for  learning hots and mixtures of hots (hot-mixtures).</s> <s>the algorithms are  global in the sense that, during the m-step, they find a structure  that yields a global maximum of the expected complete log-likelihood  rather than merely one that improves it.</s> <s>the algorithm for single hots  performs very well on reasonable-sized data sets, while that for  hot-mixtures requires data sets of sizes obtainable only with  tomorrow's more cost-efficient technologies.</s></p></d>", "label": ["<d><p><s>a global structural em algorithm for a model of cancer progression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the problem of semi-supervised learning from the vector field perspective.</s> <s>many of the existing work use the graph laplacian to ensure the smoothness of the prediction function on the data manifold.</s> <s>however, beyond smoothness, it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems.</s> <s>to achieve this goal, we show that the second order smoothness measures the linearity of the function, and the gradient field of a linear function has to be a parallel vector field.</s> <s>consequently, we propose to find a function which minimizes the empirical error, and simultaneously requires its gradient field to be as parallel as possible.</s> <s>we give a continuous objective function on the manifold and discuss how to discretize it by using random points.</s> <s>the discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently.</s> <s>the experimental results have demonstrated the effectiveness of our proposed approach.</s></p></d>", "label": ["<d><p><s>semi-supervised regression via parallel field regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>metric learning has become a very active research field.</s> <s>the most popular representative--mahalanobis metric learning--can be seen as learning a linear transformation and then computing the euclidean metric in the transformed space.</s> <s>since a linear transformation might not always be appropriate for a given learning problem, kernelized versions of various metric learning algorithms exist.</s> <s>however, the problem then becomes finding the appropriate kernel function.</s> <s>multiple kernel learning addresses this limitation by learning a linear combination of a number of predefined kernels; this approach can be also readily used in the context of multiple-source learning to fuse different data sources.</s> <s>surprisingly, and despite the extensive work on multiple kernel learning for svms, there has been no work in   the area of metric learning with multiple kernel learning.</s> <s>in this paper we fill this gap and present a general approach for metric learning with multiple kernel learning.</s> <s>our approach can be instantiated with different metric learning algorithms provided that they satisfy some constraints.</s> <s>experimental evidence suggests that our approach outperforms metric learning with an unweighted kernel combination and metric learning with cross-validation based kernel selection.</s></p></d>", "label": ["<d><p><s>metric learning with multiple kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nonparametric bayesian methods are developed for analysis of multi-channel  spike-train data, with the feature learning and spike sorting performed jointly.</s> <s>the feature learning and sorting are performed simultaneously across all channels.</s> <s>dictionary learning is implemented via the beta-bernoulli process, with  spike sorting performed via the dynamic hierarchical dirichlet process (dhdp),  with these two models coupled.</s> <s>the dhdp is augmented to eliminate refractoryperiod  violations, it allows the ?appearance?</s> <s>and ?disappearance?</s> <s>of neurons over  time, and it models smooth variation in the spike statistics.</s></p></d>", "label": ["<d><p><s>on the analysis of multi-channel neural spike data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider large matrices of low rank.</s> <s>we address the problem of recovering such matrices when most of the entries are unknown.</s> <s>matrix completion finds applications in recommender systems.</s> <s>in this setting, the rows of the matrix may correspond to items and the columns may correspond to users.</s> <s>the known entries are the ratings given by users to some items.</s> <s>the aim is to predict the unobserved ratings.</s> <s>this problem is commonly stated in a constrained optimization framework.</s> <s>we follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the grassmann manifold.</s> <s>we then apply first- and second-order riemannian trust-region methods to solve it.</s> <s>the cost of each iteration is linear in the number of known entries.</s> <s>our methods, rtrmc 1 and 2, outperform state-of-the-art algorithms on a wide range of problem instances.</s></p></d>", "label": ["<d><p><s>rtrmc: a riemannian trust-region method for low-rank matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>minwise hashing is a standard technique in the context of search for efficiently computing set similarities.</s> <s>the recent development of b-bit minwise hashing provides a  substantial improvement by storing only the lowest b bits of each hashed value.</s> <s>in this paper, we demonstrate that  b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear svm and logistic regression, to solve large-scale and high-dimensional statistical learning tasks, especially when the data do not fit in memory.</s> <s>we  compare $b$-bit minwise hashing with  the count-min (cm)  and  vowpal wabbit (vw) algorithms, which have essentially the same variances as random projections.</s> <s>our theoretical and empirical comparisons illustrate that b-bit minwise hashing is significantly more accurate (at the same storage cost) than vw (and random projections) for binary data.</s></p></d>", "label": ["<d><p><s>hashing algorithms for large-scale learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been argued that perceptual multistability reflects probabilistic inference performed by the brain when sensory input is ambiguous.</s> <s>alternatively, more traditional explanations of multistability refer to low-level mechanisms such as neuronal adaptation.</s> <s>we employ a deep boltzmann machine (dbm) model of cortical processing to demonstrate that these two different approaches can be combined in the same framework.</s> <s>based on recent developments in machine learning, we show how neuronal adaptation can be understood as a mechanism that improves probabilistic, sampling-based inference.</s> <s>using the ambiguous necker cube image, we analyze the perceptual switching exhibited by the model.</s> <s>we also examine the influence of spatial attention, and explore how binocular rivalry can be modeled with the same approach.</s> <s>our work joins earlier studies in demonstrating how the principles underlying dbms relate to cortical processing, and offers novel perspectives on the neural implementation of approximate probabilistic inference in the brain.</s></p></d>", "label": ["<d><p><s>neuronal adaptation for sampling-based probabilistic inference in perceptual bistability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inferring key unobservable features of individuals is an important task in the applied sciences.</s> <s>in particular, an important source of data in fields such as marketing, social sciences and medicine is questionnaires: answers in such questionnaires are noisy measures of target unobserved features.</s> <s>while comprehensive surveys help to better estimate the latent variables of interest, aiming at a high number of questions comes at a price: refusal to participate in surveys can go up, as well as the rate of missing data; quality of answers can decline; costs associated with applying such questionnaires can also increase.</s> <s>in this paper, we cast the problem of refining existing models for questionnaire data as follows: solve a constrained optimization problem of preserving the maximum amount of information found in a latent variable model using only a subset of existing questions.</s> <s>the goal is to find an optimal subset of a given size.</s> <s>for that, we first define an information theoretical measure for quantifying the quality of a reduced questionnaire.</s> <s>three different approximate inference methods are introduced to solve this problem.</s> <s>comparisons against a simple but powerful heuristic are presented.</s></p></d>", "label": ["<d><p><s>thinning measurement models and questionnaire design</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many of the state-of-the-art computer vision algorithms, image segmentation is an important preprocessing step.</s> <s>as such, several image segmentation algorithms have been proposed, however, with certain reservation due to high computational load and many hand-tuning parameters.</s> <s>correlation clustering, a graph-partitioning algorithm often used in natural language processing and document clustering, has the potential to perform better than previously proposed image segmentation algorithms.</s> <s>we improve the basic correlation clustering formulation by taking into account higher-order cluster relationships.</s> <s>this improves clustering in the presence of local boundary ambiguities.</s> <s>we first apply the pairwise correlation clustering to image segmentation over a pairwise superpixel graph and then develop higher-order correlation clustering over a hypergraph that considers higher-order relations among superpixels.</s> <s>fast inference is possible by linear programming relaxation, and also effective parameter learning framework by structured support vector machine is possible.</s> <s>experimental results on various datasets show that the proposed higher-order correlation clustering outperforms other state-of-the-art image segmentation algorithms.</s></p></d>", "label": ["<d><p><s>higher-order correlation clustering for image segmentation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>variational message passing (vmp) is an algorithmic implementation of the variational bayes (vb) method which applies only in the special case of conjugate exponential family models.</s> <s>we propose an extension to vmp, which we refer to as non-conjugate variational message passing (ncvmp) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: infer.net.</s> <s>we demonstrate ncvmp on logistic binary and multinomial regression.</s> <s>in the multinomial case we introduce a novel variational bound for the softmax factor which is tighter than other commonly used bounds whilst maintaining computational tractability.</s></p></d>", "label": ["<d><p><s>non-conjugate variational message passing for multinomial and binary regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>high dimensional similarity search in large scale databases becomes an important challenge due to the advent of internet.</s> <s>for such applications, specialized data structures are required to achieve computational efficiency.</s> <s>traditional approaches relied on algorithmic constructions that are often data independent (such as locality sensitive hashing) or weakly dependent (such as kd-trees, k-means trees).</s> <s>while supervised learning algorithms have been applied to related problems, those proposed in the literature mainly focused on learning hash codes optimized for compact embedding of the data rather than search efficiency.</s> <s>consequently such an embedding has to be used with linear scan or another search algorithm.</s> <s>hence learning to hash does not directly address the search efficiency issue.</s> <s>this paper considers a new framework that applies supervised learning to directly optimize a data structure that supports efficient large scale search.</s> <s>our approach takes both search quality and computational cost into consideration.</s> <s>specifically, we learn a boosted search forest that is optimized using pair-wise similarity labeled examples.</s> <s>the output of this search forest can be efficiently converted into an inverted indexing data structure, which can leverage modern text search infrastructure to achieve both scalability and efficiency.</s> <s>experimental results show that our approach significantly outperforms the start-of-the-art learning to hash methods (such as spectral hashing), as well as state-of-the-art high dimensional search algorithms (such as lsh and k-means trees).</s></p></d>", "label": ["<d><p><s>learning to search efficiently in high dimensions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the exploration-exploitation trade-off is among the central challenges of reinforcement learning.</s> <s>the optimal bayesian solution is intractable in general.</s> <s>this paper studies to what extent analytic statements about optimal learning are possible if all beliefs are gaussian processes.</s> <s>a first order approximation of learning of both loss and dynamics, for nonlinear, time-varying systems in continuous time and space, subject to a relatively weak restriction on the dynamics, is described by an infinite-dimensional partial differential equation.</s> <s>an approximate finite-dimensional projection gives an impression for how this result may be helpful.</s></p></d>", "label": ["<d><p><s>optimal reinforcement learning for gaussian systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neurons typically respond to a restricted number of stimulus features within the high-dimensional space of natural stimuli.</s> <s>here we describe an explicit model-based interpretation of traditional estimators for a neuron's multi-dimensional feature space, which allows for several important generalizations and extensions.</s> <s>first, we show that traditional estimators based on the spike-triggered average (sta) and spike-triggered covariance (stc) can be formalized in terms of the \"expected log-likelihood\" of a linear-nonlinear-poisson (lnp) model with gaussian stimuli.</s> <s>this model-based formulation allows us to define maximum-likelihood and bayesian estimators that are statistically consistent and efficient in a wider variety of settings, such as with naturalistic (non-gaussian) stimuli.</s> <s>it also allows us to employ bayesian methods for regularization, smoothing, sparsification, and model comparison, and provides bayesian confidence intervals on model parameters.</s> <s>we describe an empirical bayes method for selecting the number of features, and extend the model to accommodate an arbitrary elliptical nonlinear response function, which results in a more powerful and more flexible model for feature space inference.</s> <s>we validate these methods using neural data recorded extracellularly from macaque primary visual cortex.</s></p></d>", "label": ["<d><p><s>bayesian spike-triggered covariance analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a hallmark of modern machine learning is its ability to deal with high dimensional problems by exploiting structural assumptions that limit the degrees of freedom in the underlying model.</s> <s>a deep understanding of the capabilities and limits of high dimensional learning methods under specific assumptions such as sparsity, group sparsity, and low rank has been attained.</s> <s>efforts (negahban et al., 2010, chandrasekaran et al., 2010} are now underway to distill this valuable experience by proposing general unified frameworks that can achieve the twin goals of summarizing previous analyses and enabling their application to notions of structure hitherto unexplored.</s> <s>inspired by these developments, we propose and analyze a general computational scheme based on a greedy strategy to solve convex optimization problems that arise when dealing with structurally constrained high-dimensional problems.</s> <s>our framework not only unifies existing greedy algorithms by recovering them as special cases but also yields novel ones.</s> <s>finally, we extend our results to infinite dimensional problems by using interesting connections between smoothness of norms and behavior of martingales in banach spaces.</s></p></d>", "label": ["<d><p><s>greedy algorithms for structurally constrained high dimensional problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that for a general class of convex online learning  problems, mirror descent can always achieve a (nearly) optimal regret guarantee.</s></p></d>", "label": ["<d><p><s>on the universality of online mirror descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a nonparametric kernel-based method for realizing bayes' rule is proposed, based on kernel representations of probabilities in reproducing kernel hilbert spaces.</s> <s>the prior and conditional probabilities are expressed as empirical kernel mean and covariance operators, respectively, and the kernel mean of the posterior distribution is computed in the form of a weighted sample.</s> <s>the kernel bayes' rule can be applied to a wide variety of bayesian inference problems: we demonstrate bayesian computation without likelihood, and filtering with a nonparametric state-space model.</s> <s>a consistency rate for the posterior estimate is established.</s></p></d>", "label": ["<d><p><s>kernel bayes' rule</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of selecting the right state-representation in a reinforcement learning problem is considered.</s> <s>several models (functions mapping past observations to a finite set) of the observations are given, and it is known   that for at least one of these models the resulting state dynamics are indeed markovian.</s> <s>without  knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting   mdp, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several).</s> <s>we propose an algorithm that achieves that,   with a regret of order t^{2/3} where t is the horizon time.</s></p></d>", "label": ["<d><p><s>selecting the state-representation in reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of estimating neural spikes from extracellular voltage recordings.</s> <s>most current methods are based on clustering, which requires substantial human supervision and produces systematic errors by failing to properly handle temporally overlapping spikes.</s> <s>we formulate the problem as one of statistical inference, in which the recorded voltage is a noisy sum of the spike trains of each neuron convolved with its associated spike waveform.</s> <s>joint maximum-a-posteriori (map) estimation of the waveforms and spikes is then a blind deconvolution problem in which the coefficients are sparse.</s> <s>we develop a block-coordinate descent method for approximating the map solution.</s> <s>we validate our method on data simulated according to the generative model, as well as on real data for which ground truth is available via simultaneous intracellular recordings.</s> <s>in both cases, our method substantially reduces the number of missed spikes and false positives when compared to a standard clustering algorithm, primarily by recovering temporally overlapping spikes.</s> <s>the method offers a fully automated alternative to clustering methods that is less susceptible to systematic errors.</s></p></d>", "label": ["<d><p><s>a blind sparse deconvolution method for neural spike identification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem.</s> <s>in particular, we show that a simple modification of auer?s ucb algorithm (auer, 2002) achieves with high probability constant regret.</s> <s>more importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by auer (2002), dani et al.</s> <s>(2008), rusmevichientong and tsitsiklis (2010), li et al.</s> <s>(2010).</s> <s>our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement.</s> <s>in both cases, the improvement stems from the construction of smaller confidence sets.</s> <s>for their construction we use a novel tail inequality for vector-valued martingales.</s></p></d>", "label": ["<d><p><s>improved algorithms for linear stochastic bandits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for most scene understanding tasks (such as object detection or depth estimation), the classifiers need to consider contextual information in addition to the local features.</s> <s>we can capture such contextual information by taking as input the features/attributes from all the regions in the image.</s> <s>however, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location.</s> <s>this results in a very large number of parameters.</s> <s>in this work, we model the independence properties between the parameters for each location and for each task, by defining a markov random field (mrf) over the parameters.</s> <s>in particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close.</s> <s>our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead.</s> <s>in extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks.</s></p></d>", "label": ["<d><p><s><var>\\theta</var></s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a probabilistic algorithm for nonlinear inverse reinforcement learning.</s> <s>the goal of inverse reinforcement learning is to learn the reward function in a markov decision process from expert demonstrations.</s> <s>while most prior inverse reinforcement learning algorithms represent the reward as a linear combination of a set of features, we use gaussian processes to learn the reward as a nonlinear function, while also determining the relevance of each feature to the expert's policy.</s> <s>our probabilistic algorithm allows complex behaviors to be captured from suboptimal stochastic demonstrations, while automatically balancing the simplicity of the learned reward structure against its consistency with the observed actions.</s></p></d>", "label": ["<d><p><s>nonlinear inverse reinforcement learning with gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>log-linear models are widely used probability models for statistical pattern recognition.</s> <s>typically, log-linear models are trained according to a convex criterion.</s> <s>in recent years, the interest in log-linear models has greatly increased.</s> <s>the optimization of log-linear model parameters is costly and therefore an important topic, in particular for large-scale applications.</s> <s>different optimization algorithms have been evaluated empirically in many papers.</s> <s>in this work, we analyze the optimization problem analytically and show that the training of log-linear models can be highly ill-conditioned.</s> <s>we verify our findings on two handwriting tasks.</s> <s>by making use of our convergence analysis, we obtain good results on a large-scale continuous handwriting recognition task with a simple and generic approach.</s></p></d>", "label": ["<d><p><s>a convergence analysis of log-linear training</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while loopy belief propagation (lbp) has been utilized in a wide variety of applications with empirical success, it comes with few theoretical guarantees.</s> <s>especially, if the interactions of random variables in a graphical model are strong, the behaviors of the algorithm can be difficult to analyze due to underlying phase transitions.</s> <s>in this paper, we develop a novel approach to the uniqueness problem of the lbp fixed point; our new ?necessary and sufficient?</s> <s>condition is stated in terms of graphs and signs, where the sign denotes the types (attractive/repulsive) of the interaction (i.e., compatibility function) on the edge.</s> <s>in all previous works, uniqueness is guaranteed only in the situations where the strength of the interactions are ?sufficiently?</s> <s>small in certain senses.</s> <s>in contrast, our condition covers arbitrary strong interactions on the specified class of signed graphs.</s> <s>the result of this paper is based on the recent theoretical advance in the lbp algorithm; the connection with the graph zeta function.</s></p></d>", "label": ["<d><p><s>uniqueness of belief propagation on signed graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given one feature of a novel animal, humans readily make inferences about other features of the animal.</s> <s>for example, winged creatures often fly, and creatures that eat fish often live in the water.</s> <s>we explore the knowledge that supports these inferences and compare two approaches.</s> <s>the first approach proposes that humans rely on abstract representations of dependency relationships between features, and is formalized here as a graphical model.</s> <s>the second approach proposes that humans rely on specific knowledge of previously encountered animals, and is formalized here as a family of exemplar models.</s> <s>we evaluate these models using a task where participants reason about chimeras, or animals with pairs of features that have not previously been observed to co-occur.</s> <s>the results support the hypothesis that humans rely on explicit representations of relationships between features.</s></p></d>", "label": ["<d><p><s>inductive reasoning about chimeric creatures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a particular class of cyclic causal models, where each variable is a (possibly nonlinear) function of its parents and additive noise.</s> <s>we prove that the causal graph of such models is generically identifiable in the bivariate, gaussian-noise case.</s> <s>we also propose a method to learn such models from observational data.</s> <s>in the acyclic case, the method reduces to ordinary regression, but in the more challenging cyclic case, an additional term arises in the loss function, which makes it a special case of nonlinear independent component analysis.</s> <s>we illustrate the proposed method on synthetic data.</s></p></d>", "label": ["<d><p><s>on causal discovery with cyclic additive noise models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>increasingly, optimization problems in machine learning, especially those arising from high-dimensional statistical estimation, have a large number of variables.</s> <s>modern statistical estimators developed over the past decade have statistical or sample complexity that depends only weakly on the number of parameters when there is some structure to the problem, such as sparsity.</s> <s>a central question is whether similar advances can be made in their computational complexity as well.</s> <s>in this paper, we propose strategies that indicate that such advances can indeed be made.</s> <s>in particular, we investigate the greedy coordinate descent algorithm, and note that performing the greedy step efficiently weakens the costly dependence on the problem size provided the solution is sparse.</s> <s>we then propose a suite of methods that perform these greedy steps efficiently by a reduction to nearest neighbor search.</s> <s>we also devise a more amenable form of greedy descent for composite non-smooth objectives; as well as several approximate variants of such greedy descent.</s> <s>we develop a practical implementation of our algorithm that combines greedy coordinate descent with locality sensitive hashing.</s> <s>without tuning the latter data structure, we are not only able to significantly speed up the vanilla greedy method, but also outperform cyclic descent when the problem size becomes large.</s> <s>our results indicate the effectiveness of our nearest neighbor strategies, and also point to many open questions regarding the development of computational geometric techniques tailored towards first-order optimization methods.</s></p></d>", "label": ["<d><p><s>nearest neighbor based greedy coordinate descent</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>components estimated by independent component analysis and related methods  are typically not independent in real data.</s> <s>a very common form of nonlinear  dependency between the components is correlations in their variances or ener-  gies.</s> <s>here, we propose a principled probabilistic model to model the energy-  correlations between the latent variables.</s> <s>our two-stage model includes a linear  mixing of latent signals into the observed ones like in ica.</s> <s>the main new fea-  ture is a model of the energy-correlations based on the structural equation model  (sem), in particular, a linear non-gaussian sem.</s> <s>the sem is closely related to  divisive normalization which effectively reduces energy correlation.</s> <s>our new two-  stage model enables estimation of both the linear mixing and the interactions re-  lated to energy-correlations, without resorting to approximations of the likelihood  function or other non-principled approaches.</s> <s>we demonstrate the applicability of  our method with synthetic dataset, natural images and brain signals.</s></p></d>", "label": ["<d><p><s>structural equations and divisive normalization for energy-dependent component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper examines the problem of ranking a collection of objects using pairwise comparisons (rankings of two objects).</s> <s>in general, the ranking of $n$ objects can be identified by standard sorting methods using $n\\log_2 n$ pairwise comparisons.</s> <s>we are interested in natural situations in which relationships among the objects may allow for ranking using far fewer pairwise comparisons.</s> <s>{specifically, we assume that the objects can be embedded into a $d$-dimensional euclidean space and that the rankings reflect their relative distances from a common reference point in $\\r^d$.</s> <s>we show that under this assumption the number of possible rankings grows like $n^{2d}$ and demonstrate an algorithm that can identify a randomly selected ranking using just slightly more than $d\\log n$ adaptively selected pairwise comparisons, on average.}</s> <s>if instead the comparisons are chosen at random, then almost all pairwise comparisons must be made in order to identify any ranking.</s> <s>in addition, we propose a robust, error-tolerant algorithm that only requires that the pairwise comparisons are probably correct.</s> <s>experimental studies with synthetic and real datasets support the conclusions of our theoretical analysis.</s></p></d>", "label": ["<d><p><s>active ranking using pairwise comparisons</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>given a set $v$ of  $n$ elements we wish to linearly order them using pairwise preference labels  which may be non-transitive (due to irrationality or arbitrary noise).</s> <s>the goal is to linearly order the elements while disagreeing with  as few pairwise preference labels as possible.</s> <s>our performance is measured by two parameters:  the number of disagreements (loss) and the query complexity (number of pairwise preference labels).</s> <s>our algorithm adaptively queries  at most $o(n\\poly(\\log n,\\eps^{-1}))$ preference labels for a regret of  $\\eps$ times the optimal loss.</s> <s>this is strictly better, and often significantly better than what  non-adaptive sampling could achieve.</s> <s>our main result helps settle an open problem posed by   learning-to-rank (from pairwise information) theoreticians and practitioners:  what is a provably correct way to sample preference labels?</s></p></d>", "label": ["<d><p><s>active learning ranking from pairwise preferences with almost optimal query complexity</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>generalized linear models (glms) and single index models (sims) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor.</s> <s>in general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used.</s> <s>kalai and sastry (2009) provided the first provably efficient method, the \\emph{isotron} algorithm, for learning sims and glms, under the assumption that the data is in fact generated under a glm and under certain monotonicity and lipschitz (bounded slope) constraints.</s> <s>the isotron algorithm interleaves steps of perceptron-like updates with isotonic regression (fitting a one-dimensional non-decreasing function).</s> <s>however, to obtain provable performance, the method requires a fresh sample every iteration.</s> <s>in this paper, we provide algorithms for learning glms and sims, which are both computationally and statistically efficient.</s> <s>we modify the isotonic regression step in isotron to fit a lipschitz monotonic function, and also provide an efficient $o(n \\log(n))$ algorithm for this step, improving upon the previous $o(n^2)$ algorithm.</s> <s>we provide a brief empirical study, demonstrating the feasibility of our algorithms in practice.</s></p></d>", "label": ["<d><p><s>efficient learning of generalized linear and single index models with isotonic regression</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>latent variable mixture models are a powerful tool for exploring the structure in large datasets.</s> <s>a common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features.</s> <s>since mixture distributions are constrained in their l1 norm, typical sparsity techniques based on l1 regularization become toothless, and concave regularization becomes necessary.</s> <s>unfortunately concave regularization typically results in em algorithms that must perform problematic non-concave m-step maximizations.</s> <s>in this work, we introduce a technique for circumventing this difficulty, using the so-called mountain pass theorem to provide easily verifiable conditions under which the m-step is well-behaved despite the lacking concavity.</s> <s>we also develop a correspondence between logarithmic regularization and what we term the pseudo-dirichlet distribution, a generalization of the ordinary dirichlet distribution well-suited for inducing sparsity.</s> <s>we demonstrate our approach on a text corpus, inferring a sparse topic mixture model for 2,406 weblogs.</s></p></d>", "label": ["<d><p><s>a concave regularization technique for sparse mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the nested chinese restaurant process is extended to design a nonparametric  topic-model tree for representation of human choices.</s> <s>each tree branch corresponds  to a type of person, and each node (topic) has a corresponding probability  vector over items that may be selected.</s> <s>the observed data are assumed to have  associated temporal covariates (corresponding to the time at which choices are  made), and we wish to impose that with increasing time it is more probable that  topics deeper in the tree are utilized.</s> <s>this structure is imposed by developing  a new ?change point\" stick-breaking model that is coupled with a poisson and  product-of-gammas construction.</s> <s>to share topics across the tree nodes, topic distributions  are drawn from a dirichlet process.</s> <s>as a demonstration of this concept,  we analyze real data on course selections of undergraduate students at duke university,  with the goal of uncovering and concisely representing structure in the  curriculum and in the characteristics of the student body.</s></p></d>", "label": ["<d><p><s>hierarchical topic modeling for analysis of time-evolving personal choices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>mini-batch algorithms have recently received significant attention as a way to speed-up stochastic convex optimization problems.</s> <s>in this paper, we study how such algorithms can be improved using accelerated gradient methods.</s> <s>we provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up.</s> <s>we propose a novel accelerated gradient algorithm, which deals with this deficiency, and enjoys a uniformly superior guarantee.</s> <s>we conclude our paper with experiments  on real-world datasets, which validates our algorithm and  substantiates our theoretical insights.</s></p></d>", "label": ["<d><p><s>better mini-batch algorithms via accelerated gradient methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>domain adaptation algorithms seek to generalize a model trained in a source domain to a new target domain.</s> <s>in many  practical cases, the source and target distributions can differ substantially, and in some cases crucial target features may not have support in the source domain.</s> <s>in this paper we introduce an algorithm that bridges the gap between source and target domains by slowly adding both the target features and instances in which the current  algorithm is the most confident.</s> <s>our algorithm is a variant of co-training, and we name it coda (co-training for domain adaptation).</s> <s>unlike the original co-training work, we do not assume a particular feature split.</s> <s>instead, for each iteration of co-training, we add target features and formulate a single optimization problem which simultaneously learns a target predictor, a split of the feature space into views, and a shared subset of source  and target features to include in the predictor.</s> <s>coda significantly out-performs the state-of-the-art on the 12-domain benchmark data set of blitzer et al..</s> <s>indeed, over a wide range (65 of 84 comparisons) of target supervision, ranging from no labeled target data to a relatively large number of target labels, coda achieves the best performance.</s></p></d>", "label": ["<d><p><s>co-training for domain adaptation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>many machine learning and signal processing problems can be formulated as linearly constrained convex programs, which could be efficiently solved by the alternating direction method (adm).</s> <s>however, usually the subproblems in adm are easily solvable only when the linear mappings in the constraints are identities.</s> <s>to address this issue, we propose a linearized adm (ladm) method by linearizing the quadratic penalty term and adding a proximal term when solving the subproblems.</s> <s>for fast convergence, we also allow the penalty to change adaptively according a novel update rule.</s> <s>we prove the global convergence of ladm with adaptive penalty (ladmap).</s> <s>as an example, we apply ladmap to solve low-rank representation (lrr), which is an important subspace clustering technique yet suffers from high computation cost.</s> <s>by combining ladmap with a skinny svd representation technique, we are able to reduce the complexity $o(n^3)$  of the original adm based method to $o(rn^2)$, where $r$ and $n$ are the rank and size of the representation matrix, respectively, hence making lrr possible for large scale applications.</s> <s>numerical experiments verify that for lrr our ladmap based methods are much faster than state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>linearized alternating direction method with adaptive penalty for low-rank representation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>transfer reinforcement learning (rl) methods leverage on the experience collected on a set of source tasks to speed-up rl algorithms.</s> <s>a simple and effective approach is to transfer samples from source tasks and include them in the training set used to solve a target task.</s> <s>in this paper, we investigate the theoretical properties of this transfer method and we introduce novel algorithms adapting the transfer process on the basis of the similarity between source and target tasks.</s> <s>finally, we report illustrative experimental results in a continuous chain problem.</s></p></d>", "label": ["<d><p><s>transfer from multiple mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the development of statistical models for continuous-time longitudinal network data is of increasing interest in machine learning and social science.</s> <s>leveraging ideas from survival and event history analysis, we introduce a continuous-time regression modeling framework for network event data that can incorporate both time-dependent network statistics and time-varying regression coefficients.</s> <s>we also develop an efficient inference scheme that allows our approach to scale to large networks.</s> <s>on synthetic and real-world data, empirical results demonstrate that the proposed inference approach can accurately estimate the coefficients of the regression model, which is useful for interpreting the evolution of the network; furthermore, the learned model has systematically better predictive performance compared to standard baseline methods.</s></p></d>", "label": ["<d><p><s>continuous-time regression models for longitudinal networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reinforcement learning models address animal's behavioral adaptation to its changing \"external\" environment, and are based on the assumption that pavlovian, habitual and goal-directed responses seek to maximize reward acquisition.</s> <s>negative-feedback models of homeostatic regulation, on the other hand, are concerned with behavioral adaptation in response to the \"internal\" state of the animal, and assume that animals' behavioral objective is to minimize deviations of some key physiological variables from their hypothetical setpoints.</s> <s>building upon the drive-reduction theory of reward, we propose a new analytical framework that integrates learning and regulatory systems, such that the two seemingly unrelated objectives of reward maximization and physiological-stability prove to be identical.</s> <s>the proposed theory shows behavioral adaptation to both internal and external states in a disciplined way.</s> <s>we further show that the proposed framework allows for a unified explanation of some behavioral phenomenon like motivational sensitivity of different associative learning mechanism, anticipatory responses, interaction among competing motivational systems, and risk aversion.</s></p></d>", "label": ["<d><p><s>a reinforcement learning theory for homeostatic regulation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of recovering a matrix $\\mathbf{m}$ that is the sum of a low-rank matrix $\\mathbf{l}$ and a sparse matrix $\\mathbf{s}$ from a small set of linear measurements of the form $\\mathbf{y} = \\mathcal{a}(\\mathbf{m}) = \\mathcal{a}({\\bf l}+{\\bf s})$.</s> <s>this model subsumes three important classes of signal recovery problems:  compressive sensing, affine rank minimization, and robust principal component analysis.</s> <s>we propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called sparcs to solve it.</s> <s>sparcs inherits a number of desirable properties from the state-of-the-art cosamp and admira algorithms, including exponential convergence and efficient implementation.</s> <s>simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm.</s></p></d>", "label": ["<d><p><s>sparcs: recovering low-rank and sparse matrices from compressive measurements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, a rich variety of shrinkage priors have been proposed that have great promise in addressing massive regression problems.</s> <s>in general, these new priors can be expressed as scale mixtures of normals, but have more complex forms and better properties than traditional cauchy and double exponential priors.</s> <s>we first propose a new class of normal scale mixtures through a novel generalized beta distribution that encompasses many interesting priors as special cases.</s> <s>this encompassing framework should prove useful in comparing competing priors, considering properties and revealing close connections.</s> <s>we then develop a class of variational bayes approximations through the new hierarchy presented that will scale more efficiently to the types of truly massive data sets that are now encountered routinely.</s></p></d>", "label": ["<d><p><s>generalized beta mixtures of gaussians</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>diagnosis of alzheimer's disease (ad) at the early stage of the disease development is of great clinical importance.</s> <s>current clinical assessment that relies primarily on cognitive measures proves low sensitivity and specificity.</s> <s>the fast growing neuroimaging techniques hold great promise.</s> <s>research so far has focused on single neuroimaging modalities.</s> <s>however, as different modalities provide complementary measures for the same disease pathology, fusion of multi-modality data may increase the statistical power in identification of disease-related brain regions.</s> <s>this is especially true for early ad, at which stage the disease-related regions are most likely to be weak-effect regions that are difficult to be detected from a single modality alone.</s> <s>we propose a sparse composite linear discriminant analysis model (sclda) for identification of disease-related brain regions of early ad from multi-modality data.</s> <s>sclda uses a novel formulation that decomposes each lda parameter into a product of a common parameter shared by all the modalities and a parameter specific to each modality, which enables joint analysis of all the modalities and borrowing strength from one another.</s> <s>we prove that this formulation is equivalent to a penalized likelihood with non-convex regularization, which can be solved by the dc ((difference of convex functions) programming.</s> <s>we show that in using the dc programming, the property of the non-convex regularization in terms of preserving weak-effect features can be nicely revealed.</s> <s>we perform extensive simulations to show that sclda outperforms existing competing algorithms on feature selection, especially on the ability for identifying weak-effect features.</s> <s>we apply sclda to the magnetic resonance imaging (mri) and positron emission tomography (pet) images of 49 ad patients and 67 normal controls (nc).</s> <s>our study identifies disease-related brain regions consistent with findings in the ad literature.</s></p></d>", "label": ["<d><p><s>identifying alzheimer's disease-related brain regions from multi-modality neuroimaging data using sparse composite linear discrimination analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse coding, a method of explaining sensory data with as few dictionary bases as possible, has attracted much attention in computer vision.</s> <s>for visual object category recognition, l1 regularized sparse coding is combined with spatial pyramid representation to obtain state-of-the-art performance.</s> <s>however, because of its iterative optimization, applying sparse coding onto every local feature descriptor extracted from an image database can become a major bottleneck.</s> <s>to overcome this computational challenge, this paper presents \"generalized lasso based approximation of sparse coding\" (glas).</s> <s>by representing the distribution of sparse coefficients with slice transform, we fit a piece-wise linear mapping function with generalized lasso.</s> <s>we also propose an efficient post-refinement procedure to perform mutual inhibition between bases which is essential for an overcomplete setting.</s> <s>the experiments show that glas obtains comparable performance to l1 regularized sparse coding, yet achieves significant speed up demonstrating its effectiveness for large-scale visual recognition problems.</s></p></d>", "label": ["<d><p><s>generalized lasso based approximation of sparse coding for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>rational models of causal induction have been successful in accounting for people's judgments about the existence of causal relationships.</s> <s>however, these models have focused on explaining inferences from discrete data of the kind that can be summarized in a 2 ?</s> <s>2 contingency table.</s> <s>this severely limits the scope of these models, since the world often provides non-binary data.</s> <s>we develop a new rational model of causal induction using continuous dimensions, which aims to diminish the gap between empirical and theoretical approaches and real-world causal induction.</s> <s>this model successfully predicts human judgments from previous studies better than models of discrete causal inference, and outperforms several other plausible models of causal induction with continuous causes in accounting for people's inferences in a new experiment.</s></p></d>", "label": ["<d><p><s>a rational model of causal inference with continuous causes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning.</s> <s>traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible.</s> <s>presently, computer clusters and gpu processors make it possible to run more trials and we show that algorithmic approaches can find better results.</s> <s>we present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (dbns).</s> <s>we optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion.</s> <s>random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training dbns.</s> <s>the sequential algorithms are applied to the most difficult dbn learning problems from [larochelle et al., 2007] and find significantly better results than the best previously reported.</s> <s>this work contributes novel techniques for making response surface models p (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.</s></p></d>", "label": ["<d><p><s>algorithms for hyper-parameter optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the fundamental problem of learning an unknown large-margin halfspace in the context of parallel computation.</s> <s>our main positive result is a parallel algorithm for learning a large-margin halfspace that is based on interior point methods from convex optimization and fast parallel algorithms for matrix computations.</s> <s>we show that this algorithm learns an unknown gamma-margin halfspace over n dimensions using poly(n,1/gamma) processors and runs in time ~o(1/gamma) + o(log n).</s> <s>in contrast, naive parallel algorithms that learn a gamma-margin halfspace in time that depends polylogarithmically on n have omega(1/gamma^2) runtime dependence on gamma.</s> <s>our main negative result deals with boosting, which is a standard approach to learning large-margin halfspaces.</s> <s>we give an information-theoretic proof that in the original pac framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized: the ability to  call the weak learner multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required.</s></p></d>", "label": ["<d><p><s>algorithms and hardness results for parallel large margin learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>diversified retrieval and online learning are two core research areas in the design of modern information retrieval systems.in this paper, we propose the linear submodular bandits problem, which is an online learning setting for optimizing a general class of feature-rich submodular utility models for diversified retrieval.</s> <s>we present an algorithm, called lsbgreedy, and prove that it efficiently converges to a near-optimal model.</s> <s>as a case study, we applied our approach to the setting of personalized news recommendation, where the system must recommend small sets of news articles selected from tens of thousands of available articles each day.</s> <s>in a live user study, we found that lsbgreedy significantly outperforms existing online learning approaches.</s></p></d>", "label": ["<d><p><s>linear submodular bandits and their application to diversified retrieval</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>most online algorithms used in machine learning today are based on variants of mirror descent or follow-the-leader.</s> <s>in this paper, we present an online algorithm based on a completely different approach, which combines ``random playout'' and randomized rounding of loss subgradients.</s> <s>as an application of our approach, we provide the first computationally efficient online algorithm for collaborative filtering with trace-norm constrained matrices.</s> <s>as a second application, we solve an open question linking batch learning and transductive online learning.</s></p></d>", "label": ["<d><p><s>efficient online learning via randomized rounding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a computationally efficient technique to compute the distance of high-dimensional appearance descriptor vectors between image windows.</s> <s>the method exploits the relation between appearance distance and spatial overlap.</s> <s>we derive an upper bound on appearance distance given the spatial overlap of two windows in an image,  and use it to bound the distances of many pairs between two images.</s> <s>we propose algorithms that build on these basic operations to efficiently solve tasks relevant to many computer vision applications, such as finding all pairs of windows between two images with distance smaller than a threshold,  or finding the single pair with the smallest distance.</s> <s>in experiments on the pascal voc 07 dataset, our algorithms accurately solve these problems while greatly reducing the number of appearance distances computed, and achieve larger speedups than approximate nearest neighbour algorithms based on trees [18]and on hashing [21].</s> <s>for example, our algorithm finds the most similar pair of windows between two images while computing only 1% of all distances on average.</s></p></d>", "label": ["<d><p><s>exploiting spatial overlap to efficiently compute appearance distances between image windows</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel adaptive markov chain monte carlo algorithm to  compute the partition function.</s> <s>in particular, we show how to  accelerate a flat histogram sampling technique by significantly  reducing the number of ``null moves'' in the chain, while maintaining  asymptotic convergence properties.</s> <s>our experiments show that our  method converges quickly to highly accurate solutions on a range of  benchmark instances, outperforming other state-of-the-art methods such  as ijgp, trw, and gibbs sampling both in run-time and accuracy.</s> <s>we  also show how obtaining a so-called density of states distribution  allows for efficient weight learning in markov logic theories.</s></p></d>", "label": ["<d><p><s>accelerated adaptive markov chain for partition function computation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel class of actor-critic algorithms for actors consisting of sets of interacting modules.</s> <s>we present, analyze theoretically, and empirically evaluate an update rule for each module, which requires only local information: the module's input, output, and the td error broadcast by a critic.</s> <s>such updates are necessary when computation of compatible features becomes prohibitively difficult and are also desirable to increase the biological plausibility of reinforcement learning methods.</s></p></d>", "label": ["<d><p><s>policy gradient coagent networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of  multiclass boosting is considered.</s> <s>a new framework,based on multi-dimensional codewords and predictors is introduced.</s> <s>the optimal set of codewords is derived, and a margin enforcing loss proposed.</s> <s>the resulting risk is minimized by gradient descent on a multidimensional functional space.</s> <s>two algorithms are proposed: 1) cd-mcboost, based on coordinate descent, updates one predictor component at a time, 2) gd-mcboost, based on gradient descent, updates all components jointly.</s> <s>the algorithms differ in the weak learners that they support but are both shown to be 1) bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk.</s> <s>they also reduce to adaboost when there are only two classes.</s> <s>experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets.</s></p></d>", "label": ["<d><p><s>multiclass boosting: theory and algorithms</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>artists, advertisers, and photographers are routinely presented with the task of creating an image that a viewer will remember.</s> <s>while it may seem like image memorability is purely subjective, recent work shows that it is not an inexplicable phenomenon: variation in memorability of images is consistent across subjects, suggesting that some images are intrinsically more memorable than others, independent of a subjects' contexts and biases.</s> <s>in this paper, we used the publicly available memorability dataset of isola et al., and augmented the object and scene annotations with interpretable spatial, content, and aesthetic image properties.</s> <s>we used a feature-selection scheme with desirable explaining-away properties to determine a compact set of attributes that characterizes the memorability of any individual image.</s> <s>we find that images of enclosed spaces containing people with visible faces are memorable, while images of vistas and peaceful scenes are not.</s> <s>contrary to popular belief, unusual or aesthetically pleasing scenes do not tend to be highly memorable.</s> <s>this work represents one of the first attempts at understanding intrinsic image memorability, and opens a new domain of investigation at the interface between human cognition and computer vision.</s></p></d>", "label": ["<d><p><s>understanding the intrinsic memorability of images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of optimizing the sum of a smooth convex function and a non-smooth convex function using proximal-gradient methods, where an error is present in the calculation of the gradient of the smooth term or in the proximity operator with respect to the second term.</s> <s>we show that the basic proximal-gradient method, the basic proximal-gradient method with a strong convexity assumption, and the accelerated proximal-gradient method achieve the same convergence rates as in the error-free case, provided the errors decrease at an appropriate rate.</s> <s>our experimental results on a structured sparsity problem indicate that sequences of errors with these appealing theoretical properties can lead to practical performance improvements.</s></p></d>", "label": ["<d><p><s>convergence rates of inexact proximal-gradient methods for convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the statistical performance of a recently proposed convex tensor decomposition algorithm.</s> <s>conventionally tensor decomposition has been formulated as non-convex optimization problems, which hindered the analysis of their performance.</s> <s>we show under some conditions that the mean squared error of the convex method scales linearly with the quantity we call the normalized rank of the true tensor.</s> <s>the current analysis naturally extends the analysis of convex low-rank matrix estimation to tensors.</s> <s>furthermore, we show through numerical experiments that our theory can precisely predict the scaling behaviour in practice.</s></p></d>", "label": ["<d><p><s>statistical performance of convex tensor decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d.</s> <s>manner, many  applications involve noisy and/or missing data, possibly involving dependencies.</s> <s>we study these issues in the context of high-dimensional  sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data.</s> <s>many standard approaches to noisy or missing data, such as those using the em algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms.</s> <s>while our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers.</s> <s>on the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data.</s> <s>on the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer.</s> <s>we illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings.</s></p></d>", "label": ["<d><p><s>high-dimensional regression with noisy and missing data: provable guarantees with non-convexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many nonparametric regressors were recently shown to converge at rates that depend only on the intrinsic dimension of data.</s> <s>these regressors thus escape the curse of dimension when high-dimensional data has low intrinsic dimension (e.g.</s> <s>a manifold).</s> <s>we show that $k$-nn regression is also adaptive to intrinsic dimension.</s> <s>in particular our rates are local to a query $x$   and depend only on the way masses of balls centered at $x$ vary with radius.</s> <s>furthermore, we show a simple way to choose $k = k(x)$ locally at any $x$ so as to nearly achieve the minimax rate at $x$  in terms of the unknown intrinsic dimension in the vicinity of $x$.</s> <s>we also establish that the minimax rate does not depend   on a particular choice of metric space or distribution, but rather that this minimax rate holds for any metric space and doubling measure.</s></p></d>", "label": ["<d><p><s>k-nn regression adapts to local intrinsic dimension</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when used to learn high dimensional parametric probabilistic models, the clas- sical maximum likelihood (ml) learning often suffers from computational in- tractability, which motivates the active developments of non-ml learning meth- ods.</s> <s>yet, because of their divergent motivations and forms, the objective func- tions of many non-ml learning methods are seemingly unrelated, and there lacks a unified framework to understand them.</s> <s>in this work, based on an information geometric view of parametric learning, we introduce a general non-ml learning principle termed as minimum kl contraction, where we seek optimal parameters that minimizes the contraction of the kl divergence between the two distributions after they are transformed with a kl contraction operator.</s> <s>we then show that the objective functions of several important or recently developed non-ml learn- ing methods, including contrastive divergence [12], noise-contrastive estimation [11], partial likelihood [7], non-local contrastive objectives [31], score match- ing [14], pseudo-likelihood [3], maximum conditional likelihood [17], maximum mutual information [2], maximum marginal likelihood [9], and conditional and marginal composite likelihood [24], can be unified under the minimum kl con- traction framework with different choices of the kl contraction operators.</s></p></d>", "label": ["<d><p><s>unifying non-maximum likelihood learning objectives with minimum kl contraction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a class of sparsity-inducing regularization terms based on submodular functions.</s> <s>while previous work has focused on  non-decreasing functions, we explore symmetric submodular functions and their \\lova extensions.</s> <s>we show that the lovasz extension may be seen as the convex envelope of a function that depends on level sets  (i.e., the set of indices whose corresponding components of the underlying predictor are greater than a given constant): this leads to a class of convex structured regularization terms that impose prior knowledge on the level sets, and not only on the supports of the underlying predictors.</s> <s>we provide a unified set of optimization algorithms, such as proximal operators, and theoretical guarantees (allowed level sets and recovery conditions).</s> <s>by selecting specific submodular functions,  we   give a new interpretation to known norms, such as the total variation; we also define new norms, in particular ones that are based on order statistics with application to clustering and outlier detection, and on noisy cuts in graphs with application to change point detection in the presence of outliers.</s></p></d>", "label": ["<d><p><s>shaping level sets with submodular functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-structure model fitting has traditionally taken a two-stage approach: first, sample a (large) number of model hypotheses, then select the subset of hypotheses that optimise a joint fitting and model selection criterion.</s> <s>this disjoint two-stage approach is arguably suboptimal and inefficient - if the random sampling did not retrieve a good set of hypotheses, the optimised outcome will not represent a good fit.</s> <s>to overcome this weakness we propose a new multi-structure fitting approach based on reversible jump mcmc.</s> <s>instrumental in raising the effectiveness of our method is an adaptive hypothesis generator, whose proposal distribution is learned incrementally and online.</s> <s>we prove that this adaptive proposal satisfies the diminishing adaptation property crucial for ensuring ergodicity in mcmc.</s> <s>our method effectively conducts hypothesis sampling and optimisation simultaneously, and gives superior computational efficiency over other methods.</s></p></d>", "label": ["<d><p><s>simultaneous sampling and multi-structure fitting with adaptive reversible jump mcmc</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of computing the euclidean projection of a vector of length $p$ onto a non-negative max-heap---an ordered tree where the values of the nodes are all nonnegative and the value of any parent node is no less than the value(s) of its child node(s).</s> <s>this euclidean projection plays a building block role in the optimization problem with a non-negative max-heap constraint.</s> <s>such a constraint is desirable when the features follow an ordered tree structure, that is, a given feature is selected for the given regression/classification task only if its parent node is selected.</s> <s>in this paper, we show that such euclidean projection problem admits an analytical solution and we develop a top-down algorithm where the key operation is to find the so-called \\emph{maximal root-tree} of the subtree rooted at each node.</s> <s>a naive approach for finding the maximal root-tree is to enumerate all the possible root-trees, which, however, does not scale well.</s> <s>we reveal several important properties of the maximal root-tree, based on which we design a bottom-up algorithm with merge for efficiently finding the maximal root-tree.</s> <s>the proposed algorithm has a (worst-case) linear time complexity for a sequential list, and $o(p^2)$ for a general tree.</s> <s>we report simulation results showing the effectiveness of the max-heap for regression with an ordered tree structure.</s> <s>empirical results show that the proposed algorithm has an expected linear time complexity for many special cases including a sequential list, a full binary tree, and a tree with depth 1.</s></p></d>", "label": ["<d><p><s>projection onto a nonnegative max-heap</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel inference framework for finding maximal cliques in a weighted graph that satisfy hard constraints.</s> <s>the constraints specify the graph nodes that must belong to the solution as well as mutual exclusions of graph nodes, i.e., sets of nodes that cannot belong to the same solution.</s> <s>the proposed inference is based on a novel particle filter algorithm with state permeations.</s> <s>we apply the inference framework to a challenging problem of learning part-based, deformable object models.</s> <s>two core problems in the learning framework, matching of image patches and finding salient parts, are formulated as two instances of the problem of finding maximal cliques with hard constraints.</s> <s>our learning framework yields discriminative part based object models that achieve very good detection rate, and outperform other methods on object classes with large deformation.</s></p></d>", "label": ["<d><p><s>maximal cliques that satisfy hard constraints with application to deformable object model learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we consider the problem of compressed sensing where the goal is to recover almost all the sparse vectors using a small number of fixed linear measurements.</s> <s>for this problem, we propose a novel partial hard-thresholding operator leading to a general family of iterative algorithms.</s> <s>while one extreme of the family yields well known hard thresholding algorithms like iti and htp, the other end of the spectrum leads to a novel algorithm that we call  orthogonal matching pursuit with replacement (ompr).</s> <s>ompr, like the classic greedy algorithm omp, adds exactly one coordinate to the support at each iteration, based on the correlation with the current residual.</s> <s>however, unlike omp, ompr also removes one coordinate from the support.</s> <s>this simple change allows us to prove the best known guarantees for ompr in terms of the restricted isometry property (a condition on the measurement matrix).</s> <s>in contrast, omp is known to have very weak performance guarantees under rip.</s> <s>we also extend ompr using locality sensitive hashing to get ompr-hash, the first provably sub-linear (in dimensionality) algorithm for sparse recovery.</s> <s>our  proof techniques are novel and flexible enough to also permit the tightest known analysis of popular iterative algorithms such as cosamp and subspace pursuit.</s> <s>we provide experimental results on large problems providing  recovery for vectors of size up to million dimensions.</s> <s>we demonstrate that for large-scale problems our proposed methods are more robust and faster than the existing  methods.</s></p></d>", "label": ["<d><p><s>orthogonal matching pursuit with replacement</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the performance of markov chain monte carlo methods is often sensitive to the scaling and correlations between the random variables of interest.</s> <s>an important source of information about the local correlation and scale is given by the hessian matrix of the target distribution, but this is often either computationally expensive or infeasible.</s> <s>in this paper we propose mcmc samplers that make use of quasi-newton approximations from the optimization literature, that approximate the hessian of the target distribution from previous samples and gradients generated by the sampler.</s> <s>a key issue is that mcmc samplers that depend on the history of previous states are in general not valid.</s> <s>we address this problem by using limited memory quasi-newton methods, which depend only on a fixed window of previous samples.</s> <s>on several real world datasets, we show that the quasi-newton sampler is a more effective sampler than standard hamiltonian monte carlo at a fraction of the cost of mcmc methods that require higher-order derivatives.</s></p></d>", "label": ["<d><p><s>quasi-newton methods for markov chain monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an online prediction version of submodular set cover with connections to ranking and repeated active learning.</s> <s>in each round, the learning algorithm chooses a sequence of items.</s> <s>the algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of items needed, when items are selected in order of the chosen sequence, to achieve a coverage constraint.</s> <s>we develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight.</s> <s>our proposed algorithm is readily extended to a setting where multiple functions are revealed at each round and to bandit and contextual bandit settings.</s></p></d>", "label": ["<d><p><s>online submodular set cover, ranking, and repeated active learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the empirical strategies that humans follow as they teach a target concept with a simple 1d threshold to a robot.</s> <s>previous studies of computational teaching, particularly the teaching dimension model and the curriculum learning principle, offer contradictory predictions on what optimal strategy the teacher should follow in this teaching task.</s> <s>we show through behavioral studies that humans employ three distinct teaching strategies, one of which is consistent with the curriculum learning principle, and propose a novel theoretical framework as a potential explanation for this strategy.</s> <s>this framework, which assumes a teaching goal of minimizing the learner's expected generalization error at each iteration, extends the standard teaching dimension model and offers a theoretical justification for curriculum learning.</s></p></d>", "label": ["<d><p><s>how do humans teach: on curriculum learning and teaching dimension</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>independent components analysis (ica) and its variants have been successfully used for unsupervised feature learning.</s> <s>however, standard ica requires an orthonoramlity constraint to be enforced, which makes it dif?cult to learn overcomplete features.</s> <s>in addition, ica is sensitive to whitening.</s> <s>these properties make it challenging to scale ica to high dimensional data.</s> <s>in this paper, we propose a robust soft reconstruction cost for ica that allows us to learn highly overcomplete sparse features even on unwhitened data.</s> <s>our formulation reveals formal connections between ica and sparse autoencoders, which have previously been observed only empirically.</s> <s>our algorithm can be used in conjunction with off-the-shelf fast unconstrained optimizers.</s> <s>we show that the soft reconstruction cost can also be used to prevent replicated features in tiled convolutional neural networks.</s> <s>using our method to learn highly overcomplete sparse features and tiled convolutional neural networks, we obtain competitive performances on a wide variety of object recognition tasks.</s> <s>we achieve state-of-the-art test accuracies on the stl-10 and hollywood2 datasets.</s></p></d>", "label": ["<d><p><s>ica with reconstruction cost for efficient overcomplete feature learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>synaptic plasticity underlies learning and is thus central for development, memory, and recovery from injury.</s> <s>however, it is often difficult to detect changes in synaptic strength in vivo, since intracellular recordings are experimentally challenging.</s> <s>here we present two methods aimed at inferring changes in the coupling between pairs of neurons from extracellularly recorded spike trains.</s> <s>first, using a generalized bilinear model with poisson output we estimate time-varying coupling assuming that all changes are spike-timing-dependent.</s> <s>this approach allows model-based estimation of stdp modification functions from pairs of spike trains.</s> <s>then, using recursive point-process adaptive filtering methods we estimate more general variation in coupling strength over time.</s> <s>using simulations of neurons undergoing spike-timing dependent modification, we show that the true modification function can be recovered.</s> <s>using multi-electrode data from motor cortex we then illustrate the use of this technique on in vivo data.</s></p></d>", "label": ["<d><p><s>inferring spike-timing-dependent plasticity from spike train data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>with the advent of crowdsourcing services it has become quite cheap and reasonably effective to get a dataset labeled by multiple annotators in a short amount of time.</s> <s>various methods have been proposed to estimate the consensus labels by correcting for the bias of annotators with different kinds of expertise.</s> <s>often we have low quality annotators or spammers--annotators who assign labels randomly (e.g., without actually looking at the instance).</s> <s>spammers can make the cost of acquiring labels very expensive and can potentially degrade the quality of the consensus labels.</s> <s>in this paper we formalize the notion of a spammer and define a score which can be used to rank the annotators---with the spammers having a score close to zero and the good annotators having a high score close to one.</s></p></d>", "label": ["<d><p><s>ranking annotators for crowdsourced labeling tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop and demonstrate automatic image description methods using a large captioned photo collection.</s> <s>one contribution is our technique for the automatic collection of this new dataset -- performing a huge number of flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions.</s> <s>such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results.</s> <s>we also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results.</s> <s>finally we introduce a new objective performance measure for image captioning.</s></p></d>", "label": ["<d><p><s>im2text: describing images using 1 million captioned photographs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian filtering of stochastic stimuli has received a great deal of attention re- cently.</s> <s>it has been applied to describe the way in which biological systems dy- namically represent and make decisions about the environment.</s> <s>there have been no exact results for the error in the biologically plausible setting of inference on point process, however.</s> <s>we present an exact analysis of the evolution of the mean- squared error in a state estimation task using gaussian-tuned point processes as sensors.</s> <s>this allows us to study the dynamics of the error of an optimal bayesian decoder, providing insights into the limits obtainable in this task.</s> <s>this is done for markovian and a class of non-markovian gaussian processes.</s> <s>we find that there is an optimal tuning width for which the error is minimized.</s> <s>this leads to a char- acterization of the optimal encoding for the setting as a function of the statistics of the stimulus, providing a mathematically sound primer for an ecological theory of sensory processing.</s></p></d>", "label": ["<d><p><s>analytical results for the error in filtering of gaussian processes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show that the lambda-return target used in the td(lambda) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. we introduce the gamma-return estimator, an alternative target based on a more accurate model of variance, which defines the td_gamma family of complex-backup temporal difference learning algorithms.</s> <s>we derive td_gamma, the gamma-return equivalent of the original td(lambda) algorithm, which eliminates the lambda parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length.</s> <s>we then derive a second algorithm, td_gamma(c), with a capacity parameter c. td_gamma(c) requires c times more time and memory than td(lambda) and is incremental and online.</s> <s>we show that td_gamma outperforms td(lambda) for any setting of lambda on 4 out of 5 benchmark domains, and that td_gamma(c) performs as well as or better than td_gamma for intermediate settings of c.</s></p></d>", "label": ["<d><p><s>td_gamma: re-evaluating complex backups in temporal difference learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce hd (or ``hierarchical-deep'') models, a new compositional learning architecture that integrates deep learning models with structured hierarchical bayesian models.</s> <s>specifically we show how we can learn a hierarchical dirichlet process (hdp) prior over the activities of the top-level features in a deep boltzmann machine (dbm).</s> <s>this compound hdp-dbm model learns to learn novel concepts from very few training examples, by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts.</s> <s>we present efficient learning and inference algorithms for the hdp-dbm model and show that it is able to learn new concepts from very few examples on cifar-100 object recognition, handwritten character recognition, and human motion capture datasets.</s></p></d>", "label": ["<d><p><s>learning to learn with compound hd models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper addresses the problem of minimizing a convex, lipschitz  function $f$ over a convex, compact set $x$ under a stochastic bandit feedback model.</s> <s>in this model, the algorithm is allowed to  observe noisy realizations of the function value $f(x)$ at any query  point $x \\in x$.</s> <s>we demonstrate a generalization of the  ellipsoid algorithm that incurs $o(\\poly(d)\\sqrt{t})$ regret.</s> <s>since any algorithm has regret at least $\\omega(\\sqrt{t})$  on this problem, our algorithm is optimal in terms of the scaling  with $t$.</s></p></d>", "label": ["<d><p><s>stochastic convex optimization with bandit feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>predicting the nodes of a given graph is a fascinating   theoretical problem with applications in several domains.</s> <s>since graph sparsification via spanning trees   retains enough information while making the task much easier,   trees are an important special case of this problem.</s> <s>although it is known how to predict the nodes of an unweighted tree   in a nearly optimal way, in the weighted case a fully satisfactory   algorithm is not available yet.</s> <s>we fill this hole and introduce an efficient node predictor,   shazoo, which is nearly optimal on any weighted tree.</s> <s>moreover, we show that shazoo can   be viewed as a common nontrivial generalization of both previous approaches for   unweighted trees and weighted lines.</s> <s>experiments on real-world datasets confirm that shazoo performs well in that   it fully exploits the structure of the input tree,   and gets very close to (and sometimes better than)   less scalable energy minimization methods.</s></p></d>", "label": ["<d><p><s>see the tree through the lines: the shazoo algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>pomdp planning faces two major computational challenges: large state spaces and long planning horizons.</s> <s>the recently introduced monte carlo value iteration (mcvi) can tackle pomdps with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning horizons.</s> <s>this paper presents macro-mcvi, which extends mcvi by exploiting macro-actions for temporal abstraction.</s> <s>we provide sufficient conditions for macro-mcvi to inherit the good theoretical properties of mcvi.</s> <s>macro-mcvi does not require explicit construction of probabilistic models for macro-actions and is thus easy to apply in practice.</s> <s>experiments show that macro-mcvi substantially improves the performance of mcvi with suitable macro-actions.</s></p></d>", "label": ["<d><p><s>monte carlo value iteration with macro-actions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting.</s> <s>we first propose an algorithm called gap-based exploration (gape) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit (i.e., small gap).</s> <s>we then introduce an algorithm, called gape-v, which takes into account the variance of the arms in addition to their gap.</s> <s>we prove an upper-bound on the probability of error for both algorithms.</s> <s>since gape and gape-v need to tune an exploration parameter that depends on the complexity of the problem, which is often unknown in advance, we also introduce variations of these algorithms that estimate this complexity online.</s> <s>finally, we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems.</s></p></d>", "label": ["<d><p><s>multi-bandit best arm identification</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the difficulty in inverse reinforcement learning (irl) arises in choosing the best reward function since there are typically an infinite number of reward functions that yield the given behaviour data as optimal.</s> <s>using a bayesian framework, we address this challenge by using the maximum a posteriori (map) estimation for the reward function, and show that most of the previous irl algorithms can be modeled into our framework.</s> <s>we also present a gradient method for the map estimation based on the (sub)differentiability of the posterior distribution.</s> <s>we show the effectiveness of our approach by comparing the performance of the proposed method to those of the previous algorithms.</s></p></d>", "label": ["<d><p><s>map inference for bayesian inverse reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive algorithms for generalised tensor factorisation (gtf) by building upon the well-established theory of generalised linear models.</s> <s>our algorithms are general in the sense that we can compute arbitrary factorisations in a message passing framework, derived for a broad class of exponential family distributions including special cases such as tweedie's distributions corresponding to $\\beta$-divergences.</s> <s>by bounding the step size of the fisher scoring iteration of the glm, we obtain general updates for real data and multiplicative updates for non-negative data.</s> <s>the gtf framework is, then extended easily to address the problems when multiple observed tensors are factorised simultaneously.</s> <s>we illustrate our coupled factorisation approach on synthetic data as well as on a musical audio restoration problem.</s></p></d>", "label": ["<d><p><s>generalised coupled tensor factorisation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a novel technique for feature combination in the bag-of-words model of image classification.</s> <s>our approach builds discriminative compound words from primitive cues learned independently from training images.</s> <s>our main observation is that modeling joint-cue distributions independently is more statistically robust for typical classification problems than attempting to empirically estimate the dependent, joint-cue distribution directly.</s> <s>we use information theoretic vocabulary compression to find discriminative combinations of cues and the resulting vocabulary of portmanteau words is compact, has the cue binding property, and supports individual weighting of cues in the final image representation.</s> <s>state-of-the-art results on both the oxford flower-102 and caltech-ucsd bird-200 datasets demonstrate the effectiveness of our technique compared to other, significantly more complex approaches to multi-cue image representation</s></p></d>", "label": ["<d><p><s>portmanteau vocabularies for multi-cue image representation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>vector auto-regressive models (var) are useful tools for analyzing time series  data.</s> <s>in quite a few modern time series modelling tasks, the collection of reliable  time series turns out to be a major challenge, either due to the slow progression of  the dynamic process of interest, or inaccessibility of repetitive measurements of  the same dynamic process over time.</s> <s>in those situations, however, we observe that  it is often easier to collect a large amount of non-sequence samples, or snapshots  of the dynamic process of interest.</s> <s>in this work, we assume a small amount of time  series data are available, and propose methods to incorporate non-sequence data  into penalized least-square estimation of var models.</s> <s>we consider non-sequence  data as samples drawn from the stationary distribution of the underlying var  model, and devise a novel penalization scheme based on the discrete-time lyapunov  equation concerning the covariance of the stationary distribution.</s> <s>experiments  on synthetic and video data demonstrate the effectiveness of the proposed  methods.</s></p></d>", "label": ["<d><p><s>learning auto-regressive models from sequence and non-sequence data</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>most existing multiple-instance learning (mil) algorithms  assume data instances and/or data bags are independently and  identically distributed.</s> <s>but there often exists rich additional  dependency/structure information between instances/bags within many  applications of mil.</s> <s>ignoring this structure information limits the  performance of existing mil algorithms.</s> <s>this paper explores the  research problem as multiple instance learning on structured  data (milsd) and formulates a novel framework that considers  additional structure information.</s> <s>in particular, an effective and  efficient optimization algorithm has been proposed to solve the  original non-convex optimization problem by using a combination of  concave-convex constraint programming (cccp) method and an adapted  cutting plane method, which deals with two sets of constraints caused  by learning on  instances within individual bags and learning on  structured data.</s> <s>our method has the nice convergence property,  with specified precision on each set of constraints.</s> <s>experimental  results on three different applications, i.e., webpage  classification, market targeting, and protein fold identification,  clearly demonstrate the advantages of the proposed method over  state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>multiple instance learning on structured data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of learning to track a large quantity of homogeneous objects such as cell tracking in cell culture study and developmental biology.</s> <s>reliable cell tracking in time-lapse microscopic image sequences is important for modern biomedical research.</s> <s>existing cell tracking methods are usually kept simple and use only a small number of features to allow for manual parameter tweaking or grid search.</s> <s>we propose a structured learning approach that allows to learn optimum parameters automatically from a training set.</s> <s>this allows for the use of a richer set of features which in turn affords improved tracking compared to recently reported methods on two public benchmark sequences.</s></p></d>", "label": ["<d><p><s>structured learning for cell tracking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many practitioners of reinforcement learning problems have observed that oftentimes  the performance of the agent reaches very close to the optimal performance even though the estimated (action-)value function is still far from the optimal one.</s> <s>the goal of this paper is to explain and formalize this phenomenon by introducing the concept of the action-gap regularity.</s> <s>as a typical result, we prove that for an agent following the greedy policy \\(\\hat{\\pi}\\) with respect to an action-value function \\(\\hat{q}\\), the performance loss \\(e[v^*(x) - v^{\\hat{x}} (x)]\\) is upper bounded by \\(o(|| \\hat{q} - q^*||_\\infty^{1+\\zeta}\\)), in which \\(\\zeta >= 0\\) is the parameter quantifying the action-gap regularity.</s> <s>for \\(\\zeta > 0\\), our results indicate smaller performance loss compared to what previous analyses had suggested.</s> <s>finally, we show how this regularity affects the performance of the family of approximate value iteration algorithms.</s></p></d>", "label": ["<d><p><s>action-gap phenomenon in reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work introduces divide-factor-combine (dfc), a parallel divide-and-conquer framework for noisy matrix factorization.</s> <s>dfc divides a large-scale matrix factorization task into smaller subproblems, solves each subproblem in parallel using an arbitrary base matrix factorization algorithm, and combines the subproblem solutions using techniques from randomized matrix approximation.</s> <s>our experiments with collaborative filtering, video background modeling, and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach.</s> <s>moreover, our analysis shows that dfc enjoys high-probability recovery guarantees comparable to those of its base algorithm.</s></p></d>", "label": ["<d><p><s>divide-and-conquer matrix factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how should we design experiments to maximize performance of a complex system, taking into account uncontrollable environmental conditions?</s> <s>how should we select relevant documents (ads) to display, given information about the user?</s> <s>these tasks can be formalized as contextual bandit problems, where at each round, we receive context (about the experimental conditions, the query), and have to choose an action (parameters, documents).</s> <s>the key challenge is to trade off exploration by gathering data for estimating the mean payoff function over the context-action space, and to exploit by choosing an action deemed optimal based on the gathered data.</s> <s>we model the payoff function as a sample from a gaussian process defined over the joint context-action space, and develop cgp-ucb, an intuitive upper-confidence style algorithm.</s> <s>we show that by mixing and matching kernels for contexts and actions, cgp-ucb can handle a variety of practical applications.</s> <s>we further provide generic tools for deriving regret bounds when using such composite kernel functions.</s> <s>lastly, we evaluate our algorithm on two case studies, in the context of automated vaccine design and sensor management.</s> <s>we show that context-sensitive optimization outperforms no or naive use of context.</s></p></d>", "label": ["<d><p><s>contextual gaussian process bandit optimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors, and the latter finds a subset of predictors.</s> <s>first, a method of linear feature extraction is proposed using the gradient of regression function, based on the recent development of the kernel method.</s> <s>in comparison with other existing methods, the proposed one has wide applicability without strong assumptions on the regressor or type of variables, and uses computationally simple eigendecomposition, thus applicable to large data sets.</s> <s>second, in combination of a sparse penalty, the method is extended to variable selection, following the approach by chen et al.</s> <s>(2010).</s> <s>experimental results show that the proposed methods successfully find effective features and variables without parametric models.</s></p></d>", "label": ["<d><p><s>gradient-based kernel method for feature extraction and variable selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important way to make large training sets is to gather noisy labels from crowds of nonexperts.</s> <s>we propose a minimax entropy principle to improve the quality of these labels.</s> <s>our method assumes that labels are generated by a probability distribution over workers, items, and labels.</s> <s>by maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise.</s> <s>we infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the kullback-leibler (kl) divergence between the probability distribution and the unknown truth.</s> <s>we show that a simple coordinate descent scheme can optimize minimax entropy.</s> <s>empirically, our results are substantially better than previously published methods for the same problem.</s></p></d>", "label": ["<d><p><s>learning from the wisdom of crowds by minimax entropy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bipartite matching problems characterize many situations, ranging from ranking in information retrieval to correspondence in vision.</s> <s>exact inference in real-world applications of these problems is intractable, making efficient approximation methods essential for learning and inference.</s> <s>in this paper we propose a novel {\\it sequential matching} sampler based on the generalization of the plackett-luce model, which can effectively make large moves in the space of matchings.</s> <s>this allows the sampler to match the difficult target distributions common in these problems: highly multimodal distributions with well separated modes.</s> <s>we present experimental results with bipartite matching problems - ranking and image correspondence - which show that the sequential matching sampler efficiently approximates the target distribution, significantly outperforming other sampling approaches.</s></p></d>", "label": ["<d><p><s>efficient sampling for bipartite matching problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given $\\alpha,\\epsilon$, we study the time complexity   required to improperly learn a halfspace with misclassification   error rate of at most $(1+\\alpha)\\,l^*_\\gamma + \\epsilon$, where   $l^*_\\gamma$ is the optimal $\\gamma$-margin error rate.</s> <s>for $\\alpha   = 1/\\gamma$, polynomial time and sample complexity is achievable   using the hinge-loss.</s> <s>for $\\alpha = 0$, \\cite{shalevshsr11} showed   that $\\poly(1/\\gamma)$ time is impossible, while learning is   possible in time $\\exp(\\tilde{o}(1/\\gamma))$.</s> <s>an immediate   question, which this paper tackles, is what is achievable if $\\alpha   \\in (0,1/\\gamma)$.</s> <s>we derive positive results interpolating between   the polynomial time for $\\alpha = 1/\\gamma$ and the exponential   time for $\\alpha=0$.</s> <s>in particular, we show that there are cases in   which $\\alpha = o(1/\\gamma)$ but the problem is still solvable in   polynomial time.</s> <s>our results naturally extend to the adversarial   online learning model and to the pac learning with malicious noise   model.</s></p></d>", "label": ["<d><p><s>learning halfspaces with the zero-one loss: time-accuracy tradeoffs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering is a key component in data analysis toolbox.</s> <s>despite its   importance, scalable algorithms often eschew rich statistical models   in favor of simpler descriptions such as $k$-means clustering.</s> <s>in   this paper we present a sampler, capable of estimating   mixtures of exponential families.</s> <s>at its heart lies a novel proposal distribution using random   projections to achieve high throughput in generating proposals, which is crucial   for clustering models with large numbers of clusters.</s></p></d>", "label": ["<d><p><s>fastex: hash clustering with exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>warped gaussian processes (wgp) [1] model output observations in regression tasks as a parametric nonlinear transformation of a gaussian process (gp).</s> <s>the use of this nonlinear transformation, which is included as part of the probabilistic model, was shown to enhance performance by providing a better prior model on several data sets.</s> <s>in order to learn its parameters, maximum likelihood was used.</s> <s>in this work we show that it is possible to use a non-parametric nonlinear transformation in wgp and variationally integrate it out.</s> <s>the resulting bayesian wgp is then able to work in scenarios in which the maximum likelihood wgp failed: low data regime, data with censored values, classification, etc.</s> <s>we demonstrate the superior performance of bayesian warped gps on several real data sets.</s></p></d>", "label": ["<d><p><s>bayesian warped gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of comparing the risks of two given predictive models - for instance, a baseline model and a challenger - as confidently as possible on a fixed labeling budget.</s> <s>this problem occurs whenever models cannot be compared on held-out training data, possibly because the training data are unavailable or do not reflect the desired test distribution.</s> <s>in this case, new test instances have to be drawn and labeled at a cost.</s> <s>we devise an active comparison method that selects instances according to an instrumental sampling distribution.</s> <s>we derive the sampling distribution that maximizes the power of a statistical test applied to the observed empirical risks, and thereby minimizes the likelihood of choosing the inferior model.</s> <s>empirically, we investigate model selection problems on several classification and regression tasks and study the accuracy of the resulting p-values.</s></p></d>", "label": ["<d><p><s>active comparison of prediction models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we consider a setting where we have a very large number   of related tasks with few examples from each individual task.</s> <s>rather   than either learning each task individually (and having a large   generalization error) or learning all the tasks together using a   single hypothesis (and suffering a potentially large inherent   error), we consider learning a small pool of {\\em shared     hypotheses}.</s> <s>each task is then mapped to a single hypothesis in   the pool (hard association).</s> <s>we derive vc dimension generalization   bounds for our model, based on the number of tasks, shared   hypothesis and the vc dimension of the hypotheses   class.</s> <s>we conducted experiments with both synthetic problems and   sentiment of reviews, which strongly support our approach.</s></p></d>", "label": ["<d><p><s>learning multiple tasks using shared hypotheses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work in unsupervised feature learning has focused on the goal   of discovering high-level features from unlabeled images.</s> <s>much   progress has been made in this direction, but in most cases it is   still standard to use a large amount of labeled data in order to   construct detectors sensitive to object classes or other complex   patterns in the data.</s> <s>in this paper, we aim to test the hypothesis   that unsupervised feature learning methods, provided with only   unlabeled data, can learn high-level, invariant features that are   sensitive to commonly-occurring objects.</s> <s>though a handful of prior   results suggest that this is possible when each object class   accounts for a large fraction of the data (as in many labeled   datasets), it is unclear whether something similar can be   accomplished when dealing with completely unlabeled data.</s> <s>a major   obstacle to this test, however, is scale: we cannot expect to   succeed with small datasets or with small numbers of learned   features.</s> <s>here, we propose a large-scale feature learning system   that enables us to carry out this experiment, learning 150,000   features from tens of millions of unlabeled images.</s> <s>based on two   scalable clustering algorithms (k-means and agglomerative   clustering), we find that our simple system can discover features   sensitive to a commonly occurring object class (human faces) and can   also combine these into detectors invariant to significant global   distortions like large translations and scale.</s></p></d>", "label": ["<d><p><s>emergence of object-selective features in unsupervised feature learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the estimation of an i.i.d.\\ vector $\\xbf \\in \\r^n$ from measurements $\\ybf \\in \\r^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel.</s> <s>we present a method, called adaptive generalized approximate message passing (adaptive gamp), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\\xbf$.</s> <s>the proposed algorithm is a generalization of a recently-developed method by vila and schniter that uses expectation-maximization (em) iterations where the posteriors in the e-steps are computed via approximate message passing.</s> <s>the techniques can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes.</s> <s>we prove that for large i.i.d.\\ gaussian transform matrices the asymptotic componentwise behavior of the adaptive gamp algorithm is predicted by a simple set of scalar state evolution equations.</s> <s>this analysis shows that the adaptive gamp method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values.</s> <s>the adaptive gamp methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees.</s></p></d>", "label": ["<d><p><s>approximate message passing with consistent parameter estimation and applications to sparse learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider estimation of multiple high-dimensional gaussian graphical models corresponding to a single set of nodes under several distinct conditions.</s> <s>we assume that most aspects of the networks are shared, but that there are some structured differences between them.</s> <s>specifically, the network differences are generated from node perturbations: a few nodes are perturbed across networks, and most or all edges stemming from such nodes differ between networks.</s> <s>this corresponds to a simple model for the mechanism underlying many cancers, in which the gene regulatory network is disrupted  due to the aberrant activity of a few specific genes.</s> <s>we propose to solve this problem using the structured joint graphical lasso, a convex optimization problem that is based upon the use of a novel symmetric overlap norm penalty, which we solve using  an alternating directions method of multipliers algorithm.</s> <s>our proposal is illustrated on synthetic data and on an application to brain cancer gene expression data.</s></p></d>", "label": ["<d><p><s>structured learning of gaussian graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>margin is one of the most important concepts in machine learning.</s> <s>previous margin bounds, both for svm and for boosting, are dimensionality independent.</s> <s>a major advantage of this dimensionality independency is that it can explain the excellent performance of svm whose feature spaces are often of high or infinite dimension.</s> <s>in this paper we address the problem whether such dimensionality independency is intrinsic for the margin bounds.</s> <s>we prove a dimensionality dependent pac-bayes margin bound.</s> <s>the bound is monotone increasing with respect to the dimension when keeping all other factors fixed.</s> <s>we show that our bound is strictly sharper than a previously well-known pac-bayes margin bound if the feature space is of finite dimension; and the two bounds tend to be equivalent as the dimension goes to infinity.</s> <s>in addition, we show that the vc bound for linear classifiers can be recovered from our bound under mild conditions.</s> <s>we conduct extensive experiments on benchmark datasets and find that the new bound is useful for model selection and is significantly sharper than the dimensionality independent pac-bayes margin bound as well as the vc bound for linear classifiers.</s></p></d>", "label": ["<d><p><s>dimensionality dependent pac-bayes margin bound</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multiple-output regression models require estimating multiple functions, one for each output.</s> <s>to improve parameter estimation in such models, methods based on structural regularization of the model parameters are usually needed.</s> <s>in this paper, we present a multiple-output regression model that leverages the covariance structure of the functions (i.e., how the multiple functions are related with each other) as well as the conditional covariance structure of the outputs.</s> <s>this is in contrast with existing methods that usually take into account only one of these structures.</s> <s>more importantly, unlike most of the other existing methods, none of these structures need be known a priori in our model, and are learned from the data.</s> <s>several previously proposed structural regularization based  multiple-output regression models turn out to be special cases of our model.</s> <s>moreover, in addition to being a rich model for multiple-output regression, our model can also be used in estimating the graphical model structure of a set of variables (multivariate outputs) conditioned on another set of variables (inputs).</s> <s>experimental results on both synthetic and real datasets demonstrate the effectiveness of our method.</s></p></d>", "label": ["<d><p><s>simultaneously leveraging output and task structures for multiple-output regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>alzheimer disease (ad) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions.</s> <s>regression analysis has been studied to relate neuroimaging measures to cognitive status.</s> <s>however, whether these measures have further predictive power to infer a trajectory of cognitive performance over time is still an under-explored but important topic in ad research.</s> <s>we propose a novel high-order multi-task learning model to address this issue.</s> <s>the proposed model explores the temporal correlations existing in data features and regression tasks by the structured sparsity-inducing norms.</s> <s>in addition, the sparsity of the model enables the selection of a small number of mri measures while maintaining high prediction accuracy.</s> <s>the empirical studies, using the baseline mri and serial cognitive data of the adni cohort, have yielded promising results.</s></p></d>", "label": ["<d><p><s>high-order multi-task feature learning to identify longitudinal phenotypic markers for alzheimer's disease progression prediction</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the lovasz $\\theta$ function of a graph, is a fundamental tool in combinatorial optimization and approximation algorithms.</s> <s>computing $\\theta$ involves solving a sdp  and is extremely expensive even for moderately sized graphs.</s> <s>in this paper we establish that the lovasz $\\theta$ function is equivalent to  a kernel learning problem related to one class svm.</s> <s>this interesting connection opens up many opportunities  bridging graph theoretic algorithms and machine learning.</s> <s>we show that there exist graphs, which we call $svm-\\theta$ graphs, on which the lovasz $\\theta$ function can be approximated well by a one-class  svm.</s> <s>this leads to a novel use of svm techniques to solve algorithmic problems in large graphs e.g.</s> <s>identifying a planted clique  of size $\\theta({\\sqrt{n}})$ in a random graph $g(n,\\frac{1}{2})$.</s> <s>a classic approach for this problem involves computing  the $\\theta$ function, however it is not scalable due to sdp computation.</s> <s>we show that the random graph with a planted clique is an example of $svm-\\theta$ graph, and as a consequence a svm based approach  easily identifies the clique in large graphs and is competitive with the  state-of-the-art.</s> <s>further, we introduce  the notion of a ''common orthogonal labeling'' which extends the notion  of a ''orthogonal labelling of a single  graph (used in defining the $\\theta$ function)  to multiple graphs.</s> <s>the problem of finding the optimal common orthogonal labelling is cast as a  multiple kernel learning problem and is used to identify a large common dense region in multiple graphs.</s> <s>the proposed algorithm achieves an order of magnitude scalability compared to the state of the art.</s></p></d>", "label": ["<d><p><s>the lov?sz ? function, svms and finding large dense subgraphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of cardinality penalized optimization of a convex function over the probability simplex with additional convex constraints.</s> <s>it's well-known that the classical l1 regularizer fails to promote sparsity on the probability simplex since l1 norm on the probability simplex is trivially constant.</s> <s>we propose a direct relaxation of the minimum cardinality problem and show that it can be efficiently solved using convex programming.</s> <s>as a first application we consider recovering a sparse probability measure given moment constraints, in which our formulation becomes linear programming, hence can be solved very efficiently.</s> <s>a sufficient condition for exact recovery of the minimum cardinality solution is derived for arbitrary affine constraints.</s> <s>we then develop a penalized version for the noisy setting which can be solved using second order cone programs.</s> <s>the proposed method outperforms known heuristics based on l1 norm.</s> <s>as a second application we consider convex clustering using a sparse gaussian mixture and compare our results with the well known soft k-means algorithm.</s></p></d>", "label": ["<d><p><s>recovery of sparse probability measures via convex programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study statistical risk minimization problems under a version of privacy in which the data is kept confidential even from the learner.</s> <s>in this local privacy framework, we show sharp upper and lower bounds on the convergence rates of statistical estimation procedures.</s> <s>as a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, measured by convergence rate, of any statistical estimator.</s></p></d>", "label": ["<d><p><s>privacy aware learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning temporal dependencies between variables over continuous time is an important and challenging task.</s> <s>continuous-time bayesian networks effectively model such processes but are limited by the number of conditional intensity matrices, which grows exponentially in the number of parents per variable.</s> <s>we develop a partition-based representation using regression trees and forests whose parameter spaces grow linearly in the number of node splits.</s> <s>using a multiplicative assumption we show how to update the forest likelihood in closed form, producing efficient model updates.</s> <s>our results show multiplicative forests can be learned from few temporal trajectories with large gains in performance and scalability.</s></p></d>", "label": ["<d><p><s>multiplicative forests for continuous-time processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse linear (or generalized linear) models combine a standard likelihood function with a sparse prior on the unknown coefficients.</s> <s>these priors can conveniently be expressed as a maximization over zero-mean gaussians with different variance hyperparameters.</s> <s>standard map estimation (type i) involves maximizing over both the hyperparameters and coefficients, while an empirical bayesian alternative (type ii) first marginalizes the coefficients and then maximizes over the hyperparameters, leading to a tractable posterior approximation.</s> <s>the underlying cost functions can be related via a dual-space framework from wipf et al.</s> <s>(2011), which allows both the type i or type ii objectives to be expressed in either coefficient or hyperparmeter space.</s> <s>this perspective is useful because some analyses or extensions are more conducive to development in one space or the other.</s> <s>herein we consider the estimation of a trade-off parameter balancing sparsity and data fit.</s> <s>as this parameter is effectively a variance, natural estimators exist by assessing the problem in hyperparameter (variance) space, transitioning natural ideas from type ii to solve what is much less intuitive for type i.</s> <s>in contrast, for analyses of update rules and sparsity properties of local and global solutions, as well as extensions to more general likelihood models, we can leverage coefficient-space techniques developed for type i and apply them to type ii.</s> <s>for example, this allows us to prove that type ii-inspired techniques can be successful recovering sparse coefficients when unfavorable restricted isometry properties (rip) lead to failure of popular l1 reconstructions.</s> <s>it also facilitates the analysis of type ii when non-gaussian likelihood models lead to intractable integrations.</s></p></d>", "label": ["<d><p><s>dual-space analysis of the sparse linear model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points.</s> <s>existing work on learning with indefinite kernels has concentrated solely on binary/multiclass classification problems.</s> <s>we propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification.</s> <s>we give a ''goodness'' criterion for similarity functions w.r.t.</s> <s>a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using ''good'' similarity functions.</s> <s>we demonstrate the effectiveness of our model on three important supervised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error.</s> <s>furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error.</s> <s>finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-psd similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs.</s></p></d>", "label": ["<d><p><s>supervised learning with similarity functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>derivative free optimization (dfo) is attractive when the objective function's derivatives are not available and evaluations are costly.</s> <s>moreover, if the function evaluations are noisy, then approximating gradients by finite differences is difficult.</s> <s>this paper gives quantitative lower bounds on the performance of dfo with noisy function evaluations, exposing a fundamental and unavoidable gap between optimization performance based on noisy evaluations versus noisy gradients.</s> <s>this challenges the conventional wisdom that the method of finite differences is comparable to a stochastic gradient.</s> <s>however, there are situations in which dfo is unavoidable, and for such situations we propose a new dfo algorithm that is proved to be near optimal for the class of strongly convex objective functions.</s> <s>a distinctive feature of the algorithm is that it only uses boolean-valued function comparisons, rather than evaluations.</s> <s>this makes the algorithm useful in an even wider range of applications, including optimization based on paired comparisons from human subjects, for example.</s> <s>remarkably, we show that regardless of whether dfo is based on noisy function evaluations or boolean-valued function comparisons, the convergence rate is the same.</s></p></d>", "label": ["<d><p><s>query complexity of derivative-free optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we  show how binary classification methods developed to work on i.i.d.</s> <s>data can be  used for solving  statistical problems that are seemingly unrelated to classification and concern highly-dependent time series.</s> <s>specifically, the problems of time-series  clustering, homogeneity testing and the three-sample problem  are addressed.</s> <s>the algorithms that we construct for solving  these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods.</s> <s>universal consistency of the  proposed algorithms  is proven under most general assumptions.</s> <s>the theoretical results are illustrated with experiments on synthetic and real-world data.</s></p></d>", "label": ["<d><p><s>reducing statistical time-series problems to binary classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical relational learning models combine the power of first-order logic, the de facto tool for handling relational structure, with that of probabilistic graphical models, the de facto tool for handling uncertainty.</s> <s>lifted probabilistic inference algorithms for them have been the subject of much recent research.</s> <s>the main idea in these algorithms is to improve the speed, accuracy and scalability of existing graphical models' inference algorithms by exploiting symmetry in the first-order representation.</s> <s>in this paper, we consider blocked gibbs sampling, an advanced variation of the classic gibbs sampling algorithm and lift it to the first-order level.</s> <s>we propose to achieve this by partitioning the first-order atoms in the relational model into a set of disjoint clusters such that exact lifted inference is polynomial in each cluster given an assignment to all other atoms not in the cluster.</s> <s>we propose an approach for constructing such clusters and determining their complexity and show how it can be used to trade accuracy with computational complexity in a principled manner.</s> <s>our experimental evaluation shows that lifted gibbs sampling is superior to the propositional algorithm in terms of accuracy and convergence.</s></p></d>", "label": ["<d><p><s>on lifting the gibbs sampling algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for approximate inference for a broad class of non-conjugate probabilistic models.</s> <s>in particular, for the family of generalized linear model target densities we describe a rich class of variational approximating densities which can be best fit to the target by minimizing the kullback-leibler divergence.</s> <s>our approach is based on using the fourier representation which we show results in efficient and scalable inference.</s></p></d>", "label": ["<d><p><s>affine independent variational inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>two-alternative forced choice (2afc) and go/nogo (gng) tasks are behavioral choice paradigms commonly used to study sensory and cognitive processing in choice behavior.</s> <s>while gng is thought to isolate the sensory/decisional component by removing the need for response selection, a consistent bias towards the go response (higher hits and false alarm rates) in the gng task suggests possible fundamental differences in the sensory or cognitive processes engaged in the two tasks.</s> <s>existing mechanistic models of these choice tasks, mostly variants of the drift-diffusion model (ddm; [1,2]) and the related leaky competing accumulator models [3,4] capture various aspects of behavior but do not address the provenance of the go bias.</s> <s>we postulate that this ``impatience'' to go is a strategic adjustment in response to the implicit asymmetry in the cost structure of gng: the nogo response requires waiting until the response deadline, while a go response immediately terminates the current trial.</s> <s>we show that a bayes-risk minimizing decision policy that minimizes both error rate and average decision delay naturally exhibits the experimentally observed bias.</s> <s>the optimal decision policy is formally equivalent to a ddm with a time-varying threshold that initially rises after stimulus onset, and collapses again near the response deadline.</s> <s>the initial rise is due to the fading temporal advantage of choosing the go response over the fixed-delay nogo response.</s> <s>we show that fitting a simpler, fixed-threshold ddm to the optimal model reproduces the counterintuitive result of a higher threshold in gng than 2afc decision-making, previously observed in direct ddm fit to behavioral data [2], although such approximations cannot reproduce the go bias.</s> <s>thus, observed discrepancies between gng and 2afc decision-making may arise from rational strategic adjustments to the cost structure, and need not imply additional differences in the underlying sensory and cognitive processes.</s></p></d>", "label": ["<d><p><s>strategic impatience in go/nogo versus forced-choice decision-making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a reformulation of the information bottleneck (ib) problem in terms of copula, using the equivalence between mutual information and negative copula  entropy.</s> <s>focusing on the gaussian copula we extend the analytical ib solution available for the multivariate gaussian case to distributions with a gaussian dependence structure but arbitrary marginal densities, also called meta-gaussian distributions.</s> <s>this opens new possibles applications of ib to continuous data and provides a solution more robust to outliers.</s></p></d>", "label": ["<d><p><s>meta-gaussian information bottleneck</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes a new approach for computing nonnegative matrix factorizations (nmfs) with linear programming.</s> <s>the key idea is a data-driven model for the factorization, in which the most salient features in the data are used to express the remaining features.</s> <s>more precisely, given a data matrix x, the algorithm identifies a matrix c that satisfies x = cx and some linear constraints.</s> <s>the matrix c selects features, which are then used to compute a low-rank nmf of x.</s> <s>a theoretical analysis demonstrates that this approach has the same type of guarantees as the recent nmf algorithm of arora et al.~(2012).</s> <s>in contrast with this earlier work, the proposed method has (1) better noise tolerance, (2) extends to more general noise models, and (3) leads to efficient, scalable algorithms.</s> <s>experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice.</s> <s>an optimized c++ implementation of the new algorithm can factor a multi-gigabyte matrix in a matter of minutes.</s></p></d>", "label": ["<d><p><s>factoring nonnegative matrices with linear programs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of adaptive stratified sampling for monte carlo integration of a differentiable function given a finite number of evaluations to the function.</s> <s>we construct a sampling scheme that samples more often in regions where the function oscillates more, while allocating the samples such that they are well spread on the domain (this notion shares similitude with low discrepancy).</s> <s>we prove that the estimate returned by the algorithm is almost as accurate as the estimate that an optimal oracle strategy (that would know the variations of the function everywhere) would return, and we provide a finite-sample analysis.</s></p></d>", "label": ["<d><p><s>adaptive stratified sampling for monte-carlo integration of differentiable functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in hierarchical classification, the prediction paths may be required to always end at leaf nodes.</s> <s>this is called mandatory leaf node prediction (mlnp) and is particularly useful when the leaf nodes have much stronger semantic meaning than the internal nodes.</s> <s>however, while there have been a lot of mlnp methods in hierarchical multiclass classification, performing mlnp in hierarchical multilabel classification is much more difficult.</s> <s>in this paper, we propose a novel mlnp algorithm that (i) considers the global hierarchy structure; and (ii) can  be used on hierarchies of both trees and dags.</s> <s>we show that one can efficiently maximize the joint posterior probability of all the node labels by a simple greedy algorithm.</s> <s>moreover, this can be further extended to the minimization of the expected symmetric loss.</s> <s>experiments are performed on a number of real-world data sets with tree- and dag-structured label hierarchies.</s> <s>the  proposed method consistently outperforms other hierarchical and flat multilabel classification methods.</s></p></d>", "label": ["<d><p><s>mandatory leaf node prediction in hierarchical multilabel classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of estimating shannon's entropy h in the   under-sampled regime, where the number of possible symbols may be   unknown or countably infinite.</s> <s>pitman-yor processes (a   generalization of dirichlet processes) provide tractable prior   distributions over the space of countably infinite discrete   distributions, and have found major applications in bayesian   non-parametric statistics and machine learning.</s> <s>here we show that   they also provide natural priors for bayesian entropy estimation,   due to the remarkable fact that the moments of the induced posterior   distribution over h can be computed analytically.</s> <s>we derive   formulas for the posterior mean (bayes' least squares estimate) and   variance under such priors.</s> <s>moreover, we show that a fixed   dirichlet or pitman-yor process prior implies a narrow prior on h,   meaning the prior strongly determines the entropy estimate in the   under-sampled regime.</s> <s>we derive a family of continuous mixing   measures such that the resulting mixture of pitman-yor processes   produces an approximately flat (improper) prior over h.  we   explore the theoretical properties of the resulting estimator, and   show that it performs well on data sampled from both exponential and   power-law tailed distributions.</s></p></d>", "label": ["<d><p><s>bayesian estimation of discrete entropy with mixtures of stick-breaking priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters.</s> <s>unfortunately, this tuning is often a ?black art?</s> <s>requiring expert experience, rules of thumb, or sometimes brute-force search.</s> <s>there is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand.</s> <s>in this work, we consider this problem through the framework of bayesian optimization, in which a learning algorithm?s generalization performance is modeled as a sample from a gaussian process (gp).</s> <s>we show that certain choices for the nature of the gp, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expert-level performance.</s> <s>we describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation.</s> <s>we show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent dirichlet allocation, structured svms and convolutional neural networks.</s></p></d>", "label": ["<d><p><s>practical bayesian optimization of machine learning algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe efficient implementations of the proximity calculation for a useful class of functions; the implementations exploit the piece-wise linear nature of the dual problem.</s> <s>the second part of the paper applies the previous result to acceleration of convex minimization problems, and leads to an elegant quasi-newton method.</s> <s>the optimization method compares favorably against state-of-the-art alternatives.</s> <s>the algorithm has extensive applications including signal processing, sparse regression and recovery, and machine learning and classification.</s></p></d>", "label": ["<d><p><s>a quasi-newton proximal splitting method</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a simplex algorithm for linear programming in a linear classification formulation.</s> <s>the paramount complexity parameter in linear classification problems is called the margin.</s> <s>we prove that for margin values of practical interest our simplex variant performs a polylogarithmic number of pivot steps in the worst case, and its overall running time is near linear.</s> <s>this is in contrast to general linear programming, for which no sub-polynomial pivot rule is known.</s></p></d>", "label": ["<d><p><s>a polylog pivot steps simplex algorithm for classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a patient's risk for adverse events is affected by temporal processes including the nature and timing of diagnostic and therapeutic activities, and the overall evolution of the patient's pathophysiology over time.</s> <s>yet many investigators ignore this temporal aspect when modeling patient risk, considering only the patient's current or aggregate state.</s> <s>we explore representing patient risk as a time series.</s> <s>in doing so, patient risk stratification becomes a time-series classification task.</s> <s>the task differs from most applications of time-series analysis, like speech processing, since the time series itself must first be extracted.</s> <s>thus, we begin by defining and extracting approximate \\textit{risk processes}, the evolving approximate daily risk of a patient.</s> <s>once obtained, we use these signals to explore different approaches to time-series classification with the goal of identifying high-risk patterns.</s> <s>we apply the classification to the specific task of identifying patients at risk of testing positive for hospital acquired colonization with \\textit{clostridium difficile}.</s> <s>we achieve an area under the receiver operating characteristic curve of 0.79 on a held-out set of several hundred patients.</s> <s>our two-stage approach to risk stratification outperforms classifiers that consider only a patient's current state (p$<$0.05).</s></p></d>", "label": ["<d><p><s>patient risk stratification for hospital-associated c. diff as a time-series classification task</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>residue-residue contact prediction is a fundamental problem in protein structure prediction.</s> <s>hower, despite considerable research efforts, contact prediction methods are still largely unreliable.</s> <s>here we introduce a novel deep machine-learning architecture which consists of a multidimensional stack of learning modules.</s> <s>for contact prediction, the idea is implemented as a three-dimensional stack of neural networks nn^k_{ij}, where i and j index the spatial coordinates of the contact map and k indexes ''time''.</s> <s>the temporal dimension is introduced to capture the fact that protein folding is not an instantaneous process, but rather a progressive refinement.</s> <s>networks at level k in the stack can be trained in supervised fashion to refine the predictions produced by the previous level, hence addressing the problem of vanishing gradients, typical of deep architectures.</s> <s>increased accuracy and generalization capabilities of this approach are established by rigorous comparison with other classical machine learning approaches for contact prediction.</s> <s>the deep approach leads to an accuracy for difficult long-range contacts of about 30%, roughly 10% above the state-of-the-art.</s> <s>many variations in the architectures and the training algorithms are possible, leaving room for further improvements.</s> <s>furthermore, the approach is applicable to other problems with strong underlying spatial and temporal components.</s></p></d>", "label": ["<d><p><s>deep spatio-temporal architectures and learning for protein structure prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to learn reliable rules that can generalize to novel situations, the brain must be capable of imposing some form of regularization.</s> <s>here we suggest, through theoretical and computational arguments, that the combination of noise with synchronization provides a plausible mechanism for regularization in the nervous system.</s> <s>the functional role of regularization is considered in a general context in which coupled computational systems receive inputs corrupted by correlated noise.</s> <s>noise on the inputs is shown to impose regularization, and when synchronization upstream induces time-varying correlations across noise variables, the degree of regularization can be calibrated over time.</s> <s>the resulting qualitative behavior matches experimental data from visual cortex.</s></p></d>", "label": ["<d><p><s>synchronization can control regularization in neural systems via correlated noise processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study consistency properties of surrogate loss functions for general multiclass classification problems, defined by a general loss matrix.</s> <s>we extend the notion of classification calibration, which has been studied for binary and multiclass 0-1 classification problems (and for certain other specific learning problems), to the general multiclass setting, and derive necessary and sufficient conditions for a surrogate loss to be classification calibrated with respect to a loss matrix in this setting.</s> <s>we then introduce the notion of \\emph{classification calibration dimension} of a multiclass loss matrix, which measures the smallest `size' of a prediction space for which it is possible to design a convex surrogate that is classification calibrated with respect to the loss matrix.</s> <s>we derive both upper and lower bounds on this quantity, and use these results to analyze various loss matrices.</s> <s>in particular, as one application, we provide a different route from the recent result of duchi et al.\\ (2010) for analyzing the difficulty of designing `low-dimensional' convex surrogates that are consistent with respect to pairwise subset ranking losses.</s> <s>we anticipate the classification calibration dimension may prove to be a useful tool in the study and design of surrogate losses for general multiclass learning problems.</s></p></d>", "label": ["<d><p><s>classification calibration dimension for general multiclass losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we strengthen recent connections between prediction markets and learning by showing that a natural class of market makers can be understood as performing stochastic mirror descent when trader demands are sequentially drawn from a fixed distribution.</s> <s>this provides new insights into how market prices (and price paths) may be interpreted as a summary of the market's belief distribution by relating them to the optimization problem being solved.</s> <s>in particular, we show that the stationary point of the stochastic process of prices generated by the market is equal to the market's walrasian equilibrium of classic market analysis.</s> <s>together, these results suggest how traditional market making mechanisms might be replaced with general purpose learning algorithms while still retaining guarantees about their behaviour.</s></p></d>", "label": ["<d><p><s>interpreting prediction markets: a stochastic approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>graphical model selection refers to the problem of estimating the unknown graph structure given observations at the nodes in the model.</s> <s>we consider a challenging instance of this problem when some of the nodes are latent or hidden.</s> <s>we  characterize  conditions for tractable graph estimation and develop efficient methods with provable guarantees.</s> <s>we consider the class of ising models markov on  locally tree-like graphs, which are in the regime of correlation decay.</s> <s>we  propose an efficient method for graph estimation, and establish its structural consistency when the number of samples $n$ scales as $n = \\omega(\\theta_{\\min}^{-\\delta \\eta(\\eta+1)-2}\\log p)$, where $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is the depth (i.e., distance from a hidden node to the nearest  observed nodes), and $\\eta$ is a parameter which depends on the minimum and maximum node and edge potentials in the ising model.</s> <s>the proposed method is practical to implement and provides  flexibility to control  the number of latent variables and the cycle lengths in the output graph.</s> <s>we also present necessary conditions for graph estimation by any method and show that our method nearly matches the lower bound  on sample requirements.</s></p></d>", "label": ["<d><p><s>latent graphical model selection: efficient methods for locally tree-like graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the enduring challenges in markov chain monte carlo methodology is the development of proposal mechanisms to make moves distant from the current point, that are accepted with high probability and at low computational cost.</s> <s>the recent introduction of locally adaptive mcmc methods based on the natural underlying riemannian geometry of such models goes some way to alleviating these problems for certain classes of models for which the metric tensor is analytically tractable, however computational efficiency is not assured due to the necessity of potentially high-dimensional matrix operations at each iteration.</s> <s>in this paper we firstly investigate a sampling-based approach for approximating the metric tensor and suggest a valid mcmc algorithm that extends the applicability of riemannian manifold mcmc methods to statistical models that do not admit an analytically computable metric tensor.</s> <s>secondly, we show how the approximation scheme we consider naturally motivates the use of l1 regularisation to improve estimates and obtain a sparse approximate inverse of the metric, which enables stable and sparse approximations of the local geometry to be made.</s> <s>we demonstrate the application of this algorithm for inferring the parameters of a realistic system of ordinary differential equations using a biologically motivated robust student-t error model, for which the expected fisher information is analytically intractable.</s></p></d>", "label": ["<d><p><s>sparse approximate manifolds for differential geometric mcmc</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our personal social networks are big and cluttered, and currently there is no good way to organize them.</s> <s>social networking sites allow users to manually categorize their friends into social circles (e.g.</s> <s>`circles' on google+, and `lists' on facebook and twitter), however they are laborious to construct and must be updated whenever a user's network grows.</s> <s>we define a novel machine learning task of identifying users' social circles.</s> <s>we pose the problem as a node clustering problem on a user's ego-network, a network of connections between her friends.</s> <s>we develop a model for detecting circles that combines network structure as well as user profile information.</s> <s>for each circle we learn its members and the circle-specific user profile similarity metric.</s> <s>modeling node membership to multiple circles allows us to detect overlapping as well as hierarchically nested circles.</s> <s>experiments show that our model accurately identifies circles on a diverse set of data from facebook, google+, and twitter for all of which we obtain hand-labeled ground-truth data.</s></p></d>", "label": ["<d><p><s>learning to discover social circles in ego networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>boolean satisfiability (sat) as a canonical np-complete decision problem is one of the most important problems in computer science.</s> <s>in practice, real-world sat sentences are drawn from a distribution that may result in efficient algorithms for their solution.</s> <s>such sat instances are likely to have shared characteristics and substructures.</s> <s>this work approaches the exploration of a family of sat solvers as a learning problem.</s> <s>in particular, we relate polynomial time solvability of a sat subset to a notion of margin between sentences mapped by a feature function into a hilbert space.</s> <s>provided this mapping is based on polynomial time computable statistics of a sentence, we show that the existance of a margin between these data points implies the existance of a polynomial time solver for that sat subset based on the davis-putnam-logemann-loveland algorithm.</s> <s>furthermore, we show that a simple perceptron-style learning rule will find an optimal sat solver with a bounded number of training updates.</s> <s>we derive a linear time computable set of features and show analytically that margins exist for important polynomial special cases of sat.</s> <s>empirical results show an order of magnitude improvement over a state-of-the-art sat solver on a hardware verification task.</s></p></d>", "label": ["<d><p><s>perceptron learning of sat</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a truncation-free online variational inference algorithm for bayesian nonparametric models.</s> <s>unlike traditional (online) variational inference algorithms that require truncations for the model or the variational distribution, our method adapts model complexity on the fly.</s> <s>our experiments for dirichlet process mixture models and hierarchical dirichlet process topic models on two large-scale data sets show better performance than previous online variational inference algorithms.</s></p></d>", "label": ["<d><p><s>truncation-free online variational inference for bayesian nonparametric models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new variational inference algorithm for gaussian processes with non-conjugate likelihood functions.</s> <s>this includes binary and multi-class classification, as well as ordinal regression.</s> <s>our method constructs a convex lower bound, which can be optimized by using an efficient fixed point update method.</s> <s>we then show empirically that our new approach is much faster than existing methods without any degradation in performance.</s></p></d>", "label": ["<d><p><s>fast bayesian inference for non-conjugate gaussian process regression</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a novel bayesian approach to solve stochastic optimization problems that involve ?nding extrema of noisy, nonlinear functions.</s> <s>previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of ?rst, doing inference over the function space and second, ?nding the extrema of these functions.</s> <s>here we skip the representation step and directly model the distribution over extrema.</s> <s>to this end, we devise a non-parametric conjugate prior where the natural parameter corresponds to a given kernel function and the suf?cient statistic is composed of the observed function values.</s> <s>the resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function.</s></p></d>", "label": ["<d><p><s>a nonparametric conjugate prior distribution for the maximizing argument of a noisy function</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we derive a novel norm that corresponds to the tightest convex   relaxation of sparsity combined with an $\\ell_2$ penalty.</s> <s>we show   that this new norm provides a tighter relaxation than the elastic   net, and is thus a good replacement for the lasso or the elastic net   in sparse prediction problems.</s> <s>but through studying our new norm,   we also bound the looseness of the elastic net, thus shedding new   light on it and providing justification for its use.</s></p></d>", "label": ["<d><p><s>sparse prediction with the </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a key problem in statistics and machine learning is the determination of network structure from data.</s> <s>we consider the case where the structure of the graph to be reconstructed is known to be scale-free.</s> <s>we show that in such cases it is natural to formulate structured sparsity inducing priors using submodular functions, and we use their lovasz extension to obtain a convex relaxation.</s> <s>for tractable classes such as gaussian graphical models, this leads to a convex optimization problem that can be efficiently solved.</s> <s>we show that our method results in an improvement in the accuracy of reconstructed networks for synthetic data.</s> <s>we also show how our prior encourages scale-free reconstructions on a bioinfomatics dataset.</s></p></d>", "label": ["<d><p><s>a convex formulation for learning scale-free networks via submodular relaxation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-metric learning techniques learn local metric tensors in different parts of a feature space.</s> <s>with such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data.</s> <s>the learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way.</s> <s>we prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a riemannian manifold.</s> <s>we then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics.</s> <s>algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the riemannian manifold.</s> <s>together, these tools let many euclidean algorithms take advantage of multi-metric learning.</s> <s>we illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.</s></p></d>", "label": ["<d><p><s>a geometric take on metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hashing is a common method to reduce large, potentially infinite feature vectors to a fixed-size table.</s> <s>in reinforcement learning, hashing is often used in conjunction with tile coding to represent states in continuous spaces.</s> <s>hashing is also a promising approach to value function approximation in large discrete domains such as go and hearts, where feature vectors can be constructed by exhaustively combining a set of atomic features.</s> <s>unfortunately, the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions.</s> <s>recent work in data stream summaries has led to the development of the tug-of-war sketch, an unbiased estimator for approximating inner products.</s> <s>our work investigates the application of this new data structure to linear value function approximation.</s> <s>although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates, we show that this bias can be orders of magnitude less than that of standard hashing.</s> <s>we provide empirical results on two rl benchmark domains and fifty-five atari 2600 games to highlight the superior learning performance of tug-of-war hashing.</s></p></d>", "label": ["<d><p><s>sketch-based linear value function approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in conventional causal discovery, structural equation models (sem) are directly applied to the observed variables, meaning that the causal effect can be represented as a function of the direct causes themselves.</s> <s>however, in many real world problems, there are significant dependencies in the variances or energies, which indicates that causality may possibly take place at the level of variances or energies.</s> <s>in this paper, we propose a probabilistic causal scale-mixture model with spatiotemporal variance dependencies to represent a specific type of generating mechanism of the observations.</s> <s>in particular, the causal mechanism including contemporaneous and temporal causal relations in variances or energies is represented by a structural vector autoregressive model (svar).</s> <s>we prove the identifiability of this model under the non-gaussian assumption on the innovation processes.</s> <s>we also propose algorithms to estimate the involved parameters and discover the contemporaneous causal structure.</s> <s>experiments on synthesis and real world data are conducted to show the applicability of the proposed model and algorithms.</s></p></d>", "label": ["<d><p><s>causal discovery with scale-mixture model for spatiotemporal variance dependencies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.</s> <s>we develop a novel algorithm based on the regularized dual averaging (rda) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss.</s> <s>in particular, for strongly convex loss, it achieves the optimal  rate of $o(\\frac{1}{n}+\\frac{1}{n^2})$ for $n$  iterations, which improves the best known rate $o(\\frac{\\log n }{n})$ of previous stochastic dual averaging algorithms.</s> <s>in addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates.</s> <s>for widely used sparsity-inducing regularizers (e.g., $\\ell_1$-norm), it has the advantage of encouraging sparser solutions.</s> <s>we further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $o(\\frac{1}{n}+\\exp\\{-n\\})$ for strongly convex loss.</s></p></d>", "label": ["<d><p><s>optimal regularized dual averaging methods for stochastic optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the sum-product network (spn) is a recently-proposed deep model consisting of a network of sum and product nodes, and has been shown to be competitive with state-of-the-art deep models on certain difficult tasks such as image completion.</s> <s>designing an spn network architecture that is suitable for the task at hand is an open question.</s> <s>we propose an algorithm for learning the spn architecture from data.</s> <s>the idea is to cluster variables (as opposed to data instances) in order to identify variable subsets that strongly interact with one another.</s> <s>nodes in the spn network are then allocated towards explaining these interactions.</s> <s>experimental evidence shows that learning the spn architecture significantly improves its performance compared to using a previously-proposed static architecture.</s></p></d>", "label": ["<d><p><s>learning the architecture of sum-product networks using clustering on variables</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>imitation learning has been shown to be successful in solving many challenging real-world problems.</s> <s>some recent approaches give strong performance guarantees by training the policy iteratively.</s> <s>however, it is important to note that these guarantees depend on   how well the policy we found can imitate the oracle on the training data.</s> <s>when there is a substantial difference between the oracle's ability  and the learner's policy space, we may fail to find a policy that has low error on the training set.</s> <s>in such cases, we propose to use a coach that demonstrates easy-to-learn actions for the learner  and gradually approaches the oracle.</s> <s>by a reduction of learning by demonstration to online learning,  we prove that coaching can yield a lower regret bound than using the oracle.</s> <s>we apply our algorithm to a novel cost-sensitive dynamic feature selection problem, a hard decision problem that considers a user-specified accuracy-cost trade-off.</s> <s>experimental results on uci datasets show that our method outperforms state-of-the-art imitation learning methods in dynamic features selection and two static feature selection methods.</s></p></d>", "label": ["<d><p><s>imitation learning by coaching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the $\\ell_1$ regularized sparse inverse covariance matrix estimation problem with a very large number of variables.</s> <s>even in the face of this high dimensionality, and with limited number of samples, recent work has shown this estimator to have strong statistical guarantees in recovering the true structure of the sparse inverse covariance matrix, or alternatively the underlying graph structure of the corresponding gaussian markov random field.</s> <s>our proposed algorithm divides the problem into  smaller sub-problems, and uses the solutions of the sub-problems to build a good approximation for the original problem.</s> <s>we derive a bound on the distance of the approximate solution to the true solution.</s> <s>based on this bound, we propose a clustering algorithm that attempts to minimize this bound, and in practice, is able to find effective partitions of the variables.</s> <s>we further use the approximate solution, i.e., solution resulting from solving the sub-problems,  as an initial point to solve the original problem, and achieve a much faster computational procedure.</s> <s>as an example, a recent state-of-the-art method, quic requires 10 hours to solve a problem (with 10,000 nodes) that arises from a climate application, while our proposed algorithm, divide and conquer quic (dc-quic) only requires one hour to solve the problem.</s></p></d>", "label": ["<d><p><s>a divide-and-conquer method for sparse inverse covariance estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a novel non-parametric approximate dynamic programming (adp) algorithm that enjoys graceful, dimension-independent approximation and sample complexity guarantees.</s> <s>in particular, we establish both theoretically and computationally that our proposal can serve as a viable alternative to state-of-the-art parametric adp algorithms, freeing the designer from carefully specifying an approximation architecture.</s> <s>we accomplish this by developing a kernel-based mathematical program for adp.</s> <s>via a computational study on a controlled queueing network, we show that our non-parametric procedure is competitive with parametric adp approaches.</s></p></d>", "label": ["<d><p><s>non-parametric approximate dynamic programming via the kernel method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new algorithm for differentially private data release, based on a simple combination of the exponential mechanism with the multiplicative weights update rule.</s> <s>our mwem algorithm achieves what are the best known and nearly optimal theoretical guarantees, while at the same time being simple to implement and experimentally more accurate on actual data sets than existing techniques.</s></p></d>", "label": ["<d><p><s>a simple and practical algorithm for differentially private data release</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the paper addresses the problem of generating multiple hypotheses for prediction tasks that involve interaction with users or successive components in a cascade.</s> <s>given a set of multiple hypotheses, such components/users have the ability to automatically rank the results and thus retrieve the best one.</s> <s>the standard approach for handling this scenario is to learn a single model and then produce m-best maximum a posteriori (map) hypotheses from this model.</s> <s>in contrast, we formulate this multiple {\\em choice} learning task as a multiple-output structured-output prediction problem with a loss function that captures the natural setup of the problem.</s> <s>we present a max-margin formulation  that minimizes an upper-bound on this loss-function.</s> <s>experimental results on the problems of image co-segmentation and protein side-chain prediction show that our method outperforms conventional approaches used for this  scenario and leads to substantial improvements in prediction accuracy.</s></p></d>", "label": ["<d><p><s>multiple choice learning: learning to produce multiple structured outputs</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider derivative-free algorithms for stochastic optimization problems that use only noisy function values rather than gradients, analyzing their finite-sample convergence rates.</s> <s>we show that if pairs of function values are available, algorithms that use gradient estimates based on random perturbations suffer a factor of at most $\\sqrt{\\dim}$ in convergence rate over traditional stochastic gradient methods, where $\\dim$ is the dimension of the problem.</s> <s>we complement our algorithmic development with information-theoretic lower bounds on the minimax convergence rate of such problems, which show that our bounds are sharp with respect to all problem-dependent quantities: they cannot be improved by more than constant factors.</s></p></d>", "label": ["<d><p><s>finite sample convergence rates of zero-order stochastic optimization methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper adresses the inverse reinforcement learning (irl) problem, that is inferring a reward for which a demonstrated expert behavior is optimal.</s> <s>we introduce a new algorithm, scirl, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multi-class classifier.</s> <s>this approach produces a reward function for which the expert policy is provably near-optimal.</s> <s>contrary to most of existing irl algorithms, scirl does not require solving the direct rl problem.</s> <s>moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior.</s> <s>this is illustrated on a car driving simulator.</s></p></d>", "label": ["<d><p><s>inverse reinforcement learning through structured classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>active learning can substantially improve the yield of neurophysiology experiments by adaptively selecting stimuli to probe a neuron's receptive field (rf) in real time.</s> <s>bayesian active learning methods maintain a posterior distribution over the rf, and select stimuli to maximally reduce posterior entropy on each time step.</s> <s>however, existing methods tend to rely on simple gaussian priors, and do not exploit uncertainty at the level of hyperparameters when determining an optimal stimulus.</s> <s>this uncertainty can play a substantial role in rf characterization, particularly when rfs are smooth, sparse, or local in space and time.</s> <s>in this paper, we describe a novel framework for active learning under hierarchical, conditionally gaussian priors.</s> <s>our algorithm uses sequential markov chain monte carlo sampling (''particle filtering'' with mcmc) over hyperparameters to construct a mixture-of-gaussians representation of the rf posterior, and selects optimal stimuli using an approximate infomax criterion.</s> <s>the core elements of this algorithm are parallelizable, making it computationally efficient for real-time experiments.</s> <s>we apply our algorithm to simulated and real neural data, and show that it can provide highly accurate receptive field estimates from very limited data, even with a small number of hyperparameter samples.</s></p></d>", "label": ["<d><p><s>bayesian active learning with localized priors for fast receptive field characterization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an efficient, generalized, nonparametric, statistical kolmogorov-smirnov test for detecting distributional change in high-dimensional data.</s> <s>to implement the test, we introduce a novel, hierarchical, minimum-volume sets estimator to represent the distributions to be tested.</s> <s>our work is motivated by the need to detect changes in data streams, and the test is especially efficient in this context.</s> <s>we provide the theoretical foundations of our test and show its superiority over existing methods.</s></p></d>", "label": ["<d><p><s>learning high-density regions for a generalized kolmogorov-smirnov test in high-dimensional data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov logic is a widely used tool in statistical relational learning, which uses a weighted first-order logic knowledge base to specify a markov random field (mrf) or a conditional random field (crf).</s> <s>in many applications, a markov logic network (mln) is trained in one domain, but used in a different one.</s> <s>this paper focuses on dynamic markov logic networks, where the domain of time points typically varies between training and testing.</s> <s>it has been previously pointed out that the marginal probabilities of truth assignments to ground atoms can change if one extends or reduces the domains of predicates in an mln.</s> <s>we show that in addition to this problem, the standard way of unrolling a markov logic theory into a mrf may result in time-inhomogeneity of the underlying markov chain.</s> <s>furthermore, even if these representational problems are not significant for a given domain, we show that the more practical problem of generating samples in a sequential conditional random field for the next slice relying on the samples from the previous slice has high computational cost in the general case, due to the need to estimate a normalization factor for each sample.</s> <s>we propose a new discriminative model, slice normalized dynamic markov logic networks (sn-dmln), that suffers from none of these issues.</s> <s>it supports efficient online inference, and can directly model influences between variables within a time slice that do not have a causal direction, in contrast with fully directed models (e.g., dbns).</s> <s>experimental results show an improvement in accuracy over previous approaches to online inference in dynamic markov logic networks.</s></p></d>", "label": ["<d><p><s>slice normalized dynamic markov logic networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent experiments have demonstrated that humans and animals typically reason probabilistically about their environment.</s> <s>this ability requires a neural code that represents probability distributions and neural circuits that are capable of implementing the operations of probabilistic inference.</s> <s>the proposed probabilistic population coding (ppc) framework provides a statistically efficient neural representation of probability distributions that is both broadly consistent with physiological measurements and capable of implementing some of the basic operations of probabilistic inference in a biologically plausible way.</s> <s>however, these experiments and the corresponding neural models have largely focused on simple (tractable) probabilistic computations such as cue combination, coordinate transformations, and decision making.</s> <s>as a result it remains unclear how to generalize this framework to more complex probabilistic computations.</s> <s>here we address this short coming by showing that a very general approximate inference algorithm known as variational bayesian expectation maximization can be implemented within the linear ppc framework.</s> <s>we apply this approach to a generic problem faced by any given layer of cortex, namely the identification of latent causes of complex mixtures of spikes.</s> <s>we identify a formal equivalent between this spike pattern demixing problem and topic models used for document classification, in particular latent dirichlet allocation (lda).</s> <s>we then construct a neural network implementation of variational inference and learning for lda that utilizes a linear ppc.</s> <s>this network relies critically on two non-linear operations: divisive normalization and super-linear facilitation, both of which are ubiquitously observed in neural circuits.</s> <s>we also demonstrate how online learning can be achieved using a variation of hebb?s rule and describe an extesion of this work which allows us to deal with time varying and correlated latent causes.</s></p></d>", "label": ["<d><p><s>complex inference in neural circuits with probabilistic population codes and topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>users want natural language processing (nlp) systems to be both fast and accurate, but quality often comes at the cost of speed.</s> <s>the field has been manually exploring various speed-accuracy tradeoffs (for particular problems and datasets).</s> <s>we aim to explore this space automatically, focusing here on the case of agenda-based syntactic parsing \\cite{kay-1986}.</s> <s>unfortunately, off-the-shelf reinforcement learning techniques fail to learn good policies: the state space is simply too large to explore naively.</s> <s>an attempt to counteract this by applying imitation learning algorithms also fails: the ``teacher'' is far too good to successfully imitate with our inexpensive features.</s> <s>moreover, it is not specifically tuned for the known reward function.</s> <s>we propose a hybrid reinforcement/apprenticeship learning algorithm that, even with only a few inexpensive features, can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state-of-the-art baselines.</s></p></d>", "label": ["<d><p><s>learned prioritization for trading off accuracy and speed</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider sequential prediction algorithms that are given the predictions from a set of models as inputs.</s> <s>if the nature of the data is changing over time in that different models predict well on different segments of the data, then adaptivity is typically achieved by mixing into the weights in each round a bit of the initial prior (kind of like a weak restart).</s> <s>however, what if the favored models in each segment are from a small subset, i.e.</s> <s>the data is likely to be predicted well by models that predicted well before?</s> <s>curiously, fitting such ''sparse composite models'' is achieved by mixing in a bit of all the past posteriors.</s> <s>this self-referential updating method is rather peculiar, but it is efficient and gives superior performance on many natural data sets.</s> <s>also it is important because it introduces a long-term memory: any model that has done well in the past can be recovered quickly.</s> <s>while bayesian interpretations can be found for mixing in a bit of the initial prior, no bayesian interpretation is known for mixing in past posteriors.</s> <s>we build atop the ''specialist'' framework from the online learning literature to give the mixing past posteriors update a proper bayesian foundation.</s> <s>we apply our method to a well-studied multitask learning problem and obtain a new intriguing efficient update that achieves a significantly better bound.</s></p></d>", "label": ["<d><p><s>putting bayes to sleep</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite the variety of robust regression methods that have been developed, current regression formulations are either np-hard, or allow unbounded response to even a single leverage point.</s> <s>we present a general formulation for robust regression --variational m-estimation--that unifies a number of robust regression methods while allowing a tractable approximation strategy.</s> <s>we develop an estimator that requires only polynomial-time, while achieving certain robustness and consistency guarantees.</s> <s>an experimental evaluation demonstrates the effectiveness of the new estimation approach  compared to standard methods.</s></p></d>", "label": ["<d><p><s>a polynomial-time form of robust regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a multi-task learning approach to jointly estimate the means of multiple independent data sets.</s> <s>the proposed multi-task averaging (mta) algorithm results in a convex combination of the single-task averages.</s> <s>we derive the optimal amount of regularization, and show that it can be effectively estimated.</s> <s>simulations and real data experiments demonstrate that mta  both maximum likelihood and james-stein estimators, and that our approach to estimating the amount of regularization rivals cross-validation in performance but is more computationally efficient.</s></p></d>", "label": ["<d><p><s>multi-task averaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications, one has information, e.g., labels that are  provided in a semi-supervised manner, about a specific target region of a  large data set, and one wants to perform machine learning and data analysis  tasks nearby that pre-specified target region.</s> <s>locally-biased problems of this sort are particularly challenging for  popular eigenvector-based machine learning and data analysis tools.</s> <s>at root, the reason is that eigenvectors are inherently global quantities.</s> <s>in this paper, we address this issue by providing a methodology to construct  semi-supervised eigenvectors of a graph laplacian, and we illustrate  how these locally-biased eigenvectors can be used to perform  locally-biased machine learning.</s> <s>these semi-supervised eigenvectors capture successively-orthogonalized  directions of maximum variance, conditioned on being well-correlated with an  input seed set of nodes that is assumed to be provided in a semi-supervised  manner.</s> <s>we also provide several empirical examples demonstrating how these  semi-supervised eigenvectors can be used to perform locally-biased learning.</s></p></d>", "label": ["<d><p><s>semi-supervised eigenvectors for locally-biased learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>label space dimension reduction (lsdr) is an efficient and effective paradigm for multi-label classification with many classes.</s> <s>existing approaches to lsdr, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part.</s> <s>in this paper, we propose a novel approach to lsdr that considers both the label and the feature parts.</s> <s>the approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular hamming loss.</s> <s>the minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition.</s> <s>in addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist lsdr.</s> <s>the experimental results verify that the proposed approach is more effective than existing ones to lsdr across many real-world datasets.</s></p></d>", "label": ["<d><p><s>feature-aware label space dimension reduction for multi-label classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of category-level 3d object detection.</s> <s>given a monocular image, our aim is to localize the objects  in 3d by enclosing them with tight  oriented 3d bounding boxes.</s> <s>we propose a novel approach that extends the well-acclaimed deformable part-based model[felz.]</s> <s>to reason in 3d.</s> <s>our model represents an object class as a deformable 3d cuboid composed of faces and parts, which are both allowed to deform with respect to their anchors on the 3d box.</s> <s>we model the appearance of each face in fronto-parallel coordinates, thus effectively factoring out the appearance variation induced by viewpoint.</s> <s>our model reasons about face visibility patters called aspects.</s> <s>we train the cuboid model jointly and discriminatively and share weights across all aspects to attain efficiency.</s> <s>inference then entails sliding and rotating the box in 3d and scoring object hypotheses.</s> <s>while for inference we discretize the search space, the variables are  continuous in our model.</s> <s>we demonstrate the effectiveness of our approach in indoor and outdoor scenarios, and show that our approach outperforms the state-of-the-art in both 2d[felz09] and 3d object detection[hedau12].</s></p></d>", "label": ["<d><p><s>3d object detection and viewpoint estimation with a deformable 3d cuboid model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to k sites, and the sites are required to communicate with each other via the coordinator.</s> <s>at each time-step t, one of the k site nodes has to pick an expert from the set {1, .</s> <s>.</s> <s>.</s> <s>, n}, and the same site receives information about payoffs of all experts for that round.</s> <s>the goal of the distributed system is to minimize regret at time horizon t, while simultaneously keeping communication to a minimum.</s> <s>the two extreme solutions to this problem are: (i) full communication: this essentially simulates the non-distributed setting to obtain the optimal o(\\sqrt{log(n)t}) regret bound at the cost of t communication.</s> <s>(ii) no communication: each site runs an independent copy ?</s> <s>the regret is o(\\sqrt{log(n)kt}) and the communication is 0.</s> <s>this paper shows the difficulty of simultaneously achieving regret asymptotically better than \\sqrt{kt} and communication better than t. we give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret o(\\sqrt{k^{5(1+\\epsilon)/6} t}) and communication o(t/k^\\epsilon), for any value of \\epsilon in (0, 1/5).</s> <s>we also consider a variant of the model, where the coordinator picks the expert.</s> <s>in this model, we show that the label-efficient forecaster of cesa-bianchi et al.</s> <s>(2005) already gives us strategy that is near optimal in regret vs communication trade-off.</s></p></d>", "label": ["<d><p><s>distributed non-stochastic experts</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we argue for representing networks as a bag of {\\it triangular motifs}, particularly for important network problems that current model-based approaches handle poorly due to computational bottlenecks incurred by using edge representations.</s> <s>such approaches require both 1-edges and 0-edges (missing edges) to be provided as input, and as a consequence, approximate inference algorithms for these models usually require $\\omega(n^2)$ time per iteration, precluding their application to larger real-world networks.</s> <s>in contrast, triangular modeling requires less computation, while providing equivalent or better inference quality.</s> <s>a triangular motif is a vertex triple containing 2 or 3 edges, and the number of such motifs is $\\theta(\\sum_{i}d_{i}^{2})$ (where $d_i$ is the degree of vertex $i$), which is much smaller than $n^2$ for low-maximum-degree networks.</s> <s>using this representation, we develop a novel mixed-membership network model and approximate inference algorithm suitable for large networks with low max-degree.</s> <s>for networks with high maximum degree, the triangular motifs can be naturally subsampled in a {\\it node-centric} fashion, allowing for much faster inference at a small cost in accuracy.</s> <s>empirically, we demonstrate that our approach, when compared to that of an edge-based model, has faster runtime and improved accuracy for mixed-membership community detection.</s> <s>we conclude with a large-scale demonstration on an $n\\approx 280,000$-node network, which is infeasible for network models with $\\omega(n^2)$ inference cost.</s></p></d>", "label": ["<d><p><s>on triangular versus edge representations --- towards scalable modeling of networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>principal components analysis (pca) is a standard tool for identifying good low-dimensional approximations to data sets in high dimension.</s> <s>many current data sets of interest contain private or sensitive information about individuals.</s> <s>algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs.</s> <s>differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs.</s> <s>in this paper we investigate the theory and empirical performance of differentially private approximations to pca and propose a new method which explicitly optimizes the utility of the output.</s> <s>we demonstrate that on real data, there this a large performance gap between the existing methods and our method.</s> <s>we show that the sample complexity for the two procedures differs in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling.</s></p></d>", "label": ["<d><p><s>near-optimal differentially private principal components</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the scalability of consensus-based distributed optimization algorithms by considering two questions: how many processors should we use for a given problem, and how often should they communicate when communication is not free?</s> <s>central to our analysis is a problem-specific value $r$ which quantifies the communication/computation tradeoff.</s> <s>we show that organizing the communication among nodes as a $k$-regular expander graph~\\cite{kregexpanders} yields speedups, while when all pairs of nodes communicate (as in a complete graph), there is an optimal number of processors that depends on $r$.</s> <s>surprisingly, a speedup can be obtained, in terms of the time to reach a fixed level of accuracy, by communicating less and less frequently as the computation progresses.</s> <s>experiments on a real cluster solving metric learning and non-smooth convex minimization tasks demonstrate strong agreement between theory and practice.</s></p></d>", "label": ["<d><p><s>communication/computation tradeoffs in consensus-based distributed optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.</s> <s>the negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-poisson variability.</s> <s>here we describe a powerful data-augmentation framework for fully bayesian inference in neural models with negative-binomial spiking.</s> <s>our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a polya-gamma mixture of normals.</s> <s>this framework provides a tractable, conditionally gaussian representation of the posterior that can be used to design efficient em and gibbs sampling based algorithms for inference in regression and dynamic factor models.</s> <s>we apply the model to neural data from primate retina and show that it substantially outperforms poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains.</s></p></d>", "label": ["<d><p><s>fully bayesian inference for neural models with negative-binomial spiking</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the problem of maximum marginal prediction (mmp) in probabilistic graphical models, a task that occurs, for example, as the bayes optimal decision rule under a hamming loss.</s> <s>mmp is typically performed as a two-stage procedure: one estimates each variable's marginal probability and then forms a prediction from the states of maximal probability.</s> <s>in this work we propose a simple yet effective technique for accelerating mmp when inference is sampling-based: instead of the above two-stage procedure we directly estimate the posterior probability of each decision variable.</s> <s>this allows us to identify the point of time when we are sufficiently certain about any individual decision.</s> <s>whenever this is the case, we dynamically prune the variable we are confident about from the underlying factor graph.</s> <s>consequently, at any time only samples of variable whose decision is still uncertain need to be created.</s> <s>experiments in two prototypical scenarios, multi-label classification and image inpainting, shows that adaptive sampling can drastically accelerate mmp without sacrificing prediction accuracy.</s></p></d>", "label": ["<d><p><s>dynamic pruning of factor graphs for maximum marginal prediction</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>counterfactual regret minimization (cfr) is a popular, iterative algorithm for computing strategies in extensive-form games.</s> <s>the monte carlo cfr (mccfr) variants reduce the per iteration time cost of cfr by traversing a sampled portion of the tree.</s> <s>the previous most effective instances of mccfr can still be very slow in games with many player actions since they sample every action for a given player.</s> <s>in this paper, we present a new mccfr algorithm, average strategy sampling (as), that samples a subset of the player's actions according to the player's average strategy.</s> <s>our new algorithm is inspired by a new, tighter bound on the number of iterations required by cfr to converge to a given solution quality.</s> <s>in addition, we prove a similar, tighter bound for as and other popular mccfr variants.</s> <s>finally, we validate our work by demonstrating that as converges faster than previous mccfr algorithms in both no-limit poker and bluff.</s></p></d>", "label": ["<d><p><s>efficient monte carlo counterfactual regret minimization in games with many player actions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given their pervasive use, social media, such as twitter, have become a leading source of breaking news.</s> <s>a key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner.</s> <s>motivated by this challenge, we introduce the problem of online l1-dictionary learning where unlike traditional dictionary learning, which uses squared loss, the l1-penalty is used for measuring the reconstruction error.</s> <s>we present an efficient online algorithm for this problem based on alternating directions method of multipliers, and establish a sublinear regret bound for this algorithm.</s> <s>empirical results on news-stream and twitter data, shows that this online l1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm, without any significant loss in quality of results.</s> <s>our algorithm for online l1-dictionary learning could be of independent interest.</s></p></d>", "label": ["<d><p><s>online l1-dictionary learning with application to novel document detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a scalable algorithm for posterior inference of overlapping communities in large networks.</s> <s>our algorithm is based on stochastic variational inference in the mixed-membership stochastic blockmodel.</s> <s>it naturally interleaves subsampling the network with estimating its community structure.</s> <s>we apply our algorithm on ten large, real-world networks with up to 60,000 nodes.</s> <s>it converges several orders of magnitude faster than the state-of-the-art algorithm for mmsb, finds hundreds of communities in large real-world networks, and detects the true communities in 280 benchmark networks with equal or better accuracy compared to other scalable algorithms.</s></p></d>", "label": ["<d><p><s>scalable inference of overlapping communities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse graphical modelling/inverse covariance selection is an important problem in machine learning and has seen significant advances in recent years.</s> <s>a major focus has been on methods which perform model selection in high dimensions.</s> <s>to this end, numerous convex $\\ell_1$ regularization approaches have been proposed in the literature.</s> <s>it is not however clear which of these methods are optimal in any well-defined sense.</s> <s>a major gap in this regard pertains to the rate of convergence of proposed optimization methods.</s> <s>to address this, an iterative thresholding algorithm for numerically solving the $\\ell_1$-penalized maximum likelihood problem for sparse inverse covariance estimation is presented.</s> <s>the proximal gradient method considered in this paper is shown to converge at a linear rate, a result which is the first of its kind for numerically solving the sparse inverse covariance estimation problem.</s> <s>the convergence rate is provided in closed form, and is related to the condition number of the optimal point.</s> <s>numerical results demonstrating the proven rate of convergence are presented.</s></p></d>", "label": ["<d><p><s>iterative thresholding algorithm for sparse inverse covariance estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many practical machine learning problems, the acquisition of labeled data is often expensive and/or time consuming.</s> <s>this motivates us to study a problem as follows: given a label budget, how to select data points to label such that the learning performance is optimized.</s> <s>we propose a selective labeling method by analyzing the generalization error of laplacian regularized least squares (laprls).</s> <s>in particular, we derive a deterministic generalization error bound for laprls trained on subsampled data, and propose to select a subset of data points to label by minimizing this upper bound.</s> <s>since the minimization is a combinational problem, we relax it into continuous domain and solve it by projected gradient descent.</s> <s>experiments on benchmark datasets show that the proposed method outperforms the state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>selective labeling via error bound minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>parametric policy search algorithms are one of the methods of choice for the optimisation of markov decision processes, with expectation maximisation and natural gradient ascent being considered the current state of the art in the field.</s> <s>in this article we provide a unifying perspective of these two algorithms by showing that their step-directions in the parameter space are closely related to the search direction of an approximate newton method.</s> <s>this analysis leads naturally to the consideration of this approximate newton method as an alternative gradient-based method for markov decision processes.</s> <s>we are able show that the algorithm has numerous desirable properties, absent in the naive application of newton's method, that make it a viable alternative to either expectation maximisation or natural gradient ascent.</s> <s>empirical results suggest that the algorithm has excellent convergence and robustness properties, performing strongly in comparison to both expectation maximisation and natural gradient ascent.</s></p></d>", "label": ["<d><p><s>a unifying perspective of parametric policy search methods for markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determinantal point processes (dpps) have recently been proposed as   computationally efficient probabilistic models of diverse sets for a   variety of applications, including document summarization, image   search, and pose estimation.</s> <s>many dpp inference operations,   including normalization and sampling, are tractable; however,   finding the most likely configuration (map), which is often required   in practice for decoding, is np-hard, so we must resort to   approximate inference.</s> <s>because dpp probabilities are   log-submodular, greedy algorithms have been used in the past with   some empirical success; however, these methods only give   approximation guarantees in the special case of dpps with monotone   kernels.</s> <s>in this paper we propose a new algorithm for approximating   the map problem based on continuous techniques for submodular   function maximization.</s> <s>our method involves a novel continuous   relaxation of the log-probability function, which, in contrast to   the multilinear extension used for general submodular functions, can   be evaluated and differentiated exactly and efficiently.</s> <s>we obtain   a practical algorithm with a 1/4-approximation guarantee for a   general class of non-monotone dpps.</s> <s>our algorithm also extends to   map inference under complex polytope constraints, making it possible   to combine dpps with markov random fields, weighted matchings, and   other models.</s> <s>we demonstrate that our approach outperforms greedy   methods on both synthetic and real-world data.</s></p></d>", "label": ["<d><p><s>near-optimal map inference for determinantal point processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an abstract class of optimization problems that are parameterized concavely in a single parameter, and show that the solution path along the parameter can always be approximated with accuracy $\\varepsilon >0$ by a set of size $o(1/\\sqrt{\\varepsilon})$.</s> <s>a lower bound of size $\\omega (1/\\sqrt{\\varepsilon})$ shows that the upper bound is tight up to a constant factor.</s> <s>we also devise an algorithm that calls a step-size oracle and computes an approximate path of size $o(1/\\sqrt{\\varepsilon})$.</s> <s>finally, we provide an implementation of the oracle for soft-margin support vector machines, and a parameterized semi-definite program for matrix completion.</s></p></d>", "label": ["<d><p><s>approximating concavely parameterized optimization problems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>factor analysis models effectively summarise the covariance structure of high dimensional data, but the solutions are typically hard to interpret.</s> <s>this motivates attempting to find a disjoint partition, i.e.</s> <s>a clustering, of observed variables so that variables in a cluster are highly correlated.</s> <s>we introduce a bayesian non-parametric approach to this problem, and demonstrate advantages over heuristic methods proposed to date.</s></p></d>", "label": ["<d><p><s>a nonparametric variable clustering model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper aims to take a step forwards making the term ``intrinsic motivation'' from reinforcement learning theoretically well founded, focusing on curiosity-driven learning.</s> <s>to that end, we consider the setting where, a fixed partition p of a continuous space x being given, and a process \\nu defined on x being unknown, we are asked to sequentially decide which cell of the partition to select as well as where to sample \\nu in that cell, in order to minimize a loss function that is inspired from previous work on curiosity-driven learning.</s> <s>the loss on each cell consists of one term measuring a simple worst case quadratic sampling error, and a penalty term proportional to the range of the variance in that cell.</s> <s>the corresponding problem formulation extends the setting known as active learning for multi-armed bandits to the case when each arm is a continuous region, and we show how an adaptation of recent algorithms for that problem and of hierarchical optimistic sampling algorithms for optimization can be used in order to solve this problem.</s> <s>the resulting procedure, called hierarchical optimistic region selection driven by curiosity (horse.c) is provided together with a finite-time regret analysis.</s></p></d>", "label": ["<d><p><s>hierarchical optimistic region selection driven by curiosity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>information, disease, and influence diffuse over networks of entities in both natural systems and human society.</s> <s>analyzing these transmission networks plays an important role in understanding the diffusion processes and predicting events in the future.</s> <s>however, the underlying transmission networks are often hidden and incomplete, and we observe only the time stamps when cascades of events happen.</s> <s>in this paper, we attempt to address the challenging problem of uncovering the hidden network only from the cascades.</s> <s>the structure discovery problem is complicated by the fact that the influence among different entities in a network are heterogeneous, which can not be described by a simple parametric model.</s> <s>therefore, we propose a kernel-based method which can capture a diverse range of different types of influence without any prior assumption.</s> <s>in both synthetic and real cascade data, we show that our model can better recover the underlying diffusion network and drastically improve the estimation of the influence functions between networked entities.</s></p></d>", "label": ["<d><p><s>learning networks of heterogeneous influence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>topic modeling is a widely used approach to analyzing large text collections.</s> <s>a small number of multilingual topic models have recently been explored to discover latent topics among parallel or comparable documents, such as in wikipedia.</s> <s>other topic models that were originally proposed for structured data are also applicable to multilingual documents.</s> <s>correspondence latent dirichlet allocation (corrlda) is one such model; however, it requires a pivot language to be specified in advance.</s> <s>we propose a new topic model, symmetric correspondence lda (symcorrlda), that incorporates a hidden variable to control a pivot language, in an extension of corrlda.</s> <s>we experimented with two multilingual comparable datasets extracted from wikipedia and demonstrate that symcorrlda is more effective than some other existing multilingual topic models.</s></p></d>", "label": ["<d><p><s>symmetric correspondence topic models for multilingual text analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our central goal is to quantify the long-term progression of pediatric neurological diseases, such as a typical 10-15 years progression of child dystonia.</s> <s>to this purpose, quantitative models are convincing only if they can provide multi-scale details ranging from neuron spikes to limb biomechanics.</s> <s>the models also need to be evaluated in hyper-time, i.e.</s> <s>significantly faster than real-time, for producing useful predictions.</s> <s>we designed a platform with digital vlsi hardware for multi-scale hyper-time emulations of human motor nervous systems.</s> <s>the platform is constructed on a scalable, distributed array of field programmable gate array (fpga) devices.</s> <s>all devices operate asynchronously with 1 millisecond time granularity, and the overall system is accelerated to 365x real-time.</s> <s>each physiological component is implemented using models from well documented studies and can be flexibly modified.</s> <s>thus the validity of emulation can be easily advised by neurophysiologists and clinicians.</s> <s>for maximizing the speed of emulation, all calculations are implemented in combinational logic instead of clocked iterative circuits.</s> <s>this paper presents the methodology of building fpga modules in correspondence to components of a monosynaptic spinal loop.</s> <s>results of emulated activities are shown.</s> <s>the paper also discusses the rationale of approximating neural circuitry by organizing neurons with sparse interconnections.</s> <s>in conclusion, our platform allows introducing various abnormalities into the neural emulation such that the emerging motor symptoms can be analyzed.</s> <s>it compels us to test the origins of childhood motor disorders and predict their long-term progressions.</s></p></d>", "label": ["<d><p><s>multi-scale hyper-time hardware emulation of human motor nervous system based on spiking neurons using fpga</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes a new acoustic model based on variational gaussian process dynamical system (vgpds) for phoneme classification.</s> <s>the proposed model overcomes the limitations of the classical hmm in modeling the real speech data, by adopting a nonlinear and nonparametric model.</s> <s>in our model, the gp prior on the dynamics function enables representing the complex dynamic structure of speech, while the gp prior on the emission function successfully models the global dependency over the observations.</s> <s>additionally, we introduce variance constraint to the original vgpds for mitigating sparse approximation error of the kernel matrix.</s> <s>the effectiveness of the proposed model is demonstrated with extensive experimental results including parameter estimation, classification performance on the synthetic and benchmark datasets.</s></p></d>", "label": ["<d><p><s>phoneme classification using constrained variational gaussian process dynamical system</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>both random fourier features and the nystr?m method have been successfully applied to efficient kernel learning.</s> <s>in this work, we investigate the fundamental difference between these two approaches, and how the difference could affect their generalization performances.</s> <s>unlike approaches based on random fourier features  where the basis functions (i.e., cosine and sine functions) are sampled from a distribution  {\\it independent} from the training data, basis functions used by the nystr?m method are randomly sampled from the training examples and are therefore {\\it data dependent}.</s> <s>by exploring this difference, we show that when there is a large gap in the eigen-spectrum of the kernel matrix, approaches based the nystr?m method can yield  impressively  better generalization error bound than random fourier features based approach.</s> <s>we empirically verify our theoretical findings on a wide range of large data sets.</s></p></d>", "label": ["<d><p><s>nystr?m method vs random fourier features: a theoretical and empirical comparison</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discrete mixtures are used routinely in broad sweeping applications ranging from unsupervised settings to fully supervised multi-task learning.</s> <s>indeed, finite mixtures and infinite mixtures, relying on dirichlet processes and modifications, have become a standard tool.</s> <s>one important issue that arises in using discrete mixtures is low separation in the components; in particular, different components can be introduced that are very similar and hence redundant.</s> <s>such redundancy leads to too many clusters that are too similar, degrading performance in unsupervised learning and leading to computational problems and an unnecessarily complex model in supervised settings.</s> <s>redundancy can arise in the absence of a penalty on components placed close together even when a bayesian approach is used to learn the number of components.</s> <s>to solve this problem, we propose a novel prior that generates components from a repulsive process, automatically penalizing redundant components.</s> <s>we characterize this repulsive prior theoretically and propose a markov chain monte carlo sampling algorithm for posterior computation.</s> <s>the methods are illustrated using synthetic examples and an iris data set.</s></p></d>", "label": ["<d><p><s>repulsive mixtures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent svms (lsvms) are a class of powerful tools that have been successfully applied to many applications in computer vision.</s> <s>however, a limitation of lsvms is that they rely on linear models.</s> <s>for many computer vision tasks, linear models are suboptimal and nonlinear models learned with kernels typically perform much better.</s> <s>therefore it is desirable to develop the kernel version of lsvm.</s> <s>in this paper, we propose kernel latent svm (klsvm) -- a new learning framework that combines latent svms and kernel methods.</s> <s>we develop an iterative training algorithm to learn the model parameters.</s> <s>we demonstrate the effectiveness of klsvm using three different applications in visual recognition.</s> <s>our klsvm formulation is very general and can be applied to solve a wide range of applications in computer vision and machine learning.</s></p></d>", "label": ["<d><p><s>kernel latent svm for visual recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we present a bayesian framework for multilabel classification using compressed sensing.</s> <s>the key idea in compressed sensing for multilabel classification is to first project the label vector to a lower dimensional space using a random transformation and then learn regression functions over these projections.</s> <s>our approach considers both of these components in a single probabilistic model, thereby jointly optimizing over compression as well as learning tasks.</s> <s>we then derive an efficient variational inference scheme that provides joint posterior distribution over all the unobserved labels.</s> <s>the two key benefits of the model are that a) it can naturally handle datasets that have  missing labels and b) it can also measure uncertainty in prediction.</s> <s>the uncertainty estimate provided by the model naturally allows for active learning paradigms where an oracle provides information about labels that promise to be maximally informative for the prediction task.</s> <s>our experiments show significant boost over prior methods in terms of prediction performance over benchmark datasets, both in the fully labeled and the missing labels case.</s> <s>finally, we also highlight various useful active learning scenarios that are enabled by the probabilistic model.</s></p></d>", "label": ["<d><p><s>multilabel classification using bayesian compressed sensing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we offer a regularized, kernel extension of the multi-set, orthogonal procrustes problem, or hyperalignment.</s> <s>our new method, called kernel hyperalignment, expands the scope of hyperalignment to include nonlinear measures of similarity and enables the alignment of multiple datasets with a large number of base features.</s> <s>with direct application to fmri data analysis, kernel hyperalignment is well-suited for multi-subject alignment of large rois, including the entire cortex.</s> <s>we conducted experiments using real-world, multi-subject fmri data.</s></p></d>", "label": ["<d><p><s>kernel hyperalignment</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent spiking network models of bayesian inference and unsupervised learning frequently assume either inputs to arrive in a special format or employ complex computations in neuronal activation functions and synaptic plasticity rules.</s> <s>here we show in a rigorous mathematical treatment how homeostatic processes, which have previously received little attention in this context, can overcome common theoretical limitations and facilitate the neural implementation and performance of existing models.</s> <s>in particular, we show that homeostatic plasticity can be understood as the enforcement of a 'balancing' posterior constraint during probabilistic inference and learning with expectation maximization.</s> <s>we link homeostatic dynamics to the theory of variational inference, and show that nontrivial terms, which typically appear during probabilistic inference in a large class of models, drop out.</s> <s>we demonstrate the feasibility of our approach in a spiking winner-take-all architecture of bayesian inference and learning.</s> <s>finally, we sketch how the mathematical framework can be extended to richer recurrent network architectures.</s> <s>altogether, our theory provides a novel perspective on the interplay of homeostatic processes and synaptic plasticity in cortical microcircuits, and points to an essential role of homeostasis during inference and learning in spiking networks.</s></p></d>", "label": ["<d><p><s>homeostatic plasticity in bayesian spiking networks as expectation maximization with posterior constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe the tradeoff between the performance in a visual recognition problem and the control authority that the agent can exercise on the sensing process.</s> <s>we focus on the problem of ?visual search?</s> <s>of an object in an otherwise known and static scene, propose a measure of control authority, and relate it to the expected risk and its proxy (conditional entropy of the posterior density).</s> <s>we show this analytically, as well as empirically by simulation using the simplest known model that captures the phenomenology of image formation, including scaling and occlusions.</s> <s>we show that a ?passive?</s> <s>agent given a training set can provide no guarantees on performance beyond what is afforded by the priors, and that an ?omnipotent?</s> <s>agent, capable of infinite control authority, can achieve arbitrarily good performance (asymptotically).</s></p></d>", "label": ["<d><p><s>controlled recognition bounds for visual learning and exploration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>memristive devices have recently been proposed as efficient implementations of plastic synapses in neuromorphic systems.</s> <s>the plasticity in these memristive devices, i.e.</s> <s>their resistance change, is defined by the applied waveforms.</s> <s>this behavior resembles biological synapses, whose plasticity is also triggered by mechanisms that are determined by local waveforms.</s> <s>however, learning in memristive devices has so far been approached mostly on a pragmatic technological level.</s> <s>the focus seems to be on finding any waveform that achieves spike-timing-dependent plasticity (stdp), without regard to the biological veracity of said waveforms or to further important forms of plasticity.</s> <s>bridging this gap, we make use of a plasticity model driven by neuron waveforms that explains a large number of experimental observations and adapt it to the characteristics of the recently introduced bifeo$_3$ memristive material.</s> <s>based on this approach, we show stdp for the first time for this material, with learning window replication superior to previous memristor-based stdp implementations.</s> <s>we also demonstrate in measurements that it is possible to overlay short and long term plasticity at a memristive device in the form of the well-known triplet plasticity.</s> <s>to the best of our knowledge, this is the first implementations of triplet plasticity on any physical memristive device.</s></p></d>", "label": ["<d><p><s>waveform driven plasticity in bifeo3 memristive devices: model and implementation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we develop a novel approach to the problem of learning sparse representations in the context of fused sparsity and unknown noise level.</s> <s>we propose an algorithm, termed scaled fused dantzig selector (sfds), that accomplishes the aforementioned learning task by means of a second-order cone program.</s> <s>a special  emphasize is put on the particular instance of fused sparsity corresponding to  the learning in presence of outliers.</s> <s>we establish finite sample risk bounds and  carry out an experimental evaluation on both synthetic and real data.</s></p></d>", "label": ["<d><p><s>fused sparsity and robust estimation for linear models with unknown variance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present very efficient active learning algorithms for link classification in signed networks.</s> <s>our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes.</s> <s>we provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph $g = (v,e)$ such that $|e|$ is at least order of $|v|^{3/2}$ by querying at most order of $|v|^{3/2}$ edge labels.</s> <s>more generally, we show an algorithm that achieves optimality to within a factor of order $k$ by querying at most order of $|v| + (|v|/k)^{3/2}$ edge labels.</s> <s>the running time of this algorithm is at most of order $|e| + |v|\\log|v|$.</s></p></d>", "label": ["<d><p><s>a linear time active learning algorithm for link classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the ability to learn a policy for a sequential decision problem with continuous state space using on-line data is a long-standing challenge.</s> <s>this paper presents a new reinforcement-learning algorithm, called ikbsf, which extends the benefits of kernel-based learning to the on-line scenario.</s> <s>as a kernel-based method, the proposed algorithm is stable and has good convergence properties.</s> <s>however, unlike other similar algorithms,ikbsf's space complexity is independent of the number of sample transitions, and as a result it can process an arbitrary amount of data.</s> <s>we present theoretical results showing that ikbsf can approximate (to any level of accuracy) the value function that would be learned by an equivalent batch non-parametric kernel-based reinforcement learning approximator.</s> <s>in order to show the effectiveness of the proposed algorithm in practice, we apply ikbsf to the challenging three-pole balancing task, where the ability to process a large number of transitions is crucial for achieving a high success rate.</s></p></d>", "label": ["<d><p><s>on-line reinforcement learning using incremental kernel-based stochastic factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper shows how sparse, high-dimensional probability distributions could be represented by neurons with exponential compression.</s> <s>the representation is a novel application of compressive sensing to sparse probability distributions rather than to the usual sparse signals.</s> <s>the compressive measurements correspond to expected values of nonlinear functions of the probabilistically distributed variables.</s> <s>when these expected values are estimated by sampling, the quality of the compressed representation is limited only by the quality of sampling.</s> <s>since the compression preserves the geometric structure of the space of sparse probability distributions, probabilistic computation can be performed in the compressed domain.</s> <s>interestingly, functions satisfying the requirements of compressive sensing can be implemented as simple perceptrons.</s> <s>if we use perceptrons as a simple model of feedforward computation by neurons, these results show that the mean activity of a relatively small number of neurons can accurately represent a high-dimensional joint distribution implicitly, even without accounting for any noise correlations.</s> <s>this comprises a novel hypothesis for how neurons could encode probabilities in the brain.</s></p></d>", "label": ["<d><p><s>compressive neural representation of sparse, high-dimensional probabilities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose two classes of second-order optimization methods for solving the sparse inverse covariance estimation problem.</s> <s>the first approach, which we call the newton-lasso method, minimizes a piecewise quadratic model of the objective function at every iteration to generate a step.</s> <s>we employ the fast iterative shrinkage thresholding method (fista) to solve this subproblem.</s> <s>the second approach, which we call the orthant-based newton method, is a two-phase algorithm that first identifies an orthant face and then minimizes a smooth quadratic approximation of the objective function using the conjugate gradient method.</s> <s>these methods exploit the structure of the hessian to efficiently compute the search direction and to avoid explicitly storing the hessian.</s> <s>we show that quasi-newton methods are also effective in this context, and describe a limited memory bfgs variant of the orthant-based newton method.</s> <s>we present numerical results that suggest that all the techniques described in this paper have attractive properties and constitute useful tools for solving the sparse inverse covariance estimation problem.</s> <s>comparisons with the method implemented in the quic software package are presented.</s></p></d>", "label": ["<d><p><s>newton-like methods for sparse inverse covariance estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>pedigrees, or family trees, are directed graphs used to identify sites of the genome that are correlated with the presence or absence of a disease.</s> <s>with the advent of genotyping and sequencing technologies, there has been an explosion in the amount of data available, both in the number of individuals and in the number of sites.</s> <s>some pedigrees number in the thousands of individuals.</s> <s>meanwhile, analysis methods have remained limited to pedigrees of <100 individuals which limits analyses to many small independent pedigrees.</s> <s>disease models, such those used for the linkage analysis log-odds (lod) estimator, have similarly been limited.</s> <s>this is because linkage anlysis was originally designed with a different task in mind, that of ordering the sites in the genome, before there were technologies that could reveal the order.</s> <s>lods are difficult to interpret and nontrivial to extend to consider interactions among sites.</s> <s>these developments and difficulties call for the creation of modern methods of pedigree analysis.</s> <s>drawing from recent advances in graphical model inference and transducer theory, we introduce a simple yet powerful formalism for expressing genetic disease models.</s> <s>we show that these disease models can be turned into accurate and efficient estimators.</s> <s>the technique we use for constructing the variational approximation has potential applications to inference in other large-scale graphical models.</s> <s>this method allows inference on larger pedigrees than previously analyzed in the literature, which improves disease site prediction.</s></p></d>", "label": ["<d><p><s>bayesian pedigree analysis using measure factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new algorithm for independent component analysis (ica) which has provable performance guarantees.</s> <s>in particular, suppose we are given samples of the form $y = ax + \\eta$ where $a$ is an unknown $n \\times n$ matrix and $x$ is chosen uniformly at random from $\\{+1, -1\\}^n$, $\\eta$ is an $n$-dimensional gaussian random variable with unknown covariance $\\sigma$: we give an algorithm that provable recovers $a$ and $\\sigma$ up to an additive $\\epsilon$ whose running time and sample complexity are polynomial in $n$ and $1 / \\epsilon$.</s> <s>to accomplish this, we introduce a novel ``quasi-whitening'' step that may be useful in other contexts in which the covariance of gaussian noise is not known in advance.</s> <s>we also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $a$ one by one via local search.</s></p></d>", "label": ["<d><p><s>provable ica with unknown gaussian noise, with implications for gaussian mixtures and autoencoders</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the problem of debugging large pipelines by human labeling.</s> <s>we represent the execution of a pipeline using a directed acyclic graph of and and or nodes, where each node represents a data item produced by some operator in the pipeline.</s> <s>we assume that each operator assigns a confidence to each of its output data.</s> <s>we want to reduce the uncertainty in the output by issuing queries to a human expert, where a query consists of checking if a given data item is correct.</s> <s>in this paper, we consider the problem of asking the optimal set of queries to minimize the resulting output uncertainty.</s> <s>we perform a detailed evaluation of the complexity of the problem for various classes of graphs.</s> <s>we give efficient algorithms for the problem for trees, and show that, for a general dag, the problem is intractable.</s></p></d>", "label": ["<d><p><s>minimizing uncertainty in pipelines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new formulation for attacking binary classification problems.</s> <s>instead of relying on convex losses and regularisers such as in svms, logistic regression and boosting, or instead non-convex but continuous formulations such as those encountered in neural networks and deep belief networks, our framework entails a non-convex but \\emph{discrete} formulation, where estimation amounts to finding a map configuration in a graphical model whose potential functions are low-dimensional discrete surrogates for the misclassification loss.</s> <s>we argue that such a discrete formulation can naturally account for a number of issues that are typically encountered in either the convex or the continuous non-convex paradigms, or both.</s> <s>by reducing the learning problem to a map inference problem, we can immediately translate the guarantees available for many inference settings to the learning problem itself.</s> <s>we empirically demonstrate in a number of experiments that this approach is promising in dealing with issues such as severe label noise, while still having global optimality guarantees.</s> <s>due to the discrete nature of the formulation, it also allows for \\emph{direct} regularisation through cardinality-based penalties, such as the $\\ell_0$ pseudo-norm, thus providing the ability to perform feature selection and trade-off interpretability and predictability in a principled manner.</s> <s>we also outline a number of open problems arising from the formulation.</s></p></d>", "label": ["<d><p><s>learning as map inference in discrete graphical models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>variational methods provide a computationally scalable alternative to monte carlo methods for large-scale, bayesian nonparametric learning.</s> <s>in practice, however, conventional batch and online variational methods quickly become trapped in local optima.</s> <s>in this paper, we consider a nonparametric topic model based on the hierarchical dirichlet process (hdp), and develop a novel online variational inference algorithm based on split-merge topic updates.</s> <s>we derive a simpler and faster variational approximation of the hdp, and show that by intelligently splitting and merging components of the variational posterior, we can achieve substantially better predictions of test data than conventional online and batch variational algorithms.</s> <s>for streaming analysis of large datasets where batch analysis is infeasible, we show that our split-merge updates better capture the nonparametric properties of the underlying model, allowing continual learning of new topics.</s></p></d>", "label": ["<d><p><s>truly nonparametric online variational inference for hierarchical dirichlet processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical models for networks have been typically committed to strong prior assumptions concerning the form of the modeled distributions.</s> <s>moreover, the vast majority of currently available models are explicitly designed for capturing some specific graph properties (such as power-law degree distributions), which makes them unsuitable for application to domains where the behavior of the target quantities is not known a priori.</s> <s>the key contribution of this paper is twofold.</s> <s>first, we introduce the fiedler delta statistic, based on the laplacian spectrum of graphs, which allows to dispense with any parametric assumption concerning the modeled network properties.</s> <s>second, we use the defined statistic to develop the fiedler random field model, which allows for efficient estimation of edge distributions over large-scale random networks.</s> <s>after analyzing the dependence structure involved in fiedler random fields, we estimate them over several real-world networks, showing that they achieve a much higher modeling accuracy than other well-known statistical approaches.</s></p></d>", "label": ["<d><p><s>fiedler random fields: a large-scale spectral approach to statistical network modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces a novel classification method for functional magnetic resonance imaging datasets with tens of classes.</s> <s>the method is designed to make predictions using information from as many brain locations as possible, instead of resorting to feature selection, and does this by decomposing the pattern of brain activation into differently informative sub-regions.</s> <s>we provide results over a complex semantic processing dataset that show that the method is competitive with state-of-the-art feature selection and also suggest how the method may be used to perform group or exploratory analyses of complex class structure.</s></p></d>", "label": ["<d><p><s>a systematic approach to extracting semantic information from functional mri data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a challenging problem in hierarchical classification is to leverage the hierarchical relations among classes for  improving classification performance.</s> <s>an even greater challenge is to do so in a manner that is computationally feasible for the large scale problems usually encountered in practice.</s> <s>this paper proposes a set of bayesian methods to model hierarchical dependencies among class labels using multivari- ate logistic regression.</s> <s>specifically, the parent-child relationships are modeled by placing a hierarchical prior over the children nodes centered around the parame- ters of their parents; thereby encouraging classes nearby in the hierarchy to share similar model parameters.</s> <s>we present new, efficient variational algorithms for tractable posterior inference in these models, and provide a parallel implementa- tion that can comfortably handle large-scale problems with hundreds of thousands of dimensions and tens of thousands of classes.</s> <s>we run a comparative evaluation on multiple large-scale benchmark datasets that highlights the scalability of our approach, and shows a significant performance advantage over the other state-of- the-art hierarchical methods.</s></p></d>", "label": ["<d><p><s>bayesian models for large-scale hierarchical classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe how the pre-training algorithm for deep boltzmann machines (dbms) is related to the pre-training algorithm for deep belief networks and we show that under certain conditions, the pre-training procedure improves the variational lower bound of a two-hidden-layer dbm.</s> <s>based on this analysis, we develop a different method of pre-training dbms that distributes the modelling work more evenly over the hidden layers.</s> <s>our results on the mnist and norb datasets demonstrate that the new pre-training algorithm allows us to learn better generative models.</s></p></d>", "label": ["<d><p><s>a better way to pretrain deep boltzmann machines</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in regression problems over $\\real^d$, the unknown function $f$ often varies more in some coordinates than in others.</s> <s>we show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$  is an efficient way to significantly improve the performance of distance-based regressors, e.g.</s> <s>kernel and $k$-nn regressors.</s> <s>we propose a simple estimator of these derivative norms and prove its consistency.</s> <s>moreover, the proposed  estimator is efficiently learned online.</s></p></d>", "label": ["<d><p><s>gradient weights help nonparametric regressors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of identifying patterns in a data set that exhibit anomalous behavior, often referred to as anomaly detection.</s> <s>in most anomaly detection algorithms, the dissimilarity between data samples is calculated by a single criterion, such as euclidean distance.</s> <s>however, in many cases there may not exist a single dissimilarity measure that captures all possible anomalous patterns.</s> <s>in such a case, multiple criteria can be defined, and one can test for anomalies by scalarizing the multiple criteria by taking some linear combination of them.</s> <s>if the importance of the different criteria are not known in advance, the algorithm may need to be executed multiple times with different choices of weights in the linear combination.</s> <s>in this paper, we introduce a novel non-parametric multi-criteria anomaly detection method using pareto depth analysis (pda).</s> <s>pda uses the concept of pareto optimality to detect anomalies under multiple criteria without having to run an algorithm multiple times with different choices of weights.</s> <s>the proposed pda approach scales linearly in the number of criteria and is provably better than linear combinations of the criteria.</s></p></d>", "label": ["<d><p><s>multi-criteria anomaly detection using pareto depth analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a new model for learning meaningful representations of text documents from an unlabeled collection of documents.</s> <s>this model is inspired by the recently proposed replicated softmax, an undirected graphical model of word counts that was shown to learn a better generative model and more meaningful document representations.</s> <s>specifically, we take inspiration from the conditional mean-field recursive equations of the replicated softmax in order to define a neural network architecture that estimates the probability of observing a new word in a given document given the previously observed words.</s> <s>this paradigm also allows us to replace the expensive softmax distribution over words with a hierarchical distribution over paths in a binary tree of words.</s> <s>the end result is a model whose training complexity scales logarithmically with the vocabulary size instead of linearly as in the replicated softmax.</s> <s>our experiments show that our model is competitive both as a generative model of documents and as a document representation learning algorithm.</s></p></d>", "label": ["<d><p><s>a neural autoregressive topic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an effective strategy to exploit the supervising side information for discovering predictive topic representations is to impose discriminative constraints induced by such information on the posterior distributions under a topic model.</s> <s>this strategy has been adopted by a number of supervised topic models, such as medlda, which employs max-margin posterior constraints.</s> <s>however, unlike the likelihood-based supervised topic models, of which posterior inference can be carried out using the bayes' rule, the max-margin posterior constraints have made monte carlo methods infeasible or at least not directly applicable, thereby limited the choice of inference algorithms to be based on variational approximation with strict mean field assumptions.</s> <s>in this paper, we develop two efficient monte carlo methods under much weaker assumptions for max-margin supervised topic models based on an importance sampler and a collapsed gibbs sampler, respectively, in a convex dual formulation.</s> <s>we report thorough experimental results that compare our approach favorably against existing alternatives in both accuracy and efficiency.</s></p></d>", "label": ["<d><p><s>monte carlo methods for maximum margin supervised topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new family of matrix norms, the ''local max'' norms, generalizing existing methods such as the max norm, the trace norm (nuclear norm), and the weighted or smoothed weighted trace norms, which have been extensively used in the literature as regularizers for matrix reconstruction problems.</s> <s>we show that this new family can be used to interpolate between the (weighted or unweighted) trace norm and the more conservative max norm.</s> <s>we test this interpolation on simulated data and on the large-scale netflix and movielens ratings data, and find improved accuracy relative to the existing matrix norms.</s> <s>we also provide theoretical results showing learning guarantees for some of the new norms.</s></p></d>", "label": ["<d><p><s>matrix reconstruction with the local max norm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a brain-computer interface (bci) allows users to ?communicate?</s> <s>with a computer without using their muscles.</s> <s>bci based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand to send control signals.</s> <s>the performances of a bci can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue.</s> <s>this study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button.</s> <s>we develop for this purpose an adaptive algorithm ucb-classif based on the stochastic bandit theory.</s> <s>this shortens the training stage, thereby allowing the exploration of a greater variety of tasks.</s> <s>by not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the bci training session.</s> <s>comparing the proposed method to the standard practice in task selection, for a fixed time budget, ucb-classif leads to an improve classification rate, and for a fix classification rate, to a reduction of the time spent in training by 50%.</s></p></d>", "label": ["<d><p><s>bandit algorithms boost brain computer interfaces for motor-task selection of a brain-controlled button</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>undirected graphical models, or markov networks, such as gaussian graphical models and ising models enjoy popularity in a variety of applications.</s> <s>in many settings, however, data may not follow a gaussian or binomial distribution assumed by these models.</s> <s>we introduce a new class of graphical models based on generalized linear models (glm) by assuming that node-wise conditional distributions arise from exponential families.</s> <s>our models allow one to estimate networks for a wide class of exponential distributions, such as the poisson, negative binomial, and exponential, by fitting penalized glms to select the neighborhood for each node.</s> <s>a major contribution of this paper is the rigorous statistical analysis showing that with high probability, the neighborhood of our graphical models can be recovered exactly.</s> <s>we provide examples of high-throughput genomic networks learned via our glm graphical models for multinomial and poisson distributed data.</s></p></d>", "label": ["<d><p><s>graphical models via generalized linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while compressive sensing (cs) has been one of the most vibrant and active research fields in the past few years, most development only applies to linear models.</s> <s>this limits its application and excludes many areas where cs ideas could make a difference.</s> <s>this paper presents a novel extension of cs to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal.</s> <s>we propose a novel solution using a lifting technique -- cprl, which relaxes the np-hard problem to a nonsmooth semidefinite program.</s> <s>our analysis shows that cprl inherits many desirable properties from cs, such as guarantees for exact recovery.</s> <s>we further provide scalable numerical solvers to accelerate its implementation.</s> <s>the source code of our algorithms will be provided to the public.</s></p></d>", "label": ["<d><p><s>cprl -- an extension of compressive sensing to the phase retrieval problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>user preferences for items can be inferred from either explicit feedback, such as item ratings, or implicit feedback, such as rental histories.</s> <s>research in collaborative filtering has concentrated on explicit feedback, resulting in the development of accurate and scalable models.</s> <s>however, since explicit feedback is often difficult to collect it is important to develop effective models that take advantage of the more widely available implicit feedback.</s> <s>we introduce a probabilistic approach to collaborative filtering with implicit feedback based on modelling the user's item selection process.</s> <s>in the interests of scalability, we restrict our attention to tree-structured distributions over items and develop a principled and efficient algorithm for learning item trees from data.</s> <s>we also identify a problem with a widely used protocol for evaluating implicit feedback models and propose a way of addressing it using a small quantity of explicit feedback data.</s></p></d>", "label": ["<d><p><s>learning label trees for probabilistic modelling of implicit feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>applications of bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity.</s> <s>we develop new markov chain monte carlo methods for the beta process hidden markov model (bp-hmm), enabling discovery of shared activity patterns in large video and motion capture databases.</s> <s>by introducing split-merge moves based on sequential allocation, we allow large global changes in the shared feature structure.</s> <s>we also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors.</s> <s>our proposals apply to any choice of conjugate likelihood for observed data, and we show success with multinomial, gaussian, and autoregressive emission models.</s> <s>together, these innovations allow tractable analysis of hundreds of time series, where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.</s></p></d>", "label": ["<d><p><s>effective split-merge monte carlo methods for nonparametric models of sequential data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe an approach to speed-up inference with latent variable pcfgs, which have been shown to  be highly effective for natural language parsing.</s> <s>our approach is based on a tensor formulation recently introduced for spectral estimation of latent-variable pcfgs coupled with a tensor decomposition algorithm well-known in the multilinear algebra literature.</s> <s>we also describe an error bound for this approximation, which bounds the difference between the probabilities calculated by the algorithm and the true probabilities that the approximated model gives.</s> <s>empirical evaluation on real-world natural language parsing data demonstrates a significant speed-up at minimal cost for parsing performance.</s></p></d>", "label": ["<d><p><s>tensor decomposition for fast parsing with latent-variable pcfgs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of multiple change point estimation is  considered for sequences  with unknown number of change points.</s> <s>a consistency framework is suggested that is suitable for highly dependent time-series, and an asymptotically consistent algorithm is proposed.</s> <s>in order for the consistency to be established the only assumption  required is that the data is generated by stationary ergodic time-series distributions.</s> <s>no modeling, independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form.</s> <s>the theoretical results are complemented with experimental evaluations.</s></p></d>", "label": ["<d><p><s>locating changes in highly dependent data with unknown number of change points</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a bayesian nonparametric extension of the popular plackett-luce choice model that can handle an infinite number of choice items.</s> <s>our framework is based on the theory of random atomic measures, with the prior specified by a gamma process.</s> <s>we derive a posterior characterization and a simple and effective gibbs sampler for posterior simulation.</s> <s>we then develop a time-varying extension of our model, and apply our model to the new york times lists of weekly bestselling books.</s></p></d>", "label": ["<d><p><s>bayesian nonparametric models for ranked data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>value pursuit iteration (vpi) is an approximate value iteration algorithm that finds a close to optimal policy for  reinforcement learning and planning problems with large state spaces.</s> <s>vpi has two main features: first, it is a nonparametric algorithm that finds a good sparse approximation of the optimal value function given a dictionary of features.</s> <s>the algorithm is almost insensitive to the number of irrelevant features.</s> <s>second, after each iteration of vpi, the algorithm adds a set of functions based on the currently learned value function to the dictionary.</s> <s>this increases the representation power of the dictionary in a way that is directly relevant to the goal of having a good approximation of the optimal value function.</s> <s>we theoretically study vpi and provide a finite-sample error upper bound for it.</s></p></d>", "label": ["<d><p><s>value pursuit iteration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of recovering a sequence of vectors, $(x_k)_{k=0}^k$, for which the increments $x_k-x_{k-1}$ are $s_k$-sparse (with $s_k$ typically smaller than $s_1$), based on linear measurements $(y_k = a_k x_k + e_k)_{k=1}^k$, where $a_k$ and $e_k$ denote the measurement matrix and noise, respectively.</s> <s>assuming each $a_k$ obeys the restricted isometry property (rip) of a certain order---depending only on $s_k$---we show that in the absence of noise a convex program, which minimizes the weighted sum of the $\\ell_1$-norm of successive differences subject to the linear measurement constraints, recovers the sequence $(x_k)_{k=1}^k$ \\emph{exactly}.</s> <s>this is an interesting result because this convex program is equivalent to a standard compressive sensing problem with a highly-structured aggregate measurement matrix which does not satisfy the rip requirements in the standard sense, and yet we can achieve exact recovery.</s> <s>in the presence of bounded noise, we propose a quadratically-constrained convex program for recovery and derive bounds on the reconstruction error of the sequence.</s> <s>we supplement our theoretical analysis with simulations and an application to real video data.</s> <s>these further support the validity of the proposed approach for acquisition and recovery of signals with time-varying sparsity.</s></p></d>", "label": ["<d><p><s>exact and stable recovery of sequences of signals with sparse increments via differential _1-minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>crowdsourcing has become a popular paradigm for labeling large datasets.</s> <s>however, it has given rise to the computational task of aggregating the crowdsourced labels provided by a collection of unreliable annotators.</s> <s>we approach this problem by transforming it into a standard inference problem in graphical models, and applying approximate variational methods, including belief propagation (bp) and mean field (mf).</s> <s>we show that our bp algorithm generalizes both majority voting and a recent algorithm by karger et al, while our mf method is closely related to a commonly used em algorithm.</s> <s>in both cases, we find that the performance of the algorithms critically depends on the choice of a prior distribution on the workers' reliability; by choosing the prior properly, both bp and mf (and em) perform surprisingly well on both simulated and real-world datasets, competitive with state-of-the-art algorithms based on more complicated modeling assumptions.</s></p></d>", "label": ["<d><p><s>variational inference for crowdsourcing</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>hierarchical hidden markov models (hhmms) are sophisticated stochastic models that enable us to capture a hierarchical context characterization of sequence data.</s> <s>however, existing hhmm parameter estimation methods require large computations of time complexity o(tn^{2d}) at least for model inference, where d is the depth of the hierarchy, n is the number of states in each level, and t is the sequence length.</s> <s>in this paper, we propose a new inference method of hhmms for which the time complexity is o(tn^{d+1}).</s> <s>a key idea of our algorithm is application of the forward-backward algorithm to ''state activation probabilities''.</s> <s>the notion of a state activation, which offers a simple formalization of the hierarchical transition behavior of hhmms, enables us to conduct model inference efficiently.</s> <s>we present some experiments to demonstrate that our proposed method works more efficiently to estimate hhmm parameters than do some existing methods such as the flattening method and gibbs sampling method.</s></p></d>", "label": ["<d><p><s>forward-backward activation algorithm for hierarchical hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic approaches to computer vision typically assume a centralized setting, with the algorithm granted access to all observed data points.</s> <s>however, many problems in wide-area surveillance can benefit from distributed modeling, either because of physical or computational constraints.</s> <s>most distributed models to date use algebraic approaches (such as distributed svd) and as a result cannot explicitly deal with missing data.</s> <s>in this work we present an approach to estimation and learning of generative probabilistic models in a distributed context where certain sensor data can be missing.</s> <s>in particular, we show how traditional centralized models, such as probabilistic pca and missing-data ppca, can be learned when the data is distributed across a network of sensors.</s> <s>we demonstrate the utility of this approach on the problem of distributed affine structure from motion.</s> <s>our experiments suggest that the accuracy of the learned probabilistic structure and motion models rivals that of traditional centralized factorization methods while being able to handle challenging situations such as missing or noisy observations.</s></p></d>", "label": ["<d><p><s>distributed probabilistic learning for camera networks with missing data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in compressive sensing magnetic resonance imaging (cs-mri), one can reconstruct a mr image with good quality from only a small number of measurements.</s> <s>this can significantly reduce mr scanning time.</s> <s>according to structured sparsity theory, the measurements can be further reduced to $\\mathcal{o}(k+\\log n)$ for tree-sparse data instead of $\\mathcal{o}(k+k\\log n)$ for standard $k$-sparse data with length $n$.</s> <s>however, few of existing algorithms has utilized this for cs-mri, while most of them use total variation and wavelet sparse regularization.</s> <s>on the other side, some algorithms have been proposed for tree sparsity regularization, but few of them has validated   the benefit of tree structure in cs-mri.</s> <s>in this paper, we propose a fast convex optimization algorithm to improve cs-mri.</s> <s>wavelet sparsity, gradient sparsity and tree sparsity are all considered in our model for real mr images.</s> <s>the original complex problem is decomposed to three simpler subproblems then each of the subproblems can be efficiently solved with an iterative scheme.</s> <s>numerous experiments have been conducted and show that the proposed algorithm outperforms the state-of-the-art cs-mri algorithms, and gain better reconstructions results on real mr images than general tree based solvers or algorithms.</s></p></d>", "label": ["<d><p><s>compressive sensing mri with wavelet tree sparsity</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a model connecting visual tracking and saliency has recently been proposed.</s> <s>this model is based on the saliency hypothesis for tracking which postulates that tracking is achieved by the top-down tuning, based on target features, of discriminant center-surround saliency mechanisms over time.</s> <s>in this work, we identify three main predictions that must hold if the hypothesis were true: 1) tracking reliability should be larger for salient than for non-salient targets, 2) tracking reliability should have a dependence on the defining variables of saliency, namely feature contrast and distractor heterogeneity, and must replicate the dependence of saliency on these variables, and 3) saliency and tracking can be implemented with common low level neural mechanisms.</s> <s>we confirm that the first two predictions hold by reporting results from a set of human behavior studies on the connection between saliency and tracking.</s> <s>we also show that the third prediction holds by constructing a common neurophysiologically plausible architecture that can computationally solve both saliency and tracking.</s> <s>this architecture is fully compliant with the standard physiological models of v1 and mt, and with what is known about attentional control in area lip, while explaining the results of the human behavior experiments.</s></p></d>", "label": ["<d><p><s>on the connections between saliency and tracking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>subspace learning seeks a low dimensional representation of data that enables accurate reconstruction.</s> <s>however, in many applications, data is obtained from multiple sources rather than a single source (e.g.</s> <s>an object might be viewed by cameras at different angles, or a document might consist of text and images).</s> <s>the conditional independence of separate sources imposes constraints on their shared latent representation, which, if respected, can improve the quality of the learned low dimensional representation.</s> <s>in this paper, we present a convex formulation of multi-view subspace learning that enforces conditional independence while reducing dimensionality.</s> <s>for this formulation, we develop an efficient algorithm that recovers an optimal data reconstruction by exploiting an implicit convex regularizer, then recovers the corresponding latent representation and reconstruction model, jointly and optimally.</s> <s>experiments illustrate that the proposed method produces high quality results.</s></p></d>", "label": ["<d><p><s>convex multi-view subspace learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new stochastic gradient method for optimizing the sum of?</s> <s>a finite set of smooth functions, where the sum is strongly convex.?</s> <s>while standard stochastic gradient methods?</s> <s>converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence ?rate.</s> <s>in a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard?</s> <s>algorithms, both in terms of optimizing the training error and reducing the test error quickly.</s></p></d>", "label": ["<d><p><s>a stochastic gradient method with an exponential convergence _rate for finite training sets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a latent variable model for supervised dimensionality reduction and distance metric learning.</s> <s>the model discovers linear projections of high dimensional data that shrink the distance between similarly labeled inputs and expand the distance between differently labeled ones.</s> <s>the model?s continuous latent variables locate pairs of examples in a latent space of lower dimensionality.</s> <s>the model differs significantly from classical factor analysis in that the posterior distribution over these latent variables is not always multivariate gaussian.</s> <s>nevertheless we show that inference is completely tractable and derive an expectation-maximization (em) algorithm for parameter estimation.</s> <s>we also compare the model to other approaches in distance metric learning.</s> <s>the model?s main advantage is its simplicity: at each iteration of the em algorithm, the distance metric is re-estimated by solving an unconstrained least-squares problem.</s> <s>experiments show that these simple updates are highly effective.</s></p></d>", "label": ["<d><p><s>latent coincidence analysis: a hidden variable model for distance metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new notion of classification accuracy based on the top $\\tau$-quantile values of a scoring function, a relevant criterion in a number of problems arising for search engines.</s> <s>we define an algorithm optimizing a convex surrogate of the corresponding loss, and show how its solution can be obtained by solving several convex optimization problems.</s> <s>we also present margin-based guarantees for this algorithm based on the $\\tau$-quantile of the functions in the hypothesis set.</s> <s>finally, we report the results of several experiments evaluating the performance of our algorithm.</s> <s>in a comparison in a bipartite setting with several algorithms seeking high precision at the top, our algorithm achieves a better performance in precision at the top.</s></p></d>", "label": ["<d><p><s>accuracy at the top</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study latent factor models with the dependency structure in the latent space.</s> <s>we propose a general learning framework which induces sparsity on the undirected graphical model imposed on the vector of latent factors.</s> <s>a novel latent factor model slfa is then proposed as a matrix factorization problem with a special regularization term that encourages collaborative reconstruction.</s> <s>the main benefit (novelty) of the model is that we can simultaneously learn the lower-dimensional representation for data and model the pairwise relationships between latent factors explicitly.</s> <s>an on-line learning algorithm is devised to make the model feasible for large-scale learning problems.</s> <s>experimental results on two synthetic data and two real-world data sets demonstrate that pairwise relationships and latent factors learned by our model provide a more structured way of exploring high-dimensional data,  and the learned representations achieve the state-of-the-art classification performance.</s></p></d>", "label": ["<d><p><s>learning the dependency structure of latent factors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by \\emph{multiple} latent factors (topics), as opposed to just one.</s> <s>this increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden.</s> <s>this work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including latent dirichlet allocation (lda).</s> <s>for lda, the procedure correctly recovers both the topic-word distributions and the parameters of the dirichlet prior over the topic mixtures, using only trigram statistics (\\emph{i.e.</s> <s>}, third order moments, which may be estimated with documents containing just three words).</s> <s>the method, called excess correlation analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (svds).</s> <s>moreover, the algorithm is scalable, since the svds are carried out only on $k \\times k$ matrices, where $k$ is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.</s></p></d>", "label": ["<d><p><s>a spectral algorithm for latent dirichlet allocation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show a principled way of deriving online learning algorithms from a minimax analysis.</s> <s>various upper bounds on the minimax value, previously thought to be non-constructive, are shown to yield algorithms.</s> <s>this allows us to seamlessly recover known methods and to derive new ones, also capturing such ''unorthodox'' methods as follow the perturbed leader and the r^2 forecaster.</s> <s>understanding the inherent complexity of the learning problem thus leads to the development of algorithms.</s> <s>to illustrate our approach, we present several new algorithms, including a family of randomized methods that use the idea of a ''random play out''.</s> <s>new versions of the follow-the-perturbed-leader algorithms are presented, as well as methods based on the littlestone's dimension, efficient methods for matrix completion with trace norm, and algorithms for the problems of transductive learning and prediction with static experts.</s></p></d>", "label": ["<d><p><s>relax and randomize : from value to algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the average case performance of multi-task gaussian process (gp)   regression as captured in the learning curve, i.e.\\ the average bayes error   for a chosen task versus the total number of examples $n$ for all   tasks.</s> <s>for gp covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix, we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $t$.</s> <s>we use   these to study the asymptotic learning behaviour for large   $n$.</s> <s>surprisingly, multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation, $\\rho$, is near its maximal value   $\\rho=1$.</s> <s>this effect is most extreme for learning of smooth target   functions as described by e.g.\\ squared exponential kernels.</s> <s>we also   demonstrate that when learning {\\em many} tasks, the learning curves   separate into an initial phase, where the bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples,   and a final decay that occurs only once the number of examples is   proportional to the number of tasks.</s></p></d>", "label": ["<d><p><s>learning curves for multi-task gaussian process regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting.</s> <s>this problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence.</s> <s>we propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (ugape), with a common structure and similar theoretical analysis for these two settings.</s> <s>we prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity.</s> <s>we also show how the ugape algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits.</s> <s>finally, we evaluate the performance of ugape and compare it with a number of existing fixed budget and fixed confidence algorithms.</s></p></d>", "label": ["<d><p><s>best arm identification: a unified approach to fixed budget and fixed confidence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of estimating the difference between two probability densities.</s> <s>a naive approach  is a two-step procedure of first estimating two densities separately and then computing their difference.</s> <s>however, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small estimation error incurred in the first stage can cause a big error in the second stage.</s> <s>in this paper, we propose a single-shot procedure  for directly estimating the density difference without separately estimating two densities.</s> <s>we derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate.</s> <s>we then show how the proposed density-difference estimator can be utilized in l2-distance approximation.</s> <s>finally, we experimentally demonstrate the usefulness of the proposed method in robust distribution comparison such as class-prior estimation and change-point detection.</s></p></d>", "label": ["<d><p><s>density-difference estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error.</s> <s>for example, pac-mdp approaches such as rmax base their model certainty on the amount of collected data, while bayesian approaches assume a prior over the transition dynamics.</s> <s>we propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress.</s> <s>we provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case.</s> <s>we then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.</s></p></d>", "label": ["<d><p><s>exploration in model-based reinforcement learning by empirically estimating learning progress</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>since its inception, the modus operandi of multi-task learning (mtl) has been to minimize the task-wise mean of the empirical risks.</s> <s>we introduce a generalized loss-compositional paradigm for mtl that includes a spectrum of formulations as a subfamily.</s> <s>one endpoint of this spectrum is minimax mtl: a new mtl formulation that minimizes the maximum of the tasks' empirical risks.</s> <s>via a certain relaxation of minimax mtl, we obtain a continuum of mtl formulations spanning minimax mtl and classical mtl.</s> <s>the full paradigm itself is loss-compositional, operating on the vector of empirical risks.</s> <s>it incorporates minimax mtl, its relaxations, and many new mtl formulations as special cases.</s> <s>we show theoretically that minimax mtl tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (ltl) test setting.</s> <s>the results of several mtl formulations on synthetic and real problems in the mtl and ltl test settings are encouraging.</s></p></d>", "label": ["<d><p><s>minimax multi-task learning and a generalized loss-compositional paradigm for mtl</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop convergent minimization algorithms for bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.</s> <s>while existing message passing algorithms define fixed point iterations corresponding to stationary points of the bethe free energy, their greedy dynamics do not distinguish between local minima and maxima, and can fail to converge.</s> <s>for continuous estimation problems, this instability is linked to the creation of invalid marginal estimates, such as gaussians with negative variance.</s> <s>conversely, our approach leverages multiplier methods with well-understood convergence properties, and uses bound projection methods to ensure that marginal approximations are valid at all iterations.</s> <s>we derive general algorithms for discrete and gaussian pairwise markov random fields, showing improvements over standard loopy belief propagation.</s> <s>we also apply our method to a hybrid model with both discrete and continuous variables, showing improvements over expectation propagation.</s></p></d>", "label": ["<d><p><s>minimization of continuous bethe approximations: a positive variation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex, and the optimum is (approximately) sparse.</s> <s>previous approaches are able to exploit only one of these two structures, yielding a $\\order(\\pdim/t)$ convergence rate for strongly convex objectives in $\\pdim$ dimensions and $\\order(\\sqrt{\\spindex( \\log\\pdim)/t})$ convergence rate when the optimum is $\\spindex$-sparse.</s> <s>our algorithm is based on successively solving a series of $\\ell_1$-regularized optimization problems using nesterov's dual averaging algorithm.</s> <s>we establish that the error of our solution after $t$ iterations is at most $\\order(\\spindex(\\log\\pdim)/t)$, with natural extensions to approximate sparsity.</s> <s>our results apply to locally lipschitz losses including the logistic, exponential, hinge and least-squares losses.</s> <s>by recourse to statistical minimax results, we show that our convergence rates are optimal up to constants.</s> <s>the effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.</s></p></d>", "label": ["<d><p><s>stochastic optimization and sparse statistical recovery: optimal algorithms for high dimensions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>diversified ranking is a fundamental task in machine learning.</s> <s>it is broadly applicable in many real world problems, e.g., information retrieval, team assembling, product search, etc.</s> <s>in this paper, we consider a generic setting where we aim to diversify the top-k ranking list based on an arbitrary relevance function and an arbitrary similarity function among all the examples.</s> <s>we formulate it as an optimization problem and show that in general it is np-hard.</s> <s>then, we show that for a large volume of the parameter space, the proposed objective function enjoys the diminishing returns property, which enables us to design a scalable, greedy algorithm to find the near-optimal solution.</s> <s>experimental results on real data sets demonstrate the effectiveness of the proposed algorithm.</s></p></d>", "label": ["<d><p><s>gender: a generic diversified ranking algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel method for scalable parallelization of smc algorithms, entangled monte carlo simulation (emc).</s> <s>emc avoids the transmission of particles between  nodes, and instead reconstructs them from the particle genealogy.</s> <s>in particular, we show that we can reduce the communication to the particle weights for each machine while efficiently maintaining implicit global coherence of the parallel simulation.</s> <s>we explain methods to efficiently maintain a genealogy of particles from which any particle can be reconstructed.</s> <s>we demonstrate using examples from bayesian phylogenetic that the computational gain from parallelization using emc significantly outweighs the cost of particle reconstruction.</s> <s>the timing experiments show that reconstruction of particles is indeed much more efficient as compared to transmission of particles.</s></p></d>", "label": ["<d><p><s>entangled monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sudderth, wainwright, and willsky conjectured that the bethe approximation corresponding to any fixed point of the belief propagation algorithm over an attractive, pairwise binary graphical model provides a lower bound on the true partition function.</s> <s>in this work, we resolve this conjecture in the affirmative by demonstrating that, for any graphical model with binary variables whose potential functions (not necessarily pairwise) are all log-supermodular, the bethe partition function always lower bounds the true partition function.</s> <s>the proof of this result follows from a new variant of the ?four functions?</s> <s>theorem that may be of independent interest.</s></p></d>", "label": ["<d><p><s>the bethe partition function of log-supermodular graphical models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the empirical success of the belief propagation approximate inference algorithm has inspired numerous theoretical and algorithmic advances.</s> <s>yet, for continuous non-gaussian domains performing belief propagation remains a challenging task: recent innovations such as nonparametric or kernel belief propagation, while useful, come with a substantial computational cost and offer little theoretical guarantees, even for tree structured models.</s> <s>in this work we present nonparanormal bp  for performing efficient inference on distributions parameterized by  a gaussian copulas network and any univariate marginals.</s> <s>for  tree structured networks, our approach is guaranteed to be exact for  this powerful class of non-gaussian models.</s> <s>importantly, the method  is as efficient as standard gaussian bp, and its convergence properties do not depend on the complexity of the univariate marginals, even when a nonparametric representation is used.</s></p></d>", "label": ["<d><p><s>nonparanormal belief propagation (npnbp)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a hilbert space.</s> <s>by establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data.</s> <s>in the course of the analysis, we arrive at new lower bounds, as well as probabilistic bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.</s></p></d>", "label": ["<d><p><s>learning probability measures with respect to optimal transport metrics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>continuous relaxations play an important role in discrete optimization, but have not seen much use in approximate probabilistic inference.</s> <s>here we show that a general form of the gaussian integral trick makes it possible to transform a wide class of discrete variable undirected models into fully continuous systems.</s> <s>the continuous representation allows the use of gradient-based hamiltonian monte carlo for inference,  results in new ways of estimating normalization constants (partition functions), and in general opens up a number of new avenues for inference in difficult discrete systems.</s> <s>we demonstrate some of these continuous relaxation inference algorithms on a number of illustrative problems.</s></p></d>", "label": ["<d><p><s>continuous relaxations for discrete hamiltonian monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations.</s> <s>this paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts.</s> <s>we study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients.</s> <s>the resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues.</s> <s>we propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure.</s> <s>we experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.</s></p></d>", "label": ["<d><p><s>multiple operator-valued kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>non-linear dynamical systems (ds) have been used extensively for building generative models of human behavior.</s> <s>its applications range from modeling brain dynamics  to encoding motor commands.</s> <s>many schemes have been proposed for encoding robot motions using dynamical systems with a single attractor placed at a predefined target in state space.</s> <s>although these enable the robots to react against sudden perturbations without any re-planning, the motions are always directed towards a single target.</s> <s>in this work, we focus on combining several such ds with distinct attractors, resulting in a multi-stable ds.</s> <s>we show its applicability in reach-to-grasp tasks where the attractors represent several grasping points on the target object.</s> <s>while exploiting multiple attractors provides more flexibility in recovering from unseen perturbations, it also increases the complexity of the underlying learning problem.</s> <s>here we present the augmented-svm (a-svm) model which inherits region partitioning ability of the well known svm classifier and is augmented with novel constraints derived from the individual ds.</s> <s>the new constraints modify the original svm dual whose optimal solution then results in a new class of support vectors (sv).</s> <s>these new sv ensure that the resulting multi-stable ds incurs minimum deviation from the original dynamics and is stable at each of the attractors within a finite region of attraction.</s> <s>we show, via implementations on a simulated 10 degrees of freedom mobile robotic platform, that the model is capable of real-time motion generation and is able to adapt on-the-fly to perturbations.</s></p></d>", "label": ["<d><p><s>augmented-svm: automatic space partitioning for combining multiple non-linear dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper explores unsupervised learning of parsing models along two directions.</s> <s>first, which models are identifiable from infinite data?</s> <s>we use a general technique for numerically checking identifiability based on the rank of a jacobian matrix, and apply it to several standard constituency and dependency parsing models.</s> <s>second, for identifiable models, how do we estimate the parameters efficiently?</s> <s>em suffers from local optima, while recent work using spectral methods cannot be directly applied since the topology of the parse tree varies across sentences.</s> <s>we develop a strategy, unmixing, which deals with this additional complexity for restricted classes of parsing models.</s></p></d>", "label": ["<d><p><s>identifiability and unmixing of latent parse trees</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this work, we consider the problem of modeling the dynamic structure of human activities in  the attributes space.</s> <s>a video sequence is first represented in a semantic feature space, where each feature encodes the probability of  occurrence of an activity attribute at a given time.</s> <s>a generative model,  denoted the binary dynamic system (bds), is proposed to learn both the distribution and dynamics of different activities in this  space.</s> <s>the bds is a non-linear dynamic system, which extends both the binary  principal component analysis (pca) and classical linear dynamic systems (lds), by combining binary observation variables with a hidden gauss-markov state  process.</s> <s>in this way, it integrates the representation power of semantic  modeling with the ability of dynamic systems to capture the temporal structure of time-varying processes.</s> <s>an algorithm for learning bds  parameters, inspired by a popular lds  learning method from dynamic textures, is proposed.</s> <s>a similarity measure between bdss, which generalizes  the binet-cauchy kernel for lds, is then introduced and used to design  activity classifiers.</s> <s>the proposed method is shown to outperform similar classifiers  derived from the kernel dynamic system (kds) and state-of-the-art approaches for  dynamics-based or attribute-based action recognition.</s></p></d>", "label": ["<d><p><s>recognizing activities by attribute dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>numerical integration is an key component of many problems in scientific computing, statistical modelling, and machine learning.</s> <s>bayesian quadrature is a model-based method for numerical integration which, relative to standard monte carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral.</s> <s>we propose a novel bayesian quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model.</s> <s>our approach approximately marginalises the quadrature model's hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using monte carlo samples.</s> <s>we demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.</s></p></d>", "label": ["<d><p><s>active learning of model evidence using bayesian quadrature</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a joint model of network content and context designed for exploratory analysis of email networks via visualization of topic-specific communication patterns.</s> <s>our model is an admixture model for text and network attributes which uses multinomial distributions over words as mixture components for explaining text and latent euclidean positions of actors as mixture components for explaining network attributes.</s> <s>we validate the appropriateness of our model by achieving state-of-the-art performance on a link prediction task and by achieving semantic coherence equivalent to that of latent dirichlet allocation.</s> <s>we demonstrate the capability of our model for descriptive, explanatory, and exploratory analysis by investigating the inferred topic-specific communication patterns of a new government email dataset, the new hanover county email corpus.</s></p></d>", "label": ["<d><p><s>topic-partitioned multinetwork embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic latent variable models are one of the cornerstones of machine learning.</s> <s>they offer a convenient and coherent way to specify prior distributions over unobserved structure in data, so that these unknown properties can be inferred via posterior inference.</s> <s>such models are useful for exploratory analysis and visualization, for building density models of data, and for   providing features that can be used for later discriminative tasks.</s> <s>a significant limitation of these models, however, is that draws from the prior are often highly redundant due to i.i.d.</s> <s>assumptions   on internal parameters.</s> <s>for example, there is no preference in the prior of a mixture model to make components non-overlapping, or in topic model to ensure that co-ocurring words only appear in a small number of topics.</s> <s>in this work, we revisit these independence assumptions for probabilistic latent variable models, replacing the   underlying i.i.d.\\ prior with a determinantal point process (dpp).</s> <s>the dpp allows us to specify a preference for diversity in our latent variables using a positive definite kernel function.</s> <s>using a kernel between probability distributions, we are able to define a dpp on probability measures.</s> <s>we show how to perform map inference   with dpp priors in latent dirichlet allocation and in mixture models, leading to better intuition for the latent variable representation and quantitatively improved unsupervised feature extraction, without compromising the generative aspects of the model.</s></p></d>", "label": ["<d><p><s>priors for diversity in generative latent variable models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>approaches to audio classification and retrieval tasks largely rely on detection-based discriminative models.</s> <s>we submit that such models make a simplistic assumption in mapping acoustics directly to semantics, whereas the actual process is likely more complex.</s> <s>we present a generative model that maps acoustics in a hierarchical manner to increasingly higher-level semantics.</s> <s>our model has 2 layers with the first being generic sound units with no clear semantic associations, while the second layer attempts to find patterns over the generic sound units.</s> <s>we evaluate our model on a large-scale retrieval task from trecvid 2011, and report significant improvements over standard baselines.</s></p></d>", "label": ["<d><p><s>unsupervised structure discovery for semantic analysis of audio</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces timeline trees, which are partial models of partially observable environments.</s> <s>timeline trees are given some specific predictions to make and learn a decision tree over history.</s> <s>the main idea of timeline trees is to use temporally abstract features to identify and split on features of key events, spread arbitrarily far apart in the past (whereas previous decision-tree-based methods have been limited to a finite suffix of history).</s> <s>experiments demonstrate that timeline trees can learn to make high quality predictions in complex, partially observable environments with high-dimensional observations (e.g.</s> <s>an arcade game).</s></p></d>", "label": ["<d><p><s>learning partially observable models using temporally abstract decision trees</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>sparse learning models typically combine a smooth loss with a nonsmooth penalty, such as trace norm.</s> <s>although recent developments in sparse approximation have offered promising solution methods, current approaches either apply only to matrix-norm constrained problems or provide suboptimal convergence rates.</s> <s>in this paper, we propose a boosting method for regularized learning that guarantees $\\epsilon$ accuracy within $o(1/\\epsilon)$ iterations.</s> <s>performance is further accelerated by interlacing boosting with fixed-rank local optimization---exploiting a simpler local objective than previous work.</s> <s>the proposed method yields state-of-the-art performance on large-scale problems.</s> <s>we also demonstrate an application to latent multiview learning for which we provide the first efficient weak-oracle.</s></p></d>", "label": ["<d><p><s>accelerated training for matrix-norm regularization: a boosting approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>mirror descent with an entropic regularizer is known to achieve shifting regret bounds that are logarithmic in the dimension.</s> <s>this is done using either a carefully designed projection or by a weight sharing technique.</s> <s>via a novel unified analysis, we show that these two approaches deliver essentially equivalent bounds on a notion of regret generalizing shifting, adaptive, discounted, and other related regrets.</s> <s>our analysis also captures and extends the generalized weight sharing technique of bousquet and warmuth, and can be refined in several ways, including improvements for small losses and adaptive tuning of parameters.</s></p></d>", "label": ["<d><p><s>mirror descent meets fixed share (and feels no regret)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with the statistical consistency of ranking methods.</s> <s>recently, it was proven that many commonly used pairwise ranking methods are inconsistent with the weighted pairwise disagreement loss (wpdl), which can be viewed as the true loss of ranking, even in a low-noise setting.</s> <s>this result is interesting but also surprising, given that the pairwise ranking methods have been shown very effective in practice.</s> <s>in this paper, we argue that the aforementioned result might not be conclusive, depending on what kind of assumptions are used.</s> <s>we give a new assumption that the labels of objects to rank lie in a rank-differentiable probability space (rdps), and prove that the pairwise ranking methods become consistent with wpdl under this assumption.</s> <s>what is especially inspiring is that rdps is actually not stronger than but similar to the low-noise setting.</s> <s>our studies provide theoretical justifications of some empirical findings on pairwise ranking methods that are unexplained before, which bridge the gap between theory and applications.</s></p></d>", "label": ["<d><p><s>statistical consistency of ranking methods in a rank-differentiable probability space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space.</s> <s>the proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty.</s> <s>beside the existence of an optimal policy which satisfies the poisson equation, the only assumptions made are hoelder continuity of rewards and transition probabilities.</s></p></d>", "label": ["<d><p><s>online regret bounds for undiscounted continuous reinforcement learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>methods for efficiently estimating the shannon entropy of data streams have important applications in learning, data mining, and network anomaly detections (e.g., the ddos attacks).</s> <s>for nonnegative data streams, the method of compressed counting (cc) based on maximally-skewed stable random projections can provide accurate estimates of the shannon entropy using small storage.</s> <s>however, cc is no longer applicable when entries of data streams can be below zero, which is a common scenario when comparing two streams.</s> <s>in this paper, we propose an algorithm for entropy estimation in general data streams which allow negative entries.</s> <s>in our method, the shannon entropy is approximated by the finite difference of two correlated frequency moments estimated from correlated samples of symmetric stable random variables.</s> <s>our experiments confirm that this method is able to substantially better approximate the shannon entropy compared to the prior state-of-the-art.</s></p></d>", "label": ["<d><p><s>entropy estimations using correlated symmetric stable random projections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the restricted boltzmann machine (rbm) is a popular density model that is also good for extracting features.</s> <s>a main source of tractability in rbm models is the model's assumption that given an input, hidden units activate independently from one another.</s> <s>sparsity and competition in the hidden representation is believed to be beneficial, and while an rbm with competition among its hidden units would acquire some of the attractive properties of sparse coding, such constraints are not added  due to the widespread belief that the resulting model would become intractable.</s> <s>in this work, we show how a dynamic programming algorithm developed in 1981 can be used to implement exact sparsity in the rbm's hidden units.</s> <s>we then expand on this and show how to pass derivatives through a layer of exact sparsity, which makes it possible to fine-tune a deep belief network (dbn) consisting of rbms with sparse hidden layers.</s> <s>we show that sparsity in the rbm's hidden layer improves the performance of both the pre-trained representations and of the fine-tuned model.</s></p></d>", "label": ["<d><p><s>cardinality restricted boltzmann machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the problem of clustering data points into low-dimensional subspaces in the presence of outliers.</s> <s>we pose the problem using a density estimation formulation with an associated generative model.</s> <s>based on this probability model, we first develop an iterative expectation-maximization (em) algorithm and then derive its global solution.</s> <s>in addition, we develop two bayesian methods based on variational bayesian (vb) approximation, which are capable of automatic dimensionality selection.</s> <s>while the first method is based on an alternating optimization scheme for all unknowns, the second method makes use of recent results in vb matrix factorization leading to fast and effective estimation.</s> <s>both methods are extended to handle sparse outliers for robustness and can handle missing values.</s> <s>experimental results suggest that proposed methods are very effective in clustering and identifying outliers.</s></p></d>", "label": ["<d><p><s>probabilistic low-rank subspace clustering</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>statistical decision theory axiomatically assumes that the relative desirability of different options that humans perceive is well described by assigning them option-specific scalar utility functions.</s> <s>however, this assumption is refuted by observed human behavior, including studies wherein preferences have been shown to change systematically simply through variation in the set of choice options presented.</s> <s>in this paper, we show that interpreting desirability as a relative comparison between available options at any particular decision instance results in a rational theory of value-inference that explains heretofore intractable violations of rational choice behavior in human subjects.</s> <s>complementarily, we also characterize the conditions under which a rational agent selecting optimal options indicated by dynamic value inference in our framework will behave identically to one whose preferences are encoded using a static ordinal utility function.</s></p></d>", "label": ["<d><p><s>rational inference of relative preferences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for modeling data matrices, this paper introduces probabilistic co-subspace addition (pcsa) model by simultaneously capturing the dependent structures among both rows and columns.</s> <s>briefly, pcsa assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features, which distribute in the row-wise and column-wise latent subspaces.</s> <s>consequently, it captures the dependencies among entries intricately, and is able to model the non-gaussian and heteroscedastic density.</s> <s>variational inference is proposed on pcsa for  approximate bayesian learning, where the updating for posteriors is formulated into the problem of solving sylvester equations.</s> <s>furthermore, pcsa is extended to tackling and filling missing values, to adapting its sparseness, and to modelling tensor data.</s> <s>in comparison with several state-of-art approaches, experiments demonstrate the effectiveness and efficiency of bayesian (sparse) pcsa on modeling matrix (tensor) data and filling missing values.</s></p></d>", "label": ["<d><p><s>bayesian probabilistic co-subspace addition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we introduce context-sensitive decision forests - a new perspective to exploit contextual information in the popular decision forest framework for the object detection problem.</s> <s>they are tree-structured classifiers with the ability to access intermediate prediction (here: classification and regression) information during training and inference time.</s> <s>this intermediate prediction is available to each sample, which allows us to develop context-based decision criteria, used for refining the prediction process.</s> <s>in addition, we introduce a novel split criterion which in combination with a priority based way of constructing the trees, allows more accurate regression mode selection and hence improves the current context information.</s> <s>in our experiments, we demonstrate improved results for the task of pedestrian detection on the challenging tud data set when compared to state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>context-sensitive decision forests for object detection</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a novel method in the family of particle mcmc methods that we refer to as particle gibbs with ancestor sampling (pg-as).</s> <s>similarly to the existing pg with backward simulation (pg-bs) procedure, we use backward sampling to (considerably) improve the mixing of the pg kernel.</s> <s>instead of using separate forward and backward sweeps as in pg-bs, however, we achieve the same effect in a single forward sweep.</s> <s>we apply the pg-as framework to the challenging class of non-markovian state-space models.</s> <s>we develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the pg-as framework.</s> <s>in particular, as we show in a simulation study, pg-as can yield an order-of-magnitude improved accuracy relative to pg-bs due to its robustness to the truncation error.</s> <s>several application examples are discussed, including rao-blackwellized particle smoothing and inference in degenerate state-space models.</s></p></d>", "label": ["<d><p><s>ancestor sampling for particle gibbs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large scale $\\ell_1$-regularized loss minimization problems arise in numerous applications such as compressed sensing and high dimensional supervised learning, including classification and regression problems.</s> <s>high performance algorithms and implementations are critical to efficiently solving these problems.</s> <s>building upon previous work on coordinate descent algorithms for $\\ell_1$ regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as scd, greedy cd, shotgun, and thread-greedy.</s> <s>we give a unified convergence analysis for the family of block-greedy algorithms.</s> <s>the analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small.</s> <s>our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications.</s> <s>we hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale $\\ell_1$-regularization problems.</s></p></d>", "label": ["<d><p><s>feature clustering for accelerating parallel coordinate descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the variational bayesian (vb) approach is one of the best tractable approximations to the bayesian estimation, and it was demonstrated to perform well in many applications.</s> <s>however, its good performance was not fully understood theoretically.</s> <s>for example, vb sometimes produces a sparse solution, which is regarded as a practical advantage of vb, but such sparsity is hardly observed in the rigorous bayesian estimation.</s> <s>in this paper, we focus on probabilistic pca and give more theoretical insight into the empirical success of vb.</s> <s>more specifically, for the situation where the noise variance is unknown, we derive a sufficient condition for perfect recovery of the true pca dimensionality in the large-scale limit when the size of an observed matrix goes to infinity.</s> <s>in our analysis, we obtain bounds for a noise variance estimator and simple closed-form solutions for other parameters, which themselves are actually very useful for better implementation of vb-pca.</s></p></d>", "label": ["<d><p><s>perfect dimensionality recovery by variational bayesian pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper provides the first ---to the best of our knowledge--- analysis of online learning algorithms for multiclass problems when the {\\em confusion} matrix is taken as a performance measure.</s> <s>the work builds upon recent and elegant results on noncommutative concentration inequalities, i.e.</s> <s>concentration inequalities that apply to matrices, and more precisely to matrix martingales.</s> <s>we do establish generalization bounds for online learning algorithm and show how the theoretical study motivate the proposition of a new confusion-friendly learning procedure.</s> <s>this learning algorithm, called \\copa (for confusion passive-aggressive) is a passive-aggressive learning algorithm; it is shown that the update equations for \\copa can be computed analytically, thus allowing the user from having to recours to any optimization package to implement it.</s></p></d>", "label": ["<d><p><s>confusion-based online learning and a passive-aggressive scheme</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>by developing data augmentation methods unique to the negative binomial (nb) distribution, we unite seemingly disjoint count and mixture models  under the  nb process framework.</s> <s>we develop fundamental properties of the models and derive efficient gibbs sampling inference.</s> <s>we show that the  gamma-nb process can be reduced to the hierarchical dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages.</s> <s>a variety of nb processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the nb dispersion and probability parameters.</s></p></d>", "label": ["<d><p><s>augment-and-conquer negative binomial processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we theoretically analyze and compare the following five popular multiclass classification methods: one vs. all, all pairs, tree-based classifiers, error correcting output codes (ecoc) with randomly generated code matrices, and multiclass svm.</s> <s>in the first four methods, the classification is based on a reduction to binary classification.</s> <s>we consider the case where the binary classifier comes from a class of vc dimension $d$, and in particular from the class of halfspaces over $\\reals^d$.</s> <s>we analyze both the estimation error and the approximation error of these methods.</s> <s>our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions.</s> <s>our proof technique employs tools from vc theory to analyze the \\emph{approximation error} of hypothesis classes.</s> <s>this is in sharp contrast to most, if not all, previous uses of vc theory, which only deal with estimation error.</s></p></d>", "label": ["<d><p><s>multiclass learning approaches: a theoretical comparison with implications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of adaptive control of a high dimensional linear quadratic (lq) system.</s> <s>previous work established the asymptotic convergence to an optimal controller for various adaptive control schemes.</s> <s>more recently, an asymptotic regret bound of $\\tilde{o}(\\sqrt{t})$ was shown for $t \\gg p$ where $p$ is the dimension of the state space.</s> <s>in this work we consider the case where the matrices describing the dynamic of the lq system are sparse and their dimensions are large.</s> <s>we present an adaptive control scheme that for $p \\gg 1$ and $t \\gg \\polylog(p)$ achieves a regret bound of $\\tilde{o}(p \\sqrt{t})$.</s> <s>in particular, our algorithm has an average cost of $(1+\\eps)$ times the optimum cost after $t = \\polylog(p) o(1/\\eps^2)$.</s> <s>this is in comparison to previous work on the dense dynamics where the algorithm needs $\\omega(p)$ samples before it can estimate the unknown dynamic with any significant accuracy.</s> <s>we believe our result has prominent applications in the emerging area of computational advertising, in particular targeted online advertising and advertising in social networks.</s></p></d>", "label": ["<d><p><s>efficient reinforcement learning for high dimensional linear quadratic systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an approach to detecting and analyzing the 3d configuration of objects in real-world images with heavy occlusion and clutter.</s> <s>we focus on the application of finding and analyzing cars.</s> <s>we do so with a two-stage model; the first stage reasons about 2d shape and appearance variation due to within-class variation(station wagons look different than sedans) and changes in viewpoint.</s> <s>rather than using a view-based model, we describe a compositional representation that models a large number of effective views and shapes using a small number of local view-based templates.</s> <s>we use this model to propose candidate detections and 2d estimates of shape.</s> <s>these estimates are then refined by our second stage, using an explicit 3d model of shape and viewpoint.</s> <s>we use a morphable model to capture 3d within-class variation, and use a weak-perspective camera model to capture viewpoint.</s> <s>we learn all model parameters from 2d annotations.</s> <s>we demonstrate state-of-the-art accuracy for detection, viewpoint estimation, and 3d shape reconstruction on challenging images from the pascal voc 2011 dataset.</s></p></d>", "label": ["<d><p><s>analyzing 3d objects in cluttered images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>early stages of sensory systems face the challenge of compressing information from numerous receptors onto a much smaller number of projection neurons, a so called communication bottleneck.</s> <s>to make more efficient use of limited bandwidth, compression may be achieved using predictive coding, whereby predictable, or redundant, components of the stimulus are removed.</s> <s>in the case of the retina, srinivasan et al.</s> <s>(1982) suggested that feedforward inhibitory connections subtracting a linear prediction generated from nearby receptors implement such compression, resulting in biphasic center-surround receptive fields.</s> <s>however, feedback inhibitory circuits are common in early sensory circuits and furthermore their dynamics may be nonlinear.</s> <s>can such circuits implement predictive coding as well?</s> <s>here, solving the transient dynamics of nonlinear reciprocal feedback circuits through analogy to a signal-processing algorithm called linearized bregman iteration we show that nonlinear predictive coding can be implemented in an inhibitory feedback circuit.</s> <s>in response to a step stimulus, interneuron activity in time constructs progressively less sparse but more accurate representations of the stimulus, a temporally evolving prediction.</s> <s>this analysis provides a powerful theoretical framework to interpret and understand the dynamics of early sensory processing in a variety of physiological experiments and yields novel predictions regarding the relation between activity and stimulus statistics.</s></p></d>", "label": ["<d><p><s>a mechanistic model of early sensory processing based on subtracting sparse representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a multiresolution gaussian process to capture long-range, non-markovian dependencies while allowing for abrupt changes.</s> <s>the multiresolution gp hierarchically couples a collection of smooth gps, each defined over an element of a random nested partition.</s> <s>long-range dependencies are captured by the top-level gp while the partition points define the abrupt changes.</s> <s>due to the inherent conjugacy of the gps, one can analytically marginalize the gps and compute the conditional likelihood of the observations given the partition tree.</s> <s>this allows for efficient inference of the partition itself, for which we employ graph-theoretic techniques.</s> <s>we apply the multiresolution gp to the analysis of magnetoencephalography (meg) recordings of brain activity.</s></p></d>", "label": ["<d><p><s>multiresolution gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a deep boltzmann machine for learning a generative model of multimodal data.</s> <s>we show how to use the model to extract a meaningful representation of multimodal data.</s> <s>we find that the learned representation is useful for classification and information retreival tasks, and hence conforms to some notion of semantic similarity.</s> <s>the model defines a probability density over the space of multimodal inputs.</s> <s>by sampling from the conditional distributions over each data modality, it possible to create the representation even when some data modalities are missing.</s> <s>our experimental results on bi-modal data consisting of images and text show that the multimodal dbm can learn a good generative model of the joint space of image and text inputs that is useful for information retrieval from both unimodal and multimodal queries.</s> <s>we further demonstrate that our model can significantly outperform svms and lda on discriminative tasks.</s> <s>finally, we compare our model to other deep learning methods, including autoencoders and deep belief networks, and show that it achieves significant gains.</s></p></d>", "label": ["<d><p><s>multimodal learning with deep boltzmann machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we provide a new framework to study the generalization bound of the learning process for domain adaptation.</s> <s>without loss of generality, we consider two kinds of representative domain adaptation settings: one is domain adaptation with multiple sources and the other is domain adaptation combining source and target data.</s> <s>in particular, we introduce two quantities that capture the inherent characteristics of domains.</s> <s>for either kind of domain adaptation, based on the two quantities, we then develop the specific hoeffding-type deviation inequality and symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number.</s> <s>by using the resultant generalization bound, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation.</s> <s>meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process.</s> <s>the numerical experiments support our results.</s></p></d>", "label": ["<d><p><s>generalization bounds for domain adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel $l_1$ regularized off-policy convergent td-learning method (termed ro-td), which is able to learn sparse representations of value functions with low computational complexity.</s> <s>the algorithmic framework underlying ro-td integrates two key ideas: off-policy convergent gradient td methods, such as tdc, and a convex-concave saddle-point formulation of non-smooth convex optimization, which enables first-order solvers and feature selection using online convex regularization.</s> <s>a detailed theoretical and experimental analysis of ro-td is presented.</s> <s>a variety of experiments are presented to illustrate the off-policy convergence, sparse feature selection capability and low computational cost of the ro-td algorithm.</s></p></d>", "label": ["<d><p><s>regularized off-policy td-learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel approach to low-level vision problems that combines sparse coding and deep networks pre-trained with denoising auto-encoder (da).</s> <s>we propose an alternative training scheme that successfully adapts da, originally designed for unsupervised feature learning, to the tasks of image denoising and blind inpainting.</s> <s>our method achieves state-of-the-art performance in the image denoising task.</s> <s>more importantly, in blind image inpainting task, the proposed method provides solutions to some complex problems that have not been tackled before.</s> <s>specifically, we can automatically remove complex patterns like superimposed text from an image, rather than simple patterns like pixels missing at random.</s> <s>moreover, the proposed method does not need the information regarding the region that requires inpainting to be given a priori.</s> <s>experimental results demonstrate the effectiveness of the proposed method in the tasks of image denoising and blind inpainting.</s> <s>we also show that our new training scheme for da is more effective and can improve the performance of unsupervised feature learning.</s></p></d>", "label": ["<d><p><s>image denoising and inpainting with deep neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work in unsupervised feature learning and deep learning has shown that  being able to train large models can dramatically improve performance.</s> <s>in this  paper, we consider the problem of training a deep network with billions of  parameters using tens of thousands of cpu cores.</s> <s>we have developed a  software framework called distbelief that can utilize computing clusters  with thousands of machines to train large models.</s> <s>within this framework, we  have developed two algorithms for large-scale distributed training: (i) downpour  sgd, an asynchronous stochastic gradient descent procedure supporting a  large number of model replicas, and (ii) sandblaster, a framework that supports  for a variety of distributed batch optimization procedures, including a distributed  implementation of l-bfgs.</s> <s>downpour sgd and sandblaster l-bfgs both  increase the scale and speed of deep network training.</s> <s>we have successfully  used our system to train a deep network 100x larger than previously reported in  the literature, and achieves state-of-the-art performance on imagenet, a visual  object recognition task with 16 million images and 21k categories.</s> <s>we show that  these same techniques dramatically accelerate the training of a more modestly  sized deep network for a commercial speech recognition service.</s> <s>although we  focus on and report performance of these methods as applied to training large  neural networks, the underlying algorithms are applicable to any gradient-based  machine learning algorithm.</s></p></d>", "label": ["<d><p><s>large scale distributed deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the main challenges in data clustering is to define an appropriate similarity measure between two objects.</s> <s>crowdclustering addresses this challenge by defining the pairwise similarity based on the manual annotations obtained through crowdsourcing.</s> <s>despite its encouraging results, a key limitation of crowdclustering is that it can only cluster objects when their manual annotations are available.</s> <s>to address this limitation, we propose a new approach for clustering, called \\textit{semi-crowdsourced clustering} that effectively combines the low-level features of objects with the manual annotations of a subset of the objects obtained via crowdsourcing.</s> <s>the key idea is to learn an appropriate similarity measure, based on the low-level features of objects, from the manual annotations of only a small portion of the data to be clustered.</s> <s>one difficulty in learning the pairwise similarity measure is that there is a significant amount of noise and inter-worker variations in the manual annotations obtained via crowdsourcing.</s> <s>we address this difficulty by developing a metric learning algorithm based on the matrix completion method.</s> <s>our empirical study with two real-world image data sets shows that the proposed algorithm outperforms state-of-the-art distance metric learning algorithms in both clustering accuracy and computational efficiency.</s></p></d>", "label": ["<d><p><s>semi-crowdsourced clustering: generalizing crowd labeling by robust distance metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of diverse feature selection in linear regression: selecting a small subset of diverse features that can predict a given objective.</s> <s>diversity is useful for several reasons such as interpretability, robustness to noise, etc.</s> <s>we propose several spectral regularizers that capture a notion of diversity of features and show that these are all submodular set functions.</s> <s>these regularizers, when added to the objective function for linear regression, result in approximately submodular functions, which can then be maximized approximately by efficient greedy and local search algorithms, with provable guarantees.</s> <s>we compare our algorithms to traditional greedy and $\\ell_1$-regularization schemes and show that we obtain a more diverse set of features that result in the regression problem being stable under perturbations.</s></p></d>", "label": ["<d><p><s>selecting diverse features via spectral regularization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper suggests a learning-theoretic perspective on how synaptic plasticity benefits global brain functioning.</s> <s>we introduce a model, the selectron, that (i) arises as the fast time constant limit of leaky integrate-and-fire neurons equipped with spiking timing dependent plasticity (stdp) and (ii) is amenable to theoretical analysis.</s> <s>we show that the selectron encodes reward estimates into spikes and that an error bound on spikes is controlled by a spiking margin and the sum of synaptic weights.</s> <s>moreover, the efficacy of spikes (their usefulness to other reward maximizing selectrons) also depends on total synaptic strength.</s> <s>finally, based on our analysis, we propose a regularized version of stdp, and show the regularization improves the robustness of neuronal learning when faced with multiple stimuli.</s></p></d>", "label": ["<d><p><s>towards a learning-theoretic analysis of spike-timing dependent plasticity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data.</s> <s>in this paper, we tackle the problem of adapting object detectors learned from images to work well on videos.</s> <s>we treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video).</s> <s>our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first.</s> <s>at each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples.</s> <s>to discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes.</s> <s>we also show how rich and expressive features specific to the target domain can be incorporated under the same framework.</s> <s>we show promising results on the 2011 trecvid multimedia event detection and labelme video datasets that illustrate the benefit of our approach to adapt object detectors to video.</s></p></d>", "label": ["<d><p><s>shifting weights: adapting object detectors from image to video</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we uncover relations between robust mdps and risk-sensitive mdps.</s> <s>the objective of a robust mdp is to minimize a function, such as the expectation of cumulative cost, for the worst case when the parameters have uncertainties.</s> <s>the objective of a risk-sensitive mdp is to minimize a risk measure of the cumulative cost when the parameters are known.</s> <s>we show that a risk-sensitive mdp of minimizing the expected exponential utility is equivalent to a robust mdp of minimizing the worst-case expectation with a penalty for the deviation of the uncertain parameters from their nominal values, which is measured with the kullback-leibler divergence.</s> <s>we also show that a risk-sensitive mdp of minimizing an iterated risk measure that is composed of certain coherent risk measures is equivalent to a robust mdp of minimizing the worst-case expectation when the possible deviations of uncertain parameters from their nominal values are characterized with a concave function.</s></p></d>", "label": ["<d><p><s>robustness and risk-sensitivity in markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we explore the hypothesis that the neuronal spike generation mechanism is an analog-to-digital converter, which rectifies low-pass filtered summed synaptic currents and encodes them into spike trains linearly decodable in post-synaptic neurons.</s> <s>to digitally encode an analog current waveform, the sampling rate of the spike generation mechanism must exceed its nyquist rate.</s> <s>such oversampling is consistent with the experimental observation that the precision of the spike-generation mechanism is an order of magnitude greater than the cut-off frequency of dendritic low-pass filtering.</s> <s>to achieve additional reduction in the error of analog-to-digital conversion, electrical engineers rely on noise-shaping.</s> <s>if noise-shaping were used in neurons, it would introduce correlations in spike timing to reduce low-frequency (up to nyquist) transmission error at the cost of high-frequency one (from nyquist to sampling rate).</s> <s>using experimental data from three different classes of neurons, we demonstrate that biological neurons utilize noise-shaping.</s> <s>we also argue that rectification by the spike-generation mechanism may improve energy efficiency and carry out de-noising.</s> <s>finally, the zoo of ion channels in neurons may be viewed as a set of predictors, various subsets of which are activated depending on the statistics of the input current.</s></p></d>", "label": ["<d><p><s>neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical features of neuronal spike trains are known to be non-poisson.</s> <s>here, we investigate the extent to which the non-poissonian feature affects the efficiency of transmitting information on fluctuating firing rates.</s> <s>for this purpose, we introduce the kullbuck-leibler (kl) divergence as a measure of the efficiency of information encoding, and assume that spike trains are generated by time-rescaled renewal processes.</s> <s>we show that the kl divergence determines the lower bound of the degree of rate fluctuations below which the temporal variation of the firing rates is undetectable from sparse data.</s> <s>we also show that the kl divergence, as well as the lower bound, depends not only on the variability of spikes in terms of the coefficient of variation, but also significantly on the higher-order moments of interspike interval (isi) distributions.</s> <s>we examine three specific models that are commonly used for describing the stochastic nature of spikes (the gamma, inverse gaussian (ig) and lognormal isi distributions), and find that the time-rescaled renewal process with the ig distribution achieves the largest kl divergence, followed by the lognormal and gamma distributions.</s></p></d>", "label": ["<d><p><s>coding efficiency and detectability of rate fluctuations with non-poisson neuronal firing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>conditional markov chains (also known as linear-chain conditional random fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables.</s> <s>large-sample properties of conditional markov chains have been first studied by sinn and poupart [1].</s> <s>the paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given.</s></p></d>", "label": ["<d><p><s>mixing properties of conditional markov chains with unbounded feature functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli.</s> <s>while adaptation is an intrinsic feature of neuronal models like the hodgkin-huxley model, the challenge is to integrate adaptation in models of neural computation.</s> <s>recent computational models like the adaptive spike response model implement adaptation as spike-based addition of fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents.</s> <s>such adaptation has been shown to accurately model neural spiking behavior over a limited dynamic range.</s> <s>taking a cue from kinetic models of adaptation, we propose a multiplicative adaptive spike response model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking.</s> <s>we show that unlike the additive adaptation model, the firing rate in the multiplicative adaptation model saturates to a maximum spike-rate.</s> <s>when simulating variance switching experiments, the model also quantitatively fits the experimental data over a wide dynamic range.</s> <s>furthermore, dynamic threshold models of adaptation suggest a straightforward interpretation of neural activity in terms of dynamic signal encoding with shifted and weighted exponential kernels.</s> <s>we show that when thus encoding rectified filtered stimulus signals, the multiplicative adaptive spike response model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.</s></p></d>", "label": ["<d><p><s>efficient spike-coding with multiplicative adaptation in a spike response model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix.</s> <s>this estimator is used in a convex algorithm for robust subspace recovery (i.e., robust pca).</s> <s>our model assumes a sub-gaussian underlying distribution and an i.i.d.~sample from it.</s> <s>our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $n$ is of order $o(n^{-0.5+\\eps})$ for arbitrarily small $\\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation, i.e., $o(n^{-0.5})$.</s> <s>our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the frobenius norm is $o(d^{2+\\delta})$ for arbitrarily small $\\delta>0$ (whereas the sample complexity of direct covariance estimation with frobenius norm is $o(d^{2})$).</s> <s>these results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm, which are close to those of pca.</s> <s>to the best of our knowledge, this is the only work analyzing the sample complexity of any robust pca algorithm.</s></p></d>", "label": ["<d><p><s>on the sample complexity of robust pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new model based on gaussian processes (gps) for learning pairwise preferences expressed by multiple users.</s> <s>inference is simplified by using a \\emph{preference kernel} for gps which allows us to combine supervised gp learning of user preferences with unsupervised dimensionality reduction for multi-user systems.</s> <s>the model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available.</s> <s>approximate inference is implemented using a combination of expectation propagation and variational bayes.</s> <s>finally, we present an efficient active learning strategy for querying preferences.</s> <s>the proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.</s></p></d>", "label": ["<d><p><s>collaborative gaussian processes for preference learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the question of aggregating pairwise comparisons to obtain a global ranking over a collection of objects has been of interest for a very long time: be it ranking of online gamers (e.g.</s> <s>msr?s trueskill system) and chess players, aggregating social opinions, or deciding which product to sell based on transactions.</s> <s>in most settings, in addition to obtaining ranking, finding ?scores?</s> <s>for each object (e.g.</s> <s>player?s rating) is of interest to understanding the intensity of the preferences.</s> <s>in this paper, we propose a novel iterative rank aggregation algorithm for discovering scores for objects from pairwise comparisons.</s> <s>the algorithm has a natural random walk interpretation over the graph of objects with edges present between two objects if they are compared; the scores turn out to be the stationary probability of this random walk.</s> <s>the algorithm is model independent.</s> <s>to establish the efficacy of our method, however, we consider the popular bradley-terry-luce (btl) model in which each object has an associated score which determines the probabilistic outcomes of pairwise comparisons between objects.</s> <s>we bound the finite sample error rates between the scores assumed by the btl model and those estimated by our algorithm.</s> <s>this, in essence, leads to order-optimal dependence on the number of samples required to learn the scores well by our algorithm.</s> <s>indeed, the experimental evaluation shows that our (model independent) algorithm performs as well as the maximum likelihood estimator of the btl model and outperforms a recently proposed algorithm by ammar and shah [1].</s></p></d>", "label": ["<d><p><s>iterative ranking from pair-wise comparisons</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the computational modelling of the primary auditory cortex (a1) has been less fruitful than that of the primary visual cortex (v1) due to the less organized properties of a1.</s> <s>greater disorder has recently been demonstrated for the tonotopy of a1 that has traditionally been considered to be as ordered as the retinotopy of v1.</s> <s>this disorder appears to be incongruous, given the uniformity of the neocortex; however, we hypothesized that both a1 and v1 would adopt an efficient coding strategy and that the disorder in a1 reflects natural sound statistics.</s> <s>to provide a computational model of the tonotopic disorder in a1, we used a model that was originally proposed for the smooth v1 map.</s> <s>in contrast to natural images, natural sounds exhibit distant correlations, which were learned and reflected in the disordered map.</s> <s>the auditory model predicted harmonic relationships among neighbouring a1 cells; furthermore, the same mechanism used to model v1 complex cells reproduced nonlinear responses similar to the pitch selectivity.</s> <s>these results contribute to the understanding of the sensory cortices of different modalities in a novel and integrated manner.</s></p></d>", "label": ["<d><p><s>the topographic unsupervised learning of natural sounds in the auditory cortex</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the task of assigning a set of relevant tags to an image is challenging due to the size and variability of tag vocabularies.</s> <s>consequently, most existing algorithms focus on tag assignment and fix an often large number of hand-crafted features to describe image characteristics.</s> <s>in this paper we introduce a hierarchical model for learning representations of full sized color images from the pixel level, removing the need for engineered feature representations and subsequent feature selection.</s> <s>we benchmark our model on the stl-10 recognition dataset, achieving state-of-the-art performance.</s> <s>when our features are combined with tagprop (guillaumin et al.</s> <s>), we outperform or compete with existing annotation approaches that use over a dozen distinct image descriptors.</s> <s>furthermore, using 256-bit codes and hamming distance for training tagprop, we exchange only a small reduction in performance for efficient storage and fast comparisons.</s> <s>in our experiments, using deeper architectures always outperform shallow ones.</s></p></d>", "label": ["<d><p><s>deep representations and codes for image auto-annotation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given pairwise dissimilarities between data points, we consider the problem of finding a subset of data points called representatives or exemplars that can efficiently describe the data collection.</s> <s>we formulate the problem as a row-sparsity regularized trace minimization problem which can be solved efficiently using convex programming.</s> <s>the solution of the proposed optimization program finds the representatives and the probability that each data point is associated to each one of the representatives.</s> <s>we obtain the range of the regularization parameter for which the solution of the proposed optimization program changes from selecting one representative to selecting all data points as the representatives.</s> <s>when data points are distributed around multiple clusters according to the dissimilarities, we show that the data in each cluster select only representatives from that cluster.</s> <s>unlike metric-based methods, our algorithm does not require that the pairwise dissimilarities be metrics and can be applied to dissimilarities that are asymmetric or violate the triangle inequality.</s> <s>we demonstrate the effectiveness of the proposed algorithm on synthetic data as well as real-world datasets of images and text.</s></p></d>", "label": ["<d><p><s>finding exemplars from pairwise dissimilarities via simultaneous sparse recovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>keypoint matching between pairs of images using popular descriptors like sift or a faster variant called surf is at the heart of many computer vision algorithms including recognition, mosaicing, and structure from motion.</s> <s>for real-time mobile applications, very fast but less accurate descriptors like brief and related methods use a random sampling of pairwise comparisons of pixel intensities in an image patch.</s> <s>here, we introduce locally uniform comparison image descriptor (lucid), a simple description method based on permutation distances between the ordering of intensities of rgb values between two patches.</s> <s>lucid is computable in linear time with respect to patch size and does not require floating point computation.</s> <s>an analysis reveals an underlying issue that limits the potential of brief and related approaches compared to lucid.</s> <s>experiments demonstrate that lucid is faster than brief, and its accuracy is directly comparable to surf while being more than an order of magnitude faster.</s></p></d>", "label": ["<d><p><s>locally uniform comparison image descriptor</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>rich and complex time-series data, such as those generated from engineering sys- tems, financial markets, videos or neural recordings are now a common feature of modern data analysis.</s> <s>explaining the phenomena underlying these diverse data sets requires flexible and accurate models.</s> <s>in this paper, we promote gaussian process dynamical systems as a rich model class appropriate for such analysis.</s> <s>in particular, we present a message passing algorithm for approximate inference in gpdss based on expectation propagation.</s> <s>by phrasing inference as a general mes- sage passing problem, we iterate forward-backward smoothing.</s> <s>we obtain more accurate posterior distributions over latent structures, resulting in improved pre- dictive performance compared to state-of-the-art gpds smoothers, which are spe- cial cases of our general iterative message passing algorithm.</s> <s>hence, we provide a unifying approach within which to contextualize message passing in gpdss.</s></p></d>", "label": ["<d><p><s>expectation propagation in gaussian process dynamical systems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper proposes a novel image representation called a graphical gaussian vector, which is a counterpart of the codebook and local feature matching approaches.</s> <s>in our method, we model the distribution of local features as a gaussian markov random field (gmrf) which can efficiently represent the spatial relationship among local features.</s> <s>we consider the parameter of gmrf as a feature vector of the image.</s> <s>using concepts of information geometry, proper parameters and a metric from the gmrf can be obtained.</s> <s>finally we define a new image feature by embedding the metric into the parameters, which can be directly applied to scalable linear classifiers.</s> <s>our method obtains superior performance over the state-of-the-art methods in the standard object recognition datasets and comparable performance in the scene dataset.</s> <s>as the proposed method simply calculates the local auto-correlations of local features, it is able to achieve both high classification accuracy and high efficiency.</s></p></d>", "label": ["<d><p><s>graphical gaussian vector for image categorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is r^n.</s> <s>existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x* are known in advance.</s> <s>we present an algorithm that, without such prior knowledge, offers near-optimal regret bounds with respect to _any_ choice of x*.</s> <s>in particular, regret with respect to x* = 0 is _constant_.</s> <s>we then prove lower bounds showing that our algorithm's guarantees are optimal in this setting up to constant factors.</s></p></d>", "label": ["<d><p><s>no-regret algorithms for unconstrained online convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multiple kernel learning (mkl) generalizes svms to the setting where one simultaneously trains a linear classifier and chooses an optimal combination of given base kernels.</s> <s>model complexity is typically controlled using various norm regularizations on the vector of base kernel mixing coefficients.</s> <s>existing methods, however, neither regularize nor exploit potentially useful information pertaining to how kernels in the input set 'interact'; that is, higher order kernel-pair relationships that can be easily obtained via unsupervised (similarity, geodesics), supervised (correlation in errors), or domain knowledge driven mechanisms (which features were used to construct the kernel?).</s> <s>we show that by substituting the norm penalty with an arbitrary quadratic function q \\succeq 0, one can impose a desired covariance structure on mixing coefficient selection, and use this as an inductive bias when learning the concept.</s> <s>this formulation significantly generalizes the widely used 1- and 2-norm mkl objectives.</s> <s>we explore the model?s utility via experiments on a challenging neuroimaging problem, where the goal is to predict a subject?s conversion to alzheimer?s disease (ad) by exploiting aggregate information from several distinct imaging modalities.</s> <s>here, our new model outperforms the state of the art (p-values << 10?3 ).</s> <s>we briefly discuss ramifications in terms of learning bounds (rademacher complexity).</s></p></d>", "label": ["<d><p><s>q-mkl: matrix-induced regularization in multi-kernel learning with applications to neuroimaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel method for learning densities with bounded support which enables us to incorporate `hard' topological constraints.</s> <s>in particular, we show how emerging techniques from computational algebraic topology and the notion of persistent homology can be combined with kernel based  methods from machine learning for the purpose of density estimation.</s> <s>the proposed formalism facilitates learning of models with bounded support in a principled way, and -- by incorporating persistent homology techniques in our approach -- we are able to encode algebraic-topological constraints which are not addressed in current state-of the art probabilistic models.</s> <s>we study the behaviour of our method on two synthetic examples for various sample sizes and exemplify the benefits of the proposed approach on a real-world data-set by learning a motion model for a racecar.</s> <s>we show how to learn a model which respects the underlying topological structure of the racetrack, constraining the trajectories of the car.</s></p></d>", "label": ["<d><p><s>persistent homology for learning densities with bounded support</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a large visual multi-class detection framework, the timeliness of results can be crucial.</s> <s>our method for timely multi-class detection aims to give the best possible performance at any single point after a start time; it is terminated at a deadline time.</s> <s>toward this goal, we formulate a dynamic, closed-loop policy that infers the contents of the image in order to decide which detector to deploy next.</s> <s>in contrast to previous work, our method significantly diverges from the predominant greedy strategies, and is able to learn to take actions with deferred values.</s> <s>we evaluate our method with a novel timeliness measure, computed as the area under an average precision vs. time curve.</s> <s>experiments are conducted on the eminent pascal voc object detection dataset.</s> <s>if execution is stopped when only half the detectors have been run, our method obtains $66\\%$ better ap than a random ordering, and $14\\%$ better performance than an intelligent baseline.</s> <s>on the timeliness measure, our method obtains at least $11\\%$ better performance.</s> <s>our code, to be made available upon publication, is easily extensible as it treats detectors and classifiers as black boxes and learns from execution traces using reinforcement learning.</s></p></d>", "label": ["<d><p><s>timely object recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes gradient methods based on a scaled metric on the grassmann manifold for low-rank matrix completion.</s> <s>the proposed methods significantly improve canonical gradient methods especially on ill-conditioned matrices, while maintaining established global convegence and exact recovery guarantees.</s> <s>a connection between a form of subspace iteration for matrix completion and the scaled gradient descent procedure is also established.</s> <s>the proposed conjugate gradient method based on the scaled gradient outperforms several existing algorithms for matrix completion and is competitive with recently proposed methods.</s></p></d>", "label": ["<d><p><s>scaled gradients on grassmann manifolds for matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fine-grained recognition refers to a subordinate level of recognition, such are recognizing different species of birds, animals or plants.</s> <s>it differs from recognition of basic categories, such as humans, tables, and computers, in that there are global similarities in shape or structure shared within a category, and the differences are in the details of the object parts.</s> <s>we suggest that the key to identifying the fine-grained differences lies in finding the right alignment of image regions that contain the same object parts.</s> <s>we propose a template model for the purpose, which captures common shape patterns of object parts, as well as the co-occurence relation of the shape patterns.</s> <s>once the image regions are aligned, extracted features are used for classification.</s> <s>learning of the template model is efficient, and the recognition results we achieve significantly outperform the state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>unsupervised template learning for fine-grained object recognition</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop a probabilistic model of legislative data that uses the text of the bills to uncover lawmakers' positions on specific political issues.</s> <s>our model can be used to explore how a lawmaker's voting patterns deviate from what is expected and how that deviation depends on what is being voted on.</s> <s>we derive approximate posterior inference algorithms based on variational methods.</s> <s>across 12 years of legislative data, we demonstrate both improvement in heldout predictive performance and the model's utility in interpreting an inherently multi-dimensional space.</s></p></d>", "label": ["<d><p><s>how they vote: issue-adjusted models of legislative behavior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>graphical models are a very useful tool to describe and understand natural phenomena, from gene expression to climate change and social interactions.</s> <s>the topological structure of these graphs/networks is a fundamental part of the analysis, and in many cases the main goal of the study.</s> <s>however, little work has been done on incorporating prior topological knowledge onto the estimation of the underlying graphical models from sample data.</s> <s>in this work we propose extensions to the basic joint regression model for network estimation, which explicitly incorporate graph-topological constraints into the corresponding optimization approach.</s> <s>the first proposed extension includes an eigenvector centrality constraint, thereby promoting this important prior topological property.</s> <s>the second developed extension promotes the formation of certain motifs, triangle-shaped ones in particular, which are known to exist for example in genetic regulatory networks.</s> <s>the presentation of the underlying formulations, which serve as examples of the introduction of topological constraints in network estimation, is complemented with examples in diverse datasets demonstrating the importance of incorporating such critical prior knowledge.</s></p></d>", "label": ["<d><p><s>topology constraints in graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the dominant visual search paradigm for object class detection is sliding windows.</s> <s>although simple and effective, it is also wasteful, unnatural and rigidly hardwired.</s> <s>we propose strategies to search for objects which intelligently explore the space of windows by making sequential observations at locations decided based on previous observations.</s> <s>our strategies adapt to the class being searched and to the content of a particular test image.</s> <s>their driving force is exploiting context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set.</s> <s>in addition to being more elegant than sliding windows, we demonstrate experimentally on the pascal voc 2010 dataset that our strategies evaluate two orders of magnitude fewer windows while at the same time achieving higher detection accuracy.</s></p></d>", "label": ["<d><p><s>searching for objects driven by context</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware.</s> <s>recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations.</s> <s>operating speeds allowing for real time information operation have been reached using optoelectronic systems.</s> <s>at present the main performance bottleneck is the readout layer which uses slow, digital postprocessing.</s> <s>we have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time.</s> <s>the readout has been built and tested experimentally on a standard benchmark task.</s> <s>its performance is better than non-reservoir methods, with ample room for further improvement.</s> <s>the present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.</s></p></d>", "label": ["<d><p><s>analog readout for optical reservoir computers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian inference provides a unifying framework for addressing problems in machine learning, artificial intelligence, and robotics, as well as the problems facing the human mind.</s> <s>unfortunately, exact bayesian inference is intractable in all but the simplest models.</s> <s>therefore minds and machines have to approximate bayesian inference.</s> <s>approximate inference algorithms can achieve a wide range of time-accuracy tradeoffs, but what is the optimal tradeoff?</s> <s>we investigate time-accuracy tradeoffs using the metropolis-hastings algorithm as a metaphor for the mind's inference algorithm(s).</s> <s>we find that reasonably accurate decisions are possible long before the markov chain has converged to the posterior distribution, i.e.</s> <s>during the period known as burn-in.</s> <s>therefore the strategy that is optimal subject to the mind's bounded processing speed and opportunity costs may perform so few iterations that the resulting samples are biased towards the initial value.</s> <s>the resulting cognitive process model provides a rational basis for the anchoring-and-adjustment heuristic.</s> <s>the model's quantitative predictions are tested against published data on anchoring in numerical estimation tasks.</s> <s>our theoretical and empirical results suggest that the anchoring bias is consistent with approximate bayesian inference.</s></p></d>", "label": ["<d><p><s>burn-in, bias, and the rationality of anchoring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there is no generally accepted way to define wavelets on permutations.</s> <s>we address this issue by introducing the notion of coset based multiresolution analysis (cmra) on the symmetric group; find the corresponding wavelet functions; and describe a fast wavelet transform of o(n^p) complexity with small p for sparse signals (in contrast to the o(n^q n!)</s> <s>complexity typical of ffts).</s> <s>we discuss potential applications in ranking, sparse approximation, and multi-object tracking.</s></p></d>", "label": ["<d><p><s>multiresolution analysis on the symmetric group</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications.</s> <s>the domain knowledge is given as a weighted graph, or a kernel matrix, that loosely indicates which states should have similar optimal actions.</s> <s>we first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities.</s> <s>this distribution corresponds to a markov random field.</s> <s>we then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions.</s> <s>we also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions, gridworlds, and teaching a robot to grasp new objects.</s></p></d>", "label": ["<d><p><s>algorithms for learning markov field policies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the interaction between the patient's expected outcome of an intervention and the inherent effects of that intervention can have extraordinary effects.</s> <s>thus in clinical trials an effort is made to conceal the nature of the administered intervention from the participants in the trial i.e.</s> <s>to blind it.</s> <s>yet, in practice perfect blinding is impossible to ensure or even verify.</s> <s>the current standard is follow up the trial with an auxiliary questionnaire, which allows trial participants to express their belief concerning the assigned intervention and which is used to compute a measure of the extent of blinding in the trial.</s> <s>if the estimated extent of blinding exceeds a threshold the trial is deemed sufficiently blinded; otherwise, the trial is deemed to have failed.</s> <s>in this paper we make several important contributions.</s> <s>firstly, we identify a series of fundamental problems of the aforesaid practice and discuss them in context of the most commonly used blinding measures.</s> <s>secondly, motivated by the highlighted problems, we formulate a novel method for handling imperfectly blinded trials.</s> <s>we too adopt a post-trial feedback questionnaire but interpret the collected data using an original approach, fundamentally different from those previously proposed.</s> <s>unlike previous approaches, ours is void of any ad hoc free parameters, is robust to small changes in auxiliary data and is not predicated on any strong assumptions used to interpret participants' feedback.</s></p></d>", "label": ["<d><p><s>assessing blinding in clinical trials</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a probabilistic graphical model, its density of states is a function that, for any likelihood value, gives the number of configurations with that probability.</s> <s>we introduce a novel message-passing algorithm called density propagation (dp) for estimating this function.</s> <s>we show that dp is exact for tree-structured graphical models and is, in general, a strict generalization of both sum-product and max-product algorithms.</s> <s>further, we use density of states and tree decomposition to introduce a new family of upper and lower bounds on the partition function.</s> <s>for any tree decompostion, the new upper bound based on finer-grained density of state information is provably at least as tight as previously known bounds based on convexity of the log-partition function, and strictly stronger if a general condition holds.</s> <s>we conclude with empirical evidence of improvement over convex relaxations and mean-field based bounds.</s></p></d>", "label": ["<d><p><s>density propagation and improved bounds on the partition function</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes an efficient online learning algorithm to track the smoothing functions of additive models.</s> <s>the key idea is to combine the linear representation of additive models with a recursive least squares (rls) filter.</s> <s>in order to quickly track changes in the model and put more weight on recent data, the rls filter uses a forgetting factor which exponentially weights down observations by the order of their arrival.</s> <s>the tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors.</s> <s>using results from lyapunov stability theory, upper bounds for the learning rate are analyzed.</s> <s>the proposed algorithm is applied to 5 years of electricity load data provided by the french utility company electricite de france (edf).</s> <s>compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.</s></p></d>", "label": ["<d><p><s>adaptive learning of smoothing functions: application to electricity load forecasting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a novel approach for supervised learning based on adaptively partitioning the feature space into different regions and learning local region-specific classifiers.</s> <s>we formulate an empirical risk minimization problem that incorporates both partitioning and classification in to a single global objective.</s> <s>we show that space partitioning can be equivalently reformulated as a supervised learning problem and consequently any discriminative learning method can be utilized in conjunction with our approach.</s> <s>nevertheless, we consider locally linear schemes by learning linear partitions and linear region classifiers.</s> <s>locally linear schemes can not only approximate complex decision boundaries and ensure low training error but also provide tight control on over-fitting and generalization error.</s> <s>we train locally linear classifiers by using lda, logistic regression and perceptrons, and so our scheme is scalable to large data sizes and high-dimensions.</s> <s>we present experimental results demonstrating improved performance over state of the art classification techniques on benchmark datasets.</s> <s>we also show improved robustness to label noise.</s></p></d>", "label": ["<d><p><s>local supervised learning through space partitioning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unsupervised clustering of scattered, noisy and high-dimensional data points is an important and difficult problem.</s> <s>continuous relaxations of balanced cut problems  yield excellent clustering results.</s> <s>this paper provides rigorous convergence results for two algorithms that solve the relaxed cheeger cut minimization.</s> <s>the first algorithm is a new steepest descent algorithm and the second one is a slight modification of the inverse power method algorithm \\cite{pro:heinbuhler10onespec}.</s> <s>while the steepest descent algorithm has better theoretical convergence properties,  in practice both algorithm perform equally.</s> <s>we also completely characterize the local minima of the relaxed problem in terms of the original balanced cut problem, and relate this characterization to the convergence of the algorithms.</s></p></d>", "label": ["<d><p><s>convergence and energy landscape for cheeger cut clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>abstract given samples from distributions $p$ and $q$, a two-sample test determines whether to reject the null hypothesis that $p=q$, based on the value of a test statistic measuring the distance between the samples.</s> <s>one choice of test statistic is the maximum mean discrepancy (mmd), which is a distance between embeddings of the probability distributions in a reproducing kernel hilbert space.</s> <s>the kernel used in obtaining these embeddings is thus critical in ensuring the test has high power, and correctly distinguishes unlike distributions with high probability.</s> <s>a means of parameter selection for the two-sample test based on the mmd is proposed.</s> <s>for a given test level (an upper bound on the probability of making a type i error), the kernel is chosen so as to maximize the test power, and minimize the probability of making a type ii error.</s> <s>the test statistic, test threshold, and optimization over the kernel parameters are obtained with cost linear in the sample size.</s> <s>these properties make the kernel selection and test procedures suited to data streams, where the observations cannot all be stored in memory.</s> <s>in experiments, the new kernel selection approach yields a more powerful test than earlier kernel selection heuristics.</s></p></d>", "label": ["<d><p><s>optimal kernel choice for large-scale two-sample tests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study two communication-efficient algorithms for distributed statistical optimization on large-scale data.</s> <s>the first algorithm is an averaging method that distributes the $n$ data samples evenly to $m$ machines, performs separate minimization on each subset, and then averages the estimates.</s> <s>we provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as $\\order(n^{-1}+(n/m)^{-2})$.</s> <s>whenever $m \\le \\sqrt{n}$, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all $n$ samples.</s> <s>the second algorithm is a novel method, based on an appropriate form of the bootstrap.</s> <s>requiring only a single round of communication, it has mean-squared error that decays as $\\order(n^{-1}+(n/m)^{-3})$, and so is more robust to the amount of parallelization.</s> <s>we complement our theoretical results with experiments on large-scale problems from the microsoft learning to rank dataset.</s></p></d>", "label": ["<d><p><s>communication-efficient algorithms for statistical optimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks.</s> <s>it has been successfully applied to many applications including computer vision and biomedical informatics.</s> <s>most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\\ell_0$-type regularizer.</s> <s>in this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel regularizer.</s> <s>to solve the non-convex optimization problem, we propose a multi-stage multi-task feature learning (msmtfl) algorithm.</s> <s>moreover, we present a detailed theoretical analysis showing that msmtfl achieves a better parameter estimation error bound than the convex formulation.</s> <s>empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of msmtfl in comparison with the state of the art multi-task sparse feature learning algorithms.</s></p></d>", "label": ["<d><p><s>multi-stage multi-task feature learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we formulate clustering aggregation as a special instance of maximum-weight independent set (mwis) problem.</s> <s>for a given dataset, an attributed graph is constructed from the union of the input clusterings generated by different underlying clustering algorithms with different parameters.</s> <s>the vertices, which represent the distinct clusters, are weighted by an internal index measuring both cohesion and separation.</s> <s>the edges connect the vertices whose corresponding clusters overlap.</s> <s>intuitively, an optimal aggregated clustering can be obtained by selecting an optimal subset of non-overlapping clusters partitioning the dataset together.</s> <s>we formalize this intuition as the mwis problem on the attributed graph, i.e., finding the heaviest subset of mutually non-adjacent vertices.</s> <s>this mwis problem exhibits a special structure.</s> <s>since the clusters of each input clustering form a partition of the dataset, the vertices corresponding to each clustering form a maximal independent set (mis) in the attributed graph.</s> <s>we propose a variant of simulated annealing method that takes advantage of this special structure.</s> <s>our algorithm starts from each mis, which is close to a distinct local optimum of the mwis problem, and utilizes a local search heuristic to explore its neighborhood in order to find the mwis.</s> <s>extensive experiments on many challenging datasets show that: 1. our approach to clustering aggregation automatically decides the optimal number of clusters; 2. it does not require any parameter tuning for the underlying clustering algorithms; 3. it can combine the advantages of different underlying clustering algorithms to achieve superior performance; 4. it is robust against moderate or even bad input clusterings.</s></p></d>", "label": ["<d><p><s>clustering aggregation as maximum-weight independent set</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper examines the possibility of a `reject option' in the context of least squares regression.</s> <s>it is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\\epsilon$-pointwise track the best regressor in hindsight from the same hypothesis class, while rejecting only a bounded portion of the domain.</s> <s>moreover, the rejected volume vanishes with the training set size, under certain conditions.</s> <s>we then develop efficient and exact implementation of these selective regressors for the case of linear regression.</s> <s>empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error.</s></p></d>", "label": ["<d><p><s>pointwise tracking the optimal regression function</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, a novel, computationally fast, and alternative algorithm for com- puting weighted v-statistics in resampling both univariate and multivariate data is proposed.</s> <s>to avoid any real resampling, we have linked this problem with finite group action and converted it into a problem of orbit enumeration.</s> <s>for further computational cost reduction, an efficient method is developed to list all orbits by their symmetry order and calculate all index function orbit sums and data function orbit sums recursively.</s> <s>the computational complexity analysis shows reduction in the computational cost from n!</s> <s>or nn level to low-order polynomial level.</s></p></d>", "label": ["<d><p><s>fast resampling weighted v-statistics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper sheds light on some fundamental connections of the diffusion decision making model of neuroscience and cognitive psychology with k-nearest neighbor classification.</s> <s>we show that conventional k-nearest neighbor classification can be viewed as a special problem of the diffusion decision model in the asymptotic situation.</s> <s>applying the optimal strategy associated with the diffusion decision model, an adaptive rule is developed for determining appropriate values of k in k-nearest neighbor classification.</s> <s>making use of the sequential probability ratio test (sprt) and bayesian analysis, we propose five different criteria for adaptively acquiring nearest neighbors.</s> <s>experiments with both synthetic and real datasets demonstrate the effectivness of our classification criteria.</s></p></d>", "label": ["<d><p><s>diffusion decision making for adaptive k-nearest neighbor classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inference on high-order graphical models has become increasingly important in recent years.</s> <s>we consider energies with simple 'sparse' high-order potentials.</s> <s>previous work in this area uses either specialized message-passing  or transforms each high-order potential to the pairwise case.</s> <s>we take a fundamentally different approach, transforming the entire original problem into a comparatively small instance of a submodular vertex-cover problem.</s> <s>these vertex-cover instances can then be attacked by standard pairwise methods, where they run much faster (4--15 times) and are often more effective than on the original problem.</s> <s>we evaluate our approach on synthetic data, and we show that our algorithm can be useful in a fast hierarchical clustering and model estimation framework.</s></p></d>", "label": ["<d><p><s>minimizing sparse high-order energies by submodular vertex-cover</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a nonparametric bayesian approach to inverse reinforcement learning (irl) for multiple reward functions.</s> <s>most previous irl algorithms assume that the behaviour data is obtained from an agent who is optimizing a single reward function, but this assumption is hard to be met in practice.</s> <s>our approach is based on integrating the dirichlet process mixture model into bayesian irl.</s> <s>we provide an efficient metropolis-hastings sampling algorithm utilizing the gradient of the posterior to estimate the underlying reward functions, and demonstrate that our approach outperforms the previous ones via experiments on a number of problem domains.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian inverse reinforcement learning for multiple reward functions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the integration of excitatory inputs in dendrites is non-linear: multiple excitatory inputs can produce a local depolarization departing from the arithmetic sum of each input's response taken separately.</s> <s>if this depolarization is bigger than the arithmetic sum, the dendrite is spiking; if the depolarization is smaller, the dendrite is saturating.</s> <s>decomposing a dendritic tree into independent dendritic spiking units greatly extends its computational capacity, as the neuron then maps onto a two layer neural network, enabling it to compute linearly non-separable boolean functions (lnbfs).</s> <s>how can these lnbfs be implemented by dendritic architectures in practise?</s> <s>and can saturating dendrites equally expand computational capacity?</s> <s>to adress these questions we use a binary neuron model and boolean algebra.</s> <s>first, we confirm that spiking dendrites enable a neuron to compute lnbfs using an architecture based on the disjunctive normal form (dnf).</s> <s>second, we prove that saturating dendrites as well as spiking dendrites also enable a neuron to compute lnbfs using an architecture based on the conjunctive normal form (cnf).</s> <s>contrary to the dnf-based architecture, a cnf-based architecture leads to a dendritic unit tuning that does not imply the neuron tuning, as has been observed experimentally.</s> <s>third, we show that one cannot use a dnf-based architecture with saturating dendrites.</s> <s>consequently, we show that an important family of lnbfs implemented with a cnf-architecture can require an exponential number of saturating dendritic units, whereas the same family implemented with either a dnf-architecture or a cnf-architecture always require a linear number of spiking dendritic unit.</s> <s>this minimization could explain why a neuron spends energetic resources to make its dendrites spike.</s></p></d>", "label": ["<d><p><s>spiking and saturating dendrites differentially expand single neuron computation capacity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a new algorithm to cluster sparse unweighted graphs -- i.e.</s> <s>partition the nodes into disjoint clusters so that there is higher density within clusters, and low across clusters.</s> <s>by sparsity we mean the setting where both the in-cluster and across cluster edge densities are very small, possibly vanishing in the size of the graph.</s> <s>sparsity makes the problem noisier, and hence more difficult to solve.</s> <s>any clustering involves a tradeoff between minimizing two kinds of errors: missing edges within clusters and present edges across clusters.</s> <s>our insight is that in the sparse case, these must be {\\em penalized differently}.</s> <s>we analyze our algorithm's performance on the natural, classical and widely studied ``planted partition'' model (also called the stochastic block model); we show that our algorithm can cluster sparser graphs, and with smaller clusters, than all previous methods.</s> <s>this is seen empirically as well.</s></p></d>", "label": ["<d><p><s>clustering sparse graphs</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures  depicted in stacks of electron microscopy (em) images.</s> <s>this is necessary to efficiently map 3d brain structure and connectivity.</s> <s>to segment {\\em biological} neuron membranes, we use a special type of deep {\\em artificial} neural network as a pixel classifier.</s> <s>the label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it.</s> <s>the input layer maps each window pixel to a neuron.</s> <s>it is followed by a succession of convolutional and max-pooling layers which preserve 2d information and extract features with increasing levels of abstraction.</s> <s>the output layer produces a calibrated probability for each class.</s> <s>the classifier is trained by plain gradient descent on a $512 \\times 512 \\times 30$ stack with known ground truth, and tested on a stack of the same  size (ground truth unknown to the authors) by the organizers of the isbi 2012 em segmentation challenge.</s> <s>even without problem-specific post-processing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e.</s> <s>\\emph{rand error}, \\emph{warping error} and \\emph{pixel error}.</s> <s>for pixel error, our approach is the only one outperforming a second human observer.</s></p></d>", "label": ["<d><p><s>deep neural networks segment neuronal membranes in electron microscopy images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many visual and auditory neurons have response properties that are well explained by pooling the rectified responses of a set of self-similar linear filters.</s> <s>these filters cannot be found using spike-triggered averaging (sta), which estimates only a single filter.</s> <s>other methods, like spike-triggered covariance (stc), define a multi-dimensional response subspace, but require substantial amounts of data and do not produce unique estimates of the linear filters.</s> <s>rather, they provide a linear basis for the subspace in which the filters reside.</s> <s>here, we define a 'subunit' model as an ln-ln cascade, in which the first linear stage is restricted to a set of shifted (\"convolutional\") copies of a common filter, and the first nonlinear stage consists of rectifying nonlinearities that are identical for all filter outputs; we refer to these initial ln elements as the 'subunits' of the receptive field.</s> <s>the second linear stage then computes a weighted sum of the responses of the rectified subunits.</s> <s>we present a method for directly fitting this model to spike data.</s> <s>the method performs well for both simulated and real data (from primate v1), and the resulting model outperforms sta and stc in terms of both cross-validated accuracy and efficiency.</s></p></d>", "label": ["<d><p><s>efficient and direct estimation of a neural subunit model for sensory coding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic differential equations (sde) are a natural tool for modelling systems that are inherently noisy or contain uncertainties that can be modelled as stochastic processes.</s> <s>crucial to the process of using sde to build mathematical models is the ability to estimate parameters of those models from observed data.</s> <s>over the past few decades, significant progress has been made on this problem, but we are still far from having a definitive solution.</s> <s>we describe a novel method of approximating a diffusion process that we show to be useful in markov chain monte-carlo (mcmc) inference algorithms.</s> <s>we take the ?white?</s> <s>noise that drives a diffusion process and decompose it into two terms.</s> <s>the first is a ?coloured noise?</s> <s>term that can be deterministically controlled by a set of auxilliary variables.</s> <s>the second term is small and enables us to form a linear gaussian ?small noise?</s> <s>approximation.</s> <s>the decomposition allows us to take a diffusion process of interest and cast it in a form that is amenable to sampling by mcmc methods.</s> <s>we explain why many state-of-the-art inference methods fail on highly nonlinear inference problems.</s> <s>we demonstrate experimentally that our method performs well in such situations.</s> <s>our results show that this method is a promising new tool for use in inference and parameter estimation problems.</s></p></d>", "label": ["<d><p><s>the coloured noise expansion and parameter estimation of diffusion processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relationships between entities.</s> <s>while there is a large body of work focused on modeling these data, few considered modeling these multiple types of relationships jointly.</s> <s>further, existing approaches tend to breakdown when the number of these types grows.</s> <s>in this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations.</s> <s>our model is based on a bilinear structure, which captures the various orders of interaction of the data, but also shares sparse latent factors across different relations.</s> <s>we illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results.</s> <s>finally, a nlp application demonstrates our scalability and the ability of our model to learn efficient, and semantically meaningful verb representations.</s></p></d>", "label": ["<d><p><s>a latent factor model for highly multi-relational data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the cur matrix decomposition is an important extension of nystr?m approximation to a general matrix.</s> <s>it approximates any data matrix in terms of a small number of its columns and rows.</s> <s>in this paper we propose a novel randomized cur algorithm with an expected relative-error bound.</s> <s>the proposed algorithm has the advantages over the existing relative-error cur algorithms that it possesses tighter theoretical bound and lower time complexity, and that it can avoid maintaining the whole data matrix in main memory.</s> <s>finally, experiments on several real-world datasets demonstrate significant improvement over the existing relative-error algorithms.</s></p></d>", "label": ["<d><p><s>a scalable cur matrix decomposition algorithm: lower time complexity and tighter bound</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a simple and novel framework for mcmc inference in continuous-time discrete-state systems with pure jump trajectories.</s> <s>we construct an exact mcmc sampler for such systems by alternately sampling a random discretization of time given a trajectory of the system, and then a new trajectory given the discretization.</s> <s>the first step can be performed efficiently using properties of the poisson process, while the second step can avail of discrete-time mcmc techniques based on the forward-backward algorithm.</s> <s>we compare our approach to particle mcmc and a uniformization-based sampler, and show its advantages.</s></p></d>", "label": ["<d><p><s>mcmc for continuous-time discrete-state systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>linear support vector machines (svms) have become very popular in vision as part of state-of-the-art object recognition and other classification tasks but require high dimensional feature spaces for good performance.</s> <s>deep learning methods can find more compact representations but current methods employ multilayer perceptrons that require solving a difficult, non-convex optimization problem.</s> <s>we propose a deep non-linear classifier whose layers are svms and which incorporates random projection as its core stacking element.</s> <s>our method learns layers of linear svms recursively transforming the original data manifold through a random projection of the weak prediction computed from each layer.</s> <s>our method scales as linear svms, does not rely on any kernel computations or nonconvex optimization, and exhibits better generalization ability than kernel-based svms.</s> <s>this is especially true when the number of training samples is smaller than the dimensionality of data, a common scenario in many real-world applications.</s> <s>the use of random projections is key to our method, as we show in the experiments section, in which we observe a consistent improvement over previous --often more complicated-- methods on several vision and speech benchmarks.</s></p></d>", "label": ["<d><p><s>learning with recursive perceptual representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent approaches to collaborative filtering have concentrated on estimating an algebraic or statistical model, and using the model for predicting missing ratings.</s> <s>in this paper we observe that different models have relative advantages in different regions of the input space.</s> <s>this motivates our approach of using stagewise linear combinations of collaborative filtering algorithms, with non-constant combination coefficients based on kernel smoothing.</s> <s>the resulting stagewise model is computationally scalable and outperforms a wide selection of state-of-the-art collaborative filtering algorithms.</s></p></d>", "label": ["<d><p><s>automatic feature induction for stagewise collaborative filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a method for discovering the parts of an articulated object from aligned meshes capturing various three-dimensional (3d) poses.</s> <s>we adapt the distance dependent chinese restaurant process (ddcrp) to allow nonparametric discovery of a potentially unbounded number of parts, while simultaneously guaranteeing a spatially connected segmentation.</s> <s>to allow analysis of datasets in which object instances have varying shapes, we model part variability across poses via affine transformations.</s> <s>by placing a matrix normal-inverse-wishart prior on these affine transformations, we develop a ddcrp gibbs sampler which tractably marginalizes over transformation uncertainty.</s> <s>analyzing a dataset of humans captured in dozens of poses, we infer parts which provide quantitatively better motion predictions than conventional clustering methods.</s></p></d>", "label": ["<d><p><s>from deformations to parts: motion-based segmentation of 3d objects</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how do neural networks learn to represent information?</s> <s>here, we address this question by assuming that neural networks seek to generate an optimal population representation for a fixed linear decoder.</s> <s>we define a loss function for the quality of the population read-out and derive the dynamical equations for both neurons and synapses from the requirement to minimize this loss.</s> <s>the dynamical equations yield a network of integrate-and-fire neurons undergoing hebbian plasticity.</s> <s>we show that, through learning, initially regular and highly correlated spike trains evolve towards poisson-distributed and independent spike trains with much lower firing rates.</s> <s>the learning rule drives the network into an asynchronous, balanced regime where all inputs to the network are represented optimally for the given decoder.</s> <s>we show that the network dynamics and synaptic plasticity jointly balance the excitation and inhibition received by each unit as tightly as possible and, in doing so, minimize the prediction error between the inputs and the decoded outputs.</s> <s>in turn, spikes are only signalled whenever this prediction error exceeds a certain value, thereby implementing a predictive coding scheme.</s> <s>our work  suggests that several of the features reported in cortical networks, such as the high trial-to-trial variability, the balance between excitation and inhibition, and spike-timing dependent plasticity, are simply signatures of an efficient, spike-based code.</s></p></d>", "label": ["<d><p><s>learning optimal spike-based representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe an approach to incorporating bayesian priors in the maxq framework for hierarchical reinforcement learning (hrl).</s> <s>we define priors on the primitive environment model and on task pseudo-rewards.</s> <s>since models for composite tasks can be complex, we use a mixed model-based/model-free learning approach to find an optimal hierarchical policy.</s> <s>we show empirically that (i) our approach results in improved convergence over non-bayesian baselines, given sensible priors, (ii) task hierarchies and bayesian priors can be complementary sources of information, and using both sources is better than either alone, (iii) taking advantage of the structural decomposition induced by the task hierarchy significantly reduces the computational cost of bayesian reinforcement learning and (iv) in this framework, task pseudo-rewards can be learned instead of being manually specified, leading to automatic learning of hierarchically optimal rather than recursively optimal policies.</s></p></d>", "label": ["<d><p><s>bayesian hierarchical reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in stochastic multi--armed bandits the objective is to solve the exploration--exploitation dilemma and ultimately maximize the expected reward.</s> <s>nonetheless, in many practical problems, maximizing the expected reward is not the most desirable objective.</s> <s>in this paper, we introduce a novel setting based on the principle of risk--aversion where the objective is to compete against the arm with the best risk--return trade--off.</s> <s>this setting proves to be intrinsically more difficult than the standard multi-arm bandit setting due in part to an exploration risk which introduces a regret associated to the variability of an algorithm.</s> <s>using variance as a measure of risk, we introduce two new algorithms, we investigate their theoretical guarantees, and we report preliminary empirical results.</s></p></d>", "label": ["<d><p><s>risk-aversion in multi-armed bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>finding maximum aposteriori (map) assignments in graphical models is an important task in many applications.</s> <s>since the problem is generally hard, linear programming (lp) relaxations are often used.</s> <s>solving these relaxations efficiently is thus an important practical problem.</s> <s>in recent years, several authors have proposed message passing updates corresponding to coordinate descent in the dual lp.</s> <s>however,these are generally not guaranteed to converge to a global optimum.</s> <s>one approach to remedy this is to smooth the lp, and perform coordinate descent on the smoothed dual.</s> <s>however, little is known about the convergence rate of this procedure.</s> <s>here we perform a thorough rate analysis of such schemes and derive primal and dual convergence rates.</s> <s>we also provide a simple dual to primal mapping that yields feasible primal solutions with a guaranteed rate of convergence.</s> <s>empirical evaluation supports our theoretical claims and shows that the method is highly competitive with state of the art approaches that yield global optima.</s></p></d>", "label": ["<d><p><s>convergence rate analysis of map coordinate minimization algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many large economic markets, goods are sold through sequential auctions.</s> <s>such domains include ebay, online ad auctions, wireless spectrum auctions, and the dutch flower auctions.</s> <s>bidders in these domains face highly complex decision-making problems, as their preferences for outcomes in one auction often depend on the outcomes of other auctions, and bidders have limited information about factors that drive outcomes, such as other bidders' preferences and past actions.</s> <s>in this work, we formulate the bidder's problem as one of price prediction (i.e., learning) and optimization.</s> <s>we define the concept of stable price predictions and show that (approximate) equilibrium in sequential auctions can be characterized as a profile of strategies that (approximately) optimize with respect to such (approximately) stable price predictions.</s> <s>we show how equilibria found with our formulation compare to known theoretical equilibria for simpler auction domains, and we find new approximate equilibria for a more complex auction domain where analytical solutions were heretofore unknown.</s></p></d>", "label": ["<d><p><s>approximating equilibria in sequential auctions with incomplete information and multi-unit demand</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>partially-observable markov decision processes (pomdps) provide a powerful model for real-world sequential decision-making problems.</s> <s>in recent years, point- based value iteration methods have proven to be extremely effective techniques for ?nding (approximately) optimal dynamic programming solutions to pomdps when an initial set of belief states is known.</s> <s>however, no point-based work has provided exact point-based backups for both continuous state and observation spaces, which we tackle in this paper.</s> <s>our key insight is that while there may be an in?nite number of possible observations, there are only a ?nite number of observation partitionings that are relevant for optimal decision-making when a ?nite, ?xed set of reachable belief states is known.</s> <s>to this end, we make two important contributions: (1) we show how previous exact symbolic dynamic pro- gramming solutions for continuous state mdps can be generalized to continu- ous state pomdps with discrete observations, and (2) we show how this solution can be further extended via recently developed symbolic methods to continuous state and observations to derive the minimal relevant observation partitioning for potentially correlated, multivariate observation spaces.</s> <s>we demonstrate proof-of- concept results on uni- and multi-variate state and observation steam plant control.</s></p></d>", "label": ["<d><p><s>symbolic dynamic programming for continuous state and observation pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the setting of active learning for the multi-armed bandit, where the goal of a learner is to estimate with equal precision the mean of a finite number of arms, recent results show that it is possible to derive strategies based on finite-time confidence bounds that are competitive with the best possible strategy.</s> <s>we here consider an extension of this problem to the case when the arms are the cells of a finite partition p of a continuous sampling space x \\subset \\real^d.</s> <s>our goal is now to build a piecewise constant approximation of a noisy function (where each piece is one region of p and p is fixed beforehand) in order to maintain the local quadratic error of approximation on each cell equally low.</s> <s>although this extension is not trivial, we show that a simple algorithm based on upper confidence bounds can be proved to be adaptive to the function itself in a near-optimal way, when |p| is chosen to be of minimax-optimal order  on the class of \\alpha-h?lder functions.</s></p></d>", "label": ["<d><p><s>online allocation and homogeneous partitioning for piecewise constant mean-approximation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>simple gaussian mixture models (gmms) learned from pixels of natural image patches have been recently shown to be surprisingly strong performers in modeling the statistics of natural images.</s> <s>here we provide an in depth analysis of this simple yet rich model.</s> <s>we show that such a gmm model is able to compete with even the most successful models of natural images in log likelihood scores, denoising performance and sample quality.</s> <s>we provide an analysis of what such a model learns from natural images as a function of number of mixture components --- including covariance structure, contrast variation and intricate structures such as textures, boundaries and more.</s> <s>finally, we show that the salient properties of the gmm learned from natural images can be derived from a simplified dead leaves model which explicitly models occlusion, explaining its surprising success relative to other models.</s></p></d>", "label": ["<d><p><s>natural images, gaussian mixtures and dead leaves</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>early stages of visual processing are thought to decorrelate, or whiten, the incoming temporally varying signals.</s> <s>because the typical correlation time of natural stimuli, as well as the extent of temporal receptive fields of lateral geniculate nucleus (lgn) neurons, is much greater than neuronal time constants, such decorrelation must be done in stages combining contributions of multiple neurons.</s> <s>we propose to model temporal decorrelation in the visual pathway with the lattice filter, a signal processing device for stage-wise decorrelation of temporal signals.</s> <s>the stage-wise architecture of the lattice filter maps naturally onto the visual pathway (photoreceptors -> bipolar cells -> retinal ganglion cells -> lgn) and its filter weights can be learned using hebbian rules in a stage-wise sequential manner.</s> <s>moreover, predictions of neural activity from the lattice filter model are consistent with physiological measurements in lgn neurons and fruit fly second-order visual neurons.</s> <s>therefore, the lattice filter model is a useful abstraction that may help unravel visual system function.</s></p></d>", "label": ["<d><p><s>a lattice filter model of the visual pathway</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>when learning features for complex visual recognition problems, labeled image exemplars alone can be insufficient.</s> <s>while an \\emph{object taxonomy} specifying the categories' semantic relationships could bolster the learning process, not all relationships are relevant to a given visual classification task, nor does a single taxonomy capture all ties that \\emph{are} relevant.</s> <s>in light of these issues, we propose a discriminative feature learning approach that leverages \\emph{multiple} hierarchical taxonomies representing different semantic views of the object categories (e.g., for animal classes, one taxonomy could reflect their phylogenic ties, while another could reflect their habitats).</s> <s>for each taxonomy, we first learn a tree of semantic kernels, where each node has a mahalanobis kernel optimized to distinguish between the classes in its children nodes.</s> <s>then, using the resulting \\emph{semantic kernel forest}, we learn class-specific kernel combinations to select only those relationships relevant to recognize each object class.</s> <s>to learn the weights, we introduce a novel hierarchical regularization term that further exploits the taxonomies' structure.</s> <s>we demonstrate our method on challenging object recognition datasets, and show that interleaving multiple taxonomic views yields significant accuracy improvements.</s></p></d>", "label": ["<d><p><s>semantic kernel forests from multiple taxonomies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the human mind has a remarkable ability to store a vast amount of information in memory, and an even more remarkable ability to retrieve these experiences when needed.</s> <s>understanding the representations and algorithms that underlie human memory search could potentially be useful in other information retrieval settings, including internet search.</s> <s>psychological studies have revealed clear regularities in how people search their memory, with clusters of semantically related items tending to be retrieved together.</s> <s>these findings have recently been taken as evidence that human memory search is similar to animals foraging for food in patchy environments, with people making a rational decision to switch away from a cluster of related information as it becomes depleted.</s> <s>we demonstrate that the results that were taken as evidence for this account also emerge from a random walk on a semantic network, much like the random web surfer model used in internet search engines.</s> <s>this offers a simpler and more unified account of how people search their memory, postulating a single process rather than one process for exploring a cluster and one process for switching between clusters.</s></p></d>", "label": ["<d><p><s>human memory search as a random walk in a semantic network</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>robust policy optimization acknowledges that risk-aversion plays a vital role in real-world decision-making.</s> <s>when faced with uncertainty about the effects of actions, the policy that maximizes expected utility over the unknown parameters of the system may also carry with it a risk of intolerably poor performance.</s> <s>one might prefer to accept lower utility in expectation in order to avoid, or reduce the likelihood of, unacceptable levels of utility under harmful parameter realizations.</s> <s>in this paper, we take a bayesian approach to parameter uncertainty, but unlike other methods avoid making any distributional assumptions about the form of this uncertainty.</s> <s>instead we focus on identifying optimization objectives for which solutions can be efficiently approximated.</s> <s>we introduce percentile measures: a very general class of objectives for robust policy optimization, which encompasses most existing approaches, including ones known to be intractable.</s> <s>we then introduce a broad subclass of this family for which robust policies can be approximated efficiently.</s> <s>finally, we frame these objectives in the context of a two-player, zero-sum, extensive-form game and employ a no-regret algorithm to approximate an optimal policy, with computation only polynomial in the number of states and actions of the mdp.</s></p></d>", "label": ["<d><p><s>tractable objectives for robust policy optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we dicuss a novel  framework for multiclass learning,  defined by  a suitable coding/decoding strategy,  namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification.</s> <s>in this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class.</s> <s>moreover, we show that in this setting it is possible to derive the first provably consistent  regularized methods with training/tuning complexity which is {\\em independent} to the number of classes.</s> <s>tools from convex analysis are introduced that can be used beyond the scope of this paper.</s></p></d>", "label": ["<d><p><s>multiclass learning with simplex coding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while finding the exact solution for the map inference problem is intractable for many real-world tasks, map lp relaxations have been shown to be very effective in practice.</s> <s>however, the most efficient methods that perform block coordinate descent can get stuck in sub-optimal points as  they are not globally convergent.</s> <s>in this work we propose to augment these algorithms with an $\\epsilon$-descent approach and present a method to efficiently optimize for a descent direction in the subdifferential using a margin-based extension of the fenchel-young duality theorem.</s> <s>furthermore, the presented approach provides a methodology to construct a primal optimal solution from its dual optimal counterpart.</s> <s>we demonstrate the efficiency of the presented approach on spin glass models and protein interactions problems and show that our approach outperforms state-of-the-art solvers.</s></p></d>", "label": ["<d><p><s>globally convergent dual map lp relaxation solvers using fenchel-young margins</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a general method for deriving collapsed variational inference algorithms for probabilistic models in the conjugate exponential family.</s> <s>our method unifies many existing approaches to collapsed variational inference.</s> <s>our collapsed variational inference leads to a new lower bound on the marginal likelihood.</s> <s>we exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models.</s> <s>our approach is very general and is easily applied to any model where the mean field update equations have been derived.</s> <s>empirically we show significant speed-ups for probabilistic models optimized using our bound.</s></p></d>", "label": ["<d><p><s>fast variational inference in the conjugate exponential family</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way.</s> <s>unfortunately, finding the resulting bayes-optimal policies is notoriously taxing, since the search space becomes enormous.</s> <s>in this paper we introduce a tractable, sample-based method for approximate bayes-optimal planning which exploits monte-carlo tree search.</s> <s>our approach outperformed prior bayesian model-based rl algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of bayes rule within the search tree by lazily sampling models from the current beliefs.</s> <s>we illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in bayesian exploration.</s></p></d>", "label": ["<d><p><s>efficient bayes-adaptive reinforcement learning using sample-based search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove a new exponential concentration inequality for a plug-in estimator of the shannon mutual information.</s> <s>previous results on mutual information estimation only bounded expected error.</s> <s>the advantage of having the exponential inequality is that, combined with the union bound, we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously.</s> <s>as an application, we show how to use such a result to optimally estimate the density function and graph of a distribution which is markov to a forest graph.</s></p></d>", "label": ["<d><p><s>exponential concentration for mutual information estimation with application to forests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unsupervised joint alignment of images has been demonstrated to   improve performance on recognition tasks such as face verification.</s> <s>such alignment reduces undesired variability due to factors such as   pose, while only requiring weak supervision in the form of poorly   aligned examples.</s> <s>however, prior work on unsupervised alignment of   complex, real world images has required the careful selection of   feature representation based on hand-crafted image descriptors, in   order to achieve an appropriate, smooth optimization landscape.</s> <s>in this paper, we instead propose a novel combination of   unsupervised joint alignment with unsupervised feature learning.</s> <s>specifically, we incorporate deep learning into the {\\em congealing}   alignment framework.</s> <s>through deep learning, we obtain features that   can represent the image at differing resolutions based on network   depth, and that are tuned to the statistics of the specific data   being aligned.</s> <s>in addition, we modify the learning algorithm for   the restricted boltzmann machine by incorporating a group sparsity   penalty, leading to a topographic organization on the learned   filters and improving subsequent alignment results.</s> <s>we apply our method to the labeled faces in the wild database   (lfw).</s> <s>using the aligned images produced by our proposed   unsupervised algorithm, we achieve a significantly higher accuracy   in face verification than obtained using the original face images,   prior work in unsupervised alignment, and prior work in supervised   alignment.</s> <s>we also match the accuracy for the best available, but   unpublished method.</s></p></d>", "label": ["<d><p><s>learning to align from scratch</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies a novel discriminative part-based model to represent and recognize object shapes with an ?and-or graph?.</s> <s>we define this model consisting of three layers: the leaf-nodes with collaborative edges for localizing local parts, the or-nodes specifying the switch of leaf-nodes, and the root-node encoding the global verification.</s> <s>a discriminative learning algorithm, extended from the cccp [23], is proposed to train the model in a dynamical manner: the model structure (e.g., the configuration of the leaf-nodes associated with the or-nodes) is automatically determined with optimizing the multi-layer parameters during the iteration.</s> <s>the advantages of our method are two-fold.</s> <s>(i) the and-or graph model enables us to handle well large intra-class variance and background clutters for object shape detection from images.</s> <s>(ii) the proposed learning algorithm is able to obtain the and-or graph representation without requiring elaborate supervision and initialization.</s> <s>we validate the proposed method on several challenging databases (e.g., inria-horse, ethz-shape, and uiuc-people), and it outperforms the state-of-the-arts approaches.</s></p></d>", "label": ["<d><p><s>dynamical and-or graph learning for object shape modeling and detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>identifying patterns from the neuroimaging recordings of brain activity  related to the unobservable psychological or mental state of an individual can be treated as a unsupervised pattern recognition problem.</s> <s>the main challenges, however, for such an analysis of fmri data are: a) defining a physiologically meaningful feature-space for representing the spatial patterns across time; b) dealing with the high-dimensionality of the data; and c) robustness to the various artifacts and confounds in the fmri time-series.</s> <s>in this paper, we present a network-aware feature-space to represent the states of a general network, that enables  comparing and clustering such states in a manner that is a) meaningful in terms of the network connectivity structure; b)computationally efficient; c) low-dimensional; and d) relatively robust to structured and random noise artifacts.</s> <s>this feature-space is obtained from a spherical relaxation of the transportation distance metric which measures the cost of transporting ``mass'' over the network to transform one function into another.</s> <s>through theoretical and empirical assessments, we demonstrate the accuracy and efficiency of the approximation, especially for large problems.</s> <s>while the application presented here is for identifying distinct brain activity patterns from fmri, this feature-space can be applied to the problem of identifying recurring patterns and detecting outliers in measurements on many different types of networks, including sensor, control and social networks.</s></p></d>", "label": ["<d><p><s>identification of recurrent patterns in the activation of brain networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>probabilistic graphical models are powerful tools for analyzing constrained, continuous domains.</s> <s>however, finding most-probable explanations (mpes) in these models can be computationally expensive.</s> <s>in this paper, we improve the scalability of mpe inference in a class of graphical models with piecewise-linear and piecewise-quadratic dependencies and linear constraints over continuous domains.</s> <s>we derive algorithms based on a consensus-optimization framework and demonstrate their superior performance over state of the art.</s> <s>we show empirically that in a large-scale voter-preference modeling problem our algorithms scale linearly in the number of dependencies and constraints.</s></p></d>", "label": ["<d><p><s>scaling mpe inference for constrained continuous markov random fields with consensus optimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the shape boltzmann machine (sbm) has recently been introduced as a state-of-the-art model of foreground/background object shape.</s> <s>we extend the sbm to account for the foreground object's parts.</s> <s>our model, the multinomial sbm (msbm), can capture both local and global statistics of part shapes accurately.</s> <s>we combine the msbm with an appearance model to form a fully generative model of images of objects.</s> <s>parts-based image segmentations are obtained simply by performing probabilistic inference in the model.</s> <s>we apply the model to two challenging datasets which exhibit significant shape and appearance variability, and find that it obtains results that are comparable to the state-of-the-art.</s></p></d>", "label": ["<d><p><s>a generative model for parts-based object segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the usability of brain computer interfaces (bci) based on the p300 speller is severely hindered by the need for long training times and many repetitions of the same stimulus.</s> <s>in this contribution we introduce a set of unsupervised hierarchical probabilistic models that tackle both problems simultaneously by incorporating prior knowledge from two sources: information from other training subjects (through transfer learning) and information about the words being spelled (through language models).</s> <s>we show, that due to this prior knowledge, the performance of the unsupervised models parallels and in some cases even surpasses that of supervised models, while eliminating the tedious training session.</s></p></d>", "label": ["<d><p><s>a p300 bci for the masses: prior information enables instant unsupervised spelling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>time delay is pervasive in neural information processing.</s> <s>to achieve real-time tracking, it is critical to compensate the transmission and processing delays in a neural system.</s> <s>in the present study we show that dynamical synapses with short-term depression can enhance the mobility of a continuous attractor network to the extent that the system tracks time-varying stimuli in a timely manner.</s> <s>the state of the network can either track the instantaneous position of a moving stimulus perfectly (with zero-lag) or lead it with an effectively constant time, in agreement with experiments on the head-direction systems in rodents.</s> <s>the parameter regions for delayed, perfect and anticipative tracking correspond to network states that are static, ready-to-move and spontaneously moving, respectively, demonstrating the strong correlation between tracking performance and the intrinsic dynamics of the network.</s> <s>we also find that when the speed of the stimulus coincides with the natural speed of the network state, the delay becomes effectively independent of the stimulus amplitude.</s></p></d>", "label": ["<d><p><s>delay compensation with dynamical synapses</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of estimating a manifold from random samples.</s> <s>in particular, we consider piecewise constant and piecewise linear estimators induced by  k-means and k-?ats, and analyze their performance.</s> <s>we extend previous results  for k-means in two separate directions.</s> <s>first, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-?ats), for which no known results were previously available.</s> <s>while the results for k-means are novel, some of the technical tools are well-established in the literature.</s> <s>in the case of k-?ats, both the results and the mathematical tools are  new.</s></p></d>", "label": ["<d><p><s>learning manifolds with k-means and k-flats</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>while minwise hashing is promising for large-scale learning in massive binary data, the preprocessing cost is prohibitive as it requires applying (e.g.,) $k=500$ permutations on the data.</s> <s>the testing time is also  expensive if a new data point (e.g., a new document or a new image) has not been processed.</s> <s>in this paper, we develop a simple \\textbf{one permutation hashing} scheme to address this important issue.</s> <s>while it is true that the preprocessing step can be parallelized, it comes at the cost of additional hardware and implementation.</s> <s>also, reducing $k$ permutations to just one  would be much more \\textbf{energy-efficient}, which might be an important perspective as minwise hashing is commonly deployed in the search industry.</s> <s>while the theoretical probability analysis is  interesting, our experiments on similarity estimation and   svm \\& logistic regression also confirm the theoretical results.</s></p></d>", "label": ["<d><p><s>one permutation hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we derive a novel algorithm to cluster  hidden markov models (hmms) according to their probability distributions.</s> <s>we propose a variational hierarchical em algorithm that i) clusters a given collection of hmms into groups of hmms that are similar, in terms of the distributions they represent, and ii) characterizes each group by a ``cluster center'', i.e., a novel hmm that is representative for the group.</s> <s>we illustrate the benefits of the proposed algorithm on hierarchical clustering of motion capture sequences as well as on automatic music tagging.</s></p></d>", "label": ["<d><p><s>the variational hierarchical em algorithm for clustering hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a probabilistic generative model for representing acoustic event structure at multiple scales via a two-stage hierarchy.</s> <s>the first stage consists of a spiking representation which encodes a sound with a sparse set of kernels at different frequencies positioned precisely in time.</s> <s>the coarse time and frequency statistical structure of the first-stage spikes is encoded by a second stage spiking representation, while fine-scale statistical regularities are encoded by recurrent interactions within the first-stage.</s> <s>when fitted to speech data, the model encodes acoustic features such as harmonic stacks, sweeps, and frequency modulations, that can be composed to represent complex acoustic events.</s> <s>the model is also able to synthesize sounds from the higher-level representation and provides significant improvement over wavelet thresholding techniques on a denoising task.</s></p></d>", "label": ["<d><p><s>hierarchical spike coding of sound</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a large-volume box classification for binary   prediction, which maintains a subset of weight vectors, and   specifically axis-aligned boxes.</s> <s>our learning algorithm seeks for a   box of large volume that contains ``simple'' weight vectors which   most of are accurate on the training set.</s> <s>two versions of the   learning process are cast as convex optimization problems, and it   is shown how to solve them efficiently.</s> <s>the formulation yields a   natural pac-bayesian performance bound and it is shown to minimize a   quantity directly aligned with it.</s> <s>the algorithm outperforms svm and   the recently proposed arow algorithm on a majority of $30$ nlp   datasets and binarized usps optical character recognition datasets.</s></p></d>", "label": ["<d><p><s>volume regularization for binary classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a bayesian nonparametric model for genetic sequence data in which a set of genetic sequences is modelled using a markov model of  partitions.</s> <s>the partitions at consecutive locations in the genome are related by their clusters first splitting and then merging.</s> <s>our model can be thought of as a discrete time analogue of continuous time fragmentation-coagulation processes [teh et al 2011], preserving the important properties of projectivity, exchangeability and reversibility, while being more scalable.</s> <s>we apply this model to the problem of genotype imputation, showing improved computational efficiency while maintaining the same accuracies as in [teh et al 2011].</s></p></d>", "label": ["<d><p><s>scalable imputation of genetic data with a discrete fragmentation-coagulation process</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this work we study how the stimulus distribution influences the optimal coding of an individual neuron.</s> <s>closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying poisson statistics under a given stimulus distribution.</s> <s>we consider a variety of optimality criteria, including maximizing discriminability, maximizing mutual information and minimizing estimation error under a general $l_p$ norm.</s> <s>we generalize the cramer-rao lower bound and show how the $l_p$ loss can be written as a functional of the fisher information in the asymptotic limit, by proving the moment convergence of certain functions of poisson random variables.</s> <s>in this manner, we show how the optimal tuning curve depends upon the loss function, and the equivalence of maximizing mutual information with minimizing $l_p$ loss in the limit as $p$ goes to zero.</s></p></d>", "label": ["<d><p><s>optimal neural tuning curves for arbitrary stimulus distributions: discrimax, infomax and minimum </s></p></d>"], "set": "test"},
  {"data": "<d><p><s>multi-dimensional latent variable models can capture the many latent factors in a text corpus, such as topic, author perspective and sentiment.</s> <s>we introduce factorial lda, a multi-dimensional latent variable model in which a document is influenced by k different factors, and each word token depends on a k-dimensional vector of latent variables.</s> <s>our model incorporates structured word priors and learns a sparse product of factors.</s> <s>experiments on research abstracts show that our model can learn latent factors such as research topic, scientific discipline, and focus (e.g.</s> <s>methods vs.</s> <s>applications.)</s> <s>our modeling improvements reduce test perplexity and improve human interpretability of the discovered factors.</s></p></d>", "label": ["<d><p><s>factorial lda: sparse multi-dimensional text models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study large-scale, nonsmooth, nonconconvex optimization problems.</s> <s>in particular, we focus on nonconvex problems with \\emph{composite} objectives.</s> <s>this class of problems includes the extensively studied convex, composite objective problems as a special case.</s> <s>to tackle composite nonconvex problems, we introduce a powerful new framework based on asymptotically \\emph{nonvanishing} errors, avoiding the common convenient assumption of eventually vanishing errors.</s> <s>within our framework we derive both batch and incremental nonconvex proximal splitting algorithms.</s> <s>to our knowledge, our framework is first to develop and analyze incremental \\emph{nonconvex} proximal-splitting algorithms, even if we disregard the ability to handle nonvanishing errors.</s> <s>we illustrate our theoretical framework by showing how it applies to difficult large-scale, nonsmooth, and nonconvex problems.</s></p></d>", "label": ["<d><p><s>scalable nonconvex inexact proximal splitting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new prior for use in nonparametric bayesian hierarchical clustering.</s> <s>the prior is constructed by marginalizing out the time information of kingman?s coalescent, providing a prior over tree structures which we call the time-marginalized coalescent (tmc).</s> <s>this allows for models which factorize the tree structure and times, providing two benefits: more flexible priors may be constructed and more efficient gibbs type inference can be used.</s> <s>we demonstrate this on an example model for density estimation and show the tmc achieves competitive experimental results.</s></p></d>", "label": ["<d><p><s>the time-marginalized coalescent prior for hierarchical clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>finding contours in natural images is a fundamental problem that serves as the basis of many tasks such as image segmentation and object recognition.</s> <s>at the core of contour detection technologies are a set of hand-designed gradient features, used by most existing approaches including the state-of-the-art global pb (gpb) operator.</s> <s>in this work, we show that contour detection accuracy can be significantly improved by computing sparse code gradients (scg), which measure contrast using patch representations automatically learned through sparse coding.</s> <s>we use k-svd and orthogonal matching pursuit for efficient dictionary learning and encoding, and use multi-scale pooling and power transforms to code oriented local neighborhoods before computing gradients and applying linear svm.</s> <s>by extracting rich representations from pixels and avoiding collapsing them prematurely, sparse code gradients effectively learn how to measure local contrasts and find contours.</s> <s>we improve the f-measure metric on the bsds500 benchmark to 0.74 (up from 0.71 of gpb contours).</s> <s>moreover, our learning approach can easily adapt to novel sensor data such as kinect-style rgb-d cameras: sparse code gradients on depth images and surface normals lead to promising contour detection using depth and depth+color, as verified on the nyu depth dataset.</s> <s>our work combines the concept of oriented gradients with sparse representation and opens up future possibilities for learning contour detection and segmentation.</s></p></d>", "label": ["<d><p><s>discriminatively trained sparse code gradients for contour detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new methodology is developed for joint analysis of a matrix and accompanying documents, with the documents associated with the matrix rows/columns.</s> <s>the documents are modeled with a focused topic model, inferring latent binary features (topics) for each document.</s> <s>a new matrix decomposition is developed, with latent binary features associated with the rows/columns, and with imposition of a low-rank constraint.</s> <s>the matrix decomposition and topic model are coupled by sharing the latent binary feature vectors associated with each.</s> <s>the model is applied to roll-call data, with the associated documents defined by the legislation.</s> <s>state-of-the-art results are manifested for prediction of votes on a new piece of legislation, based only on the observed text legislation.</s> <s>the coupling of the text and legislation is also demonstrated to yield insight into the properties of the matrix decomposition for roll-call data.</s></p></d>", "label": ["<d><p><s>joint modeling of a matrix with associated text via latent binary features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper discusses the problem of calibrating posterior class probabilities from partially labelled data.</s> <s>each instance is assumed to be labelled as belonging to one of several candidate categories, at most one of them being true.</s> <s>we generalize the concept of proper loss to this scenario, establish a necessary and sufficient condition for a loss function to be proper, and we show a direct procedure to construct a proper loss for partial labels from a conventional proper loss.</s> <s>the problem can be characterized by the mixing probability matrix relating the true class of the data and the observed labels.</s> <s>an interesting result is that the full knowledge of this matrix is not required, and losses can be constructed that are proper in a subset of the probability simplex.</s></p></d>", "label": ["<d><p><s>proper losses for learning from partial labels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>mixture distributions are often used to model complex data.</s> <s>in this paper, we develop a new method that jointly estimates mixture models over multiple data sets by exploiting the statistical dependencies between them.</s> <s>specifically, we introduce a set of latent dirichlet processes as sources of component models (atoms), and for each data set, we construct a nonparametric mixture model by combining sub-sampled versions of the latent dps.</s> <s>each mixture model may acquire atoms from different latent dps, while each atom may be shared by multiple mixtures.</s> <s>this multi-to-multi association distinguishes the proposed method from prior constructions that rely on tree or chain structures, allowing mixture models to be coupled more flexibly.</s> <s>in addition, we derive a sampling algorithm that jointly infers the model parameters and present experiments on both document analysis and image modeling.</s></p></d>", "label": ["<d><p><s>coupling nonparametric mixtures via latent dirichlet processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications classification systems often require in the loop human intervention.</s> <s>in such cases the decision process must be transparent and comprehensible simultaneously requiring minimal assumptions on the underlying data distribution.</s> <s>to tackle this problem, we formulate it as an axis-alligned subspacefinding task under the assumption that query specific information dictates the complementary use of the subspaces.</s> <s>we develop a regression-based approach called recip that efficiently solves this problem by finding projections that minimize a nonparametric conditional entropy estimator.</s> <s>experiments show that the method is accurate in identifying the informative projections of the dataset, picking the correct ones to classify query points and facilitates visual evaluation by users.</s></p></d>", "label": ["<d><p><s>projection retrieval for classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a weighted graph is used as an underlying structure of many algorithms like semi-supervised learning and spectral clustering.</s> <s>the edge weights are usually deter-mined by a single similarity measure, but it often hard if not impossible to capture all relevant aspects of similarity when using a single similarity measure.</s> <s>in par-ticular, in the case of visual object matching it is beneficial to integrate different similarity measures that focus on different visual representations.</s> <s>in this paper, a novel approach to integrate multiple similarity measures is pro-posed.</s> <s>first pairs of similarity measures are combined with a diffusion process on their tensor product graph (tpg).</s> <s>hence the diffused similarity of each pair of ob-jects becomes a function of joint diffusion of the two original similarities, which in turn depends on the neighborhood structure of the tpg.</s> <s>we call this process fusion with diffusion (fd).</s> <s>however, a higher order graph like the tpg usually means significant increase in time complexity.</s> <s>this is not the case in the proposed approach.</s> <s>a key feature of our approach is that the time complexity of the dif-fusion on the tpg is the same as the diffusion process on each of the original graphs, moreover, it is not necessary to explicitly construct the tpg in our frame-work.</s> <s>finally all diffused pairs of similarity measures are combined as a weighted sum.</s> <s>we demonstrate the advantages of the proposed approach on the task of visual tracking, where different aspects of the appearance similarity between the target object in frame t and target object candidates in frame t+1 are integrated.</s> <s>the obtained method is tested on several challenge video sequences and the experimental results show that it outperforms state-of-the-art tracking methods.</s></p></d>", "label": ["<d><p><s>fusion with diffusion for robust visual tracking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hashing-based methods provide a very promising approach to large-scale similarity search.</s> <s>to obtain compact hash codes, a recent trend seeks to learn the hash functions from data automatically.</s> <s>in this paper, we study hash function learning in the context of multimodal data.</s> <s>we propose a novel multimodal hash function learning method, called co-regularized hashing (crh), based on a boosted co-regularization framework.</s> <s>the hash functions for each bit of the hash codes are learned by solving dc (difference of convex functions) programs, while the learning for multiple bits proceeds via a boosting procedure so that the bias introduced by the hash functions can be sequentially minimized.</s> <s>we empirically compare crh with two state-of-the-art multimodal hash function learning methods on two publicly available data sets.</s></p></d>", "label": ["<d><p><s>co-regularized hashing for multimodal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>structured output learning has been successfully applied to object localization, where the mapping between an image and an object bounding box can be well captured.</s> <s>its extension to action localization in videos, however, is much more challenging, because one needs to predict the locations of the action patterns both spatially and temporally, i.e., identifying a sequence of bounding boxes that track the action in video.</s> <s>the problem becomes intractable due to the exponentially large size of the structured video space where actions could occur.</s> <s>we propose a novel structured learning approach for spatio-temporal action localization.</s> <s>the mapping between a video and a spatio-temporal action trajectory is learned.</s> <s>the intractable inference and learning problems are addressed by leveraging an efficient max-path search method, thus makes it feasible to optimize the model over the whole structured space.</s> <s>experiments on two challenging benchmark datasets show that our proposed method outperforms the state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>max-margin structured output regression for spatio-temporal action localization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning the number of clusters is a key problem in data clustering.</s> <s>we present dip-means, a novel robust incremental method to learn the number of data clusters that may be used as a wrapper around any iterative clustering algorithm of the k-means family.</s> <s>in contrast to many popular methods which make assumptions about the underlying cluster distributions, dip-means only assumes a fundamental cluster property: each cluster to admit a unimodal distribution.</s> <s>the proposed algorithm considers each cluster member as a ''viewer'' and applies a univariate statistic hypothesis test for unimodality (dip-test) on the distribution of the distances between the viewer and the cluster members.</s> <s>two important advantages are: i) the unimodality test is applied on univariate distance vectors, ii) it can be directly applied with kernel-based methods, since only the pairwise distances are involved in the computations.</s> <s>experimental results on artificial and real datasets indicate the effectiveness of our method and its superiority over analogous approaches.</s></p></d>", "label": ["<d><p><s>dip-means: an incremental clustering method for estimating the number of clusters</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>hypothesis testing on signals de?ned on surfaces (such as the cortical surface) is a fundamental component of a variety of studies in neuroscience.</s> <s>the goal here is to identify regions that exhibit changes as a function of the clinical condition under study.</s> <s>as the clinical questions of interest move towards identifying very early signs of diseases, the corresponding statistical differences at the group level invariably become weaker and increasingly hard to identify.</s> <s>indeed, after a multiple comparisons correction is adopted (to account for correlated statistical tests over all surface points), very few regions may survive.</s> <s>in contrast to hypothesis tests on point-wise measurements, in this paper, we make the case for performing statistical analysis on multi-scale shape descriptors that characterize the local topological context of the signal around each surface vertex.</s> <s>our descriptors are based on recent results from harmonic analysis, that show how wavelet theory extends to non-euclidean settings (i.e., irregular weighted graphs).</s> <s>we provide strong evidence that these descriptors successfully pick up group-wise differences, where traditional methods either fail or yield unsatisfactory results.</s> <s>other than this primary application, we show how the framework allows performing cortical surface smoothing in the native space without mappint to a unit sphere.</s></p></d>", "label": ["<d><p><s>wavelet based multi-scale shape features on arbitrary surfaces for cortical thickness discrimination</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although many variants of stochastic gradient descent have been proposed for large-scale convex optimization, most of them require projecting the solution at {\\it each} iteration to ensure that the obtained solution stays within the feasible domain.</s> <s>for complex domains (e.g., positive semidefinite cone), the projection step can be computationally expensive, making stochastic gradient descent unattractive for large-scale optimization problems.</s> <s>we address this limitation by developing a novel stochastic gradient descent algorithm that does not need intermediate projections.</s> <s>instead, only one projection at the last iteration is needed to obtain a feasible solution in the given domain.</s> <s>our theoretical analysis shows that with a high probability, the proposed algorithms achieve an $o(1/\\sqrt{t})$ convergence rate for general convex optimization, and an $o(\\ln t/t)$  rate for  strongly convex optimization under mild conditions about the domain and the objective function.</s></p></d>", "label": ["<d><p><s>stochastic gradient descent with only one projection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the problem of estimation of entropy functionals of probability densities has received much attention in the information theory, machine learning and statistics communities.</s> <s>kernel density plug-in estimators are simple, easy to implement and widely used for estimation of entropy.</s> <s>however, kernel plug-in estimators suffer from the curse of dimensionality, wherein the mse rate of convergence is glacially slow - of order  $o(t^{-{\\gamma}/{d}})$, where $t$ is the number of samples, and $\\gamma>0$ is a rate parameter.</s> <s>in this paper, it is shown that for sufficiently smooth densities, an ensemble of kernel plug-in estimators can be combined via a weighted convex combination, such that the resulting weighted estimator has a superior parametric mse rate of convergence of order $o(t^{-1})$.</s> <s>furthermore, it is shown that these optimal weights can be determined by solving a convex optimization problem which does not require training data or knowledge of the underlying density, and therefore can be performed offline.</s> <s>this novel result is remarkable in that, while each of the individual kernel plug-in estimators belonging to the ensemble suffer from the curse of dimensionality, by appropriate ensemble averaging we can achieve parametric convergence rates.</s></p></d>", "label": ["<d><p><s>ensemble weighted kernel estimators for multivariate entropy estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>category-level object detection has a crucial need for informative object representations.</s> <s>this demand has led to feature descriptors of ever increasing dimensionality  like co-occurrence statistics and self-similarity.</s> <s>in this paper we propose a new object representation based on curvature self-similarity that goes beyond the currently popular  approximation of objects using straight lines.</s> <s>however, like all descriptors using second order statistics, ours also exhibits a high dimensionality.</s> <s>although improving discriminability,  the high dimensionality becomes a critical issue due to lack of generalization ability and curse of dimensionality.</s> <s>given only a limited amount of training data, even sophisticated  learning algorithms such as the popular kernel methods are not able to suppress noisy or superfluous dimensions of such high-dimensional data.</s> <s>consequently, there is a natural need for  feature selection when using present-day informative features and, particularly, curvature self-similarity.</s> <s>we therefore suggest an embedded feature selection method for svms that  reduces complexity and improves generalization capability of object models.</s> <s>by successfully integrating the proposed curvature self-similarity representation together with the embedded  feature selection in a widely used state-of-the-art object detection framework we show the general pertinence of the approach.</s></p></d>", "label": ["<d><p><s>visual recognition using embedded feature selection for curvature self-similarity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider infinite-horizon stationary $\\gamma$-discounted markov   decision processes, for which it is known that there exists a   stationary optimal policy.</s> <s>using value and policy iteration with   some error $\\epsilon$ at each iteration, it is well-known that one   can compute stationary policies that are $\\frac{2\\gamma{(1-\\gamma)^2}\\epsilon$-optimal.</s> <s>after arguing that this   guarantee is tight, we develop variations of value and policy   iteration for computing non-stationary policies that can be up to   $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant   improvement in the usual situation when $\\gamma$ is close to   $1$.</s> <s>surprisingly, this shows that the problem of ``computing near-optimal non-stationary policies'' is much simpler than that   of ``computing near-optimal stationary policies''.</s></p></d>", "label": ["<d><p><s>on the use of non-stationary policies for stationary infinite-horizon markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many graph-based machine learning and data mining approaches, the quality of the graph is critical.</s> <s>however, in real-world applications, especially in semi-supervised learning and unsupervised learning, the evaluation of the quality of a graph is often expensive and sometimes even impossible, due the cost or the unavailability of ground truth.</s> <s>in this paper, we proposed a robust approach with convex optimization to ``forge'' a graph: with an input of a graph, to learn a graph with higher quality.</s> <s>our major concern is that an ideal graph shall satisfy all the following constraints: non-negative, symmetric, low rank, and positive semidefinite.</s> <s>we develop a graph learning algorithm by solving a convex optimization problem and further develop an efficient optimization to obtain global optimal solutions with theoretical guarantees.</s> <s>with only one non-sensitive parameter, our method is shown by experimental results to be robust and achieve higher accuracy in semi-supervised learning and clustering under various settings.</s> <s>as a preprocessing of graphs, our method has a wide range of potential applications machine learning and data mining.</s></p></d>", "label": ["<d><p><s>forging the graphs: a low rank and positive semidefinite graph learning approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new framework based on the theory of copulas is proposed to address semi-supervised domain adaptation problems.</s> <s>the presented method factorizes any multivariate density into a product of marginal distributions and bivariate copula functions.</s> <s>therefore, changes in each of these factors can be detected and corrected to adapt a density model across different learning domains.</s> <s>importantly, we introduce a novel vine copula model, which allows for this factorization in a non-parametric manner.</s> <s>experimental results on regression problems with real-world data illustrate the efficacy of the proposed approach when compared to state-of-the-art techniques.</s></p></d>", "label": ["<d><p><s>semi-supervised domain adaptation with non-parametric copulas</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider bayesian reinforcement learning (brl) where actions incur costs in addition to rewards, and thus exploration has to be constrained in terms of the expected total cost while learning to maximize the expected long-term total reward.</s> <s>in order to formalize cost-sensitive exploration, we use the constrained markov decision process (cmdp) as the model of the environment, in which we can naturally encode exploration requirements using the cost function.</s> <s>we extend beetle, a model-based brl method, for learning in the environment with cost constraints.</s> <s>we demonstrate the cost-sensitive exploration behaviour in a number of simulated problems.</s></p></d>", "label": ["<d><p><s>cost-sensitive exploration in bayesian reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how does the brain combine prior knowledge with sensory evidence when making decisions under uncertainty?</s> <s>two competing descriptive models have been proposed based on experimental data.</s> <s>the first posits an additive offset to a decision variable, implying a static effect of the prior.</s> <s>however, this model is inconsistent with recent data from a motion discrimination task involving temporal integration of uncertain sensory evidence.</s> <s>to explain this data, a second model has been proposed which assumes a time-varying influence of the prior.</s> <s>here we present a normative model of decision making that incorporates prior knowledge in a principled way.</s> <s>we show that the additive offset model and the time-varying prior model emerge naturally when decision making is viewed within the framework of partially observable markov decision processes (pomdps).</s> <s>decision making in the model reduces to (1) computing beliefs given observations and prior information in a bayesian manner, and (2) selecting actions based on these beliefs to maximize the  expected sum of future rewards.</s> <s>we show that the model can explain both  data previously explained using the additive offset model as well as more  recent data on the time-varying influence of prior knowledge on decision making.</s></p></d>", "label": ["<d><p><s>how prior probability influences decision making: a unifying probabilistic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning control policies via trajectory preference queries to an expert.</s> <s>in particular, the learning agent can present an expert with short runs of a pair of policies originating from the same state and the expert then indicates the preferred trajectory.</s> <s>the agent's goal is to elicit a latent target policy from the expert with as few queries as possible.</s> <s>to tackle this problem we propose a novel bayesian model of the querying process and introduce two methods that exploit this model to actively select expert queries.</s> <s>experimental results on four benchmark problems indicate that our model can effectively learn policies from trajectory preference queries and that active query selection can be substantially more efficient than random selection.</s></p></d>", "label": ["<d><p><s>a bayesian approach for policy learning from trajectory preference queries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new discrepancy score between two distributions that gives an indication on their \\emph{similarity}.</s> <s>while much research has been done to determine if two samples come from exactly the same distribution, much less research considered the problem of determining if two finite samples come from similar distributions.</s> <s>the new score gives an intuitive interpretation of similarity;  it optimally perturbs the distributions so that they best fit each other.</s> <s>the score is defined between distributions, and can be efficiently estimated from samples.</s> <s>we provide convergence bounds of the estimated score, and develop hypothesis testing procedures that test if two data sets come from similar distributions.</s> <s>the statistical power of this procedures is presented in simulations.</s> <s>we also compare the score's capacity to detect similarity with that of other known measures on real data.</s></p></d>", "label": ["<d><p><s>the perturbed variation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-task learning (mtl) aims to improve generalization performance by learning multiple related tasks simultaneously and identifying the shared information among tasks.</s> <s>most of existing mtl methods focus on learning linear models under the supervised setting.</s> <s>we propose a novel semi-supervised and nonlinear approach for mtl using vector fields.</s> <s>a vector field is a smooth mapping from the manifold to the tangent spaces which can be viewed as a directional derivative of functions on the manifold.</s> <s>we argue that vector fields provide a natural way to exploit the geometric structure of data as well as the shared differential structure of tasks, both are crucial for semi-supervised multi-task learning.</s> <s>in this paper, we develop multi-task vector field learning (mtvfl) which learns the prediction functions and the vector fields simultaneously.</s> <s>mtvfl has the following key properties: (1) the vector fields we learned are close to the gradient fields of the prediction functions; (2) within each task, the vector field is required to be as parallel as possible which is expected to span a low dimensional subspace; (3) the vector fields from all tasks share a low dimensional subspace.</s> <s>we formalize our idea in a regularization framework and also provide a convex relaxation method to solve the original non-convex problem.</s> <s>the experimental results on synthetic and real data demonstrate the effectiveness of our proposed approach.</s></p></d>", "label": ["<d><p><s>multi-task vector field learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity.</s> <s>binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear knn search.</s> <s>the framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss.</s> <s>we overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural svms.</s> <s>we develop a new loss-augmented inference algorithm that is quadratic in the code length.</s> <s>we show strong retrieval performance on cifar-10 and mnist, with promising classification results using no more than knn on the binary codes.</s></p></d>", "label": ["<d><p><s>hamming distance metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several machine learning methods allow for abstaining from uncertain predictions.</s> <s>while being common for settings like conventional classification, abstention has been studied much less in learning to rank.</s> <s>we address abstention for the label ranking setting, allowing the learner to declare certain pairs of labels as being incomparable and, thus, to predict partial instead of total orders.</s> <s>in our method, such predictions are produced via thresholding the probabilities of pairwise preferences between labels, as induced by a predicted probability distribution on the set of all rankings.</s> <s>we formally analyze this approach for the mallows and the plackett-luce model, showing that it produces proper partial orders as predictions and characterizing the expressiveness of the induced class of partial orders.</s> <s>these theoretical results are complemented by experiments demonstrating the practical usefulness of the approach.</s></p></d>", "label": ["<d><p><s>label ranking with partial abstention based on thresholded probabilistic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-agent plan recognition (mapr) aims to recognize dynamic team structures and team behaviors from the observed team traces (activity sequences) of a set of intelligent agents.</s> <s>previous mapr approaches required a library of team activity sequences (team plans) be given as input.</s> <s>however, collecting a library of team plans to ensure adequate coverage is often difficult and costly.</s> <s>in this paper, we relax this constraint, so that team plans are not required to be provided beforehand.</s> <s>we assume instead that a set of action models are available.</s> <s>such models are often already created to describe domain physics; i.e., the  preconditions and effects of effects actions.</s> <s>we propose a novel approach for recognizing multi-agent team plans based on such action models rather than libraries of team plans.</s> <s>we encode the resulting mapr problem as a \\emph{satisfiability problem} and solve the problem using a state-of-the-art weighted max-sat solver.</s> <s>our approach also allows for incompleteness in the observed plan traces.</s> <s>our empirical studies demonstrate that our algorithm is both effective and efficient in comparison to state-of-the-art mapr methods based on plan libraries.</s></p></d>", "label": ["<d><p><s>action-model based multi-agent plan recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a key function of brains is undoubtedly the abstraction and maintenance of information from the environment for later use.</s> <s>neurons in association cortex play an important role in this process: during learning these neurons become tuned to relevant features and represent the information that is required later as a persistent elevation of their activity.</s> <s>it is however not well known how these neurons acquire their task-relevant tuning.</s> <s>here we introduce a biologically plausible learning scheme that explains how neurons become selective for relevant information when animals learn by trial and error.</s> <s>we propose that the action selection stage feeds back attentional signals to earlier processing levels.</s> <s>these feedback signals interact with feedforward signals to form synaptic tags at those connections that are responsible for the stimulus-response mapping.</s> <s>a globally released neuromodulatory signal interacts with these tagged synapses to determine the sign and strength of plasticity.</s> <s>the learning scheme is generic because it can train networks in different tasks, simply by varying inputs and rewards.</s> <s>it explains how neurons in association cortex learn to (1) temporarily store task-relevant information in non-linear stimulus-response mapping tasks and (2) learn to optimally integrate probabilistic evidence for perceptual decision making.</s></p></d>", "label": ["<d><p><s>neurally plausible reinforcement learning of working memory tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a dynamic nonlinear generative model for visual motion based on a latent representation of binary-gated gaussian variables.</s> <s>trained on sequences of images, the model learns to represent different movement directions in different variables.</s> <s>we use an online approximate-inference scheme that can be mapped to the dynamics of networks of neurons.</s> <s>probed with drifting grating stimuli and moving bars of light, neurons in the model show patterns of responses analogous to those of direction-selective simple cells in primary visual cortex.</s> <s>most model neurons also show speed tuning and respond equally well to a range of motion directions and speeds aligned to the constraint line of their respective preferred speed.</s> <s>we show how these computations are enabled by a specific pattern of recurrent connections learned by the model.</s></p></d>", "label": ["<d><p><s>learning visual motion in recurrent neural networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>reinforcement learning (rl) methods based on direct policy search (dps) have been actively discussed to achieve an efficient approach to complicated markov decision processes (mdps).</s> <s>although they have brought much progress in practical applications of rl, there still remains an unsolved problem in dps related to model  selection for the policy.</s> <s>in this paper, we propose a novel dps method, {\\it  weighted likelihood policy search (wlps)}, where a policy is efficiently learned through the weighted likelihood estimation.</s> <s>wlps naturally connects dps to the statistical inference problem and thus various sophisticated techniques in statistics can be applied to dps problems directly.</s> <s>hence, by following the idea of the {\\it information criterion}, we develop a new measurement for model comparison in dps based on the weighted log-likelihood.</s></p></d>", "label": ["<d><p><s>weighted likelihood policy search with model selection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>probabilistic planning captures the uncertainty of plan execution by probabilistically modeling the effects of actions in the environment, and therefore the probability of reaching different states from a given state and action.</s> <s>in order to compute a solution for a probabilistic planning problem, planners need to manage the uncertainty associated with the different paths from the initial state to a goal state.</s> <s>several approaches to manage uncertainty were proposed, e.g., consider all paths at once, perform determinization of actions, and sampling.</s> <s>in this paper, we introduce trajectory-based short-sighted stochastic shortest path problems (ssps), a novel approach to manage uncertainty for probabilistic planning problems in which states reachable with low probability are substituted by artificial goals that heuristically estimate their cost to reach a goal state.</s> <s>we also extend the theoretical results of short-sighted probabilistic planner (ssipp) [ref] by proving that ssipp always finishes and is asymptotically optimal under sufficient conditions on the structure of short-sighted ssps.</s> <s>we empirically compare ssipp using trajectory-based short-sighted ssps with the winners of the previous probabilistic planning competitions and other state-of-the-art planners in the triangle tireworld problems.</s> <s>trajectory-based ssipp outperforms all the competitors and is the only planner able to scale up to problem number 60, a problem in which the optimal solution contains approximately $10^{70}$ states.</s></p></d>", "label": ["<d><p><s>trajectory-based short-sighted probabilistic planning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the application of the maximum entropy principle to sequence   modeling has been popularized by methods such as conditional random   fields (crfs).</s> <s>however, these approaches are generally limited to   modeling paths in discrete spaces of low dimensionality.</s> <s>we   consider the problem of modeling distributions over paths in   continuous spaces of high dimensionality---a problem for which   inference is generally intractable.</s> <s>our main contribution is to   show that maximum entropy modeling of high-dimensional, continuous   paths is tractable as long as the constrained features    possess a certain kind of low dimensional structure.</s> <s>in this case, we show that the associated {\\em partition function} is   symmetric and that this symmetry can be exploited to compute the   partition function efficiently in a compressed form.</s> <s>empirical   results are given showing an application of our method to maximum   entropy modeling of high dimensional human motion capture data.</s></p></d>", "label": ["<d><p><s>efficient high dimensional maximum entropy modeling via symmetric partition functions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of learning local metrics for nearest neighbor classification.</s> <s>most previous works on local metric learning learn a number of local unrelated metrics.</s> <s>while this ''independence'' approach delivers an increased flexibility its downside is the considerable risk of overfitting.</s> <s>we present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold.</s> <s>using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space.</s> <s>we constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold.</s> <s>our metric learning method has excellent performance both in terms of predictive power and scalability.</s> <s>we experimented with several large-scale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to svm with automatic kernel selection, all of which it outperforms in a significant manner.</s></p></d>", "label": ["<d><p><s>parametric local metric learning for nearest neighbor classification</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>linear chains and trees are basic building blocks in many applications of graphical models.</s> <s>although exact inference in these models can be performed by dynamic programming, this computation can still be prohibitively expensive with non-trivial target variable domain sizes due to the quadratic dependence on this size.</s> <s>standard message-passing algorithms for these problems are inefficient because they compute scores on hypotheses for which there is strong negative local evidence.</s> <s>for this reason there has been significant previous interest in beam search and its variants; however, these methods provide only approximate inference.</s> <s>this paper presents new efficient exact inference algorithms based on the combination of it column generation and pre-computed bounds on the model's cost structure.</s> <s>improving worst-case performance is impossible.</s> <s>however, our method substantially speeds real-world, typical-case inference in chains and trees.</s> <s>experiments show our method to be twice as fast as exact viterbi for wall street journal part-of-speech tagging and over thirteen times faster for a joint part-of-speed and named-entity-recognition task.</s> <s>our algorithm is also extendable to new techniques for approximate inference, to faster two-best inference, and new opportunities for connections between inference and learning.</s></p></d>", "label": ["<d><p><s>map inference in chains using column generation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of actively learning \\textit{multi-index} functions of the form $f(\\vecx) = g(\\mata\\vecx)= \\sum_{i=1}^k g_i(\\veca_i^t\\vecx)$ from point evaluations of $f$.</s> <s>we assume that the function $f$ is defined on an $\\ell_2$-ball in $\\real^d$, $g$ is twice continuously differentiable almost everywhere, and $\\mata \\in \\mathbb{r}^{k \\times d}$ is a rank $k$ matrix, where $k \\ll d$.</s> <s>we propose a randomized, active sampling scheme for estimating such functions with uniform approximation guarantees.</s> <s>our theoretical developments leverage recent techniques from low rank matrix recovery, which enables us to derive an estimator of the function $f$ along with sample complexity bounds.</s> <s>we also characterize the noise robustness of the scheme, and provide empirical evidence that the high-dimensional scaling of our sample complexity bounds are quite accurate.</s></p></d>", "label": ["<d><p><s>active learning of multi-index function models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a novel multilabel/ranking algorithm working in partial information settings.</s> <s>the algorithm is based on 2nd-order descent methods, and relies on upper-confidence bounds to trade-off exploration and exploitation.</s> <s>we analyze this algorithm in a partial adversarial setting, where covariates can be adversarial, but multilabel probabilities are ruled by (generalized) linear models.</s> <s>we show $o(t^{1/2}\\log t)$ regret bounds, which improve in several ways on the existing results.</s> <s>we test the effectiveness of our upper-confidence scheme by contrasting against full-information baselines on real-world multilabel datasets, often obtaining comparable performance.</s></p></d>", "label": ["<d><p><s>on multilabel classification and ranking with partial feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices.</s> <s>we propose a calibrated spectrum elastic net method with a sum of the nuclear and frobenius penalties and develop an iterative algorithm to solve the convex minimization problem.</s> <s>the iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges.</s> <s>a calibration step follows to correct the bias caused by the frobenius penalty.</s> <s>under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level.</s> <s>this provides a unified analysis of the noisy and noiseless matrix completion problems.</s> <s>simulation results are presented to compare our proposal with previous ones.</s></p></d>", "label": ["<d><p><s>calibrated elastic regularization in matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the lsvrc-2010 imagenet training set into the 1000 different classes.</s> <s>on the test data, we achieved top-1 and top-5 error rates of 39.7\\% and 18.9\\% which is considerably better than the previous state-of-the-art results.</s> <s>the neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax.</s> <s>to make training faster, we used non-saturating neurons and a very efficient gpu implementation of convolutional nets.</s> <s>to reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.</s></p></d>", "label": ["<d><p><s>imagenet classification with deep convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a kernel-based discriminative learning framework on probability measures.</s> <s>rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data.</s> <s>by representing these probability distributions as mean embeddings in the reproducing kernel hilbert space (rkhs), we are able to apply many standard kernel-based learning techniques in straightforward fashion.</s> <s>to accomplish this, we construct a generalization of the support vector machine (svm) called a support measure machine (smm).</s> <s>our analyses of smms provides several insights into their relationship to traditional svms.</s> <s>based on such insights, we propose a flexible svm (flex-svm) that places different kernel functions on each training example.</s> <s>experimental results on both synthetic and real-world data demonstrate the effectiveness of our proposed framework.</s></p></d>", "label": ["<d><p><s>learning from distributions via support measure machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the national epidemiologic survey on alcohol and related conditions (nesarc) database contains a large amount of information, regarding the way of life, medical conditions, depression, etc., of a representative sample of the u.s. population.</s> <s>in the present paper, we are interested in seeking the hidden causes behind the suicide attempts, for which we propose to model the subjects using a nonparametric latent model based on the indian buffet process (ibp).</s> <s>due to the nature of the data, we need to adapt the observation model for discrete random variables.</s> <s>we propose a generative model in which the observations are drawn from a multinomial-logit distribution given the ibp matrix.</s> <s>the implementation of an efficient gibbs sampler is accomplished using the laplace approximation, which allows us to integrate out the weighting factors of the  multinomial-logit likelihood model.</s> <s>finally, the experiments over the nesarc database show that our model properly captures some of the hidden causes that model suicide attempts.</s></p></d>", "label": ["<d><p><s>bayesian nonparametric modeling of suicide attempts</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>although human object recognition is supposedly robust to viewpoint, much research on human perception indicates that there is a preferred or ?canonical?</s> <s>view of objects.</s> <s>this phenomenon was discovered more than 30 years ago but the canonical view of only a small number of categories has been validated experimentally.</s> <s>moreover, the explanation for why humans prefer the canonical view over other views remains elusive.</s> <s>in this paper we ask: can we use internet image collections to learn more about canonical views?</s> <s>we start by manually finding the most common view in the results returned by internet search engines when queried with the objects used in psychophysical experiments.</s> <s>our results clearly show that the most likely view in the search engine corresponds to the same view preferred by human subjects in experiments.</s> <s>we also present a simple method to find the most likely view in an image collection and apply it to hundreds of categories.</s> <s>using the new data we have collected we present strong evidence against the two most prominent formal theories of canonical views and provide novel constraints for new theories.</s></p></d>", "label": ["<d><p><s>learning about canonical views from internet image collections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the primary application of collaborate filtering (cf) is to recommend a small set of items to a user, which entails ranking.</s> <s>most approaches, however, formulate the cf problem as rating prediction, overlooking the ranking perspective.</s> <s>in this work we present a method for collaborative ranking that leverages the strengths of the two main cf approaches, neighborhood- and model-based.</s> <s>our novel method is highly efficient, with only seventeen parameters to optimize and a single hyperparameter to tune, and beats the state-of-the-art collaborative ranking methods.</s> <s>we also show that parameters learned on one dataset yield excellent results on a very different dataset, without any retraining.</s></p></d>", "label": ["<d><p><s>collaborative ranking with 17 parameters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the accurate prediction of molecular energetics in chemical compound space is a crucial ingredient for rational compound design.</s> <s>the inherently graph-like, non-vectorial nature of molecular data gives rise to a unique and difficult machine learning problem.</s> <s>in this paper, we adopt a learning-from-scratch approach where quantum-mechanical molecular energies are predicted directly from the raw molecular geometry.</s> <s>the study suggests a benefit from setting flexible priors and enforcing invariance stochastically rather than structurally.</s> <s>our results improve the state-of-the-art by a factor of almost three, bringing statistical methods one step closer to the holy grail of ''chemical accuracy''.</s></p></d>", "label": ["<d><p><s>learning invariant representations of molecules for atomization energy prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper focuses on the problem of learning binary embeddings for efficient retrieval of high-dimensional non-negative data.</s> <s>such data typically arises in a large number of vision and text applications where counts or frequencies are used as features.</s> <s>also, cosine distance is commonly used as a measure of dissimilarity between such vectors.</s> <s>in this work, we introduce a novel spherical quantization scheme to generate binary embedding of such data and analyze its properties.</s> <s>the number of quantization landmarks in this scheme grows exponentially with data dimensionality resulting in low-distortion quantization.</s> <s>we propose a very efficient method for computing the binary embedding using such large number of landmarks.</s> <s>further, a linear transformation is learned to minimize the quantization error by adapting the method to the input data resulting in improved embedding.</s> <s>experiments on image and text retrieval applications show superior performance of the proposed method over other existing state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>angular quantization-based binary codes for fast similarity search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new learning strategy based on an efficient blocked gibbs sampler for sparse overcomplete linear models.</s> <s>particular emphasis is placed on statistical image modeling, where overcomplete models have played an important role in discovering sparse representations.</s> <s>our gibbs sampler is faster than general purpose sampling schemes while also requiring no tuning as it is free of parameters.</s> <s>using the gibbs sampler and a persistent variant of expectation maximization, we are able to extract highly sparse distributions over latent sources from data.</s> <s>when applied to natural images, our algorithm learns source distributions which resemble spike-and-slab distributions.</s> <s>we evaluate the likelihood and quantitatively compare the performance of the overcomplete linear model to its complete counterpart as well as a product of experts model, which represents another overcomplete generalization of the complete linear model.</s> <s>in contrast to previous claims, we find that overcomplete representations lead to significant improvements, but that the overcomplete linear model still underperforms other models.</s></p></d>", "label": ["<d><p><s>training sparse natural image models with a fast gibbs sampler of an extended state space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel stochastic process that is with probability $\\alpha_i$ being absorbed at current state $i$, and with probability $1-\\alpha_i$ follows a random edge out of it.</s> <s>we analyze its properties and show its potential for exploring graph structures.</s> <s>we prove that under proper absorption rates, a random walk starting from a set $\\mathcal{s}$ of low conductance will be mostly absorbed in $\\mathcal{s}$.</s> <s>moreover, the absorption probabilities vary slowly inside $\\mathcal{s}$, while dropping sharply outside $\\mathcal{s}$, thus implementing the desirable cluster assumption for graph-based learning.</s> <s>remarkably, the partially absorbing process unifies many popular models arising in a variety of contexts, provides new insights into them, and makes it possible for transferring findings from one paradigm to another.</s> <s>simulation results demonstrate its promising applications in graph-based learning.</s></p></d>", "label": ["<d><p><s>learning with partially absorbing random walks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a bayesian nonparametric model that discovers implicit social structure from interaction time-series data.</s> <s>social groups are often formed implicitly, through actions among members of groups.</s> <s>yet many models of social networks use explicitly declared relationships to infer social structure.</s> <s>we consider a particular class of hawkes processes, a doubly stochastic point process, that is able to model reciprocity between groups of individuals.</s> <s>we then extend the infinite relational model by using these reciprocating hawkes processes to parameterise its edges, making events associated with edges co-dependent through time.</s> <s>our model outperforms general, unstructured hawkes processes as well as structured poisson process-based models at predicting verbal and email turn-taking, and military conflicts among nations.</s></p></d>", "label": ["<d><p><s>modelling reciprocating relationships with hawkes processes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>statistical learning and sequential prediction are two different but related formalisms to study the quality of predictions.</s> <s>mapping out their relations and transferring ideas is an active area of investigation.</s> <s>we provide another piece of the puzzle by showing that an important concept in sequential prediction, the mixability of a loss, has a natural counterpart in the statistical setting, which we call stochastic mixability.</s> <s>just as ordinary mixability characterizes fast rates for the worst-case regret in sequential prediction, stochastic mixability characterizes fast rates in statistical learning.</s> <s>we show that, in the special case of log-loss, stochastic mixability reduces to a well-known (but usually unnamed) martingale condition, which is used in existing convergence theorems for minimum description length and bayesian inference.</s> <s>in the case of 0/1-loss, it reduces to the margin condition of mammen and tsybakov, and in the case that the model under consideration contains all possible predictors, it is equivalent to ordinary mixability.</s></p></d>", "label": ["<d><p><s>mixability in statistical learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent linear dynamical systems with generalised-linear observation models arise in a variety of applications, for example when modelling the spiking activity of populations of neurons.</s> <s>here, we show how  spectral learning methods for linear systems with gaussian observations   (usually called subspace identification in this context) can be extended to estimate the parameters of dynamical system models observed through non-gaussian noise models.</s> <s>we use this approach to obtain estimates of parameters for a dynamical model of neural population data, where the observed spike-counts are poisson-distributed with log-rates determined by the latent dynamical process, possibly driven by external inputs.</s> <s>we show that the extended system identification algorithm is consistent and accurately recovers the correct parameters on large simulated data sets with much smaller computational cost than approximate expectation-maximisation (em) due to the non-iterative nature of subspace identification.</s> <s>even on smaller data sets, it provides an effective initialization for em, leading to more robust performance and faster convergence.</s> <s>these benefits are shown to extend to real neural data.</s></p></d>", "label": ["<d><p><s>spectral learning of linear dynamics from generalised-linear observations with application to neural population data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a novel bayesian nonparametric model for random bipartite graphs.</s> <s>the model is based on the theory of completely random measures and is able to handle a potentially infinite number of nodes.</s> <s>we show that the model has appealing properties and in particular it may exhibit a power-law behavior.</s> <s>we derive a posterior characterization, an indian buffet-like generative process for network growth, and a simple and efficient gibbs sampler for posterior simulation.</s> <s>our model is shown to be well fitted to several real-world social networks.</s></p></d>", "label": ["<d><p><s>bayesian nonparametric models for bipartite graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while human listeners excel at selectively attending to a conversation in a cocktail party, machine performance is still far inferior by comparison.</s> <s>we show that the cocktail party problem, or the speech separation problem, can be effectively approached via structured prediction.</s> <s>to account for temporal dynamics in speech, we employ conditional random fields (crfs) to classify speech dominance within each time-frequency unit for a sound mixture.</s> <s>to capture complex, nonlinear relationship between input and output, both state and transition feature functions in crfs are learned by deep neural networks.</s> <s>the formulation of the problem as classification allows us to directly optimize a measure that is well correlated with human speech intelligibility.</s> <s>the proposed system substantially outperforms existing ones in a variety of noises.</s></p></d>", "label": ["<d><p><s>cocktail party processing via structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a number of dependent nonparametric processes have been proposed to model non-stationary data with unknown latent dimensionality.</s> <s>however, the inference algorithms are often slow and unwieldy, and are in general highly specific to a given model formulation.</s> <s>in this paper, we describe a wide class of nonparametric processes, including several existing models, and present a slice sampler that allows efficient inference across this class of models.</s></p></d>", "label": ["<d><p><s>slice sampling normalized kernel-weighted completely random measure mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we introduce two novel metric learning algorithms, ?2-lmnn and gb-lmnn, which are explicitly designed to be non-linear and  easy-to-use.</s> <s>the two approaches achieve this goal in fundamentally different ways: ?2-lmnn inherits the computational benefits of a linear mapping from linear metric learning, but uses a non-linear ?2-distance to explicitly capture similarities within histogram data sets; gb-lmnn applies gradient-boosting to learn non-linear mappings directly in function space and takes advantage of this approach's robustness, speed, parallelizability and insensitivity towards the single additional hyper-parameter.</s> <s>on various benchmark data sets, we demonstrate these methods not only match the current state-of-the-art in terms of knn classification error, but in the case of ?2-lmnn, obtain best results in 19 out of 20 learning settings.</s></p></d>", "label": ["<d><p><s>non-linear metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the representer theorem is a property that lies at the foundation of regularization theory and kernel methods.</s> <s>a class of regularization functionals is said to admit a linear representer theorem if every member of the class admits minimizers that lie in the finite dimensional subspace spanned by the representers of the data.</s> <s>a recent characterization states that certain classes of regularization functionals with differentiable regularization term admit a linear representer theorem for any choice of the data if and only if the regularization term is a radial nondecreasing function.</s> <s>in this paper, we extend such result by weakening the assumptions on the regularization term.</s> <s>in particular, the main result of this paper implies that, for a sufficiently large family of regularization functionals, radial nondecreasing functions are the only lower semicontinuous regularization terms that guarantee existence of a representer theorem for any choice of the data.</s></p></d>", "label": ["<d><p><s>the representer theorem for hilbert spaces: a necessary and sufficient condition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we seek to detect rectangular cuboids and localize their corners in uncalibrated single-view images depicting everyday scenes.</s> <s>in contrast to recent approaches that rely on detecting vanishing points of the scene and grouping line segments to form cuboids, we build a discriminative parts-based detector that models the appearance of the cuboid corners and internal edges while enforcing consistency to a 3d cuboid model.</s> <s>our model is invariant to the different 3d viewpoints and aspect ratios and is able to detect cuboids across many different object categories.</s> <s>we introduce a database of images with cuboid annotations that spans a variety of indoor and outdoor scenes and show qualitative and quantitative results on our collected database.</s> <s>our model out-performs baseline detectors that use 2d constraints alone on the task of localizing cuboid corners.</s></p></d>", "label": ["<d><p><s>localizing 3d cuboids in single-view images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an approach to multivariate nonparametric regression that generalizes reduced rank regression for linear models.</s> <s>an additive model is estimated for each dimension of a $q$-dimensional response, with a shared $p$-dimensional predictor variable.</s> <s>to control the complexity of the model, we employ a functional form of the ky-fan or nuclear norm, resulting in a set of function estimates that have low rank.</s> <s>backfitting algorithms are derived and justified using a nonparametric form of the nuclear norm subdifferential.</s> <s>oracle inequalities on excess risk are derived that exhibit the scaling behavior of the procedure in the high dimensional setting.</s> <s>the methods are illustrated on gene expression data.</s></p></d>", "label": ["<d><p><s>nonparametric reduced rank regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>symmetric positive definite (spd) matrices are remarkably pervasive in a multitude of scientific disciplines, including machine learning and optimization.</s> <s>we consider the fundamental task of measuring distances between two spd matrices; a task that is often nontrivial whenever an application demands the distance function to respect the non-euclidean geometry of spd matrices.</s> <s>unfortunately, typical non-euclidean distance measures such as the riemannian metric $\\riem(x,y)=\\frob{\\log(x\\inv{y})}$, are computationally demanding and also complicated to use.</s> <s>to allay some of these difficulties, we introduce a new metric on spd matrices: this metric not only respects non-euclidean geometry, it also offers faster computation than $\\riem$ while being less complicated to use.</s> <s>we support our claims theoretically via a series of theorems that relate our metric to $\\riem(x,y)$, and experimentally by studying the nonconvex problem of computing matrix geometric means based on squared distances.</s></p></d>", "label": ["<d><p><s>a new metric on the manifold of kernel matrices with application to matrix geometric means</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nonnegative matrix factorization (nmf) is a promising relaxation technique for clustering analysis.</s> <s>however, conventional nmf methods that directly approximate the pairwise similarities using the least square error often yield mediocre performance for data in curved manifolds because they can capture only the immediate similarities between data samples.</s> <s>here we propose a new nmf clustering method which replaces the approximated matrix with its smoothed version using random walk.</s> <s>our method can thus accommodate farther relationships between data samples.</s> <s>furthermore, we introduce a novel regularization in the proposed objective function in order to improve over spectral clustering.</s> <s>the new learning objective is optimized by a multiplicative majorization-minimization algorithm with a scalable implementation for learning the factorizing matrix.</s> <s>extensive experimental results on real-world datasets show that our method has strong performance in terms of cluster purity.</s></p></d>", "label": ["<d><p><s>clustering by nonnegative matrix factorization using graph random walk</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most existing hashing methods adopt some projection functions to project the original data into several dimensions of real values, and then each of these projected dimensions is quantized into one bit (zero or one) by thresholding.</s> <s>typically, the variances of different projected dimensions are different for existing projection functions such as principal component analysis (pca).</s> <s>using the same number of bits for different projected dimensions is unreasonable because larger-variance dimensions will carry more information.</s> <s>although this viewpoint has been widely accepted by many researchers, it is still not verified by either theory or experiment because no methods have been proposed to find a projection with equal variances for different dimensions.</s> <s>in this paper, we propose a novel method, called isotropic hashing (isohash), to learn projection functions which can produce projected dimensions with isotropic variances (equal variances).</s> <s>experimental results on real data sets show that isohash can outperform its counterpart with different variances for different dimensions, which verifies the viewpoint that projections with isotropic variances will be better than those with anisotropic variances.</s></p></d>", "label": ["<d><p><s>isotropic hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sign-random-projection locality-sensitive hashing (srp-lsh) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity, yet suffers from the large variance of its estimation.</s> <s>in this work, we propose the super-bit locality-sensitive hashing (sblsh).</s> <s>it is easy to implement, which orthogonalizes the random projection vectors in batches, and it is theoretically guaranteed that sblsh also provides an unbiased estimate of angular similarity, yet with a smaller variance when the angle to estimate is within $(0,\\pi/2]$.</s> <s>the extensive experiments on real data well validate that given the same length of binary code, sblsh may achieve significant mean squared error reduction in estimating pairwise angular similarity.</s> <s>moreover, sblsh shows the superiority over srp-lsh in approximate nearest neighbor (ann) retrieval experiments.</s></p></d>", "label": ["<d><p><s>super-bit locality-sensitive hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we  apply   boosting  to  learn   complex  non-linear  local   visual  feature representations, drawing  inspiration from its successful  application to visual object detection.</s> <s>the main  goal of  local feature descriptors  is to distinctively  represent a salient image  region while remaining invariant to  viewpoint and illumination changes.</s> <s>this representation can  be improved using machine learning, however, past approaches  have been mostly limited to learning  linear feature mappings in either the original input or a  kernelized input feature space.</s> <s>while kernelized  methods have proven somewhat effective for learning non-linear local  feature descriptors,  they rely heavily  on the choice  of an appropriate kernel  function whose selection is often  difficult and non-intuitive.</s> <s>we propose  to use the boosting-trick  to  obtain a  non-linear  mapping  of  the input  to  a high-dimensional feature space.</s> <s>the non-linear feature mapping  obtained with the  boosting-trick is  highly intuitive.</s> <s>we employ gradient-based weak learners resulting in a learned descriptor that closely resembles the well-known sift.</s> <s>as demonstrated in our experiments, the resulting descriptor   can  be  learned   directly  from   intensity  patches  achieving state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>learning image descriptors with the boosting-trick</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in the conventional approaches for supervised parametric learning, relations between data and target variables are provided through training sets consisting of pairs of corresponded data and target variables.</s> <s>in this work, we describe a new learning scheme for parametric learning, in which the target variables $\\y$ can be modeled with a prior model $p(\\y)$ and the relations between data and target variables are estimated through $p(\\y)$ and a set of uncorresponded data $\\x$ in training.</s> <s>we term this method as learning with target priors (ltp).</s> <s>specifically, ltp learning seeks parameter $\\t$ that maximizes the log likelihood of $f_\\t(\\x)$ on a uncorresponded training set with regards to $p(\\y)$.</s> <s>compared to the conventional (semi)supervised learning approach, ltp can make efficient use of prior knowledge of the target variables in the form of probabilistic distributions, and thus removes/reduces the reliance on training data in learning.</s> <s>compared to the bayesian approach, the learned parametric regressor in ltp can be more efficiently implemented and deployed in tasks where running efficiency is critical, such as on-line bci signal decoding.</s> <s>we demonstrate the effectiveness of the proposed approach on parametric regression tasks for bci signal decoding and pose estimation from video.</s></p></d>", "label": ["<d><p><s>learning with target prior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel marginalized particle gaussian process (mpgp) regression, which provides a fast, accurate online bayesian filtering framework to model the latent function.</s> <s>using a state space model established by the data construction procedure, our mpgp recursively filters out the estimation of hidden function values by a gaussian mixture.</s> <s>meanwhile, it provides a new online method for training hyperparameters with a number of weighted particles.</s> <s>we demonstrate the estimated performance of our mpgp on both simulated and real large data sets.</s> <s>the results show that our mpgp is a robust estimation algorithm with high computational efficiency, which outperforms other state-of-art sparse gp methods.</s></p></d>", "label": ["<d><p><s>a marginalized particle gaussian process regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider  unsupervised estimation of mixtures of discrete graphical models, where the class variable   is hidden and each mixture component  can have a potentially different markov graph structure and parameters over the observed variables.</s> <s>we propose a novel method for estimating the mixture components with  provable guarantees.</s> <s>our output is   a tree-mixture model which serves as a good approximation to the underlying graphical model mixture.</s> <s>the   sample and computational requirements for our method scale as $\\poly(p,  r)$,   for an $r$-component mixture of $p$-variate graphical models, for a wide class of models which includes tree mixtures and mixtures over bounded degree graphs.</s></p></d>", "label": ["<d><p><s>learning mixtures of tree graphical models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in the paper, we consider the problem of link prediction in time-evolving graphs.</s> <s>we assume that certain graph features, such as the node degree, follow a vector autoregressive (var) model and we propose to use this information to improve the accuracy of prediction.</s> <s>our strategy involves a joint optimization procedure over the space of adjacency matrices and var matrices which takes into account both sparsity and low rank properties of the matrices.</s> <s>oracle inequalities are derived and illustrate the trade-offs in the choice of smoothing parameters when modeling the joint effect of sparsity and low rank property.</s> <s>the estimate is computed efficiently using proximal methods through a generalized forward-backward agorithm.</s></p></d>", "label": ["<d><p><s>link prediction in graphs with autoregressive features</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>links between probabilistic and non-probabilistic learning algorithms can arise by performing small-variance asymptotics, i.e., letting the variance of particular distributions in a graphical model go to zero.</s> <s>for instance, in the context of clustering, such an approach yields precise connections between the k-means and em algorithms.</s> <s>in this paper, we explore small-variance asymptotics for exponential family dirichlet process (dp) and hierarchical dirichlet process (hdp) mixture models.</s> <s>utilizing connections between exponential family distributions and bregman divergences, we derive novel clustering algorithms from the asymptotic limit of the dp and hdp mixtures that feature the scalability of existing hard clustering methods as well as the flexibility of bayesian nonparametric models.</s> <s>we focus on special cases of our analysis for discrete-data problems, including topic modeling, and we demonstrate the utility of our results by applying variants of our algorithms to problems arising in vision and document analysis.</s></p></d>", "label": ["<d><p><s>small-variance asymptotics for exponential family dirichlet process mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study how to automatically select and adapt multiple abstractions or representations of the world to support model-based reinforcement learning.</s> <s>we address the challenges of transfer learning in heterogeneous environments with varying tasks.</s> <s>we present an efficient, online framework that, through a sequence of tasks, learns a set of relevant representations to be used in future tasks.</s> <s>without pre-defined mapping strategies, we introduce a general approach to support transfer learning across different state spaces.</s> <s>we demonstrate the potential impact of our system through improved jumpstart and faster convergence to near optimum policy in two benchmark domains.</s></p></d>", "label": ["<d><p><s>transferring expectations in model-based reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the partition function plays a key role in probabilistic modeling including conditional random fields, graphical models, and maximum likelihood estimation.</s> <s>to optimize partition functions, this article introduces a quadratic variational upper bound.</s> <s>this inequality facilitates majorization methods: optimization of complicated functions through the iterative solution of simpler sub-problems.</s> <s>such bounds remain efficient to compute even when the partition function involves a graphical model (with small tree-width) or in latent likelihood settings.</s> <s>for large-scale problems, low-rank versions of the bound are provided and outperform lbfgs as well as first-order methods.</s> <s>several learning applications are shown and reduce to fast and convergent update rules.</s> <s>experimental results show advantages over state-of-the-art optimization methods.</s></p></d>", "label": ["<d><p><s>majorization for crfs and latent likelihoods</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>accurate and detailed models of the progression of neurodegenerative diseases such as  alzheimer's (ad) are crucially important for reliable early diagnosis and the determination and deployment of effective treatments.</s> <s>in this paper, we introduce the alpaca (alzheimer's disease probabilistic cascades) model, a generative model linking latent alzheimer's progression dynamics to observable biomarker data.</s> <s>in contrast with previous works which model disease progression as a fixed ordering of events, we explicitly model the variability over such orderings among patients which is more realistic, particularly for highly detailed disease progression models.</s> <s>we describe efficient learning algorithms for alpaca and discuss promising experimental results on a real cohort of alzheimer's patients from the  alzheimer's disease neuroimaging initiative.</s></p></d>", "label": ["<d><p><s>probabilistic event cascades for alzheimer's disease</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>if a piece of information is released from a media site, can it spread, in 1 month, to a million web pages?</s> <s>this influence estimation problem is very challenging since both the time-sensitive nature of the problem and the issue of scalability need to be addressed simultaneously.</s> <s>in this paper, we propose a randomized algorithm for influence estimation in continuous-time diffusion networks.</s> <s>our algorithm can estimate the influence of every node in a network with $|\\vcal|$ nodes and $|\\ecal|$ edges to an accuracy of $\\epsilon$ using  $n=o(1/\\epsilon^2)$ randomizations and up to logarithmic factors $o(n|\\ecal|+n|\\vcal|)$ computations.</s> <s>when used as a subroutine in a greedy influence maximization algorithm, our proposed method is guaranteed to find a set of nodes with an influence of at least $(1 - 1/e)\\operatorname{opt} - 2\\epsilon$, where $\\operatorname{opt}$ is the optimal value.</s> <s>experiments on both synthetic and real-world data show that the proposed method can easily scale up to networks of millions of nodes while significantly improves over previous state-of-the-arts in terms of the accuracy of the estimated influence and the quality of the selected nodes in maximizing the influence.</s></p></d>", "label": ["<d><p><s>scalable influence estimation in continuous-time diffusion networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the adaptive anonymity problem is formalized where each individual shares their data along with an integer value to indicate their personal level of desired privacy.</s> <s>this problem leads to a generalization of $k$-anonymity to the $b$-matching setting.</s> <s>novel algorithms and theory are provided to implement this type of anonymity.</s> <s>the relaxation achieves better utility, admits theoretical privacy guarantees that are as strong, and, most importantly, accommodates a variable level of anonymity for each individual.</s> <s>empirical results confirm improved utility on benchmark and social data-sets.</s></p></d>", "label": ["<d><p><s>adaptive anonymity via </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor completion from incomplete observations is a problem of significant practical interest.</s> <s>however, it is unlikely that there exists an efficient algorithm with provable guarantee to recover a general tensor from a limited number of observations.</s> <s>in this paper, we study the recovery algorithm for pairwise interaction tensors, which has recently gained considerable attention for modeling multiple attribute data due to its simplicity and effectiveness.</s> <s>specifically, in the absence of noise, we show that one can exactly recover a pairwise interaction tensor by solving a constrained convex program which minimizes the weighted sum of nuclear norms of matrices from $o(nr\\log^2(n))$ observations.</s> <s>for the noisy cases, we also prove error bounds for a constrained convex program for recovering the tensors.</s> <s>our experiments on the synthetic dataset demonstrate that the recovery performance of our algorithm agrees well with the theory.</s> <s>in addition, we apply our algorithm on a temporal collaborative filtering task and obtain state-of-the-art results.</s></p></d>", "label": ["<d><p><s>exact and stable recovery of pairwise interaction tensors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by an application in computational biology, we consider constrained low-rank matrix factorization problems with $\\{0,1\\}$-constraints on one of the factors.</s> <s>in addition to the the non-convexity shared with more general matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size $2^{m \\cdot r}$, where $m$ is the dimension of the data points and $r$ the rank of the factorization.</s> <s>despite apparent intractability, we provide $-$in the line of recent work on non-negative matrix factorization by arora et al.~(2012)$-$ an algorithm that provably recovers the underlying factorization in the exact case with operations of the order $o(m r 2^r + mnr)$ in the worst case.</s> <s>to obtain that result, we invoke theory centered around a fundamental result in combinatorics, the littlewood-offord lemma.</s></p></d>", "label": ["<d><p><s>matrix factorization with binary components</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lifted inference algorithms exploit symmetries in probabilistic models to speed up inference.</s> <s>they show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities.</s> <s>the reason is that conditioning on evidence breaks many of the model's symmetries, which preempts standard lifting techniques.</s> <s>recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #p-hard, suggesting that no lifting is to be expected in the worst case.</s> <s>in this paper, we balance this grim result by identifying the boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference.</s> <s>in particular, we show that conditioning on binary evidence with bounded boolean rank is efficient.</s> <s>this opens up the possibility of approximating evidence by a low-rank boolean matrix factorization, which we investigate both theoretically and empirically.</s></p></d>", "label": ["<d><p><s>on the complexity and approximation of binary evidence in lifted inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>finite-state transducers (fst) are a standard tool for modeling paired input-output sequences and are used in numerous applications, ranging from computational biology to natural language processing.</s> <s>recently balle et al.</s> <s>presented a spectral algorithm for learning fst from samples of aligned input-output sequences.</s> <s>in this paper we address the more realistic, yet challenging setting where the alignments are unknown to the learning algorithm.</s> <s>we frame fst learning as finding a low rank hankel matrix satisfying constraints derived from observable statistics.</s> <s>under this formulation, we provide identifiability results for fst distributions.</s> <s>then, following previous work on rank minimization, we propose a regularized convex relaxation of this objective which is based on minimizing a nuclear norm penalty subject to linear constraints and can be solved efficiently.</s></p></d>", "label": ["<d><p><s>unsupervised spectral learning of finite state transducers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the proximal map is the key step in gradient-type algorithms, which have become prevalent in large-scale high-dimensional problems.</s> <s>for simple functions this proximal map is available in closed-form while for more complicated functions it can become highly nontrivial.</s> <s>motivated by the need of combining regularizers to simultaneously induce different types of structures, this paper initiates a systematic investigation of when the proximal map of a sum of functions decomposes into the composition of the proximal maps of the individual summands.</s> <s>we not only unify a few known results scattered in the literature but also discover several new decompositions obtained almost effortlessly from our theory.</s></p></d>", "label": ["<d><p><s>on decomposing the proximal map</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point.</s> <s>consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatially-varying or non-uniform blur operator.</s> <s>using ideas from bayesian inference and convex analysis, this paper derives a non-uniform blind deblurring algorithm with several desirable, yet previously-unexplored attributes.</s> <s>the underlying objective function includes a spatially-adaptive penalty that couples the latent sharp image, non-uniform blur operator, and noise level together.</s> <s>this coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted.</s> <s>remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics.</s> <s>the algorithm can be implemented using an optimization strategy  that is virtually parameter free and simpler than existing methods.</s> <s>detailed theoretical analysis and empirical validation on real images serve to validate the proposed method.</s></p></d>", "label": ["<d><p><s>non-uniform camera shake removal using a spatially-adaptive sparse penalty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse subspace clustering (ssc) and low-rank representation (lrr) are both considered as the state-of-the-art methods for {\\em subspace clustering}.</s> <s>the two methods are fundamentally similar in that both are convex optimizations exploiting the intuition of self-expressiveness''.</s> <s>the main difference is that ssc minimizes the vector $\\ell_1$ norm of the representation matrix to induce sparsity while lrr minimizes nuclear norm (aka trace norm) to promote a low-rank structure.</s> <s>because the representation matrix is often simultaneously sparse and low-rank,  we propose a new algorithm, termed low-rank sparse subspace clustering (lrssc), by combining ssc and lrr, and develops theoretical guarantees of when the algorithm succeeds.</s> <s>the results reveal interesting insights into the strength and weakness of ssc and lrr and demonstrate how lrssc can take the advantages of both methods in preserving the \"self-expressiveness property'' and \"graph connectivity'' at the same time.\"</s></p></d>", "label": ["<d><p><s>provable subspace clustering: when lrr meets ssc</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in the matrix completion problem the aim is to recover an unknown real matrix from a subset of its entries.</s> <s>this problem comes up in many application areas, and has received a great deal of attention in the context of the netflix prize.</s> <s>a central approach to this problem is to output a matrix of lowest  possible complexity (e.g.</s> <s>rank or trace norm) that agrees with the partially  specified matrix.</s> <s>the performance of this approach under the assumption that the revealed entries are sampled randomly   has received considerable attention.</s> <s>in practice, often the set of revealed entries is not chosen at random and these results do not apply.</s> <s>we are therefore left with no guarantees on the performance of the algorithm we are using.</s> <s>we present a means to obtain performance guarantees with respect to any set of initial observations.</s> <s>the first  step remains the same: find a matrix of lowest possible complexity that agrees with the partially specified matrix.</s> <s>we give a new way to interpret the output of this algorithm by next finding a probability distribution over  the non-revealed entries with respect to which a bound on the generalization error can be proven.</s> <s>the more  complex the set of revealed entries according to a certain measure, the better the bound on the generalization  error.</s></p></d>", "label": ["<d><p><s>matrix completion from any given set of observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction.</s> <s>unfortunately, such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization---creating a highly non-convex problem.</s> <s>instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training.</s> <s>our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer.</s> <s>the resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics.</s></p></d>", "label": ["<d><p><s>convex two-layer modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there are two major routes to address linear inverse problems.</s> <s>whereas regularization-based approaches build estimators as solutions of penalized regression optimization problems, bayesian estimators rely on the posterior distribution of the unknown, given some assumed family of priors.</s> <s>while these may seem radically different approaches, recent results have shown that, in the context of additive white gaussian denoising, the bayesian conditional mean estimator is always the solution of a penalized regression problem.</s> <s>the contribution of this paper is twofold.</s> <s>first, we extend the additive white gaussian denoising results to general linear inverse problems with colored gaussian noise.</s> <s>second, we characterize conditions under which the penalty function associated to the conditional mean estimator can satisfy certain popular properties such as convexity, separability, and smoothness.</s> <s>this sheds light on some tradeoff between computational efficiency and estimation accuracy in sparse regularization, and draws some connections between bayesian estimation and proximal optimization.</s></p></d>", "label": ["<d><p><s>reconciling \"priors\" & \"priors\" without prejudice?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we focus on the principal component regression and its application to high dimension non-gaussian data.</s> <s>the major contributions are in two folds.</s> <s>first, in low dimensions and under a double asymptotic framework where both the dimension $d$ and sample size $n$ can increase, by borrowing the strength from recent development in minimax optimal principal component estimation, we first time sharply characterize the potential advantage of classical principal component regression over least square estimation under the gaussian model.</s> <s>secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data.</s> <s>the elliptical distribution is a semiparametric generalization of the gaussian, including many well known distributions such as multivariate gaussian, rank-deficient gaussian, $t$, cauchy, and logistic.</s> <s>it allows the random vector to be heavy tailed and have tail dependence.</s> <s>these extra flexibilities make it very suitable for modeling finance and biomedical imaging data.</s> <s>under the elliptical model, we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the gaussian based methods.</s> <s>experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method.</s></p></d>", "label": ["<d><p><s>robust sparse principal component regression under the high dimensional elliptical model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a successful approach to structured learning is to write the learning objective as a joint function of linear parameters and inference messages, and iterate between updates to each.</s> <s>this paper observes that if the inference problem is ?smoothed?</s> <s>through the addition of entropy terms, for fixed messages, the learning objective reduces to a traditional (non-structured) logistic regression problem with respect to parameters.</s> <s>in these logistic regression problems, each training example has a bias term determined by the current set of messages.</s> <s>based on this insight, the structured energy function can be extended from linear factors to any function class where an ?oracle?</s> <s>exists to minimize a logistic loss.</s></p></d>", "label": ["<d><p><s>structured learning via logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population.</s> <s>less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory.</s> <s>we show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall.</s> <s>we derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules.</s> <s>these dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities.</s> <s>we therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate.</s></p></d>", "label": ["<d><p><s>correlations strike back (again): the case of associative memory retrieval</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses.</s> <s>to understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states.</s> <s>moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity.</s> <s>this raises the fundamental question, how does synaptic complexity give rise to memory?</s> <s>to address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks.</s> <s>moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.</s></p></d>", "label": ["<d><p><s>a memory frontier for complex synapses</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.</s> <s>estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.</s> <s>however,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.</s> <s>for example, existing  bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.</s> <s>here we develop bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.</s> <s>we define two prior distributions over spike words using   mixtures of dirichlet distributions centered on simple parametric  models.</s> <s>the parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5).</s> <s>conversely, the   dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.</s> <s>we devise a compact representation of the data and  prior that allow for computationally efficient implementations of   bayesian least squares and empirical bayes entropy estimators with  large numbers of neurons.</s> <s>we apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.</s></p></d>", "label": ["<d><p><s>bayesian entropy estimation for binary spike train data using parametric prior knowledge</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>simultaneous recordings of the activity of large neural populations are extremely valuable as they can be used to infer the dynamics and interactions of neurons in a local circuit, shedding light on the computations performed.</s> <s>it is now possible to measure the activity of hundreds of neurons  using 2-photon calcium imaging.</s> <s>however, many computations are thought to involve circuits consisting of thousands of neurons, such as cortical barrels in rodent somatosensory cortex.</s> <s>here we contribute a statistical method for stitching\" together sequentially imaged sets of neurons into one model  by phrasing the problem as fitting a latent dynamical system with missing observations.</s> <s>this method allows us to substantially expand the population-sizes for which population dynamics can be characterized---beyond the number of simultaneously imaged neurons.</s> <s>in particular, we demonstrate using recordings in mouse somatosensory cortex that this method makes it possible to predict noise correlations between non-simultaneously recorded neuron pairs.\"</s></p></d>", "label": ["<d><p><s>inferring neural population dynamics from multiple partial recordings of the same neural circuit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns.</s> <s>although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex.</s> <s>here we consider associative memories with noisy internal computations and analytically characterize performance.</s> <s>as long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small.</s> <s>more surprisingly, we show that internal noise actually improves the performance of the recall phase.</s> <s>computational experiments lend additional support to our theoretical analysis.</s> <s>this work suggests a functional benefit to noisy neurons in biological neuronal networks.</s></p></d>", "label": ["<d><p><s>noise-enhanced associative memories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the olfactory system faces a difficult inference problem: it has to determine what odors are present based on the distributed activation of its receptor neurons.</s> <s>here we derive neural implementations of two approximate inference algorithms that could be used by the brain.</s> <s>one is a variational algorithm (which builds on the work of beck.</s> <s>et al., 2012), the other is based on sampling.</s> <s>importantly, we use a more realistic prior distribution over odors than has been used in the past: we use a spike and slab'' prior, for which most odors have zero concentration.</s> <s>after mapping the two algorithms onto neural dynamics, we find that both can infer correct odors in less than 100 ms, although it takes ~500 ms to eliminate false positives.</s> <s>thus, at the behavioral level, the two algorithms make very similar predictions.</s> <s>however, they make different assumptions about connectivity and neural computations, and make different predictions about neural activity.</s> <s>thus, they should be distinguishable experimentally.</s> <s>if so, that would provide insight into the mechanisms employed by the olfactory system, and, because the two algorithms use very different coding strategies, that would also provide insight into how networks represent probabilities.\"</s></p></d>", "label": ["<d><p><s>demixing odors - fast inference in olfaction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>population neural recordings with long-range temporal structure are often best understood in terms of a shared underlying low-dimensional dynamical process.</s> <s>advances in recording technology provide access to an ever larger fraction of the population, but the standard computational approaches available to identify the collective dynamics scale poorly with the size of the dataset.</s> <s>here we describe a new, scalable approach to discovering the low-dimensional dynamics that underlie simultaneously recorded spike trains from a neural population.</s> <s>our method is based on recurrent linear models (rlms), and relates closely to timeseries models based on recurrent neural networks.</s> <s>we formulate rlms for neural data by generalising the kalman-filter-based likelihood calculation for latent linear dynamical systems (lds) models to incorporate a generalised-linear observation process.</s> <s>we show that rlms describe motor-cortical population data better than either directly-coupled generalised-linear models or latent linear dynamical system models with generalised-linear observations.</s> <s>we also introduce the cascaded linear model (clm) to capture low-dimensional instantaneous correlations in neural populations.</s> <s>the clm describes the cortical recordings better than either ising or gaussian models and, like the rlm, can be fit exactly and quickly.</s> <s>the clm can also be seen as a generalization of a low-rank gaussian model, in this case factor analysis.</s> <s>the computational tractability of the rlm and clm allow both to scale to very high-dimensional neural data.</s></p></d>", "label": ["<d><p><s>recurrent linear models of simultaneously-recorded neural   populations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out'' neurons during training in order to avoid the co-adaptation of feature detectors.</s> <s>we introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks.</s> <s>for deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means.</s> <s>we provide estimates and bounds for these approximations and corroborate the results with simulations.</s> <s>we also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.\"</s></p></d>", "label": ["<d><p><s>understanding dropout</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many powerful monte carlo techniques for estimating partition functions, such as annealed importance sampling (ais), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution.</s> <s>the near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better.</s> <s>we present a novel sequence of intermediate distributions for exponential families: averaging the moments of the initial and target distributions.</s> <s>we derive an asymptotically optimal piecewise linear schedule for the moments path and show that it performs at least as well as geometric averages with a linear schedule.</s> <s>moment averaging performs well empirically at estimating partition functions of restricted boltzmann machines (rbms), which form the building blocks of many deep learning models, including deep belief networks and deep boltzmann machines.</s></p></d>", "label": ["<d><p><s>annealing between distributions by averaging moments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for data assumed to come from a finite mixture with an unknown number of components, it has become common to use dirichlet process mixtures (dpms) not only for density estimation, but also for inferences about the number of components.</s> <s>the typical approach is to use the posterior distribution on the number of components occurring so far --- that is, the posterior on the number of clusters in the observed data.</s> <s>however, it turns out that this posterior is not consistent --- it does not converge to the true number of components.</s> <s>in this note, we give an elementary demonstration of this inconsistency in what is perhaps the simplest possible setting: a dpm with normal components of unit variance, applied to data from a mixture\" with one standard normal component.</s> <s>further, we find that this example exhibits severe inconsistency: instead of going to 1, the posterior probability that there is one cluster goes to 0.\"</s></p></d>", "label": ["<d><p><s>a simple example of dirichlet process mixture inconsistency for the number of components</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the idea of computer vision as the bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement.</s> <s>instead, most vision tasks are approached via complex bottom-up processing pipelines.</s> <s>here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images.</s> <s>generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model.</s> <s>representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models.</s> <s>this formulation combines probabilistic programming, computer graphics, and approximate bayesian computation, and depends only on general-purpose, automatic inference techniques.</s> <s>we describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3d road models from vehicle-mounted camera images.</s> <s>each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately bayesian inferences about ambiguous real-world images.</s></p></d>", "label": ["<d><p><s>approximate bayesian image interpretation using generative probabilistic graphics programs</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>dropout and other feature noising schemes control overfitting by artificially corrupting the training data.</s> <s>for generalized linear models, dropout performs a form of adaptive regularization.</s> <s>using this viewpoint, we show that the dropout regularizer is first-order equivalent to an $\\lii$ regularizer applied after scaling the features by an estimate of the inverse diagonal fisher information matrix.</s> <s>we also establish a connection to adagrad, an online learner, and find that a close relative of adagrad operates by repeatedly solving linear dropout-regularized problems.</s> <s>by casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer.</s> <s>we apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the imdb reviews dataset.</s></p></d>", "label": ["<d><p><s>dropout training as adaptive regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we investigate the use of langevin monte carlo methods on the probability simplex and propose a new method, stochastic gradient riemannian langevin dynamics, which is simple to implement and can be applied online.</s> <s>we apply this method to latent dirichlet allocation in an online setting, and demonstrate that it achieves substantial performance improvements to the state of the art online variational bayesian methods.</s></p></d>", "label": ["<d><p><s>stochastic gradient riemannian langevin dynamics on the probability simplex</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>distributions over exchangeable matrices with infinitely many columns are useful in constructing nonparametric latent variable models.</s> <s>however, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks.</s> <s>in this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models.</s> <s>such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution.</s></p></d>", "label": ["<d><p><s>restricting exchangeable nonparametric distributions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose an approximate inference algorithm for continuous time gaussian-markov process models with both discrete and continuous time likelihoods.</s> <s>we show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for the discrete time terms and (2) variational updates for the continuous time term.</s> <s>we introduce corrections methods that improve on the marginals of the approximation.</s> <s>this approach extends the classical kalman-bucy smoothing procedure to non-gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models.</s> <s>experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.</s></p></d>", "label": ["<d><p><s>approximate inference in latent gaussian-markov models from continuous time observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general formalism of iterated random functions with semigroup property, under which exact and approximate bayesian posterior updates can be viewed as specific instances.</s> <s>a convergence theory for iterated random functions is presented.</s> <s>as an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model.</s> <s>the sequential inference algorithm and its supporting theory are  illustrated by simulated examples.</s></p></d>", "label": ["<d><p><s>bayesian inference as iterated random functions with  applications to sequential inference in graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>psychologists are interested in developing instructional policies that boost student learning.</s> <s>an instructional policy specifies the manner and content of instruction.</s> <s>for example, in the domain of concept learning, a policy might specify the nature of exemplars chosen over a training sequence.</s> <s>traditional psychological studies compare several hand-selected policies, e.g., contrasting a policy that selects only difficult-to-classify exemplars with a policy that gradually progresses over the training sequence from easy exemplars to more difficult (known as {\\em fading}).</s> <s>we propose an alternative to the traditional methodology in which we define a  parameterized space of policies and search this space to identify the optimum policy.</s> <s>for example, in concept learning, policies might be described by a fading function that specifies exemplar difficulty over time.</s> <s>we propose an experimental technique for searching policy spaces using gaussian process surrogate-based optimization and a generative model of student performance.</s> <s>instead of evaluating a few experimental conditions each with many human subjects, as the traditional methodology does, our technique evaluates many experimental conditions each with a few subjects.</s> <s>even though individual subjects provide only a noisy estimate of the population mean, the optimization method allows us to determine the shape of the policy space and identify the global optimum, and is as efficient in its subject budget as a traditional a-b comparison.</s> <s>we evaluate the method via two behavioral studies, and suggest that the method has broad applicability to optimization problems involving humans in domains beyond the educational arena.</s></p></d>", "label": ["<d><p><s>optimizing instructional policies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many attempts to understand the success of simple decision heuristics have examined heuristics as an approximation to a linear decision rule.</s> <s>this research has identified three environmental structures that aid heuristics: dominance, cumulative dominance, and noncompensatoriness.</s> <s>here, we further develop these ideas and examine their empirical relevance in 51 natural environments.</s> <s>we find that all three structures are prevalent, making it possible for some simple rules to reach the accuracy levels of the linear decision rule using less information.</s></p></d>", "label": ["<d><p><s>linear decision rule as aspiration for simple decision heuristics</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach.</s> <s>a challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse.</s> <s>control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers.</s> <s>this raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa.</s> <s>we give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners.</s> <s>as a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.</s></p></d>", "label": ["<d><p><s>scoring workers in crowdsourcing: how many control questions are enough?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an inference and optimal design procedure for recovering synaptic weights in neural microcircuits.</s> <s>we base our procedure on data from an experiment in which populations of putative presynaptic neurons can be stimulated while a subthreshold recording is made from a single postsynaptic neuron.</s> <s>we present a realistic statistical model which accounts for the main sources of variability in this experiment and allows for large amounts of information about the biological system to be incorporated if available.</s> <s>we then present a simpler model to facilitate online experimental design which entails the use of efficient bayesian inference.</s> <s>the optimized approach results in equal quality posterior estimates of the synaptic weights in roughly half the number of experimental trials under experimentally realistic conditions, tested on synthetic data generated from the full model.</s></p></d>", "label": ["<d><p><s>bayesian inference and online experimental design for mapping neural microcircuits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features.</s> <s>in this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks.</s> <s>the main contribution of this paper is a new procedure called {\\em sparse overlapping sets (sos) lasso}, a convex optimization that automatically selects similar features for related learning tasks.</s> <s>error bounds are derived for soslasso and its consistency is established for squared error loss.</s> <s>in particular,  soslasso is motivated by multi-subject fmri studies in which functional activity is classified using brain voxels as features.</s> <s>experiments with real and synthetic data demonstrate the advantages of soslasso compared to the lasso and group lasso.</s></p></d>", "label": ["<d><p><s>sparse overlapping sets lasso for multitask learning and its application to fmri analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lasso is a widely used regression technique to find sparse representations.</s> <s>when the dimension of the feature space and the number of samples are extremely large, solving the lasso problem remains challenging.</s> <s>to improve the efficiency of solving large-scale lasso problems, el ghaoui and his colleagues have proposed the safe rules which are able to quickly identify the inactive predictors, i.e., predictors that have $0$ components in the solution vector.</s> <s>then, the inactive predictors or features can be removed from the optimization problem to reduce its scale.</s> <s>by transforming the standard lasso to its dual form, it can be shown that the inactive predictors include the set of inactive constraints on the optimal dual solution.</s> <s>in this paper, we propose an efficient and effective screening rule via dual polytope projections (dpp), which is mainly based on the uniqueness and nonexpansiveness  of the optimal dual solution due to the fact that the feasible set in the dual space is a convex and closed polytope.</s> <s>moreover, we show that our screening rule can be extended to identify inactive groups in group lasso.</s> <s>to the best of our knowledge, there is currently no exact\" screening rule for group lasso.</s> <s>we have evaluated our screening rule using many real data sets.</s> <s>results show that our rule is more effective to identify inactive predictors than existing state-of-the-art screening rules for lasso.\"</s></p></d>", "label": ["<d><p><s>lasso screening rules via dual polytope projection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce kernel nonparametric tests for lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel hilbert space.</s> <s>the resulting test statistics are straightforward to compute, and are used in powerful three-variable interaction tests, which are consistent against all alternatives for a large family of reproducing kernels.</s> <s>we show the lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable, but their combined effect has a strong influence.</s> <s>this makes the lancaster test especially suited to finding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such v-structures.</s></p></d>", "label": ["<d><p><s>a kernel test for three-variable interactions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a parameter server system for distributed ml, which follows a stale synchronous parallel (ssp) model of computation that maximizes the time computational workers spend doing useful work on ml algorithms, while still providing correctness guarantees.</s> <s>the parameter server provides an easy-to-use shared interface for read/write access to an ml model's values (parameters and variables), and the ssp model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage.</s> <s>this significantly increases the proportion of time workers spend computing, as opposed to waiting.</s> <s>furthermore, the ssp model ensures ml algorithm correctness by limiting the maximum age of the stale values.</s> <s>we provide a proof of correctness under ssp, as well as empirical results demonstrating that the ssp model achieves faster algorithm convergence on several different ml problems, compared to fully-synchronous and asynchronous schemes.</s></p></d>", "label": ["<d><p><s>more effective distributed ml via a stale synchronous parallel parameter server</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>incorporating invariance information is important for many learning problems.</s> <s>to exploit invariances, most existing methods resort to approximations that either lead to expensive optimization problems such as semi-definite programming, or rely on separation oracles to retain tractability.</s> <s>some methods further limit the space of functions and settle for non-convex models.</s> <s>in this paper, we propose a framework for learning in reproducing kernel hilbert spaces (rkhs) using local invariances that explicitly characterize the behavior of the target function around data instances.</s> <s>these invariances are \\emph{compactly} encoded as linear functionals whose value are penalized by some loss function.</s> <s>based on a representer theorem that we establish, our formulation can be efficiently optimized via a convex program.</s> <s>for the representer theorem to hold, the linear functionals are required to be bounded in the rkhs, and we show that this is true for a variety of commonly used rkhs and invariances.</s> <s>experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art.</s></p></d>", "label": ["<d><p><s>learning with invariance via linear functionals on reproducing kernel hilbert space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we use the notion of local rademacher complexity to design new algorithms for learning kernels.</s> <s>our algorithms thereby benefit from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate.</s> <s>we devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques, and another one that can be formulated as a dc-programming problem for which we describe a solution in detail.</s> <s>we also report the results of experiments with both algorithms in both binary and multi-class classification tasks.</s></p></d>", "label": ["<d><p><s>learning kernels using local rademacher complexity</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we address the problem of estimating the ratio $\\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function.</s> <s>knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another.</s> <s>it is often referred as {\\it importance sampling} in statistical inference and is  also closely related to the problem of {\\it covariate shift} in transfer learning as well as to various mcmc methods.</s> <s>our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the fredholm problem of the first kind.</s> <s>this formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically.</s> <s>the resulting family of algorithms (fire, for fredholm inverse regularized estimator) is flexible,  simple and  easy to implement.</s> <s>we provide detailed theoretical analysis including concentration bounds and convergence rates for the gaussian kernel for densities defined on $\\r^d$ and smooth $d$-dimensional sub-manifolds of the euclidean space.</s> <s>model selection for unsupervised or semi-supervised inference is generally a difficult problem.</s> <s>interestingly, it turns out that in the density ratio estimation setting, when samples from both distributions are available, there are simple completely unsupervised methods for choosing parameters.</s> <s>we  call this model selection mechanism cd-cv for cross-density cross-validation.</s> <s>finally, we show encouraging experimental results including applications to classification  within the covariate shift framework.</s></p></d>", "label": ["<d><p><s>inverse density as an inverse problem: the fredholm equation approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of maintaining the data-structures of a partition-based regression procedure in a setting where the training data arrives sequentially over time.</s> <s>we prove that it is possible to maintain such a structure in time $o(\\log n)$ at any time step $n$ while achieving a nearly-optimal regression rate of $\\tilde{o}(n^{-2/(2+d)})$ in terms of the unknown metric dimension $d$.</s> <s>finally we prove a new regression lower-bound which is independent of a given data size, and hence is more appropriate for the streaming setting.</s></p></d>", "label": ["<d><p><s>regression-tree tuning in a streaming setting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of deciding whether there exists a consistent estimator of a given relation q, when data are missing not at random.</s> <s>we employ a formal representation called `missingness graphs' to explicitly portray the causal mechanisms responsible for missingness and to encode dependencies between these mechanisms and the variables being measured.</s> <s>using this representation, we define the notion of \\textit{recoverability} which ensures that, for a given missingness-graph $g$ and a given query $q$ an algorithm exists such that in the limit of large samples, it produces an estimate of $q$ \\textit{as if} no data were missing.</s> <s>we further present conditions that the graph should satisfy in order for recoverability to hold and devise algorithms to detect the presence of these conditions.</s></p></d>", "label": ["<d><p><s>graphical models for inference with missing data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk.</s> <s>we focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of $o(1/\\sqrt{n})$.</s> <s>we consider and analyze two algorithms that achieve a rate of $o(1/n)$ for classical   supervised learning problems.</s> <s>for least-squares regression, we show that   averaged stochastic gradient descent with constant step-size achieves the desired rate.</s> <s>for logistic regression, this is achieved by a simple novel stochastic gradient algorithm that (a) constructs successive local quadratic approximations of the loss functions, while (b) preserving the same running time complexity as stochastic gradient descent.</s> <s>for these algorithms, we provide a non-asymptotic analysis of the generalization error (in expectation, and also in high probability for least-squares), and run extensive experiments showing that they often outperform existing approaches.</s></p></d>", "label": ["<d><p><s>non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work on molecular programming has explored new possibilities for computational abstractions with biomolecules, including logic gates, neural networks, and linear systems.</s> <s>in the future such abstractions might enable nanoscale devices that can sense and control the world at a molecular scale.</s> <s>just as in macroscale robotics, it is critical that such devices can learn about their environment and reason under uncertainty.</s> <s>at this small scale, systems are typically modeled as chemical reaction networks.</s> <s>in this work, we develop a procedure that can take arbitrary probabilistic graphical models, represented as factor graphs over discrete random variables, and compile them into chemical reaction networks that implement inference.</s> <s>in particular, we show that marginalization based on sum-product message passing can be implemented in terms of reactions between chemical species whose concentrations represent probabilities.</s> <s>we show algebraically that the steady state concentration of these species correspond to the marginal distributions of the random variables in the graph and validate the results in simulations.</s> <s>as with standard sum-product inference, this procedure yields exact results for tree-structured graphs, and approximate solutions for loopy graphs.</s></p></d>", "label": ["<d><p><s>message passing inference with chemical reaction networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we establish minimax risk lower bounds for distributed statistical estimation given a budget $b$ of the total number of bits that may be communicated.</s> <s>such lower bounds in turn reveal the minimum amount of communication required by any procedure to achieve the classical optimal rate for statistical estimation.</s> <s>we study two classes of protocols in which machines send messages either independently or interactively.</s> <s>the lower bounds are established for a variety of problems, from estimating the mean of a population to estimating parameters in linear regression or binary classification.</s></p></d>", "label": ["<d><p><s>information-theoretic lower bounds for distributed statistical estimation with communication constraints</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present pac-bayes-empirical-bernstein inequality.</s> <s>the inequality is based on combination of pac-bayesian bounding technique with empirical bernstein bound.</s> <s>it allows to take advantage of small empirical variance and is especially useful in regression.</s> <s>we show that when the empirical variance is significantly smaller than the empirical loss pac-bayes-empirical-bernstein inequality is significantly tighter than pac-bayes-kl inequality of seeger (2002) and otherwise it is comparable.</s> <s>pac-bayes-empirical-bernstein inequality is an interesting example of application of pac-bayesian bounding technique to self-bounding functions.</s> <s>we provide empirical comparison of pac-bayes-empirical-bernstein inequality with pac-bayes-kl inequality on a synthetic example and several uci datasets.</s></p></d>", "label": ["<d><p><s>pac-bayes-empirical-bernstein inequality</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we establish theoretical results concerning all local optima of various regularized m-estimators, where both loss and penalty functions are allowed to be nonconvex.</s> <s>our results show that as long as the loss function satisfies restricted strong convexity and the penalty function satisfies suitable regularity conditions, any local optimum of the composite objective function lies within statistical precision of the true parameter vector.</s> <s>our theory covers a broad class of nonconvex objective functions, including corrected versions of the lasso for errors-in-variables linear models; regression in generalized linear models using nonconvex regularizers such as scad and mcp; and graph and inverse covariance matrix estimation.</s> <s>on the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision epsilon in log(1/epsilon) iterations, which is the fastest possible rate of any first-order method.</s> <s>we provide a variety of simulations to illustrate the sharpness of our theoretical predictions.</s></p></d>", "label": ["<d><p><s>regularized m-estimators with nonconvexity: statistical and algorithmic theory for local optima</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the increased availability of data in recent years led several authors to ask whether it is possible to use data as  a {\\em computational} resource.</s> <s>that is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task?</s> <s>we give the first positive answer to this question for a {\\em natural supervised learning problem} --- we consider agnostic pac learning of halfspaces over $3$-sparse vectors in $\\{-1,1,0\\}^n$.</s> <s>this class is inefficiently learnable using $o\\left(n/\\epsilon^2\\right)$ examples.</s> <s>our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random $\\mathrm{3cnf}$ formulas is hard, efficiently learning this class using $o\\left(n/\\epsilon^2\\right)$ examples is impossible.</s> <s>we further show that under stronger hardness assumptions, even $o\\left(n^{1.499}/\\epsilon^2\\right)$ examples do not suffice.</s> <s>on the other hand, we show a new algorithm that learns this class efficiently using $\\tilde{\\omega}\\left(n^2/\\epsilon^2\\right)$ examples.</s> <s>this formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.</s></p></d>", "label": ["<d><p><s>more data speeds up training time in learning halfspaces over sparse vectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years.</s> <s>we give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss.</s> <s>we use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement.</s></p></d>", "label": ["<d><p><s>convex calibrated surrogates for low-rank loss matrices with applications to subset ranking losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the relationship between three fundamental problems in machine learning: binary classification, bipartite ranking, and binary class probability estimation (cpe).</s> <s>it is known that a good binary cpe model can be used to obtain a good binary classification model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the cpe model directly as a ranking model); it is also known that a binary classification model does not necessarily yield a cpe model.</s> <s>however, not much is known about other directions.</s> <s>formally, these relationships involve regret transfer bounds.</s> <s>in this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data).</s> <s>we then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classification model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary cpe model (by calibrating the scores of the ranking model).</s></p></d>", "label": ["<d><p><s>on the relationship between binary classification, bipartite ranking, and binary class probability estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the partial observability model for multi-armed bandits, introduced by mannor and shamir (2011).</s> <s>our main result is a characterization of regret in the directed observability model in terms of the dominating and independence numbers of the observability graph.</s> <s>we also show that in the undirected case, the learner can achieve optimal regret without even accessing the observability graph before selecting an action.</s> <s>both results are shown using variants of the exp3 algorithm operating on the observability graph in a time-efficient manner.</s></p></d>", "label": ["<d><p><s>from bandits to experts: a tale of domination and independence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper considers the sample complexity of the multi-armed bandit with dependencies among the arms.</s> <s>some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration.</s> <s>the clearest example of this is the class of upper confidence bound (ucb) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called thompson sampling, also shares a close theoretical connection with optimistic approaches.</s> <s>in this paper, we develop a regret bound that holds for both classes of algorithms.</s> <s>this bound applies broadly and can be specialized to many model classes.</s> <s>it depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards.</s> <s>compared to ucb algorithm regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models.</s></p></d>", "label": ["<d><p><s>eluder dimension and the sample complexity of optimistic exploration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the design of strategies for \\emph{market making} in a market like a stock, commodity, or currency exchange.</s> <s>in order to obtain profit guarantees for a market maker one typically requires very particular stochastic assumptions on the sequence of price fluctuations of the asset in question.</s> <s>we propose a class of spread-based market making strategies whose performance can be controlled even under worst-case (adversarial) settings.</s> <s>we prove structural properties of these strategies which allows us to design a master algorithm which obtains low regret relative to the best such strategy in hindsight.</s> <s>we run a set of experiments showing favorable performance on real-world price data.</s></p></d>", "label": ["<d><p><s>adaptive market making via online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate two new optimization problems ?</s> <s>minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack).</s> <s>we are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost).</s> <s>these problems are often posed as minimizing the difference between submodular functions [9, 23] which is in the worst case inapproximable.</s> <s>we show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees.</s> <s>we also show that both these problems are closely related and, an approximation algorithm solving one can be used to obtain an approximation guarantee for the other.</s> <s>we provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors.</s> <s>finally, we empirically demonstrate the performance and good scalability properties of our algorithms.</s></p></d>", "label": ["<d><p><s>submodular optimization with submodular cover and submodular knapsack constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a popular problem in finance, option pricing, through the lens of an online learning game between nature and an investor.</s> <s>in the black-scholes option pricing model from 1973, the investor can continuously hedge the risk of an option by trading the underlying asset, assuming that the asset's price fluctuates according to geometric brownian motion (gbm).</s> <s>we consider a worst-case model, in which nature chooses a sequence of price fluctuations under a cumulative quadratic volatility constraint, and the investor can make a sequence of hedging decisions.</s> <s>our main result is to show that the value of our proposed game, which is the regret'' of hedging strategy, converges to the black-scholes option price.</s> <s>we use significantly weaker assumptions than previous work---for instance, we allow large jumps in the asset price---and show that the black-scholes hedging strategy is near-optimal for the investor even in this non-stochastic framework.\"</s></p></d>", "label": ["<d><p><s>how to hedge an option against an adversary: black-scholes pricing is minimax optimal</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>small-variance asymptotics provide an emerging technique for obtaining scalable combinatorial algorithms from rich probabilistic models.</s> <s>we present a small-variance asymptotic analysis of the hidden markov model and its infinite-state bayesian nonparametric extension.</s> <s>starting with the standard hmm, we first derive a ?hard?</s> <s>inference algorithm analogous to k-means that arises when particular variances in the model tend to zero.</s> <s>this analysis is then extended to the bayesian nonparametric case, yielding a simple, scalable, and flexible algorithm for discrete-state sequence data with a non-fixed number of states.</s> <s>we also derive the corresponding combinatorial objective functions arising from our analysis, which involve a k-means-like term along with penalties based on state transitions and the number of states.</s> <s>a key property of such algorithms is that ?</s> <s>particularly in the nonparametric setting ?</s> <s>standard probabilistic inference algorithms lack scalability and are heavily dependent on good initialization.</s> <s>a number of results on synthetic and real data sets demonstrate the advantages of the proposed framework.</s></p></d>", "label": ["<d><p><s>small-variance asymptotics for hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hypergraphs allow to encode higher-order relationships in data and are thus a very flexible modeling tool.</s> <s>current learning methods are either based on approximations of the hypergraphs via graphs or  on tensor methods which are only applicable under special conditions.</s> <s>in this paper we present a new learning framework on hypergraphs which fully uses the hypergraph structure.</s> <s>the key element  is a family of regularization functionals based on  the total variation on hypergraphs.</s></p></d>", "label": ["<d><p><s>the total variation on hypergraphs - learning on hypergraphs revisited</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the mixture models problem it is assumed that there are $k$ distributions $\\theta_{1},\\ldots,\\theta_{k}$ and one gets to observe a sample from a mixture of these distributions with unknown coefficients.</s> <s>the goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions.</s> <s>in this work we make the assumption that we have access to several samples drawn from the same $k$ underlying distributions, but with different mixing weights.</s> <s>as with topic modeling, having multiple samples is often a reasonable assumption.</s> <s>instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure.</s> <s>we present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high.</s> <s>the methods, when applied to topic modeling, allow generalization to words not present in the training data.</s></p></d>", "label": ["<d><p><s>using multiple samples to learn mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determinantal point processes (dpps) are random point processes well-suited for modeling repulsion.</s> <s>in machine learning, the focus of dpp-based models has been on diverse subset selection from a discrete and finite base set.</s> <s>this discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix.</s> <s>recently, there has been growing interest in using dpps defined on continuous spaces.</s> <s>while the discrete-dpp sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases.</s> <s>in this paper, we present efficient approximate dpp sampling schemes based on nystrom and random fourier feature approximations that apply to a wide range of kernel functions.</s> <s>we demonstrate the utility of continuous dpps in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.</s></p></d>", "label": ["<d><p><s>approximate inference in continuous determinantal processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion.</s> <s>variance related risk measures are among the most common risk-sensitive criteria in finance and operations research.</s> <s>however, optimizing many such criteria is known to be a hard problem.</s> <s>in this paper, we consider both discounted and average reward markov decision processes.</s> <s>for each formulation, we first define a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize.</s> <s>for each of these criteria, we derive a formula for computing its gradient.</s> <s>we then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction.</s> <s>we establish the convergence of our algorithms to locally risk-sensitive optimal policies.</s> <s>finally, we demonstrate the usefulness of our algorithms in a traffic signal control application.</s></p></d>", "label": ["<d><p><s>actor-critic algorithms for risk-sensitive mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an approach to learning from demonstration (lfd) which leverages expert data, even if the expert examples are very few or inaccurate.</s> <s>we achieve this by integrating lfd in an approximate policy iteration algorithm.</s> <s>the key idea of our approach is that expert examples are used to generate linear constraints on the optimization, in a similar fashion to large-margin classification.</s> <s>we prove an upper bound on the true bellman error of the approximation computed by the algorithm at each iteration.</s> <s>we show empirically that the algorithm outperforms both pure policy iteration, as well as dagger (a state-of-art lfd algorithm) and supervised learning in a variety of scenarios, including when very few and/or imperfect demonstrations are available.</s> <s>our experiments include simulations as well as a real robotic navigation task.</s></p></d>", "label": ["<d><p><s>learning from limited demonstrations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study exploration in multi-armed bandits (mab) in a setting where~$k$ players collaborate in order to identify an $\\epsilon$-optimal arm.</s> <s>our motivation comes from recent employment of mab algorithms in computationally intensive, large-scale applications.</s> <s>our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them.</s> <s>in particular, our main result shows that by allowing the $k$ players to communicate \\emph{only once}, they are able to learn $\\sqrt{k}$ times faster than a single player.</s> <s>that is, distributing learning to $k$ players gives rise to a factor~$\\sqrt{k}$ parallel speed-up.</s> <s>we complement this result with a lower bound showing this is in general the best possible.</s> <s>on the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in~$1/\\epsilon$.</s></p></d>", "label": ["<d><p><s>distributed exploration in multi-armed bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces.</s> <s>our analysis shows that the algorithm is implicitly able to estimate the $l_2$ norm of the unknown competitor, $u$, achieving a regret bound of the order of $o(u \\log (u t+1))\\sqrt{t})$, instead of the standard $o((u^2 +1) \\sqrt{t})$, achievable without knowing $u$.</s> <s>for this analysis, we introduce novel tools for algorithms with time-varying regularizers, through the use of local smoothness.</s> <s>through a lower bound, we also show that the algorithm is optimal up to $\\sqrt{\\log t}$ term for linear and lipschitz losses.</s></p></d>", "label": ["<d><p><s>dimension-free exponentiated gradient</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>analytic shrinkage is a statistical technique that offers a fast alternative to cross-validation for the regularization of covariance matrices and has appealing consistency properties.</s> <s>we show that the proof of consistency implies bounds on the growth rates of eigenvalues and their dispersion, which are often violated in data.</s> <s>we prove consistency under assumptions which do not restrict the covariance structure and therefore better match real world data.</s> <s>in addition, we propose an extension of analytic shrinkage --orthogonal complement shrinkage-- which adapts to the covariance structure.</s> <s>finally we demonstrate the superior performance of our novel approach on data from the domains of finance, spoken letter and optical character recognition, and neuroscience.</s></p></d>", "label": ["<d><p><s>generalizing analytic shrinkage for arbitrary covariance structures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the efficiency of brain-computer interfaces (bci) largely depends upon a reliable extraction of informative features from the high-dimensional eeg signal.</s> <s>a crucial step in this protocol is the computation of spatial filters.</s> <s>the common spatial patterns (csp) algorithm computes filters that maximize the difference in band power between two conditions, thus it is tailored to extract the relevant information in motor imagery experiments.</s> <s>however, csp is highly sensitive to artifacts in the eeg data, i.e.</s> <s>few outliers may alter the estimate drastically and decrease classification performance.</s> <s>inspired by concepts from the field of information geometry we propose a novel approach for robustifying csp.</s> <s>more precisely, we formulate csp as a divergence maximization problem and utilize the property of a particular type of divergence, namely beta divergence, for robustifying the estimation of spatial filters in the presence of artifacts in the data.</s> <s>we demonstrate the usefulness of our method on toy data and on eeg recordings from 80 subjects.</s></p></d>", "label": ["<d><p><s>robust spatial filtering with beta divergence</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the l1-regularized gaussian maximum likelihood estimator (mle) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings.</s> <s>however, it requires solving a difficult non-smooth log-determinant program with number of parameters scaling quadratically with the number of gaussian variables.</s> <s>state-of-the-art methods thus do not scale to problems with more than 20,000 variables.</s> <s>in this paper, we develop an algorithm bigquic, which can solve 1 million dimensional l1-regularized gaussian mle problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory.</s> <s>in order to do so, we carefully exploit the underlying structure of the problem.</s> <s>our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of specific components.</s> <s>in spite of these modifications,  we are able to theoretically analyze our procedure and show that bigquic can achieve super-linear or even quadratic convergence rates.</s></p></d>", "label": ["<d><p><s>big & quic: sparse inverse covariance estimation for a million variables</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>multiple hypothesis testing is a significant problem in nearly all neuroimaging studies.</s> <s>in order to correct for this phenomena, we require a reliable estimate of the family-wise error rate (fwer).</s> <s>the well known bonferroni correction method, while being simple to implement, is quite conservative, and can substantially under-power a study because it ignores dependencies between test statistics.</s> <s>permutation testing, on the other hand, is an exact, non parametric method of estimating the fwer for a given ?</s> <s>threshold, but for acceptably low thresholds the computational burden can be prohibitive.</s> <s>in this paper, we observe that permutation testing in fact amounts to populating the columns of a very large matrix p. by analyzing the spectrum of this matrix, under certain conditions, we see that p has a low-rank plus a low-variance residual decomposition which makes it suitable for highly sub?sampled ?</s> <s>on the order of 0.5% ?</s> <s>matrix completion methods.</s> <s>thus, we propose a novel permutation testing methodology which offers a large speedup, without sacrificing the fidelity of the estimated fwer.</s> <s>our valuations on four different neuroimaging datasets show that a computational speedup factor of roughly 50?</s> <s>can be achieved while recovering the fwer distribution up to very high accuracy.</s> <s>further, we show that the estimated ?</s> <s>threshold is also recovered faithfully, and is stable.</s></p></d>", "label": ["<d><p><s>speeding up permutation testing in neuroimaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems.</s> <s>we propose a robust graph matching algorithm inspired in sparsity-related techniques.</s> <s>we cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented lagrangian techniques.</s> <s>the method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data.</s> <s>the proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence.</s> <s>the algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs.</s> <s>we also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fmri) data.</s></p></d>", "label": ["<d><p><s>robust multimodal graph matching: sparse coding meets graph matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>as massively parallel computations have become broadly available with modern gpus, deep architectures trained on very large datasets have risen in popularity.</s> <s>discriminatively trained convolutional neural networks, in particular, were recently shown to yield state-of-the-art performance in challenging image classification benchmarks such as imagenet.</s> <s>however, elements of these architectures are similar to standard hand-crafted representations used in computer vision.</s> <s>in this paper, we explore the extent of this analogy, proposing a version of the state-of-the-art fisher vector image encoding that can be stacked in multiple layers.</s> <s>this architecture significantly improves on standard fisher vectors, and obtains competitive results with deep convolutional networks at a significantly smaller computational cost.</s> <s>our hybrid architecture allows us to measure the performance improvement brought by a deeper image classification pipeline, while staying in the realms of conventional sift features and fv encodings.</s></p></d>", "label": ["<d><p><s>deep fisher networks for large-scale image classification</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>optimal transportation distances are a fundamental family of parameterized distances for histograms in the probability simplex.</s> <s>despite their appealing theoretical properties, excellent performance and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds.</s> <s>we propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective.</s> <s>we smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers.</s> <s>we also report improved performance on the mnist benchmark problem over competing distances.</s></p></d>", "label": ["<d><p><s>sinkhorn distances: lightspeed computation of optimal transport</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite growing interest and practical use in various scientific areas, variable importances derived from tree-based ensemble methods are not well understood from a theoretical point of view.</s> <s>in this work we characterize the mean decrease impurity (mdi) variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions.</s> <s>we derive a three-level decomposition of the  information jointly provided by all input variables about the output in terms of i) the mdi importance of each input variable, ii) the degree of interaction of a given input variable with the other input variables, iii) the different interaction terms of a given degree.</s> <s>we then show that this mdi importance of a variable is equal to zero if and only if the variable is irrelevant and that the mdi importance of a relevant variable is invariant with respect to the removal or the addition of irrelevant variables.</s> <s>we illustrate these properties on a simple example and discuss how they may change in the case of  non-totally randomized trees such as random forests and extra-trees.</s></p></d>", "label": ["<d><p><s>understanding variable importances in forests of randomized trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>language users are remarkably good at making inferences about speakers' intentions in context, and children learning their native language also display substantial skill in acquiring the meanings of unknown words.</s> <s>these two cases are deeply related: language users invent new terms in conversation, and language learners learn the literal meanings of words based on their pragmatic inferences about how those words are used.</s> <s>while pragmatic inference and word learning have both been independently characterized in probabilistic terms, no current work unifies these two.</s> <s>we describe a model in which language learners assume that they jointly approximate a shared, external lexicon and reason recursively about the goals of others in using this lexicon.</s> <s>this model captures phenomena in word learning and pragmatic inference; it additionally leads to insights about the emergence of communicative systems in conversation and the mechanisms by which pragmatic inferences become incorporated into word meanings.</s></p></d>", "label": ["<d><p><s>learning and using language via recursive pragmatic reasoning about other agents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the high-dimensional regression model a response variable is linearly related to $p$ covariates,  but the sample size $n$ is smaller than $p$.</s> <s>we assume that only a small subset of covariates is `active'  (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying  the active covariates.</s> <s>a popular approach is to estimate the regression coefficients through the lasso ($\\ell_1$-regularized least squares).</s> <s>this is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition.</s> <s>in this paper we study the `gauss-lasso' selector, a simple two-stage method that first solves the lasso, and then performs ordinary least squares restricted to the lasso active set.</s> <s>we formulate `generalized irrepresentability condition' (gic), an assumption that is substantially weaker than  irrepresentability.</s> <s>we prove that, under gic, the gauss-lasso correctly recovers the active set.</s></p></d>", "label": ["<d><p><s>model selection for high-dimensional regression under the generalized irrepresentability condition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures.</s> <s>as a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates.</s> <s>this in turn implies that it is extremely challenging to quantify the `uncertainty' associated with a certain parameter estimate.</s> <s>concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values.</s> <s>we consider here a broad class of regression problems, and propose  an efficient algorithm  for constructing confidence intervals and p-values.</s> <s>the resulting confidence intervals have nearly optimal size.</s> <s>when testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power.</s> <s>our approach is based on constructing a `de-biased' version of regularized m-estimators.</s> <s>the new construction  improves over recent work in the field in that it does not assume a special structure on the design matrix.</s> <s>furthermore, proofs are remarkably simple.</s> <s>we test our method on a diabetes prediction problem.</s></p></d>", "label": ["<d><p><s>confidence intervals and hypothesis testing for high-dimensional statistical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of unsupervised feature learning for text data.</s> <s>our method is grounded in the principle of minimum description length and uses a dictionary-based compression scheme to extract a succinct feature set.</s> <s>specifically, our method finds a set of word $k$-grams that minimizes the cost of reconstructing the text losslessly.</s> <s>we formulate document compression as a binary optimization task and show how to solve it approximately via a sequence of reweighted linear programs that are efficient to solve and parallelizable.</s> <s>as our method is unsupervised, features may be extracted once and subsequently used in a variety of tasks.</s> <s>we demonstrate the performance of these features over a range of scenarios including unsupervised exploratory analysis and supervised text categorization.</s> <s>our compressed feature space is two orders of magnitude smaller than the full $k$-gram space and matches the text categorization accuracy achieved in the full feature space.</s> <s>this dimensionality reduction not only results in faster training times, but it can also help elucidate structure in unsupervised learning tasks and reduce the amount of training data necessary for supervised learning.</s></p></d>", "label": ["<d><p><s>compressive feature learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the goal of unsupervised feature selection is to identify a small number of important features that can represent the data.</s> <s>we propose a new algorithm, a modification of the classical pivoted qr algorithm of businger and golub, that requires a small number of passes over the data.</s> <s>the improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the final selection.</s> <s>our algorithm selects the exact same features as the classical pivoted qr algorithm, and has the same favorable numerical stability.</s> <s>we describe experiments on real-world datasets which sometimes show improvements of {\\em several orders of magnitude} over the classical algorithm.</s> <s>these results appear to be competitive with  recently proposed randomized algorithms in terms of pass efficiency and run time.</s> <s>on the other hand, the randomized algorithms may produce better features, at the cost of small probability of failure.</s></p></d>", "label": ["<d><p><s>pass-efficient unsupervised feature selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is a common practice to approximate complicated'' functions with more friendly ones.</s> <s>in large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions.</s> <s>we re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends  the linearity of the proximal map.</s> <s>the new approximation is justified using a recent convex analysis tool---proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead.</s> <s>numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims.\"</s></p></d>", "label": ["<d><p><s>better approximation and faster algorithm using the proximal average</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>structured sparse estimation has become an important technique in many areas of data analysis.</s> <s>unfortunately, these estimators normally create computational difficulties that entail sophisticated algorithms.</s> <s>our first contribution is to uncover a rich class of structured sparse regularizers whose polar operator can be evaluated efficiently.</s> <s>with such an operator, a simple conditional gradient method can then be developed that, when combined with smoothing and local optimization, significantly reduces training time vs. the state of the art.</s> <s>we also demonstrate a new reduction of polar to proximal maps that enables more efficient latent fused lasso.</s></p></d>", "label": ["<d><p><s>polar operators for structured sparse estimation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately.</s> <s>currently, a popular method for solving such problem is the proximal gradient method (pgm), which is known to have a sublinear rate of convergence.</s> <s>in this paper, we show that for a large class of loss functions, the convergence rate of the pgm is in fact linear.</s> <s>our result is established without any strong convexity assumption on the loss function.</s> <s>a key ingredient in our proof is a new lipschitzian error bound for the aforementioned trace norm-regularized problem, which may be of independent interest.</s></p></d>", "label": ["<d><p><s>on the linear convergence of the proximal gradient method for trace norm regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance.</s> <s>to remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (svrg).</s> <s>for smooth and strongly convex functions, we  prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (sdca) and stochastic average gradient (sag).</s> <s>however, our analysis is significantly simpler and more intuitive.</s> <s>moreover, unlike sdca or sag, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.</s></p></d>", "label": ["<d><p><s>accelerating stochastic gradient descent using predictive variance reduction</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>stochastic dual coordinate ascent (sdca) is an effective technique for solving regularized loss minimization problems in machine learning.</s> <s>this paper considers an extension of sdca under the mini-batch setting that is often used in practice.</s> <s>our main contribution is to introduce an accelerated mini-batch version of sdca and prove a fast convergence rate for this method.</s> <s>we discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of nesterov [2007].</s></p></d>", "label": ["<d><p><s>accelerated mini-batch stochastic dual coordinate ascent</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study stochastic optimization problems when the \\emph{data} is sparse,  which is in a sense dual to the current understanding of high-dimensional statistical learning and optimization.</s> <s>we highlight both the difficulties---in terms of increased sample complexity that sparse data necessitates---and the potential benefits, in terms of allowing parallelism and asynchrony in the design of algorithms.</s> <s>concretely, we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data, and we exhibit algorithms achieving these rates.</s> <s>our algorithms are adaptive: they achieve the best possible rate for the data observed.</s> <s>we also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms, providing experimental evidence complementing our theoretical results on medium to large-scale learning tasks.</s></p></d>", "label": ["<d><p><s>estimation, optimization, and parallelism when data is sparse</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for smooth and strongly convex optimization, the optimal iteration complexity of the gradient-based algorithm is $o(\\sqrt{\\kappa}\\log 1/\\epsilon)$, where $\\kappa$ is the conditional number.</s> <s>in the case that the optimization problem is ill-conditioned, we need to evaluate a larger number of full gradients, which could be computationally expensive.</s> <s>in this paper, we propose to reduce the number of full gradient required by allowing the algorithm to access the stochastic gradients of the objective function.</s> <s>to this end, we present a novel algorithm named epoch mixed gradient descent (emgd) that is able to utilize two kinds of gradients.</s> <s>a distinctive step in emgd is the mixed gradient descent, where we use an combination of the gradient and the stochastic gradient to update the intermediate solutions.</s> <s>by performing a fixed number of mixed gradient descents, we are able to improve the sub-optimality of the solution by a constant factor, and thus achieve a linear convergence rate.</s> <s>theoretical analysis shows that emgd is able to find an $\\epsilon$-optimal solution by computing $o(\\log 1/\\epsilon)$ full gradients and $o(\\kappa^2\\log 1/\\epsilon)$ stochastic gradients.</s></p></d>", "label": ["<d><p><s>linear convergence with condition number independent access of full gradients</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is well known that the optimal convergence rate for stochastic optimization of smooth functions is $[o(1/\\sqrt{t})]$, which is same as stochastic optimization of lipschitz continuous convex functions.</s> <s>this is in contrast to optimizing smooth functions using full gradients, which yields a convergence rate of $[o(1/t^2)]$.</s> <s>in this work, we consider a new setup for optimizing smooth functions, termed as {\\bf mixed optimization}, which allows to access  both a stochastic oracle  and a full gradient oracle.</s> <s>our goal is to significantly improve the convergence rate of stochastic optimization of smooth functions by having an additional small number of accesses to the full gradient oracle.</s> <s>we show that, with an $[o(\\ln t)]$ calls to the full gradient oracle  and an $o(t)$ calls to the stochastic oracle, the proposed mixed optimization algorithm is able to achieve an optimization error of $[o(1/t)]$.</s></p></d>", "label": ["<d><p><s>mixed optimization for smooth functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we are interested in the development of efficient algorithms  for convex optimization problems in the simultaneous presence of multiple objectives and stochasticity in the first-order information.</s> <s>we  cast the stochastic multiple objective optimization problem into a constrained optimization problem by choosing one function as the objective and try to bound other objectives by appropriate thresholds.</s> <s>we first examine  a two stages exploration-exploitation based algorithm which first approximates  the stochastic objectives by sampling  and  then solves a constrained stochastic optimization problem by projected gradient method.</s> <s>this method  attains a  suboptimal  convergence rate  even under  strong assumption on the objectives.</s> <s>our second approach is an efficient primal-dual stochastic algorithm.</s> <s>it leverages on the theory of lagrangian method in constrained optimization and attains  the optimal convergence rate of $[o(1/ \\sqrt{t})]$ in high probability for general lipschitz continuous objectives.</s></p></d>", "label": ["<d><p><s>stochastic convex optimization with multiple  objectives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider robust optimization for polynomial optimization problems where the uncertainty set is a set of candidate probability density functions.</s> <s>this set is a ball around a density function estimated from data samples, i.e., it is data-driven and random.</s> <s>polynomial optimization problems are inherently hard due to nonconvex objectives and constraints.</s> <s>however, we show that by employing polynomial and histogram density estimates, we can introduce robustness with respect to distributional uncertainty sets without making the problem harder.</s> <s>we show that the solution to the distributionally robust problem is the limit of a sequence of tractable semidefinite programming relaxations.</s> <s>we also give finite-sample consistency guarantees for the data-driven uncertainty  sets.</s> <s>finally, we apply our model and solution method in a water network problem.</s></p></d>", "label": ["<d><p><s>data-driven distributionally robust polynomial optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nonparametric estimation of the conditional distribution of a response given high-dimensional features is a challenging problem.</s> <s>it is important to allow not only the mean but also the variance and shape of the response density to change flexibly with features, which are massive-dimensional.</s> <s>we propose a multiscale dictionary learning model, which expresses the conditional response density as a convex combination of dictionary densities, with the densities used and their weights dependent on the path through a tree decomposition of the feature space.</s> <s>a fast graph partitioning algorithm is applied to obtain the tree decomposition, with bayesian methods then used to adaptively prune and average over different sub-trees in a soft probabilistic manner.</s> <s>the algorithm scales efficiently to approximately one million features.</s> <s>state of the art predictive performance is demonstrated for toy examples and two neuroscience applications including up to a million features.</s></p></d>", "label": ["<d><p><s>multiscale dictionary learning for estimating conditional distributions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a large number of algorithms in machine learning, from principal component analysis (pca), and its non-linear (kernel) extensions, to more recent spectral embedding and support estimation methods, rely on estimating a linear subspace from samples.</s> <s>in this paper we introduce  a general formulation  of this problem and derive novel learning error estimates.</s> <s>our results rely on  natural  assumptions on the spectral properties of the covariance operator associated to  the data distribution, and hold for a wide class of metrics between subspaces.</s> <s>as special cases, we discuss sharp error estimates  for the reconstruction properties of pca and spectral support estimation.</s> <s>key to our analysis is an operator theoretic approach that has broad applicability to spectral learning methods.</s></p></d>", "label": ["<d><p><s>on the sample complexity of subspace learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel non-parametric method for finding a subspace of stimulus features that contains all information about the response of a system.</s> <s>our method generalizes similar approaches to this problem such as spike triggered average, spike triggered covariance, or maximally informative dimensions.</s> <s>instead of maximizing the mutual information between features and responses directly, we use integral probability metrics in kernel hilbert spaces to minimize the information between uninformative features and the combination of informative features and responses.</s> <s>since estimators of these metrics access the data via kernels, are easy to compute, and exhibit good theoretical convergence properties, our method can easily be generalized to populations of neurons or spike patterns.</s> <s>by using a particular expansion of the mutual information, we can show that the informative features must contain all information if we can make the uninformative features independent of the rest.</s></p></d>", "label": ["<d><p><s>least informative dimensions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>compressed sensing (cs) is a concept that allows to acquire compressible signals with a small number of measurements.</s> <s>as such, it is very attractive for hardware implementations.</s> <s>therefore, correct calibration of the hardware is a central issue.</s> <s>in this paper we study the so-called blind calibration, i.e.</s> <s>when the training signals that are available to perform the calibration are sparse but unknown.</s> <s>we extend the approximate message passing (amp) algorithm used in cs to the case of blind calibration.</s> <s>in the calibration-amp, both the gains on the sensors and the elements of the signals are treated as unknowns.</s> <s>our algorithm is also applicable to settings in which the sensors distort the measurements in other ways than multiplication by a gain, unlike previously suggested blind calibration algorithms based on convex relaxations.</s> <s>we study numerically the phase diagram of the blind calibration problem, and show that even in cases where convex relaxation is possible, our algorithm requires a smaller number of measurements and/or signals in order to perform well.</s></p></d>", "label": ["<d><p><s>blind calibration in compressed sensing using message passing algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the fundamental problems of variance and risk estimation in high dimensional statistical modeling.</s> <s>in particular, we consider the problem of learning a coefficient vector $\\theta_0\\in r^p$ from noisy linear observation $y=x\\theta_0+w\\in r^n$ and the popular estimation procedure of solving an $\\ell_1$-penalized least squares objective known as the lasso or basis pursuit denoising (bpdn).</s> <s>in this context, we develop new estimators for the $\\ell_2$ estimation risk $\\|\\hat{\\theta}-\\theta_0\\|_2$ and the variance of the noise.</s> <s>these can be used to select the regularization parameter optimally.</s> <s>our approach combines stein unbiased risk estimate (stein'81) and recent results of (bayati and montanari'11-12) on the analysis of approximate message passing and risk of lasso.</s> <s>we establish high-dimensional consistency of our estimators for sequences of matrices $x$ of increasing dimensions, with independent gaussian entries.</s> <s>we establish validity for a broader class of gaussian designs, conditional on the validity of a certain conjecture from statistical physics.</s> <s>our approach is the first that provides an asymptotically consistent risk estimator.</s> <s>in addition, we demonstrate through simulation that our variance estimation outperforms several existing methods in the literature.</s></p></d>", "label": ["<d><p><s>estimating lasso risk and noise level</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>max-product ?belief propagation?</s> <s>(bp) is a popular distributed heuristic for finding the maximum a posteriori (map) assignment in a joint probability distribution represented by a graphical model (gm).</s> <s>it was recently shown that bp converges to the correct map assignment for a class of loopy gms with the following common feature: the linear programming (lp) relaxation to the map problem is tight (has no integrality gap).</s> <s>unfortunately, tightness of the lp relaxation does not, in general, guarantee convergence and correctness of the bp algorithm.</s> <s>the failure of bp in such cases motivates reverse engineering a solution ?</s> <s>namely, given a tight lp, can we design a ?good?</s> <s>bp algorithm.</s> <s>in this paper, we design a bp algorithm for the maximum weight matching (mwm) problem over general graphs.</s> <s>we prove that the algorithm converges to the correct optimum if the respective lp relaxation, which may include inequalities associated with non-intersecting odd-sized cycles, is tight.</s> <s>the most significant part of our approach is the introduction of a novel graph transformation designed to force convergence of bp.</s> <s>our theoretical result suggests an efficient bp-based heuristic for the mwm problem, which consists of making sequential, ?cutting plane?, modifications to the underlying gm.</s> <s>our experiments show that this heuristic performs as well as traditional cutting-plane algorithms using lp solvers on mwm problems.</s></p></d>", "label": ["<d><p><s>a graphical transformation for belief propagation: maximum weight matchings and odd-sized cycles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the sensor selection problem on multivariate gaussian distributions where only a \\emph{subset} of latent variables is of inferential interest.</s> <s>for pairs of vertices connected by a unique path in the graph, we show that there exist decompositions of nonlocal mutual information into local information measures that can be computed efficiently from the output of message passing algorithms.</s> <s>we integrate these decompositions into a computationally efficient greedy selector where the computational expense of quantification can be distributed across nodes in the network.</s> <s>experimental results demonstrate the comparative efficiency of our algorithms for sensor selection in high-dimensional distributions.</s> <s>we additionally derive an online-computable performance bound based on augmentations of the relevant latent variable set that, when such a valid augmentation exists, is applicable for \\emph{any} distribution with nuisances.</s></p></d>", "label": ["<d><p><s>sensor selection in high-dimensional gaussian trees with nuisances</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes, equivalent to the harmonic predictor on gaussian random fields (grfs).</s> <s>for active learning on grfs, the commonly used v-optimality criterion queries nodes that reduce the l2 (regression) loss.</s> <s>v-optimality satisfies a submodularity property showing that greedy reduction produces a (1 ?</s> <s>1/e) globally optimal solution.</s> <s>however, l2 loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning.</s> <s>we consider a new criterion we call ?-optimality, which queries the node that minimizes the sum of the elements in the predictive covariance.</s> <s>?-optimality directly optimizes the risk of the surveying problem, which is to determine the proportion of nodes belonging to one class.</s> <s>in this paper we extend submodularity guarantees from v-optimality to ?-optimality using properties specific to grfs.</s> <s>we further show that grfs satisfy the suppressor-free condition in addition to the conditional independence inherited from markov random fields.</s> <s>we test ?-optimality on real-world graphs with both synthetic and real data and show that it outperforms v-optimality and other related methods on classification.</s></p></d>", "label": ["<d><p><s>?-optimality for active learning on gaussian random fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many real-world problems have complicated objective functions.</s> <s>to optimize such functions, humans utilize sophisticated sequential decision-making strategies.</s> <s>many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior?</s> <s>we try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1d function.</s> <s>subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location.</s> <s>their task is to find the function?s maximum in as few clicks as possible.</s> <s>subjects win if they get close enough to the maximum location.</s> <s>analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms.</s> <s>bayesian optimization based on gaussian processes, which exploit all the x values tried and all the f(x) values obtained so far to pick the next x, predicts human performance and searched locations better.</s> <s>in 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that gaussian processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.</s></p></d>", "label": ["<d><p><s>bayesian optimization explains human active search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we present active learning algorithms in the context of structured prediction problems.</s> <s>to reduce the amount of labeling necessary to learn good models, our algorithms only label subsets of the output.</s> <s>to this end, we query examples using entropies of local marginals, which are a good surrogate for uncertainty.</s> <s>we demonstrate the effectiveness of our approach in the task of 3d layout prediction from single images, and show that good models are learned when labeling only a handful of random variables.</s> <s>in particular, the same performance as using the full training set can be obtained while only labeling ~10\\% of the random variables.</s></p></d>", "label": ["<d><p><s>latent structured active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees for these problems.</s> <s>our algorithms exploit adaptivity to identify entries that are highly informative for identifying the column space of the matrix (tensor) and consequently, our results hold even when the row space is highly coherent, in contrast with previous analysis of matrix completion.</s> <s>in the absence of noise, we show that one can exactly recover a $n \\times n$ matrix of rank $r$ using $o(r^2 n \\log(r))$ observations, which is better than the best known bound under random sampling.</s> <s>we also show that one can recover an order $t$ tensor using $o(r^{2(t-1)}t^2 n \\log(r))$.</s> <s>for noisy recovery, we show that one can consistently estimate a low rank matrix corrupted with noise using $o(nr \\textrm{polylog}(n))$ observations.</s> <s>we complement our study with simulations that verify our theoretical guarantees and demonstrate the scalability of our algorithms.</s></p></d>", "label": ["<d><p><s>low-rank matrix and tensor completion via adaptive sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>maximization of submodular functions has wide applications in machine learning and artificial intelligence.</s> <s>adaptive submodular maximization has been traditionally studied under the assumption that the model of the world, the expected gain of choosing an item given previously selected items and their states, is known.</s> <s>in this paper, we study the scenario where the expected gain is initially unknown and it is learned by interacting repeatedly with the optimized function.</s> <s>we propose an efficient algorithm for solving our problem and prove that its expected cumulative regret increases logarithmically with time.</s> <s>our regret bound captures the inherent property of submodular maximization, earlier mistakes are more costly than later ones.</s> <s>we refer to our approach as optimistic adaptive submodular maximization (oasm) because it trades off exploration and exploitation based on the optimism in the face of uncertainty principle.</s> <s>we evaluate our method on a preference elicitation problem and show that non-trivial k-step policies can be learned from just a few hundred interactions with the problem.</s></p></d>", "label": ["<d><p><s>adaptive submodular maximization in bandit setting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance.</s> <s>we study binary classification in an extreme case, where the algorithm only pays for negative labels.</s> <s>our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible.</s> <s>we term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative points it labels to learn a hypothesis with low relative error.</s> <s>we design auditing algorithms for thresholds on the line and axis-aligned rectangles, and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity.</s> <s>we discuss a general approach for auditing for a general hypothesis class, and describe several interesting directions for future work.</s></p></d>", "label": ["<d><p><s>auditing: active learning with outcome-dependent query costs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many practical applications of active learning, it is more cost-effective to request labels in large batches, rather than one-at-a-time.</s> <s>this is because the cost of labeling a large batch of examples at once is often sublinear in the number of examples in the batch.</s> <s>in this work, we study the label complexity of active learning algorithms that request labels in a given number of batches, as well as the tradeoff between the total number of queries and the number of rounds allowed.</s> <s>we additionally study the total cost sufficient for learning, for an abstract notion of the cost of requesting the labels of a given number of examples at once.</s> <s>in particular, we find that for sublinear cost functions, it is often desirable to request labels in large batches (i.e., buying in bulk); although this may increase the total number of labels requested, it reduces the total cost required for learning.</s></p></d>", "label": ["<d><p><s>buy-in-bulk active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new objective function for pool-based bayesian active learning with probabilistic hypotheses.</s> <s>this objective function, called the policy gibbs error, is the expected error rate of a random classifier drawn from the prior distribution on the examples adaptively selected by the active learning policy.</s> <s>exact maximization of the policy gibbs error is hard, so we propose a greedy strategy that maximizes the gibbs error at each iteration, where the gibbs error on an instance is the expected error of a random classifier selected from the posterior label distribution on that instance.</s> <s>we apply this maximum gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning.</s> <s>in each scenario, we prove that the criterion achieves near-maximal policy gibbs error when constrained to a fixed budget.</s> <s>for practical implementations, we provide approximations to the maximum gibbs error criterion for bayesian conditional random fields and transductive naive bayes.</s> <s>our experimental results on a named entity recognition task and a text classification task show that the maximum gibbs error criterion is an effective active learning criterion for noisy models.</s></p></d>", "label": ["<d><p><s>active learning for probabilistic hypotheses using the maximum gibbs error criterion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a number of classical and new computational problems regarding marginal distributions, and inference in models specifying a full joint distribution.</s> <s>we prove general and efficient reductions between a number of these problems, which demonstrate that algorithmic progress in inference automatically yields progress for ?pure data?</s> <s>problems.</s> <s>our main technique involves formulating the problems as linear programs, and proving that the dual separation oracle for the ellipsoid method is provided by the target problem.</s> <s>this technique may be of independent interest in probabilistic inference.</s></p></d>", "label": ["<d><p><s>marginals-to-models reducibility</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the problem of learning the structure of a markov network from data.</s> <s>it is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data.</s> <s>to achieve efficient encodings, we develop a novel characterization of markov network structure using a balancing condition on the separators between cliques forming the network.</s> <s>the resulting translations into propositional satisfiability and its extensions such as maximum satisfiability, satisfiability modulo theories, and answer set programming, enable us to prove the optimality of networks which have been previously found by stochastic search.</s></p></d>", "label": ["<d><p><s>learning chordal markov networks by constraint satisfaction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in large-scale applications of undirected graphical models, such as social networks and biological networks, similar patterns occur frequently and give rise to similar parameters.</s> <s>in this situation, it is beneficial to group the parameters for more efficient learning.</s> <s>we show that even when the grouping is unknown, we can infer these parameter groups during learning via a bayesian approach.</s> <s>we impose a dirichlet process prior on the parameters.</s> <s>posterior inference usually involves calculating intractable terms, and we propose two approximation algorithms, namely a metropolis-hastings algorithm with auxiliary variables and a gibbs sampling algorithm with stripped beta approximation (gibbs_sba).</s> <s>simulations show that both algorithms outperform conventional maximum likelihood estimation (mle).</s> <s>gibbs_sba's performance is close to gibbs sampling with exact likelihood calculation.</s> <s>models learned with gibbs_sba also generalize better than the models learned by mle on real-world senate voting data.</s></p></d>", "label": ["<d><p><s>bayesian estimation of latently-grouped parameters in undirected graphical models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we describe how map inference can be used to sample efficiently from gibbs distributions.</s> <s>specifically, we provide  means for drawing either approximate or unbiased samples from gibbs' distributions by introducing low dimensional perturbations and solving the corresponding map assignments.</s> <s>our approach also leads to new ways to derive lower bounds on partition functions.</s> <s>we demonstrate empirically that our method excels in the typical high signal - high coupling'' regime.</s> <s>the setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. \"</s></p></d>", "label": ["<d><p><s>on sampling from the gibbs distribution with random maximum a-posteriori perturbations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>edml is a recently proposed algorithm for learning parameters in bayesian networks.</s> <s>it was originally derived in terms of approximate inference on a meta-network, which underlies the bayesian approach to parameter estimation.</s> <s>while this initial derivation helped discover edml in the first place and provided a concrete context for identifying some of its properties (e.g., in contrast to em), the formal setting was somewhat tedious in the number of concepts it drew on.</s> <s>in this paper, we propose a greatly simplified perspective on edml, which casts it as a general approach to continuous optimization.</s> <s>the new perspective has several advantages.</s> <s>first, it makes immediate some results that were non-trivial to prove initially.</s> <s>second, it facilitates the design of edml algorithms for new graphical models, leading to a new algorithm for learning parameters in markov networks.</s> <s>we derive this algorithm in this paper, and show, empirically, that it can sometimes learn better estimates from complete data, several times faster than commonly used optimization methods, such as conjugate gradient and l-bfgs.</s></p></d>", "label": ["<d><p><s>edml for learning parameters in directed and undirected graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inference in general ising models is difficult, due to high treewidth making tree-based algorithms intractable.</s> <s>moreover, when interactions are strong, gibbs sampling may take exponential time to converge to the stationary distribution.</s> <s>we present an algorithm to project ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences.</s> <s>we find that gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.</s></p></d>", "label": ["<d><p><s>projecting ising model parameters for fast mixing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of sampling from a probability distribution defined over a high-dimensional discrete set, specified for instance by a graphical model.</s> <s>we propose a sampling algorithm, called paws, based on embedding the set into a higher-dimensional space which is then randomly projected using universal hash functions to a lower-dimensional subspace and explored using combinatorial search methods.</s> <s>our scheme can leverage fast combinatorial optimization tools as a blackbox and, unlike mcmc methods, samples produced are guaranteed to be within an (arbitrarily small) constant factor of the true probability distribution.</s> <s>we demonstrate that by using state-of-the-art combinatorial search tools, paws can efficiently sample from ising grids with strong interactions and from software verification instances, while mcmc and variational methods fail in both cases.</s></p></d>", "label": ["<d><p><s>embed and project: discrete sampling with universal hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a class of algorithms for amortized inference in bayesian networks.</s> <s>in this setting, we invest computation upfront to support rapid online inference for a wide range of queries.</s> <s>our approach is based on learning an inverse factorization of a model's joint distribution: a factorization that turns observations into root nodes.</s> <s>our algorithms accumulate information to estimate the local conditional distributions that constitute such a factorization.</s> <s>these stochastic inverses can be used to invert each of the computation steps leading to an observation, sampling backwards in order to quickly find a likely explanation.</s> <s>we show that estimated inverses converge asymptotically in number of (prior or posterior) training samples.</s> <s>to make use of inverses before convergence, we describe the inverse mcmc algorithm, which uses stochastic inverses to make block proposals for a metropolis-hastings sampler.</s> <s>we explore the efficiency of this sampler for a variety of parameter regimes and bayes nets.</s></p></d>", "label": ["<d><p><s>learning stochastic inverses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector.</s> <s>using a gaussian process prior over the drift as a function of the state vector, we develop an approximate em algorithm to deal with the unobserved, latent dynamics between observations.</s> <s>the posterior over states is approximated by a piecewise linearized process and the map estimation of the drift is facilitated by a sparse gaussian process regression.</s></p></d>", "label": ["<d><p><s>approximate gaussian process inference for the drift function in stochastic differential equations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reliance on computationally expensive algorithms for inference has been limiting the use of bayesian nonparametric models in large scale applications.</s> <s>to tackle this problem, we propose a bayesian learning algorithm for dp mixture models.</s> <s>instead of following the conventional paradigm -- random initialization plus iterative update, we take an progressive approach.</s> <s>starting with a given prior, our method recursively transforms it into an approximate posterior through sequential variational approximation.</s> <s>in this process, new components will be incorporated on the fly when needed.</s> <s>the algorithm can reliably estimate a dp mixture model in one pass, making it particularly suited for applications with massive data.</s> <s>experiments on both synthetic data and real datasets demonstrate remarkable improvement on efficiency -- orders of magnitude speed-up compared to the state-of-the-art.</s></p></d>", "label": ["<d><p><s>online learning of nonparametric mixture models via sequential variational approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational inference algorithms provide the most effective framework for large-scale training  of bayesian nonparametric models.</s> <s>stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima.</s> <s>we present a new algorithm, memoized online variational inference, which scales to very large (yet finite) datasets while avoiding the complexities of stochastic gradient.</s> <s>our algorithm maintains finite-dimensional sufficient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples.</s> <s>exploiting nested families of variational bounds for infinite nonparametric models, we develop principled birth and merge moves allowing non-local optimization.</s> <s>births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed.</s> <s>using dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.</s></p></d>", "label": ["<d><p><s>memoized online variational inference for dirichlet process mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we seek robust policies for uncertain markov decision processes (mdps).</s> <s>most robust optimization approaches for these problems have focussed on the computation of {\\em maximin} policies which maximize the value corresponding to the worst realization of the uncertainty.</s> <s>recent work has proposed {\\em minimax} regret as a suitable alternative to the {\\em maximin} objective for robust optimization.</s> <s>however, existing algorithms for handling {\\em minimax} regret are restricted to models with uncertainty over rewards only.</s> <s>we provide algorithms that employ sampling to improve across multiple dimensions: (a) handle uncertainties over both transition and reward models; (b) dependence of model uncertainties across state, action pairs and decision epochs; (c) scalability and quality bounds.</s> <s>finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature.</s> <s>we also provide a sample average approximation (saa) analysis to compute a posteriori error bounds.</s></p></d>", "label": ["<d><p><s>regret based robust solutions for uncertain markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a markov decision process (mdp) with $n$ states and $m$ actions per state, we study the number of iterations needed by policy iteration (pi) algorithms to converge to the optimal $\\gamma$-discounted optimal policy.</s> <s>we consider two variations of pi: howard's pi that changes the actions in all states with a positive advantage, and simplex-pi that only changes the action in the state with maximal advantage.</s> <s>we show that howard's pi terminates after at most  $ o \\left( \\frac{ n m}{1-\\gamma} \\log \\left( \\frac{1}{1-\\gamma} \\right)\\right) $ iterations, improving by a factor $o(\\log n)$ a result by hansen et al.</s> <s>(2013), while simplex-pi terminates after at most $ o \\left(  \\frac{n^2 m}{1-\\gamma} \\log \\left( \\frac{1}{1-\\gamma} \\right)\\right) $ iterations, improving by a factor $o(\\log n)$ a result by ye (2011).</s> <s>under some structural assumptions of the mdp, we then consider bounds that are independent of the discount factor~$\\gamma$: given a measure of the maximal transient time $\\tau_t$ and the maximal time $\\tau_r$ to revisit states in recurrent classes under all policies, we show that simplex-pi terminates after at most $ \\tilde o \\left( n^3 m^2 \\tau_t \\tau_r \\right) $ iterations.</s> <s>this generalizes a recent result for deterministic mdps by post & ye (2012), in which $\\tau_t \\le n$ and $\\tau_r \\le n$.</s> <s>we explain why similar results seem hard to derive for howard's pi.</s> <s>finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that simplex-pi and howard's pi terminate after at most  $ \\tilde o(nm (\\tau_t+\\tau_r))$ iterations.</s></p></d>", "label": ["<d><p><s>improved and generalized upper bounds on the complexity of policy iteration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system  and as a solution propose optimistic constraint propagation (ocp), an algorithm designed to synthesize efficient exploration and value function generalization.</s> <s>we establish that when the true value function lies within the given hypothesis class, ocp selects optimal actions over all but at most k episodes, where k is the eluder dimension of the given hypothesis class.</s> <s>we establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis space, for  the special case where the hypothesis space is the span of pre-specified indicator functions over disjoint sets.</s></p></d>", "label": ["<d><p><s>efficient exploration and value function generalization in deterministic systems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper addresses the problem of online planning in markov decision processes using only a generative model.</s> <s>we propose a new algorithm which is based on the construction of a forest of single successor state planning trees.</s> <s>for every explored state-action, such a tree contains exactly one successor state, drawn from the generative model.</s> <s>the trees are built using a planning algorithm which follows the optimism in the face of uncertainty principle, in assuming the most favorable outcome in the absence of further information.</s> <s>in the decision making step of the algorithm, the individual trees are combined.</s> <s>we discuss the approach, prove that our proposed algorithm is consistent, and empirically show that it performs better than a related algorithm which additionally assumes the knowledge of all transition distributions.</s></p></d>", "label": ["<d><p><s>aggregating optimistic planning trees for solving markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of online learning in finite episodic markov decision processes where the loss function is allowed to change between episodes.</s> <s>the natural performance measure in this learning problem is the regret defined as the difference between the total loss of the best stationary policy and the total loss suffered by the learner.</s> <s>we assume that the learner is given access to a finite action space $\\a$ and the state space $\\x$ has a layered structure with $l$ layers, so that state transitions are only possible between consecutive layers.</s> <s>we describe a variant of the recently proposed relative entropy policy search algorithm and show that its regret after $t$ episodes is $2\\sqrt{l\\nx\\na t\\log(\\nx\\na/l)}$ in the bandit setting and $2l\\sqrt{t\\log(\\nx\\na/l)}$ in the full information setting.</s> <s>these guarantees largely improve previously known results under much milder assumptions and cannot be significantly improved under general assumptions.</s></p></d>", "label": ["<d><p><s>online learning in episodic markovian decision processes by relative entropy policy search</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of online learning markov decision processes (mdps) when both the transition distributions and loss functions are chosen by an adversary.</s> <s>we present an algorithm that, under a mixing assumption, achieves $o(\\sqrt{t\\log|\\pi|}+\\log|\\pi|)$ regret with respect to a comparison set of policies $\\pi$.</s> <s>the regret is independent of the size of the state and action spaces.</s> <s>when expectations over sample paths can be computed efficiently and the comparison set $\\pi$ has polynomial size, this algorithm is efficient.</s> <s>we also consider the episodic adversarial online shortest path problem.</s> <s>here, in each episode an adversary may choose a weighted directed acyclic graph with an identified start and finish node.</s> <s>the goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to finish node.</s> <s>at the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm.</s> <s>the goal is to minimize regret with respect to a fixed policy for selecting paths.</s> <s>this problem is a special case of the online mdp problem.</s> <s>for randomly chosen graphs and adversarial losses, this problem can be efficiently solved.</s> <s>we show that it also can be efficiently solved for adversarial graphs and randomly chosen losses.</s> <s>when both graphs and losses are adversarially chosen, we present an efficient algorithm whose regret scales linearly with the number of distinct graphs.</s> <s>finally, we show that designing efficient algorithms for the adversarial online shortest path problem (and hence for the adversarial mdp problem) is as hard as learning parity with noise, a notoriously difficult problem that has been used to design efficient cryptographic schemes.</s></p></d>", "label": ["<d><p><s>online learning in markov decision processes with adversarially chosen transition probability distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of online learning in a dynamic setting.</s> <s>we consider a social network in which each individual observes a private signal about the underlying state of the world and communicates with her neighbors at each time period.</s> <s>unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk.</s> <s>we view the scenario as an optimization problem where agents aim to learn the true state while suffering the smallest possible loss.</s> <s>based on the decomposition of the global loss function, we introduce two update mechanisms, each of which generates an estimate of the true state.</s> <s>we establish a tight bound on the rate of change of the underlying state, under which individuals can track the parameter with a bounded variance.</s> <s>then, we characterize explicit expressions for the steady state mean-square deviation(msd) of the estimates from the truth, per individual.</s> <s>we observe that only one of the estimators recovers the optimal msd, which underscores the impact of the objective function decomposition on the learning quality.</s> <s>finally, we provide an upper bound on the regret of the proposed methods, measured as an average of errors in estimating the parameter in a finite time.</s></p></d>", "label": ["<d><p><s>online learning of dynamic parameters in social networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop a probabilistic approach for accurate network modeling using node popularities within the framework of the mixed-membership stochastic blockmodel (mmsb).</s> <s>our model integrates some of the basic properties of nodes in social networks: homophily and preferential connection to popular nodes.</s> <s>we develop a scalable algorithm for posterior inference, based on a novel nonconjugate variant of stochastic variational inference.</s> <s>we evaluate the link prediction accuracy of our algorithm on eight real-world networks with up to 60,000 nodes, and 24 benchmark networks.</s> <s>we demonstrate that our algorithm predicts better than the mmsb.</s> <s>further, using benchmark networks we show that node popularities are essential to achieving high accuracy in the presence of skewed degree distribution and noisy links---both characteristics of real networks.</s></p></d>", "label": ["<d><p><s>modeling overlapping communities with node popularities</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a scalable approach for making inference about latent spaces of large networks.</s> <s>with a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods.</s> <s>when compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction.</s></p></d>", "label": ["<d><p><s>a scalable approach to probabilistic latent space inference of large-scale networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data.</s> <s>to tackle this problem, we propose a relevance topic model\" for jointly learning meaningful mid-level representations upon bag-of-words (bow) video representations and a classifier with sparse weights.</s> <s>in our approach, sparse bayesian learning is incorporated into an undirected topic model (i.e., replicated softmax) to discover topics which are relevant to video classes and suitable for prediction.</s> <s>rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model.</s> <s>an efficient variational em algorithm is presented for model parameter estimation and inference.</s> <s>experimental results on the unstructured social activity attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy, particularly in the case of a very small number of labeled training videos.\"</s></p></d>", "label": ["<d><p><s>relevance topic model for unstructured social group activity recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present sda-bayes, a framework for (s)treaming, (d)istributed, (a)synchronous computation of a bayesian posterior.</s> <s>the framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function.</s> <s>we demonstrate the usefulness of our framework, with variational bayes (vb) as the primitive, by fitting the latent dirichlet allocation model to two large-scale document collections.</s> <s>we demonstrate the advantages of our algorithm over stochastic variational inference (svi), both in the single-pass setting svi was designed for and in the streaming setting, to which svi does not apply.</s></p></d>", "label": ["<d><p><s>streaming variational bayes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>logistic-normal topic models can effectively discover correlation structures among latent topics.</s> <s>however, their inference remains a challenge because of the non-conjugacy between the logistic-normal prior and multinomial topic mixing proportions.</s> <s>existing algorithms either make restricting mean-field assumptions or are not scalable to large-scale applications.</s> <s>this paper presents a partially collapsed gibbs sampling algorithm that approaches the provably correct distribution by exploring the ideas of data augmentation.</s> <s>to improve time efficiency, we further present a parallel implementation that can deal with large-scale applications and learn the correlation structures of thousands of topics from millions of documents.</s> <s>extensive empirical results demonstrate the promise.</s></p></d>", "label": ["<d><p><s>scalable inference for logistic-normal topic models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>overcomplete latent representations have been very popular for unsupervised feature learning in recent years.</s> <s>in this paper, we specify which overcomplete models can be identified given observable moments of a certain order.</s> <s>we consider   probabilistic admixture or topic models  in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary.</s> <s>while   general  overcomplete topic models are not   identifiable, we establish {\\em generic} identifiability under  a  constraint, referred to as  {\\em topic persistence}.</s> <s>our sufficient conditions for identifiability involve a novel set of higher order'' expansion conditions on   the {\\em topic-word matrix} or the {\\em population structure}   of the   model.</s> <s>this set of higher-order expansion conditions allow for overcomplete models, and require  the existence of a perfect  matching from latent topics to higher order observed words.</s> <s>we establish that random structured topic models are identifiable w.h.p.</s> <s>in the overcomplete regime.</s> <s>our identifiability results allow for   general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework.</s> <s>our identifiability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of {\\em tucker} decompositions, but is more general than the {\\em candecomp/parafac} (cp)   decomposition.\"</s></p></d>", "label": ["<d><p><s>when are overcomplete topic models identifiable? uniqueness of tensor tucker decompositions with structured sparsity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while several papers have investigated computationally and statistically efficient methods for learning gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood.</s> <s>in this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic gaussians in high dimensions under small mean separation.</s> <s>if there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efficient procedure.</s> <s>our results provide the first step of a theoretical basis for recent methods that combine feature selection and clustering.</s></p></d>", "label": ["<d><p><s>minimax theory for high-dimensional gaussian mixtures with sparse mean separation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the problem of estimating the cluster tree for a density $f$ supported on or near a smooth $d$-dimensional manifold $m$ isometrically embedded in $\\mathbb{r}^d$.</s> <s>we study a $k$-nearest neighbor based algorithm recently proposed by chaudhuri and dasgupta.</s> <s>under mild assumptions on $f$ and $m$, we obtain rates of convergence that depend on $d$ only but not on the ambient dimension $d$.</s> <s>we also provide a sample complexity lower bound for a natural class of clustering algorithms that use $d$-dimensional neighborhoods.</s></p></d>", "label": ["<d><p><s>cluster trees on manifolds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new class of structured schatten norms for tensors that includes two recently  proposed norms (overlapped'' and \"latent'') for convex-optimization-based  tensor decomposition.</s> <s>based on the properties of the structured schatten norms, we mathematically analyze the performance of \"latent'' approach for tensor decomposition, which was empirically found to perform better than the \"overlapped'' approach in some settings.</s> <s>we show theoretically that this is indeed the case.</s> <s>in particular, when the unknown true tensor is low-rank in a specific mode, this approach performs as well as knowing the mode with the smallest rank.</s> <s>along the way, we show a novel duality result for structures schatten norms, which is also interesting in the general context of structured sparsity.</s> <s>we confirm through  numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.  \"</s></p></d>", "label": ["<d><p><s>convex tensor decomposition via structured schatten norm regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>seriation seeks to reconstruct a linear order between variables using unsorted similarity information.</s> <s>it has direct applications in archeology and shotgun gene sequencing for example.</s> <s>we prove the equivalence between the seriation and the combinatorial 2-sum problem (a quadratic minimization problem over permutations) over a class of similarity matrices.</s> <s>the seriation problem can be solved exactly by a spectral algorithm in the noiseless case and we produce a convex relaxation for the 2-sum problem to improve the robustness of solutions in a noisy setting.</s> <s>this relaxation also allows us to impose additional structural constraints on the solution, to solve semi-supervised seriation problems.</s> <s>we present numerical experiments on archeological data, markov chains and gene sequences.</s></p></d>", "label": ["<d><p><s>convex relaxations for permutation problems</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the problem of matching not just two, but m different sets of objects to each other arises in a variety of contexts, including finding the correspondence between feature points across multiple images in computer vision.</s> <s>at present it is usually solved by matching the sets pairwise, in series.</s> <s>in contrast, we propose a new method, permutation synchronization, which finds all the matchings jointly, in one shot, via a relaxation to eigenvector decomposition.</s> <s>the resulting algorithm is both computationally efficient, and, as we demonstrate with theoretical arguments as well as experimental results, much more stable to noise than previous methods.</s></p></d>", "label": ["<d><p><s>solving the multi-way matching problem by permutation synchronization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision.</s> <s>in consequence, there is need for efficient optimization procedures for submodular functions, in particular for minimization problems.</s> <s>while general submodular minimization is challenging, we propose a new approach that exploits existing decomposability of submodular functions.</s> <s>in contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning.</s> <s>moreover, it is easy to implement and parallelize.</s> <s>a key component of our approach is a formulation of the discrete submodular minimization problem as a continuous best approximation problem.</s> <s>it is solved through a sequence of reflections and its solution can be  automatically thresholded to obtain an optimal discrete solution.</s> <s>our method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction.</s> <s>in our experiments, we show the benefits of our new algorithms for two image segmentation tasks.</s></p></d>", "label": ["<d><p><s>reflection methods for user-friendly submodular optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate three related and important problems connected to machine learning, namely approximating a submodular function everywhere, learning a submodular function (in a pac like setting [26]), and constrained minimization of submodular functions.</s> <s>in all three problems, we provide improved bounds which depend on the ?curvature?</s> <s>of a submodular function and improve on the previously known best results for these problems [9, 3, 7, 25] when the function is not too curved ?</s> <s>a property which is true of many real-world submodular functions.</s> <s>in the former two problems, we obtain these bounds through a generic black-box transformation (which can potentially work for any algorithm), while in the case of submodular minimization, we propose a framework of algorithms which depend on choosing an appropriate surrogate for the submodular function.</s> <s>in all these cases, we provide almost matching lower bounds.</s> <s>while improved curvature-dependent bounds were shown for monotone submodular maximization [4, 27], the existence of similar improved bounds for the aforementioned problems has been open.</s> <s>we resolve this question in this paper by showing that the same notion of curvature provides these improved results.</s> <s>empirical experiments add further support to our claims.</s></p></d>", "label": ["<d><p><s>curvature and optimal algorithms for learning and minimizing submodular functions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>many problems in machine learning can be solved by rounding the solution of an appropriate linear program.</s> <s>we propose a scheme that is based on a quadratic program relaxation which allows us to use parallel stochastic-coordinate-descent to approximately solve large linear programs efficiently.</s> <s>our software is an order of magnitude faster than cplex (a commercial linear programming solver) and yields similar solution quality.</s> <s>our results include a novel perturbation analysis of a quadratic-penalty formulation of linear programming and a convergence result, which we use to derive running time and quality guarantees.</s></p></d>", "label": ["<d><p><s>an approximate, efficient lp solver for lp rounding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans recognize visually-presented objects rapidly and accurately.</s> <s>to understand this ability, we seek to construct models of the ventral stream, the series of cortical areas thought to subserve object recognition.</s> <s>one tool to assess the quality of a model of the ventral stream is the representation dissimilarity matrix (rdm), which uses a set of visual stimuli and measures the distances produced in either the brain (i.e.</s> <s>fmri voxel responses, neural firing rates) or in models (features).</s> <s>previous work has shown that all known models of the ventral stream fail to capture the rdm pattern observed in either it cortex, the highest ventral area, or in the human ventral stream.</s> <s>in this work, we construct models of the ventral stream using a novel optimization procedure for category-level object recognition problems, and produce rdms resembling both macaque it and human ventral stream.</s> <s>the model, while novel in the optimization procedure, further develops a long-standing functional hypothesis that the ventral visual stream is a hierarchically arranged series of processing stages optimized for visual object recognition.</s></p></d>", "label": ["<d><p><s>hierarchical modular optimization of convolutional networks achieves representations similar to macaque it and human ventral stream</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the receptive field (rf) of a sensory neuron describes how the   neuron integrates sensory stimuli over time and space.</s> <s>in typical   experiments with naturalistic or flickering spatiotemporal stimuli,   rfs are very high-dimensional, due to the large number of   coefficients needed to specify an integration profile across time   and space.</s> <s>estimating these coefficients from small amounts of data   poses a variety of challenging statistical and computational   problems.</s> <s>here we address these challenges by developing bayesian   reduced rank regression methods for rf estimation.</s> <s>this corresponds   to modeling the rf as a sum of several space-time separable (i.e.,   rank-1) filters, which proves accurate even for neurons with   strongly oriented space-time rfs.</s> <s>this approach substantially   reduces the number of parameters needed to specify the rf, from   1k-100k down to mere 100s in the examples we consider, and confers   substantial benefits in statistical power and computational   efficiency.</s> <s>in particular, we introduce a novel prior over low-rank   rfs using the restriction of a matrix normal prior to the manifold   of low-rank matrices.</s> <s>we then use a localized'' prior over row and   column covariances to obtain sparse, smooth, localized estimates of   the spatial and temporal rf components.</s> <s>we develop two methods for   inference in the resulting hierarchical model: (1) a fully bayesian   method using blocked-gibbs sampling; and (2) a fast, approximate   method that employs alternating coordinate ascent of the conditional   marginal likelihood.</s> <s>we develop these methods under gaussian and   poisson noise models, and show that low-rank estimates substantially   outperform full rank estimates in accuracy and speed using neural   data from retina and v1.\"</s></p></d>", "label": ["<d><p><s>bayesian inference for low rank spatiotemporal neural receptive fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a set of fast, tractable methods for characterizing neural responses to high-dimensional sensory stimuli using a model we refer to as the generalized quadratic model (gqm).</s> <s>the gqm consists of a low-rank quadratic form followed by a point nonlinearity and exponential-family noise.</s> <s>the quadratic form characterizes the neuron's stimulus selectivity in terms of a set linear receptive fields followed by a quadratic combination rule, and the invertible nonlinearity maps this output to the desired response range.</s> <s>special cases of the gqm include the 2nd-order volterra model (marmarelis and marmarelis 1978, koh and powers 1985) and the elliptical linear-nonlinear-poisson model (park and pillow 2011).</s> <s>here we show that for canonical form\" gqms, spectral decomposition of the first two response-weighted moments yields approximate maximum-likelihood estimators via a quantity called the expected log-likelihood.</s> <s>the resulting theory generalizes moment-based estimators such as the spike-triggered covariance, and, in the gaussian noise case, provides closed-form estimators under a large class of non-gaussian stimulus distributions.</s> <s>we show that these estimators are fast and provide highly accurate estimates with far lower computational cost than full maximum likelihood.</s> <s>moreover, the gqm provides a natural framework for combining multi-dimensional stimulus sensitivity and spike-history dependencies within a single model.</s> <s>we show applications to both analog and spiking data using intracellular recordings of v1 membrane potential and extracellular recordings of retinal spike trains.\"</s></p></d>", "label": ["<d><p><s>spectral methods for neural characterization using generalized quadratic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how does neural population process sensory information?</s> <s>optimal coding theories assume that neural tuning curves are adapted to the prior distribution of the stimulus variable.</s> <s>most of the previous work has discussed optimal solutions for only one-dimensional stimulus variables.</s> <s>here, we expand some of these ideas and present new solutions that define optimal tuning curves for high-dimensional stimulus variables.</s> <s>we consider solutions for a minimal case where the number of neurons in the population is equal to the number of stimulus dimensions (diffeomorphic).</s> <s>in the case of two-dimensional stimulus variables, we analytically derive optimal solutions for different optimal criteria such as minimal l2 reconstruction error or maximal mutual information.</s> <s>for higher dimensional case, the learning rule to improve the population code is provided.</s></p></d>", "label": ["<d><p><s>optimal neural population codes for high-dimensional stimulus variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recordings from large populations of neurons make it possible to search for hypothesized low-dimensional dynamics.</s> <s>finding these dynamics requires models that take into account biophysical constraints and can be fit efficiently and robustly.</s> <s>here, we present an approach to dimensionality reduction for neural data that is convex, does not make strong assumptions about dynamics, does not require averaging over many trials and is extensible to more complex statistical models that combine local and global influences.</s> <s>the results can be combined with spectral methods to learn dynamical systems models.</s> <s>the basic method can be seen as an extension of pca to the exponential family using nuclear norm minimization.</s> <s>we evaluate the effectiveness of this method using an exact decomposition of the bregman divergence that is analogous to variance explained for pca.</s> <s>we show on model data that the parameters of latent linear dynamical systems can be recovered, and that even if the dynamics are not stationary we can still recover the true latent subspace.</s> <s>we also demonstrate an extension of nuclear norm minimization that can separate sparse local connections from global latent dynamics.</s> <s>finally, we demonstrate improved prediction on real neural data from monkey motor cortex compared to fitting linear dynamical models without nuclear norm smoothing.</s></p></d>", "label": ["<d><p><s>robust learning of low-dimensional dynamics from large neural ensembles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a compressed sensing (cs) calcium imaging framework for monitoring large neuronal populations, where we image randomized projections of the spatial calcium concentration at each timestep, instead of measuring the concentration at individual locations.</s> <s>we develop scalable nonnegative deconvolution methods for extracting the neuronal spike time series from such observations.</s> <s>we also address the problem of demixing the spatial locations of the neurons using rank-penalized matrix factorization methods.</s> <s>by exploiting the sparsity of neural spiking we demonstrate that the number of measurements needed per timestep is significantly smaller than the total number of neurons, a result that can potentially enable imaging of larger populations at considerably faster rates compared to traditional raster-scanning techniques.</s> <s>unlike traditional cs setups, our problem involves a block-diagonal sensing matrix and a non-orthogonal sparse basis that spans multiple timesteps.</s> <s>we study the effect of these distinctive features in a noiseless setup using recent results relating conic geometry to cs.</s> <s>we provide tight approximations to the number of measurements needed for perfect deconvolution for certain classes of spiking processes, and show that this number displays a phase transition,\" similar to phenomena observed in more standard cs settings; however, in this case the required measurement rate depends not just on the mean sparsity level but also on other details of the underlying spiking process.\"</s></p></d>", "label": ["<d><p><s>sparse nonnegative deconvolution for compressive calcium imaging: algorithms and phase transitions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we propose a class of efficient generalized method-of-moments(gmm) algorithms for computing parameters of the plackett-luce model, where the data consists of full rankings over alternatives.</s> <s>our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions.</s> <s>we identify conditions for the output of gmm to be unique, and identify a general class of consistent and inconsistent breakings.</s> <s>we then show by theory and experiments that our algorithms run significantly faster than the classical minorize-maximization (mm) algorithm, while achieving competitive statistical efficiency.</s></p></d>", "label": ["<d><p><s>generalized method-of-moments for rank aggregation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a model for demand estimation in multi-agent, differentiated product settings and present an estimation algorithm that uses reversible jump mcmc techniques to classify agents' types.</s> <s>our model extends the popular setup in berry, levinsohn and pakes (1995) to allow for the data-driven classification of agents' types using agent-level data.</s> <s>we focus on applications involving data on agents' ranking over alternatives, and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior.</s> <s>results on both real and simulated data provide support for the scalability of our approach.</s></p></d>", "label": ["<d><p><s>generalized random utility models with multiple types</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in standard matrix completion theory, it is required to have at least $o(n\\ln^2 n)$ observed entries to perfectly recover a low-rank matrix $m$ of size $n\\times n$, leading to a large number of observations when $n$ is large.</s> <s>in many real tasks, side information in addition to the observed entries is often available.</s> <s>in this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries.</s> <s>we show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix $m$ can be dramatically reduced to $o(\\ln n)$.</s> <s>we demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning.</s></p></d>", "label": ["<d><p><s>speedup matrix completion with side information: application to multi-label learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents correlated nystrom views (xnv), a fast semi-supervised algorithm for regression and classification.</s> <s>the algorithm draws on two main ideas.</s> <s>first, it generates two views consisting of computationally inexpensive random features.</s> <s>second, multiview regression, using canonical correlation analysis (cca) on unlabeled data, biases the regression towards useful features.</s> <s>it has been shown that cca regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators.</s> <s>recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views.</s> <s>we show that xnv consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude.</s></p></d>", "label": ["<d><p><s>correlated random features for fast semi-supervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>label propagation is one of the state-of-the-art methods for semi-supervised learning, which estimates labels by propagating label information through a graph.</s> <s>label propagation assumes that data points (nodes) connected in a graph should have similar labels.</s> <s>consequently, the label estimation heavily depends on edge weights in a graph which represent similarity of each node pair.</s> <s>we propose a method for a graph to capture the manifold structure of input features using edge weights parameterized by a similarity function.</s> <s>in this approach, edge weights represent both similarity and local reconstruction weight simultaneously, both being reasonable for label propagation.</s> <s>for further justification, we provide analytical considerations including an interpretation as a cross-validation of a propagation model in the feature space, and an error analysis based on a low dimensional manifold model.</s> <s>experimental results demonstrated the effectiveness of our approach both in synthetic and real datasets.</s></p></d>", "label": ["<d><p><s>manifold-based similarity adaptation for label propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a new and computationally efficient framework for learning sparse models.</s> <s>we formulate a unified approach that contains as particular cases models promoting sparse synthesis and analysis type of priors, and mixtures thereof.</s> <s>the supervised training of the proposed model is formulated as a bilevel optimization problem, in which the operators are optimized to achieve the best possible performance on a specific task, e.g., reconstruction or classification.</s> <s>by restricting the operators to be shift invariant, our approach can be thought as a way of learning analysis+synthesis sparsity-promoting convolutional operators.</s> <s>leveraging recent ideas on fast trainable regressors designed to approximate exact sparse codes, we propose a way of constructing feed-forward neural networks capable of approximating the learned models at a fraction of the computational cost of exact solvers.</s> <s>in the shift-invariant case, this leads to a principled way of constructing task-specific convolutional networks.</s> <s>we illustrate the proposed models on several experiments in music analysis and image processing applications.</s></p></d>", "label": ["<d><p><s>supervised sparse analysis and synthesis operators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise.</s> <s>it is well known that standard computationally tractable sparse recovery algorithms, such as the lasso, omp, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns.</s> <s>we develop a simple greedy algorithm, called swap, that iteratively swaps variables until a desired loss function cannot be decreased any further.</s> <s>swap is surprisingly effective in handling measurement matrices with high correlations.</s> <s>we prove that swap can be easily used as a wrapper around standard sparse recovery algorithms for improved performance.</s> <s>we theoretically quantify the statistical guarantees of swap and complement our analysis with numerical results on synthetic and real data.</s></p></d>", "label": ["<d><p><s>when in doubt, swap: high-dimensional sparse recovery from correlated measurements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>automatic music recommendation has become an increasingly relevant problem in recent years, since a lot of music is now sold and consumed digitally.</s> <s>most recommender systems rely on collaborative filtering.</s> <s>however, this approach suffers from the cold start problem: it fails when no usage data is available, so it is not effective for recommending new and unpopular songs.</s> <s>in this paper, we propose to use a latent factor model for recommendation, and predict the latent factors from music audio when they cannot be obtained from usage data.</s> <s>we compare a traditional approach using a bag-of-words representation of the audio signals with deep convolutional neural networks, and evaluate the predictions quantitatively and qualitatively on the million song dataset.</s> <s>we show that using predicted latent factors produces sensible recommendations, despite the fact that there is a large semantic gap between the characteristics of a song that affect user preference and the corresponding audio signal.</s> <s>we also show that recent advances in deep learning translate very well to the music recommendation setting, with deep convolutional neural networks significantly outperforming the traditional approach.</s></p></d>", "label": ["<d><p><s>deep content-based music recommendation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel class of algorithms for low rank matrix completion.</s> <s>our approach builds on novel penalty functions on the singular values of the low rank matrix.</s> <s>by exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an expectation-maximization algorithm to obtain a maximum a posteriori estimate of the completed low rank matrix.</s> <s>the resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values.</s> <s>the algorithm is simple to implement and can scale to large matrices.</s> <s>we provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion.</s></p></d>", "label": ["<d><p><s>probabilistic low-rank matrix completion with adaptive spectral regularization algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems.</s> <s>in many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase.</s> <s>for instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them.</s> <s>in this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems.</s> <s>more specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to ?share?</s> <s>signals (contexts and payoffs) with the neghboring nodes.</s> <s>we then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes.</s> <s>we experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information.</s> <s>our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.</s></p></d>", "label": ["<d><p><s>a gang of bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many natural settings, the analysis goal is not to characterize a single data set in isolation, but rather to understand the difference between one set of observations and another.</s> <s>for example, given a background corpus of news articles together with writings of a particular author, one may want a topic model that explains word patterns and themes specific to the author.</s> <s>another example comes from genomics, in which biological signals may be collected from different regions of a genome, and one wants a model that captures the differential statistics observed in these regions.</s> <s>this paper formalizes this notion of contrastive learning for  mixture models, and develops spectral algorithms for inferring mixture components specific to a foreground data set when contrasted with a background data set.</s> <s>the method builds on recent moment-based estimators and tensor decompositions for latent variable models, and has the intuitive feature of using background data statistics to appropriately modify moments estimated from foreground data.</s> <s>a key advantage of the method is that the background data need only be coarsely modeled, which is important when the background is too complex, noisy, or not of interest.</s> <s>the method is demonstrated on applications in contrastive topic modeling and genomic sequence analysis.</s></p></d>", "label": ["<d><p><s>contrastive learning using spectral methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determinantal point process (dpp) has gained much popularity for modeling sets of diverse items.</s> <s>the gist of dpp is that the probability of choosing a particular set of items is proportional to the determinant of a positive definite matrix that defines the similarity of those items.</s> <s>however, computing the determinant requires time cubic in the number of items, and is hence impractical for large sets.</s> <s>in this paper, we address this problem by constructing a rapidly mixing markov chain, from which we can acquire a sample from the given dpp in sub-cubic time.</s> <s>in addition, we show that this framework can be extended to sampling from cardinality-constrained dpps.</s> <s>as an application, we show how our sampling algorithm can be used to provide a fast heuristic for determining the number of clusters, resulting in better clustering.</s></p></d>", "label": ["<d><p><s>fast determinantal point process sampling with application to clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing the stationary distribution of a large finite or countably infinite state space markov chain (mc) has become central in many problems such as statistical inference and network analysis.</s> <s>standard methods involve large matrix multiplications as in power iteration, or simulations of long random walks to sample states from the stationary distribution, as in markov chain monte carlo (mcmc).</s> <s>however these methods are computationally costly; either they involve operations at every state or they scale (in computation time) at least linearly in the size of the state space.</s> <s>in this paper, we provide a novel algorithm that answers whether a chosen state in a mc has stationary probability larger than some $\\delta \\in (0,1)$.</s> <s>if so, it estimates the stationary probability.</s> <s>our algorithm uses information from a local neighborhood of the state on the graph induced by the mc, which has constant size relative to the state space.</s> <s>we provide correctness and convergence guarantees that depend on the algorithm parameters and mixing properties of the mc.</s> <s>simulation results show mcs for which this method gives tight estimates.</s></p></d>", "label": ["<d><p><s>computing the stationary distribution locally</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer's value distribution for a good when the buyer is repeatedly interacting with a seller through a posted-price mechanism.</s> <s>we model the buyer as a strategic agent, whose goal is to maximize her long-term surplus, and we are interested in mechanisms that maximize the seller's long-term revenue.</s> <s>we present seller algorithms that are no-regret when the buyer discounts her future surplus --- i.e.</s> <s>the buyer prefers showing advertisements to users sooner rather than later.</s> <s>we also give a lower bound on regret that increases as the buyer's discounting weakens and shows, in particular, that any seller algorithm will suffer linear regret if there is no discounting.</s></p></d>", "label": ["<d><p><s>learning prices for repeated auctions with strategic buyers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study differentially private mechanisms for answering \\emph{smooth} queries on databases consisting of data points in $\\mathbb{r}^d$.</s> <s>a $k$-smooth query is specified by a function whose partial derivatives up to order $k$ are all bounded.</s> <s>we develop an $\\epsilon$-differentially private mechanism which for the class of $k$-smooth queries has accuracy $o (\\left(\\frac{1}{n}\\right)^{\\frac{k}{2d+k}}/\\epsilon)$.</s> <s>the mechanism first outputs a summary of the database.</s> <s>to obtain an answer of a query, the user runs a public evaluation algorithm which contains no information of the database.</s> <s>outputting the summary runs in time $o(n^{1+\\frac{d}{2d+k}})$, and the evaluation algorithm for answering a query runs in time $\\tilde o (n^{\\frac{d+2+\\frac{2d}{k}}{2d+k}} )$.</s> <s>our mechanism is based on $l_{\\infty}$-approximation of (transformed) smooth functions by low degree even trigonometric polynomials with small and efficiently computable coefficients.</s></p></d>", "label": ["<d><p><s>efficient algorithm for privately releasing smooth queries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a general technique for making online learning algorithms differentially private, in both the full information and bandit settings.</s> <s>our technique applies to algorithms that aim to minimize a \\emph{convex} loss function which is a sum of smaller convex loss terms, one for each data point.</s> <s>we modify the popular \\emph{mirror descent} approach, or rather a variant called \\emph{follow the approximate leader}.</s> <s>the technique leads to the first nonprivate algorithms for private online learning in the bandit setting.</s> <s>in the full information setting, our algorithms improve over the regret bounds of previous work.</s> <s>in many cases, our algorithms (in both settings) matching the dependence on the input length, $t$, of the \\emph{optimal nonprivate} regret bounds up to logarithmic factors in $t$.</s> <s>our algorithms require logarithmic space and update time.</s></p></d>", "label": ["<d><p><s>(nearly) optimal algorithms for private online learning in full-information and bandit settings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a detailed study of the estimation of probability distributions---discrete and continuous---in a stringent setting in which data is kept private even from the statistician.</s> <s>we give sharp minimax rates of convergence for estimation in these locally private settings, exhibiting fundamental tradeoffs between privacy and convergence rate, as well as providing tools to allow movement along the privacy-statistical efficiency continuum.</s> <s>one of the consequences of our results is that warner's classical work on randomized response is an optimal way to perform survey sampling while maintaining privacy of the respondents.</s></p></d>", "label": ["<d><p><s>local privacy and minimax bounds: sharp rates for probability estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>differential privacy is a cryptographically motivated definition of privacy which has gained considerable attention in the algorithms, machine-learning and data-mining communities.</s> <s>while there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application.</s> <s>in this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric.</s> <s>the training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over.</s> <s>we apply our generic procedure to two fundamental tasks in statistics and machine-learning -- training a regularized linear classifier and building a histogram density estimator that result in end-to-end differentially private solutions for these problems.</s></p></d>", "label": ["<d><p><s>a stability-based validation procedure for differentially private machine learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>measuring similarity is crucial to many learning tasks.</s> <s>it is also a richer and broader notion than what most metric learning algorithms can model.</s> <s>for example, similarity can arise from the process of aggregating the decisions of multiple latent components, where each latent component compares data in its own way by focusing on a different subset of features.</s> <s>in this paper, we propose similarity component analysis (sca), a probabilistic graphical model that discovers those latent components from data.</s> <s>in sca, a latent component generates a local similarity value, computed with its own metric, independently of other components.</s> <s>the final similarity measure is then obtained by combining the local similarity values with a (noisy-)or gate.</s> <s>we derive an em-based algorithm for fitting the model parameters with similarity-annotated data from pairwise comparisons.</s> <s>we validate the sca model on synthetic datasets where sca discovers the ground-truth about the latent components.</s> <s>we also apply sca to a multiway classification task and a link prediction task.</s> <s>for both tasks, sca attains significantly better prediction accuracies than competing methods.</s> <s>moreover, we show how sca can be instrumental in exploratory analysis of data, where we gain insights about the data by examining patterns hidden in its latent components' local similarity values.</s></p></d>", "label": ["<d><p><s>similarity component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a novel approach for computing collision-free \\emph{global} trajectories for $p$ agents with specified initial and final configurations, based on an improved version of the alternating direction method of multipliers (admm) algorithm.</s> <s>compared with existing methods, our approach is naturally parallelizable and allows for incorporating different cost functionals with only minor adjustments.</s> <s>we apply our method to classical challenging instances and observe that its computational requirements scale well with $p$ for several cost functionals.</s> <s>we also show that a specialization of our algorithm can be used for {\\em local} motion planning by solving the problem of joint optimization in velocity space.</s></p></d>", "label": ["<d><p><s>a message-passing algorithm for multi-agent trajectory planning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when approximating binary similarity using the hamming distance between short binary hashes, we shown that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps.</s> <s>i.e.~by approximating the similarity between $x$ and $x'$ as the hamming distance between $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as the hamming distance between $f(x)$ and $f(x')$.</s></p></d>", "label": ["<d><p><s>the power of asymmetry in binary hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces.</s> <s>we employ a vp-tree and explore two simple yet effective learning-to prune approaches: density estimation through sampling and ?stretching?</s> <s>of the triangle inequality.</s> <s>both methods are evaluated using data sets with metric (euclidean) and non-metric (kl-divergence and itakura-saito) distance functions.</s> <s>conditions on spaces where the vp-tree is applicable are discussed.</s> <s>the vp-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (lsh), and permutation methods.</s> <s>our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.</s></p></d>", "label": ["<d><p><s>learning to prune in metric and non-metric spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents).</s> <s>the matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space.</s> <s>this schema, although proven successful on a range of matching tasks, is insufficient for capturing the rich structure in the matching process of more complicated objects.</s> <s>in this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains.</s> <s>more specifically, we apply this model to matching tasks in natural language, e.g., finding sensible responses for a tweet, or  relevant answers to a given question.</s> <s>this new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models.</s></p></d>", "label": ["<d><p><s>a deep architecture for matching short texts</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper examines the question: what kinds of distributions can be efficiently represented by restricted boltzmann machines (rbms)?</s> <s>we characterize the rbm's unnormalized log-likelihood function as a type of neural network (called an rbm network), and through a series of simulation results relate these networks to types that are better understood.</s> <s>we show the surprising result that rbm networks can efficiently compute any function that depends on the number of 1's in the input, such as parity.</s> <s>we also provide the first known example of a particular type of distribution which provably cannot be efficiently represented by an rbm (or equivalently, cannot be efficiently computed by an rbm network), assuming a realistic exponential upper bound on the size of the weights.</s> <s>by formally demonstrating that a relatively simple distribution cannot be represented efficiently by an rbm our results provide a new rigorous justification for the use of potentially more expressive generative models, such as deeper ones.</s></p></d>", "label": ["<d><p><s>on the representational efficiency of restricted boltzmann machines</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the recently introduced continuous skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.</s> <s>in this paper we present several improvements that make the skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.</s> <s>we show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks.</s> <s>we also introduce negative sampling, a simplified variant of noise contrastive estimation (nce) that learns more accurate vectors for frequent words compared to the hierarchical softmax.</s> <s>an inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.</s> <s>for example, the meanings of canada'' and \"air'' cannot be easily combined to obtain \"air canada''.</s> <s>motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the skip-gram model. \"</s></p></d>", "label": ["<d><p><s>distributed representations of words and phrases and their compositionality</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised.</s> <s>unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and rbms, because they involve a reconstruction step where the whole input vector is predicted from the current feature values.</s> <s>an algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling  scheme stochastically selecting which input elements to actually reconstruct during training for each particular example.</s> <s>to generalize this idea to rbms, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme.</s> <s>we show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 newsgroups and rcv1), while keeping computational cost linear in the number of non-zeros.</s></p></d>", "label": ["<d><p><s>stochastic ratio matching of rbms for sparse high-dimensional inputs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data generating density, in the case where the corruption noise is gaussian, the reconstruction error is the squared error, and the data is continuous-valued.</s> <s>this has led to various proposals for sampling from this implicitly learned density function, using langevin and metropolis-hastings mcmc.</s> <s>however, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors.</s> <s>another issue is the mathematical justification which is only valid in the limit of small corruption noise.</s> <s>we propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).</s></p></d>", "label": ["<d><p><s>generalized denoising auto-encoders as generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the multi-prediction deep boltzmann machine (mp-dbm).</s> <s>the mp-dbm can be seen as a single probabilistic model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent nets that share parameters and approximately solve different inference problems.</s> <s>prior methods of training dbms either do not perform well on classification tasks or require an initial learning pass that trains the dbm greedily, one layer at a time.</s> <s>the mp-dbm does not require greedy layerwise pretraining, and outperforms the standard dbm at classification, classification with missing inputs, and mean field prediction tasks.</s></p></d>", "label": ["<d><p><s>multi-prediction deep boltzmann machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we demonstrate that there is significant redundancy in the parameterization of   several deep learning models.</s> <s>given only a few weight values for each feature   it is possible to accurately predict the remaining values.</s> <s>moreover, we show   that not only can the parameter values be predicted, but many of them need not   be learned at all.</s> <s>we train several different architectures by learning only   a small number of weights and predicting the rest.</s> <s>in the best case we are   able to predict more than 95% of the weights of a network without any drop in   accuracy.</s></p></d>", "label": ["<d><p><s>predicting parameters in deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multilayer perceptrons (mlps) or neural networks are popular models used for nonlinear regression and classification tasks.</s> <s>as regressors, mlps model the conditional distribution of the predictor variables y given the input variables x.</s> <s>however, this predictive distribution is assumed to be unimodal (e.g.</s> <s>gaussian).</s> <s>for tasks such as structured prediction problems, the conditional distribution should be multimodal, forming one-to-many mappings.</s> <s>by using stochastic hidden variables rather than deterministic ones, sigmoid belief nets (sbns) can induce a rich multimodal distribution in the output space.</s> <s>however, previously proposed learning algorithms for sbns are very slow and do not work well for real-valued data.</s> <s>in this paper, we propose a stochastic feedforward network with hidden layers having \\emph{both deterministic and stochastic} variables.</s> <s>a new generalized em training procedure using importance sampling allows us to efficiently learn complicated conditional distributions.</s> <s>we demonstrate the superiority of our model to conditional restricted boltzmann machines and mixture density networks on synthetic datasets and on modeling facial expressions.</s> <s>moreover, we show that latent features of our model improves classification and provide additional qualitative results on color images.</s></p></d>", "label": ["<d><p><s>learning stochastic feedforward neural networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this work introduces a model that can recognize objects in images even if no training data is available for the object class.</s> <s>the only necessary knowledge about unseen categories comes from unsupervised text corpora.</s> <s>unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of objects, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes.</s> <s>this is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like.</s> <s>our deep learning model does not require any manually defined semantic or visual features for either words or images.</s> <s>images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class.</s> <s>then, a separate recognition model can be employed for each type.</s> <s>we demonstrate two strategies, the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.</s></p></d>", "label": ["<d><p><s>zero-shot learning through cross-modal transfer</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a common problem in knowledge representation and related fields is reasoning over a large joint knowledge graph, represented as triples of a relation between two entities.</s> <s>the goal of this paper is to develop a more powerful neural network model suitable for inference over these relationships.</s> <s>previous models suffer from weak interaction between entities or simple linear projection of the vector space.</s> <s>we address these problems by introducing a neural tensor network (ntn) model which allow the entities and relations to interact multiplicatively.</s> <s>additionally, we observe that such knowledge base models can be further improved by representing each entity as the average of vectors for the words in the entity name, giving an additional dimension of similarity by which entities can share statistical strength.</s> <s>we assess the model by considering the problem of predicting additional true relations between entities given a partial knowledge base.</s> <s>our model outperforms previous models and can classify unseen relationships in wordnet and freebase with an accuracy of 86.2% and 90.0%, respectively.</s></p></d>", "label": ["<d><p><s>reasoning with neural tensor networks for knowledge base completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a way of improving classification performance for classes which have very few training examples.</s> <s>the key idea is to discover classes which are similar and transfer knowledge among them.</s> <s>our method organizes the classes into a tree hierarchy.</s> <s>the tree structure can be used to impose a generative prior over classification parameters.</s> <s>we show that these priors can be combined with discriminative models such as deep neural networks.</s> <s>our method benefits from the power of discriminative training of deep neural networks, at the same time using tree-based generative priors over classification parameters.</s> <s>we also propose an algorithm for learning the underlying tree structure.</s> <s>this gives the model some flexibility to tune the tree so that the tree is pertinent to task being solved.</s> <s>we show that the model can transfer knowledge across related classes using fixed semantic trees.</s> <s>moreover, it can learn new meaningful trees usually leading to improved performance.</s> <s>our method achieves state-of-the-art classification results on the cifar-100 image data set and the mir flickr multimodal data set.</s></p></d>", "label": ["<d><p><s>discriminative transfer learning with tree-based priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stacked sparse denoising auto-encoders (ssdas) have recently been shown to be successful at removing noise from corrupted images.</s> <s>however, like most denoising techniques, the ssda is not robust to variation in noise types beyond what it has seen during training.</s> <s>we present the multi-column stacked sparse denoising autoencoder, a novel technique of combining multiple ssdas into a multi-column ssda (mc-ssda) by combining the outputs of each ssda.</s> <s>we eliminate the need to determine the type of noise, let alone its statistics, at test time.</s> <s>we show that good denoising performance can be achieved with a single system on a variety of different noise types, including ones not seen in the training set.</s> <s>additionally, we experimentally demonstrate the efficacy of mc-ssda denoising by achieving mnist digit error rates on denoised images at close to that of the uncorrupted images.</s></p></d>", "label": ["<d><p><s>adaptive multi-column deep neural networks with application to robust image denoising</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>designing a principled and effective algorithm for learning deep architectures is a challenging problem.</s> <s>the current approach involves two training phases: a fully unsupervised learning followed by a strongly discriminative optimization.</s> <s>we suggest a deep learning strategy that bridges the gap between the two phases, resulting in a three-phase learning procedure.</s> <s>we propose to implement the scheme using a method to regularize deep belief networks with top-down information.</s> <s>the network is constructed from building blocks of restricted boltzmann machines learned by combining bottom-up and top-down sampled signals.</s> <s>a global optimization procedure that merges samples from a forward bottom-up pass and a top-down pass is used.</s> <s>experiments on the mnist dataset show improvements over the existing algorithms for deep belief networks.</s> <s>object recognition results on the caltech-101 dataset also yield competitive results.</s></p></d>", "label": ["<d><p><s>top-down regularization of deep belief networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, it was shown that by dropping out hidden activities with a probability of 0.5, deep neural networks can perform very well.</s> <s>we describe a model in which a binary belief network is overlaid on a neural network and is used to decrease the information content of its hidden units by selectively setting activities to zero.</s> <s>this ''dropout network can be trained jointly with the neural network by approximately computing local expectations of binary dropout variables, computing derivatives using back-propagation, and using stochastic gradient descent.</s> <s>interestingly, experiments show that the learnt dropout network parameters recapitulate the neural network parameters, suggesting that a good dropout network regularizes activities according to magnitude.</s> <s>when evaluated on the mnist and norb datasets, we found our method can be used to achieve lower classification error rates than other feather learning methods, including standard dropout, denoising auto-encoders, and restricted boltzmann machines.</s> <s>for example, our model achieves 5.8% error on the norb test set, which is better than state-of-the-art results obtained using convolutional architectures. \"</s></p></d>", "label": ["<d><p><s>adaptive dropout for training deep neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study pca as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as matrix stochastic gradient'' (msg), as well as a practical variant, capped msg.</s> <s>we study the method both theoretically and empirically. \"</s></p></d>", "label": ["<d><p><s>stochastic optimization of pca with capped msg</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient optimization is a class of widely used algorithms for training machine learning models.</s> <s>to optimize an objective, it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset.</s> <s>however, when the variance of the noisy gradient is large, the algorithm might spend much time bouncing around, leading to slower convergence and worse performance.</s> <s>in this paper, we develop a general approach of using control variate for variance reduction in stochastic gradient.</s> <s>data statistics such as low-order moments (pre-computed or estimated online) is used to form the control variate.</s> <s>we demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization.</s> <s>one is convex---the map estimation for logistic regression, and the other is non-convex---stochastic variational inference for latent dirichlet allocation.</s> <s>on both problems, our approach shows faster convergence and better performance than the classical approach.</s></p></d>", "label": ["<d><p><s>variance reduction for stochastic gradient optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider streaming, one-pass principal component analysis (pca), in the high-dimensional regime, with limited memory.</s> <s>here, $p$-dimensional samples are presented sequentially, and the goal is to produce the $k$-dimensional subspace that best approximates these points.</s> <s>standard algorithms require $o(p^2)$ memory; meanwhile no algorithm can do better than $o(kp)$ memory, since this is what the output itself requires.</s> <s>memory (or storage) complexity is most meaningful when understood in the context of computational and sample complexity.</s> <s>sample complexity for high-dimensional pca is typically studied in the setting of the {\\em spiked covariance model}, where $p$-dimensional points are generated from a population covariance equal to the identity (white noise) plus a low-dimensional perturbation (the spike) which is the signal to be recovered.</s> <s>it is now well-understood that the spike can be recovered when the number of samples, $n$, scales proportionally with the dimension, $p$.</s> <s>yet, all algorithms that provably achieve this, have memory complexity $o(p^2)$.</s> <s>meanwhile, algorithms with memory-complexity $o(kp)$ do not have provable bounds on sample complexity comparable to $p$.</s> <s>we present an algorithm that achieves both: it uses $o(kp)$ memory (meaning storage of any kind) and is able to compute the $k$-dimensional spike with $o(p \\log p)$ sample-complexity -- the first algorithm of its kind.</s> <s>while our theoretical analysis focuses on the spiked covariance model, our simulations show that our algorithm is successful on much more general models for the data.</s></p></d>", "label": ["<d><p><s>memory limited, streaming pca</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of independently sampling $s$ non-zero entries of a matrix $a$ in order to produce a sparse sketch of it, $b$, that minimizes $\\|a-b\\|_2$.</s> <s>for large $m \\times n$ matrices, such that $n \\gg m$ (for example, representing $n$ observations over $m$ attributes) we give  distributions exhibiting four important properties.</s> <s>first, they have closed forms for the probability of sampling each item which are computable from minimal information regarding $a$.</s> <s>second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with $o(1)$ computation per non-zero.</s> <s>third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible.</s> <s>lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution.</s> <s>note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix.</s> <s>therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.</s></p></d>", "label": ["<d><p><s>near-optimal entrywise sampling for data matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of sparse precision matrix estimation in high dimensions using the clime estimator, which has several desirable theoretical properties.</s> <s>we present an inexact alternating direction method of multiplier (admm) algorithm for clime, and establish rates of convergence for both the objective and optimality conditions.</s> <s>further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores.</s> <s>the proposed framework solves clime in column-blocks and only involves elementwise operations and parallel matrix multiplications.</s> <s>we evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies.</s> <s>experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores.</s></p></d>", "label": ["<d><p><s>large scale distributed sparse precision estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>research on distributed machine learning algorithms has focused primarily on one of two extremes---algorithms that obey strict concurrency constraints or algorithms that obey few or no such constraints.</s> <s>we consider an intermediate alternative in which algorithms optimistically assume that conflicts are unlikely and if conflicts do arise a conflict-resolution protocol is invoked.</s> <s>we view this optimistic concurrency control'' paradigm as particularly appropriate for large-scale machine learning algorithms, particularly in the unsupervised setting.</s> <s>we demonstrate our approach in three problem areas: clustering, feature learning and online facility location.</s> <s>we evaluate  our methods via large-scale experiments in a cluster computing environment.  \"</s></p></d>", "label": ["<d><p><s>optimistic concurrency control for distributed unsupervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many large-scale machine learning problems (such as clustering, non-parametric learning, kernel machines, etc.)</s> <s>require selecting, out of a massive data set, a manageable, representative subset.</s> <s>such problems can often be reduced to maximizing a submodular set function subject to cardinality constraints.</s> <s>classical approaches require centralized access to the full data set; but for truly large-scale problems, rendering the data centrally is often impractical.</s> <s>in this paper, we consider the problem of submodular function maximization in a distributed fashion.</s> <s>we develop a simple, two-stage protocol greedi, that is easily implemented using mapreduce style computations.</s> <s>we theoretically analyze our approach, and show, that under certain natural conditions, performance close to the (impractical) centralized approach can be achieved.</s> <s>in our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including sparse gaussian process inference on tens of millions of examples using hadoop.</s></p></d>", "label": ["<d><p><s>distributed submodular maximization: identifying representative elements in massive data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work, we propose a general method for recovering low-rank three-order tensors, in which the data can be deformed by some unknown transformation and corrupted by arbitrary sparse errors.</s> <s>since the unfolding matrices of a tensor are interdependent, we introduce auxiliary variables and relax the hard equality constraints by the augmented lagrange multiplier method.</s> <s>to improve the computational efficiency, we introduce a proximal gradient step to the alternating direction minimization method.</s> <s>we have provided proof for the convergence of the linearized version of the problem which is the inner loop of the overall algorithm.</s> <s>both simulations and experiments show that our methods are more efficient and effective than previous work.</s> <s>the proposed method can be easily applied to simultaneously  rectify and align multiple images or videos frames.</s> <s>in this context, the state-of-the-art algorithms rasl'' and \"tilt'' can be viewed as two special cases of our work, and yet each only performs part of the function of our method.\"</s></p></d>", "label": ["<d><p><s>simultaneous rectification and alignment via robust recovery of low-rank tensors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>phase retrieval problems involve solving linear equations, but with missing sign (or phase, for complex numbers) information.</s> <s>over the last two decades, a popular generic empirical approach to the many variants of this problem has been one of alternating minimization; i.e.</s> <s>alternating between estimating the missing phase information, and the candidate solution.</s> <s>in this paper, we show that a simple alternating minimization algorithm geometrically converges to the solution of one such problem -- finding a vector $x$ from $y,a$, where $y = |a'x|$ and $|z|$ denotes a vector of element-wise magnitudes of $z$ -- under the assumption that $a$ is gaussian.</s> <s>empirically, our algorithm performs similar to recently proposed convex techniques for this variant (which are based on lifting\" to a convex matrix problem) in sample complexity and robustness to noise.</s> <s>however, our algorithm is much more efficient and can scale to large problems.</s> <s>analytically, we show geometric convergence to the solution, and sample complexity that is off by log factors from obvious lower bounds.</s> <s>we also establish close to optimal scaling for the case when the unknown vector is sparse.</s> <s>our work represents the only known proof of alternating minimization for any variant of phase retrieval problems in the non-convex setting.\"</s></p></d>", "label": ["<d><p><s>phase retrieval using alternating minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>what if there is a teacher who knows the learning goal and wants to design good training data for a machine learner?</s> <s>we propose an optimal teaching framework aimed at learners who employ bayesian models.</s> <s>our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher.</s> <s>this optimization problem is in general hard.</s> <s>in the case where the learner employs conjugate exponential family models, we present an approximate algorithm for finding the optimal teaching set.</s> <s>our algorithm optimizes the aggregate sufficient statistics, then unpacks them into actual teaching examples.</s> <s>we give several examples to illustrate our framework.</s></p></d>", "label": ["<d><p><s>machine teaching for bayesian learners in the exponential family</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sampling inference methods are computationally difficult to scale for many models in part because global dependencies can reduce opportunities for parallel computation.</s> <s>without strict conditional independence structure among variables, standard gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong.</s> <s>empirical work has shown that some models can be sampled effectively by going hogwild'' and simply running gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood.</s> <s>as a step towards such an understanding, we study the hogwild gibbs sampling strategy in the context of gaussian distributions.</s> <s>we develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra.</s> <s>in particular, we show that if the gaussian precision matrix is generalized diagonally dominant, then any hogwild gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean. \"</s></p></d>", "label": ["<d><p><s>analyzing hogwild parallel gaussian gibbs sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>learning the joint dependence of discrete variables is a fundamental problem in machine learning, with many applications including prediction, clustering and dimensionality reduction.</s> <s>more recently, the framework of copula modeling has gained popularity due to its modular parametrization of joint distributions.</s> <s>among other properties, copulas provide a recipe for combining flexible models for univariate marginal distributions with parametric families suitable for potentially high dimensional dependence structures.</s> <s>more radically, the extended rank likelihood approach of hoff (2007) bypasses learning marginal models completely when such information is ancillary to the learning task at hand as in, e.g., standard dimensionality reduction problems or copula parameter estimation.</s> <s>the main idea is to represent data by their observable rank statistics, ignoring any other information from the marginals.</s> <s>inference is typically done in a bayesian framework with gaussian copulas, and it is complicated by the fact this implies sampling within a space where the number of constraints increase quadratically with the number of data points.</s> <s>the result is slow mixing when using off-the-shelf gibbs sampling.</s> <s>we present an efficient algorithm based on recent advances on constrained hamiltonian markov chain monte carlo that is simple to implement and does not require paying for a quadratic cost in sample size.</s></p></d>", "label": ["<d><p><s>flexible sampling of discrete data correlations without the marginal distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new approach to sample from generic binary distributions, based on an exact  hamiltonian monte carlo algorithm applied to a piecewise continuous augmentation  of the binary distribution of interest.</s> <s>an extension of this idea to distributions over mixtures of binary and continuous variables allows us to sample from posteriors of   linear and probit regression models with spike-and-slab priors and truncated parameters.</s> <s>we illustrate the advantages of these algorithms in several examples in which they outperform the metropolis or gibbs samplers.</s></p></d>", "label": ["<d><p><s>auxiliary-variable exact hamiltonian monte carlo samplers for binary distributions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>an increasing number of applications require processing of signals defined on weighted graphs.</s> <s>while wavelets provide a flexible tool for signal processing in the classical setting of regular domains, the existing graph wavelet constructions are less flexible -- they are guided solely by the structure of the underlying graph and do not take directly into consideration the particular class of signals to be processed.</s> <s>this paper introduces a machine learning framework for constructing graph wavelets that can sparsely represent a given class of signals.</s> <s>our construction uses the lifting scheme, and is based on the observation that the recurrent nature of the lifting scheme gives rise to a structure resembling a deep auto-encoder network.</s> <s>particular properties that the resulting wavelets must satisfy determine the training objective and the structure of the involved neural networks.</s> <s>the training is unsupervised, and is conducted similarly to the greedy pre-training of a stack of auto-encoders.</s> <s>after training is completed, we obtain a linear wavelet transform that can be applied to any graph signal in time and memory linear in the size of the graph.</s> <s>improved sparsity of our wavelet transform for the test signals is confirmed via experiments both on synthetic and real data.</s></p></d>", "label": ["<d><p><s>wavelets on graphs via deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a convergent sequence of graphs, there exists a limit object called the graphon from which random graphs are generated.</s> <s>this nonparametric perspective of random graphs opens the door to study graphs beyond the traditional parametric models, but at the same time also poses the challenging question of how to estimate the graphon underlying observed graphs.</s> <s>in this paper, we propose a computationally efficient algorithm to estimate a graphon from a set of observed graphs generated from it.</s> <s>we show that, by approximating the graphon with stochastic block models, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.</s></p></d>", "label": ["<d><p><s>stochastic blockmodel approximation of a graphon: theory and consistent estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an efficient bayesian nonparametric model for discovering hierarchical community structure in social networks.</s> <s>our model is a tree-structured mixture of potentially exponentially many stochastic blockmodels.</s> <s>we describe a family of greedy agglomerative model selection algorithms whose worst case scales quadratically in the number of vertices of the network, but independent of the number of communities.</s> <s>our algorithms are two orders of magnitude faster than the infinite relational model, achieving comparable or better accuracy.</s></p></d>", "label": ["<d><p><s>bayesian hierarchical community discovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>relational data?like graphs, networks, and matrices?is often dynamic, where the relational structure evolves over time.</s> <s>a fundamental problem in the analysis of time-varying network data is to extract a summary of the common structure and the dynamics of underlying relations between entities.</s> <s>here we build on the intuition that changes in the network structure are driven by the dynamics at the level of groups of nodes.</s> <s>we propose a nonparametric multi-group membership model for dynamic networks.</s> <s>our model contains three main components.</s> <s>we model the birth and death of groups with respect to the dynamics of the network structure via a distance dependent indian buffet process.</s> <s>we capture the evolution of individual node group memberships via a factorial hidden markov model.</s> <s>and, we explain the dynamics of the network structure by explicitly modeling the connectivity structure.</s> <s>we demonstrate our model?s capability of identifying the dynamics of latent groups in a number of different types of network data.</s> <s>experimental results show our model achieves higher predictive performance on the future network forecasting and missing link prediction.</s></p></d>", "label": ["<d><p><s>nonparametric multi-group membership model for dynamic networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>probabilistic models for binary spike patterns provide a powerful   tool for understanding the statistical dependencies in large-scale   neural recordings.</s> <s>maximum entropy (or maxent'') models, which   seek to explain dependencies in terms of low-order interactions   between neurons, have enjoyed remarkable success in modeling such   patterns, particularly for small groups of neurons.</s> <s>however, these   models are computationally intractable for large populations, and   low-order maxent models have been shown to be inadequate for some   datasets.</s> <s>to overcome these limitations, we propose a family of   \"universal'' models for binary spike patterns, where universality   refers to the ability to model arbitrary distributions over all   $2^m$ binary patterns.</s> <s>we construct universal models using a   dirichlet process centered on a well-behaved parametric base   measure, which naturally combines the flexibility of a histogram and   the parsimony of a parametric model.</s> <s>we derive computationally   efficient inference methods using bernoulli and cascade-logistic   base measures, which scale tractably to large populations.</s> <s>we also   establish a condition for equivalence between the cascade-logistic   and the 2nd-order maxent or \"ising'' model, making cascade-logistic   a reasonable choice for base measure in a universal model.</s> <s>we illustrate the performance of these models using neural data.\"</s></p></d>", "label": ["<d><p><s>universal models for binary spike patterns using centered dirichlet processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>point processes are popular models of neural spiking behavior as they provide a statistical distribution over temporal sequences of spikes and help to reveal the complexities underlying a series of recorded action potentials.</s> <s>however, the most common neural point process models, the poisson process and the gamma renewal process, do not capture interactions and correlations that are critical to modeling populations of neurons.</s> <s>we develop a novel model based on a determinantal point process over latent embeddings of neurons that effectively captures and helps visualize complex inhibitory and competitive interaction.</s> <s>we show that this model is a natural extension of the popular generalized linear model to sets of interacting neurons.</s> <s>the model is extended to incorporate gain control or divisive normalization, and the modulation of neural spiking based on periodic phenomena.</s> <s>applied to neural spike recordings from the rat hippocampus, we see that the model captures inhibitory relationships, a dichotomy of classes of neurons, and a periodic modulation by the theta rhythm known to be present in the data.</s></p></d>", "label": ["<d><p><s>a determinantal point process latent variable model for inhibition in neural spiking data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the macaque superior temporal sulcus (sts) is a brain area that receives and integrates inputs from both the ventral and dorsal visual processing streams (thought to specialize in form and motion processing respectively).</s> <s>for the processing of articulated actions, prior work has shown that even a small population of sts neurons contains sufficient information for the decoding of actor invariant to action, action invariant to actor, as well as the specific conjunction of actor and action.</s> <s>this paper addresses two questions.</s> <s>first, what are the invariance properties of individual neural representations (rather than the population representation) in sts?</s> <s>second, what are the neural encoding mechanisms that can produce such individual neural representations from streams of pixel images?</s> <s>we find that a baseline model, one that simply computes a linear weighted sum of ventral and dorsal responses to short action ?snippets?, produces surprisingly good fits to the neural data.</s> <s>interestingly, even using inputs from a single stream, both actor-invariance and action-invariance can be produced simply by having different linear weights.</s></p></d>", "label": ["<d><p><s>neural representation of action sequences: how far can a simple snippet-matching model take us?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how are firing rates in a spiking network related to neural input, connectivity and network function?</s> <s>this is an important problem because firing rates are one of the most important measures of network activity, in both the study of neural computation and neural network dynamics.</s> <s>however, it is a difficult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity.</s> <s>we develop a new technique for calculating firing rates in optimal balanced networks.</s> <s>these are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition.</s> <s>we can calculate firing rates by treating balanced network dynamics as an algorithm for optimizing signal representation.</s> <s>we identify this algorithm and then calculate firing rates by finding the solution to the algorithm.</s> <s>our firing rate calculation relates network firing rates directly to network input, connectivity and function.</s> <s>this allows us to explain the function and underlying mechanism of tuning curves in a variety of systems.</s></p></d>", "label": ["<d><p><s>firing rate predictions in optimal balanced networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent extensions of the perceptron, as e.g.</s> <s>the tempotron, suggest that this theoretical concept is highly relevant also for understanding networks of spiking neurons in the brain.</s> <s>it is not known, however, how the computational power of the perceptron and of its variants might be accomplished by the plasticity mechanisms of real synapses.</s> <s>here we prove that spike-timing-dependent plasticity having an anti-hebbian form for excitatory synapses as well as a spike-timing-dependent plasticity of hebbian shape for inhibitory synapses are sufficient for realizing the original perceptron learning rule if the respective plasticity mechanisms act in concert with the hyperpolarisation of the post-synaptic neurons.</s> <s>we also show that with these simple yet biologically realistic dynamics tempotrons are efficiently learned.</s> <s>the proposed mechanism might underly the acquisition of mappings of spatio-temporal activity patterns in one area of the brain onto other spatio-temporal spike patterns in another region and of long term memories in cortex.</s> <s>our results underline that learning processes in realistic networks of spiking neurons depend crucially on the interactions of synaptic plasticity mechanisms with the dynamics of participating neurons.</s></p></d>", "label": ["<d><p><s>perfect associative learning with spike-timing-dependent plasticity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>psychophysical experiments have demonstrated that the brain integrates information from multiple sensory cues in a near bayesian optimal manner.</s> <s>the present study proposes a novel mechanism to achieve this.</s> <s>we consider two reciprocally connected networks, mimicking the integration of heading direction information between the dorsal medial superior temporal (mstd) and the ventral intraparietal (vip) areas.</s> <s>each network serves as a local estimator and receives an independent cue, either the visual or the vestibular, as direct input for the external stimulus.</s> <s>we find that positive reciprocal interactions can improve the decoding accuracy of each individual network as if it implements bayesian inference from two cues.</s> <s>our model successfully explains the experimental finding that both mstd and vip achieve bayesian multisensory integration, though each of them only receives a single cue as direct external input.</s> <s>our result suggests that the brain may implement optimal information integration distributively at each local estimator through the reciprocal connections between cortical regions.</s></p></d>", "label": ["<d><p><s>reciprocally coupled local estimators implement bayesian information integration distributively</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate a spiking neuron model of multisensory integration.</s> <s>multiple stimuli from different sensory modalities are encoded by a single neural circuit comprised of a multisensory bank of receptive fields in cascade with a population of biophysical spike generators.</s> <s>we demonstrate that stimuli of different dimensions can be faithfully multiplexed and encoded in the spike domain and derive tractable algorithms for decoding each stimulus from the common pool of spikes.</s> <s>we also show that the identification of multisensory processing in a single neuron is dual to the recovery of stimuli encoded with a population of multisensory neurons, and prove that only a projection of the circuit onto input stimuli can be identified.</s> <s>we provide an example of multisensory integration using natural audio and video and discuss the performance of the proposed decoding and identification algorithms.</s></p></d>", "label": ["<d><p><s>multisensory encoding, decoding, and identification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a recurrent neuronal network, modeled as a continuous-time dynamical system, that can solve constraint satisfaction problems.</s> <s>discrete variables are represented by coupled winner-take-all (wta) networks, and their values are encoded in localized patterns of oscillations that are learned by the recurrent weights in these networks.</s> <s>constraints over the variables are encoded in the network connectivity.</s> <s>although there are no sources of noise, the network can escape from local optima in its search for solutions that satisfy all constraints by modifying the effective network connectivity through oscillations.</s> <s>if there is no solution that satisfies all constraints, the network state changes in a pseudo-random manner and its trajectory approximates a sampling procedure that selects a variable assignment with a probability that increases with the fraction of constraints satisfied by this assignment.</s> <s>external evidence, or input to the network, can force variables to specific values.</s> <s>when new inputs are applied, the network re-evaluates the entire set of variables in its search for the states that satisfy the maximum number of constraints, while being consistent with the external input.</s> <s>our results demonstrate that the proposed network architecture can perform a deterministic search for the optimal solution to problems with non-convex cost functions.</s> <s>the network is inspired by canonical microcircuit models of the cortex and suggests possible dynamical mechanisms to solve constraint satisfaction problems that can be present in biological networks, or implemented in neuromorphic electronic circuits.</s></p></d>", "label": ["<d><p><s>recurrent networks of coupled winner-take-all oscillators for solving constraint satisfaction problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we solve the mean field equations for a stochastic hopfield network with temperature (noise) in the presence of strong, i.e., multiply stored patterns, and use this solution to obtain the storage capacity of such a network.</s> <s>our result provides for the first time a rigorous solution of the mean field equations for the standard hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been hitherto used for this derivation.</s> <s>we show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity, when sum of the cubes of degrees of all stored patterns is negligible compared to the network size.</s> <s>in the case of a single strong pattern in the presence of simple patterns, when the ratio of the number of all stored patterns and the network size is a positive constant, we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern.</s> <s>this square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy.</s></p></d>", "label": ["<d><p><s>capacity of strong attractor patterns to model behavioural and cognitive prototypes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>local competition among neighboring neurons is common in biological neural networks (nns).</s> <s>we apply the concept to gradient-based, backprop-trained artificial multilayer nns.</s> <s>nns with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.</s></p></d>", "label": ["<d><p><s>compete to compute</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce rnade, a new model for joint density estimation of real-valued vectors.</s> <s>our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters.</s> <s>rnade learns a distributed representation of the data, while having a tractable expression for the calculation of densities.</s> <s>a tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers.</s> <s>we compare the performance of rnade on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.</s></p></d>", "label": ["<d><p><s>rnade: the real-valued neural autoregressive density-estimator</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>with simultaneous measurements from ever increasing populations of neurons, there is a growing need for sophisticated tools to recover signals from individual neurons.</s> <s>in electrophysiology experiments, this classically proceeds in a two-step process: (i) threshold the waveforms to detect putative spikes and (ii) cluster the waveforms into single units (neurons).</s> <s>we extend previous bayesian nonparamet- ric models of neural spiking to jointly detect and cluster neurons using a gamma process model.</s> <s>importantly, we develop an online approximate inference scheme enabling real-time analysis, with performance exceeding the previous state-of-the- art.</s> <s>via exploratory data analysis?using data with partial ground truth as well as two novel data sets?we find several features of our model collectively contribute to our improved performance including: (i) accounting for colored noise, (ii) de- tecting overlapping spikes, (iii) tracking waveform dynamics, and (iv) using mul- tiple channels.</s> <s>we hope to enable novel experiments simultaneously measuring many thousands of neurons and possibly adapting stimuli dynamically to probe ever deeper into the mysteries of the brain.</s></p></d>", "label": ["<d><p><s>real-time inference for a gamma process model of neural spiking</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper considers the problem of transferring experimental findings learned from multiple heterogeneous domains to a target environment, in which only limited experiments can be performed.</s> <s>we reduce questions of transportability from multiple domains and with limited scope to symbolic derivations in the do-calculus, thus extending the treatment of transportability from full experiments introduced in pearl and bareinboim (2011).</s> <s>we further provide different graphical and algorithmic conditions for computing the transport formula for this setting, that is, a way of fusing the observational and experimental information scattered throughout different domains to synthesize a consistent estimate of the desired effects.</s></p></d>", "label": ["<d><p><s>transportability from multiple environments with limited experiments</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>causal inference uses observational data to infer the causal structure of the data generating system.</s> <s>we study a class of restricted structural equation models for time series that we call time series models with independent noise (timino).</s> <s>these models require independent residual time series, whereas traditional methods like granger causality exploit the variance of residuals.</s> <s>this work contains two main contributions: (1) theoretical: by restricting the model class (e.g.</s> <s>to additive noise) we provide more general identifiability results than existing ones.</s> <s>the results cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series.</s> <s>(2) practical: if there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series.</s> <s>when the data are causally insufficient, or the data generating process does not satisfy the model assumptions, this algorithm may still give partial results, but mostly avoids incorrect answers.</s> <s>the structural equation model point of view allows us to extend both the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays (as may happen for fmri data, for example).</s> <s>timino outperforms existing methods on artificial and real data.</s> <s>code is provided.</s></p></d>", "label": ["<d><p><s>causal inference on time series using restricted structural equation models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we give a polynomial-time algorithm for provably learning the structure and parameters of bipartite noisy-or bayesian networks of binary variables where the top layer is completely hidden.</s> <s>unsupervised learning of these models is a form of discrete factor analysis, enabling the discovery of hidden variables and their causal relationships with observed data.</s> <s>we obtain an efficient learning algorithm for a family of bayesian networks that we call quartet-learnable, meaning that every latent variable has four children that do not have any other parents in common.</s> <s>we show that the existence of such a quartet allows us to uniquely identify each latent variable and to learn all parameters involving that latent variable.</s> <s>underlying our algorithm are two new techniques for structure learning: a quartet test to determine whether a set of binary variables are singly coupled, and a conditional mutual information test that we use to learn parameters.</s> <s>we also show how to subtract already learned latent variables from the model to create new singly-coupled quartets, which substantially expands the class of structures that we can learn.</s> <s>finally, we give a proof of the polynomial sample complexity of our learning algorithm, and experimentally compare it to variational em.</s></p></d>", "label": ["<d><p><s>discovering hidden variables in noisy-or networks using quartet tests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning dynamic models from observed data has been a central issue in many scientific  studies or engineering tasks.</s> <s>the usual setting is that data are collected sequentially  from trajectories of some dynamical system operation.</s> <s>in quite a few modern scientific modeling tasks, however, it turns out that  reliable sequential data are rather difficult to gather, whereas out-of-order snapshots are much easier to obtain.</s> <s>examples include the modeling of galaxies, chronic diseases such alzheimer's, or certain biological processes.</s> <s>existing methods for learning dynamic model from non-sequence data are mostly based on expectation-maximization, which involves non-convex optimization and is thus hard to analyze.</s> <s>inspired by recent advances in spectral learning methods, we propose to study this problem  from a different perspective: moment matching and spectral decomposition.</s> <s>under that framework,  we identify reasonable assumptions on the generative process of non-sequence data,  and propose learning algorithms based on the tensor decomposition method  \\cite{anandkumar2012tensor} to \\textit{provably} recover first-order markov models and hidden markov models.</s> <s>to the best of our knowledge, this is the first formal guarantee on learning from non-sequence data.</s> <s>preliminary simulation results confirm our theoretical findings.</s></p></d>", "label": ["<d><p><s>learning hidden markov models from non-sequence data via tensor decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we develop efficient methods for learning random map predictors for structured label problems.</s> <s>in particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods.</s> <s>we show that every smooth posterior distribution would suffice to define a smooth pac-bayesian risk bound suitable for gradient methods.</s> <s>in addition, we relate the posterior distributions to computational properties of the map predictors.</s> <s>we suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized map predictors such as graph-cuts.</s> <s>we also describe label-augmented posterior models that can use efficient map approximations, such as those arising from linear program relaxations.</s></p></d>", "label": ["<d><p><s>learning efficient random maximum a-posteriori predictors with non-decomposable loss functions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>markov decision processes (mdps) are extremely useful for modeling and solving sequential decision making problems.</s> <s>graph-based mdps provide a compact representation for mdps with large numbers of random variables.</s> <s>however, the complexity of exactly solving a graph-based mdp usually grows exponentially in the number of variables, which limits their application.</s> <s>we present a new variational framework to describe and solve the planning problem of mdps, and derive both exact and approximate planning algorithms.</s> <s>in particular, by exploiting the graph structure of graph-based mdps, we propose a factored variational value iteration algorithm in which the value function is first approximated by the multiplication of local-scope value functions, then solved by minimizing a kullback-leibler (kl) divergence.</s> <s>the kl divergence is optimized using the belief propagation algorithm, with complexity exponential in only the cluster size of the graph.</s> <s>experimental comparison on different models shows that our algorithm outperforms existing approximation algorithms at finding good policies.</s></p></d>", "label": ["<d><p><s>variational planning for graph-based mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a non-factorized variational method for full posterior inference in bayesian hierarchical models,  with the goal of capturing the posterior variable dependencies via efficient and possibly parallel computation.</s> <s>our approach unifies the integrated nested laplace approximation (inla) under the variational framework.</s> <s>the proposed method is applicable in more challenging scenarios than typically assumed by inla,  such as bayesian lasso,  which is characterized by the non-differentiability of the $\\ell_{1}$ norm arising from independent laplace priors.</s> <s>we derive an upper bound for the kullback-leibler divergence,  which yields a fast closed-form solution via decoupled optimization.</s> <s>our method is a reliable analytic alternative to markov chain monte carlo (mcmc), and it results in a tighter evidence lower bound than that of mean-field variational bayes (vb) method.</s></p></d>", "label": ["<d><p><s>integrated non-factorized variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>when a probabilistic model and its  prior are given, bayesian learning offers inference with automatic parameter tuning.</s> <s>however, bayesian learning is often obstructed by computational difficulty: the rigorous bayesian learning is  intractable in many models, and its variational bayesian (vb) approximation is prone to suffer from local minima.</s> <s>in this paper, we overcome this difficulty for low-rank subspace clustering (lrsc) by providing an exact global solver and its efficient approximation.</s> <s>lrsc extracts a low-dimensional structure of data by embedding  samples into the union of low-dimensional subspaces, and its variational bayesian variant has shown good performance.</s> <s>we first prove a key property that the vb-lrsc model is highly redundant.</s> <s>thanks to this property, the optimization problem of vb-lrsc can be separated into small subproblems, each of which has only a small number of unknown variables.</s> <s>our exact global solver relies on another key property that the stationary condition of each subproblem is written as a set of polynomial equations, which is solvable with the homotopy method.</s> <s>for further computational efficiency,  we also propose an efficient approximate variant, of which the stationary condition can be written as a polynomial equation with a single variable.</s> <s>experimental results show the usefulness of our approach.</s></p></d>", "label": ["<d><p><s>global solver and its efficient approximation for variational bayesian low-rank subspace clustering</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>expectation propagation (ep) is a popular approximate posterior inference algorithm that often  provides a fast and accurate alternative to sampling-based methods.</s> <s>however, while the ep framework in theory allows for complex non-gaussian factors, there is still a significant practical barrier to using them within ep, because doing so requires the implementation of message update operators, which can be difficult and require hand-crafted approximations.</s> <s>in this work, we study the question of whether it is possible to automatically derive fast and accurate ep updates by learning a discriminative model e.g., a neural network or random forest) to map ep message inputs to ep message outputs.</s> <s>we address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising.</s></p></d>", "label": ["<d><p><s>learning to pass expectation propagation messages</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces.</s> <s>our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases.</s> <s>hence, we propose, transe, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities.</s> <s>despite its simplicity, this assumption proves to be powerful since extensive experiments show that transe significantly outperforms state-of-the-art methods in link prediction on two knowledge bases.</s> <s>besides, it can be successfully trained on a large scale data set with 1m entities, 25k relationships and more than 17m training samples.</s></p></d>", "label": ["<d><p><s>translating embeddings for modeling multi-relational data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic block models characterize observed network relationships via latent community memberships.</s> <s>in large social networks, we expect entities to participate in multiple communities, and the number of communities to grow with the network size.</s> <s>we introduce a new model for these phenomena, the hierarchical dirichlet process relational model, which allows nodes to have mixed membership in an unbounded set of communities.</s> <s>to allow scalable learning, we derive an online stochastic variational inference algorithm.</s> <s>focusing on assortative models of undirected networks, we also propose an efficient structured mean field variational bound, and online methods for automatically pruning unused communities.</s> <s>compared to state-of-the-art online learning methods for parametric relational models, we show significantly improved perplexity and link prediction accuracy for sparse networks with tens of thousands of nodes.</s> <s>we also showcase an analysis of littlesis, a large network of who-knows-who at the heights of business and government.</s></p></d>", "label": ["<d><p><s>efficient online inference for bayesian nonparametric relational models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we theoretically study the problem of binary classification in the presence of random classification noise --- the learner, instead of seeing the true labels, sees labels that have independently been flipped with some small probability.</s> <s>moreover, random label noise is \\emph{class-conditional} --- the flip probability depends on the class.</s> <s>we provide two approaches to suitably modify any given surrogate loss function.</s> <s>first, we provide a simple unbiased estimator of any loss, and obtain performance bounds for empirical risk minimization in the presence of iid data with noisy labels.</s> <s>if the loss function satisfies a simple symmetry condition, we show that the method leads to an efficient algorithm for empirical minimization.</s> <s>second, by leveraging a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss, we suggest the use of a simple weighted surrogate loss, for which we are able to obtain strong empirical risk bounds.</s> <s>this approach has a very remarkable consequence --- methods used in practice such as biased svm and weighted logistic regression are provably noise-tolerant.</s> <s>on a synthetic non-separable dataset, our methods achieve over 88\\% accuracy even when 40\\% of the labels are corrupted, and are competitive with respect to recently proposed methods for dealing with label noise in several benchmark datasets.</s></p></d>", "label": ["<d><p><s>learning with noisy labels</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the problem of reconstructing low-rank matrices from their noisy observations.</s> <s>we formulate the problem in the bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity.</s> <s>we propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the bayesian inference for matrix reconstruction.</s> <s>we have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property.</s> <s>numerical experiments show that the proposed algorithm outperforms lloyd's k-means algorithm.</s></p></d>", "label": ["<d><p><s>low-rank matrix reconstruction and clustering via approximate message passing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the method of cauchy random projections is popular  for computing the $l_1$ distance in high dimension.</s> <s>in this paper, we propose to use only the signs of the projected data and show that the  probability of collision (i.e., when the two signs differ) can be accurately approximated as a function of the chi-square ($\\chi^2$) similarity, which is a popular  measure for nonnegative data (e.g., when features are generated from histograms as common in text and vision applications).</s> <s>our experiments   confirm that this method of sign cauchy random projections is promising for large-scale  learning applications.</s> <s>furthermore, we extend the idea to sign $\\alpha$-stable random projections and derive a bound of the collision probability.</s></p></d>", "label": ["<d><p><s>sign cauchy projections and chi-square kernel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bilinear approximation of a matrix is a powerful paradigm of unsupervised learning.</s> <s>in some applications, however, there is a natural hierarchy of concepts that ought to be reflected in the unsupervised analysis.</s> <s>for example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel $\\rightarrow$ neuron $\\rightarrow$ assembly that should find their counterpart in the unsupervised analysis.</s> <s>driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels.</s> <s>in contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts.</s> <s>in addition, we learn the nature of these relations rather than imposing them.</s> <s>finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion.</s> <s>the proposed bilevel shmf (sparse heterarchical matrix factorization) is the first formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies.</s> <s>experiments show that the proposed model fully recovers the structure from difficult synthetic data designed to imitate the experimental data.</s> <s>more importantly, bilevel shmf yields plausible interpretations of real-world calcium imaging data.</s></p></d>", "label": ["<d><p><s>learning multi-level sparse representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of learning a tensor from a set of linear measurements.</s> <s>a prominent methodology for this problem is based on the extension of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting.</s> <s>in this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the euclidean unit ball.</s> <s>we then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers.</s> <s>experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.</s></p></d>", "label": ["<d><p><s>a new convex relaxation for tensor completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a maximum margin framework that clusters data using latent variables.</s> <s>using latent representations enables our framework to model unobserved information embedded in the data.</s> <s>we implement our idea by large margin learning, and develop an alternating descent algorithm to effectively solve the resultant non-convex optimization problem.</s> <s>we instantiate our latent maximum margin clustering framework with tag-based video clustering tasks, where each video is represented by a latent tag model describing the presence or absence of video tags.</s> <s>experimental results obtained on three standard datasets show that the proposed method outperforms non-latent maximum margin clustering as well as conventional clustering approaches.</s></p></d>", "label": ["<d><p><s>latent maximum margin clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many applications require the analysis of complex interactions between time series.</s> <s>these interactions can be non-linear and involve vector valued as well as complex data structures such as graphs or strings.</s> <s>here we provide a general framework for the statistical analysis of these interactions when random variables are sampled from stationary time-series of arbitrary objects.</s> <s>to achieve this goal we analyze the properties of the kernel cross-spectral density operator induced by positive definite kernels on arbitrary input domains.</s> <s>this framework enables us to develop an independence test between time series as well as a similarity measure to compare different types of coupling.</s> <s>the performance of our test is compared to the hsic test using i.i.d.</s> <s>assumptions, showing improvement in terms of detection errors as well as the suitability of this approach for testing dependency in complex dynamical systems.</s> <s>finally, we use this approach to characterize complex interactions in electrophysiological neural time series.</s></p></d>", "label": ["<d><p><s>statistical analysis of coupled time series with kernel cross-spectral density operators.</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel embedding of distributions has led to many recent advances in machine learning.</s> <s>however, latent and low rank structures prevalent in real world distributions have rarely been taken into account in this setting.</s> <s>furthermore, no prior work in kernel embedding literature has addressed the issue of robust embedding when the latent and low rank information are misspecified.</s> <s>in this paper, we propose a hierarchical low rank decomposition of kernels embeddings which can exploit such low rank structures in data while being robust to model misspecification.</s> <s>we also illustrate with empirical evidence that the estimated low rank embeddings lead to improved performance in density estimation.</s></p></d>", "label": ["<d><p><s>robust low rank kernel embeddings of multivariate distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a family of maximum mean discrepancy (mmd) kernel two-sample tests that have low sample complexity and are consistent.</s> <s>the test has a hyperparameter that allows one to control the tradeoff between sample complexity and computational time.</s> <s>our family of tests, which we denote as b-tests, is both computationally and statistically efficient, combining favorable properties of previously proposed mmd two-sample tests.</s> <s>it does so by better leveraging samples to produce low variance estimates in the finite sample case, while avoiding a quadratic number of kernel evaluations and complex null-hypothesis approximation as would be required by tests relying on one sample u-statistics.</s> <s>the b-test uses a smaller than quadratic number of kernel evaluations and avoids completely the computational burden of complex null-hypothesis approximation while maintaining consistency and probabilistically conservative thresholds on type i error.</s> <s>finally, recent results of combining multiple kernels transfer seamlessly to our hypothesis test, allowing a further increase in discriminative power and decrease in sample complexity.</s></p></d>", "label": ["<d><p><s>b-test: a non-parametric, low variance kernel two-sample test</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies.</s> <s>to this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies.</s> <s>this bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers.</s> <s>we then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy.</s> <s>we finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.</s></p></d>", "label": ["<d><p><s>on flat versus hierarchical classification in large-scale taxonomies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this  paper presents an approach to multilabel classification (mlc) with a large number of labels.</s> <s>our approach is a reduction to binary classification in which label sets are represented by low dimensional binary vectors.</s> <s>this representation follows the principle of bloom filters, a space-efficient data structure originally designed for approximate membership testing.</s> <s>we show that a naive application of bloom filters in mlc is not robust to individual binary classifiers' errors.</s> <s>we then present an approach that exploits a specific feature of real-world datasets when the number of labels is large: many labels (almost) never appear together.</s> <s>our approch is provably robust, has sublinear training and inference complexity with respect to the number of labels, and compares favorably to state-of-the-art algorithms on two large scale multilabel datasets.</s></p></d>", "label": ["<d><p><s>robust bloom filters for large multilabel classification tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the estimation of dependencies between multiple variables is a central problem in the analysis of financial time series.</s> <s>a common approach is to express these dependencies in terms of a copula function.</s> <s>typically the copula function is assumed to be constant but this may be innacurate when there are covariates that could have a large influence on the dependence structure of the data.</s> <s>to account for this, a bayesian framework for the estimation of conditional copulas is proposed.</s> <s>in this framework the parameters of a copula are non-linearly related to some arbitrary conditioning variables.</s> <s>we evaluate the ability of our method to predict time-varying dependencies on several equities and currencies and observe consistent performance gains compared to static copula models and other time-varying copula methods.</s></p></d>", "label": ["<d><p><s>gaussian process conditional copulas with applications to financial time series</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>state-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems.</s> <s>we present a fully bayesian approach to inference and learning in nonlinear nonparametric state-space models.</s> <s>we place a gaussian process prior over the transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena.</s> <s>however, to enable efficient inference, we marginalize over the dynamics of the model and instead infer directly the joint smoothing distribution through the use of specially tailored particle markov chain monte carlo samplers.</s> <s>once an approximation of the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically.</s> <s>we make use of sparse gaussian process models to greatly reduce the computational complexity of the approach.</s></p></d>", "label": ["<d><p><s>bayesian inference and learning in gaussian process state-space models with particle mcmc</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency.</s> <s>in this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently.</s> <s>our approach is based on extending multi-task gaussian processes to the framework of bayesian optimization.</s> <s>we show that this method significantly speeds up the optimization process when compared to the standard single-task approach.</s> <s>we further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up $k$-fold cross-validation.</s> <s>lastly, our most significant contribution is an adaptation of a recently proposed acquisition function, entropy search, to the cost-sensitive and multi-task settings.</s> <s>we demonstrate the utility of this new acquisition function by utilizing a small dataset in order to explore hyperparameter settings for a large dataset.</s> <s>our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.</s></p></d>", "label": ["<d><p><s>multi-task bayesian optimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose an efficient discrete optimization algorithm for selecting a subset of training data to induce sparsity for gaussian process regression.</s> <s>the algorithm estimates this inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy.</s> <s>the space and time complexity are linear in the training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains.</s> <s>empirical evaluation shows state-of-art performance in the discrete case and competitive results in the continuous case.</s></p></d>", "label": ["<d><p><s>efficient optimization for sparse gaussian process regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in gaussian process regression.</s> <s>this approach consists of a novel variant of the variational framework that has been recently developed for the gaussian process latent variable model which additionally makes use of a standardised representation of the gaussian process.</s> <s>we consider this technique for learning mahalanobis distance metrics in a gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering  datasets with high-dimensional inputs.</s></p></d>", "label": ["<d><p><s>variational inference for mahalanobis distance metrics in gaussian process regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-task prediction models are widely being used to couple regressors or classification models by sharing information across related tasks.</s> <s>a common pitfall of these models is that they assume that the output tasks are independent conditioned on the inputs.</s> <s>here, we propose a multi-task gaussian process approach to model both the relatedness between regressors as well as the task correlations in the residuals, in order to more accurately identify true sharing between regressors.</s> <s>the resulting gaussian model has a covariance term that is the sum of kronecker products, for which efficient parameter inference and out of sample prediction are feasible.</s> <s>on both synthetic examples and applications to phenotype prediction in genetics, we find substantial benefits of modeling structured noise compared to established alternatives.</s></p></d>", "label": ["<d><p><s>it is all in the noise: efficient multi-task gaussian process inference with structured residuals</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>entropy rate quantifies the amount of disorder in a stochastic process.</s> <s>for spiking neurons, the entropy rate places an upper bound on the rate at which the spike train can convey stimulus information, and a large literature has focused on the problem of estimating entropy rate from spike train data.</s> <s>here we present bayes least squares and empirical bayesian entropy rate estimators for binary spike trains using hierarchical dirichlet process (hdp) priors.</s> <s>our estimator leverages the fact that the entropy rate of an ergodic markov chain with known transition probabilities can be calculated analytically, and many stochastic processes that are non-markovian can still be well approximated by markov processes of sufficient depth.</s> <s>choosing an appropriate depth of markov model presents challenges due to possibly long time dependencies and short data sequences: a deeper model can better account for long time-dependencies, but is more difficult to infer from limited data.</s> <s>our approach mitigates this difficulty by using a hierarchical prior to share statistical power across markov chains of different depths.</s> <s>we present both a fully bayesian and empirical bayes entropy rate estimator based on this model, and demonstrate their performance on simulated and real neural spike train data.</s></p></d>", "label": ["<d><p><s>spike train entropy-rate estimation using hierarchical dirichlet process priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider design of linear projection measurements for a vector poisson signal model.</s> <s>the projections are performed on the vector poisson rate, $x\\in\\mathbb{r}_+^n$, and the observed data are a vector of counts, $y\\in\\mathbb{z}_+^m$.</s> <s>the projection matrix is designed by maximizing mutual information between $y$ and $x$, $i(y;x)$.</s> <s>when there is a latent class label $c\\in\\{1,\\dots,l\\}$ associated with $x$, we consider the mutual information with respect to $y$ and $c$, $i(y;c)$.</s> <s>new analytic expressions for the gradient of $i(y;x)$ and $i(y;c)$ are presented, with gradient performed with respect to the measurement matrix.</s> <s>connections are made to the more widely studied gaussian measurement model.</s> <s>example results are presented for compressive topic modeling of a document corpora (word counting), and hyperspectral compressive sensing for chemical classification (photon counting).</s></p></d>", "label": ["<d><p><s>designed measurements for vector count data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a unified framework for the high-dimensional analysis of ?superposition-structured?</s> <s>or ?dirty?</s> <s>statistical models: where the model parameters are a ?superposition?</s> <s>of structurally constrained parameters.</s> <s>we allow for any number and types of structures, and any statistical model.</s> <s>we consider the general class of $m$-estimators that minimize the sum of any loss function, and an instance of what we call a ?hybrid?</s> <s>regularization, that is the infimal convolution of weighted regularization functions, one for each structural component.</s> <s>we provide corollaries showcasing our unified framework for varied statistical models such as linear regression, multiple regression and principal component analysis, over varied superposition structures.</s></p></d>", "label": ["<d><p><s>dirty statistical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>infinite mixture models are commonly used for clustering.</s> <s>one can sample from the posterior of mixture assignments by monte carlo methods or find its maximum a posteriori solution by optimization.</s> <s>however, in some problems the posterior is diffuse and it is hard to interpret the sampled partitionings.</s> <s>in this paper, we introduce novel statistics based on block sizes for representing sample sets of partitionings and feature allocations.</s> <s>we develop an element-based definition of entropy to quantify segmentation among their elements.</s> <s>then we propose a simple algorithm called entropy agglomeration (ea) to summarize and visualize this information.</s> <s>experiments on various infinite mixture posteriors as well as a feature allocation dataset demonstrate that the proposed statistics are useful in practice.</s></p></d>", "label": ["<d><p><s>summary statistics for partitionings and feature allocations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a novel algorithm, based upon the dependent dirichlet process mixture model (ddpmm), for clustering batch-sequential data containing an unknown number of evolving clusters.</s> <s>the algorithm is derived via a low-variance asymptotic analysis of the gibbs sampling algorithm for the ddpmm, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm.</s> <s>empirical results from a synthetic test with moving gaussian clusters and a test with real ads-b aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets.</s></p></d>", "label": ["<d><p><s>dynamic clustering via asymptotics of the dependent dirichlet process mixture</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the following new variant of prototype learning, called {\\em $k$-prototype learning problem for 3d rigid structures}: given a set of 3d rigid structures, find a set of $k$ rigid structures so that each of them is a prototype for a cluster of the given rigid structures and the total cost (or dissimilarity) is minimized.</s> <s>prototype learning is a core problem in machine learning and has a wide range of applications in many areas.</s> <s>existing results on this problem have mainly focused on the graph domain.</s> <s>in this paper, we present the first algorithm for learning multiple prototypes from 3d rigid structures.</s> <s>our result is based on a number of new insights to rigid structures alignment, clustering, and prototype reconstruction, and is practically efficient with quality guarantee.</s> <s>we validate our approach using two type of data sets, random data and biological data of chromosome territories.</s> <s>experiments suggest that our approach can effectively learn prototypes in both types of data.</s></p></d>", "label": ["<d><p><s>k-prototype learning for 3d rigid structures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper provides new algorithms for distributed clustering for two popular center-based objectives, $k$-median and $k$-means.</s> <s>these algorithms have provable guarantees and improve communication complexity over existing approaches.</s> <s>following a classic approach in clustering by \\cite{har2004coresets}, we reduce the problem of finding a clustering with low cost to the problem of finding a `coreset' of small size.</s> <s>we provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies.</s> <s>we provide experimental evidence for this approach on both synthetic and real data sets.</s></p></d>", "label": ["<d><p><s>distributed </s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation.</s> <s>while these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks.</s> <s>this paper presents a general framework for multiclass total variation clustering that does not rely on recursion.</s> <s>the results greatly outperform previous total variation algorithms and compare well with state-of-the-art nmf approaches.</s></p></d>", "label": ["<d><p><s>multiclass total variation clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the general problem of multiple model learning (mml) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases.</s> <s>a common approach to solving new mml problems is to generalize lloyd's algorithm  for clustering (or expectation-maximization for soft clustering).</s> <s>however this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.</s> <s>we propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out.</s> <s>this enhances robustness by making assumptions on class balance.</s> <s>we further provide generalization bounds and explain how the new iterations may be computed efficiently.</s> <s>we demonstrate the robustness benefits of our approach with some experimental results and prove for the important  case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.</s></p></d>", "label": ["<d><p><s>learning multiple models via regularized weighting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral clustering is a fast and popular algorithm for finding clusters in networks.</s> <s>recently, chaudhuri et al.</s> <s>and amini et al.</s> <s>proposed variations on the algorithm that artificially inflate the node degrees for improved statistical performance.</s> <s>the current paper extends the previous theoretical results to the more canonical spectral clustering algorithm in a way that removes any assumption on the minimum degree and  provides guidance on the choice of tuning parameter.</s> <s>moreover, our results show how the star shape\" in the eigenvectors--which are consistently observed in empirical networks--can be explained by the degree-corrected stochastic blockmodel and the extended planted partition model, two statistical model that allow for highly heterogeneous degrees.</s> <s>throughout, the paper characterizes and justifies several of the variations of the spectral clustering algorithm in terms of these models.  \"</s></p></d>", "label": ["<d><p><s>regularized spectral clustering under the degree-corrected stochastic blockmodel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>suppose $k$ centers are fit to $m$ points by heuristically minimizing the $k$-means cost; what is the corresponding fit over the source distribution?</s> <s>this question is resolved here for distributions with $p\\geq 4$ bounded moments; in particular, the difference between the sample cost and distribution cost decays with $m$ and $p$ as $m^{\\min\\{-1/4, -1/2+2/p\\}}$.</s> <s>the essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions.</s> <s>to further demonstrate this mechanism, a soft clustering variant of $k$-means cost is also considered, namely the log likelihood of a gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum.</s> <s>lastly, a rate with refined constants is provided for $k$-means instances possessing some cluster structure.</s></p></d>", "label": ["<d><p><s>moment-based uniform deviation bounds for </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a framework for designing efficient active learning algorithms that are tolerant to random classification noise.</s> <s>the framework is based on active learning algorithms that are statistical in the sense that they rely on estimates of expectations of functions of filtered random examples.</s> <s>it builds on the powerful statistical query framework of kearns (1993).</s> <s>we show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of uncorrelated\" noise.</s> <s>the complexity of the resulting algorithms has information-theoretically optimal quadratic dependence on $1/(1-2\\eta)$, where $\\eta$ is the noise rate.</s> <s>we demonstrate the power of our framework by showing that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework.</s> <s>these results combined with our generic conversion lead to the first known computationally-efficient algorithms for actively learning some of these concept classes in the presence of random classification noise that provide exponential improvement in the dependence on the error $\\epsilon$ over their passive counterparts.</s> <s>in addition, we show that our algorithms can be automatically converted to efficient active differentially-private algorithms.</s> <s>this leads to the first differentially-private active learning algorithms with exponential label savings over the passive case.\"</s></p></d>", "label": ["<d><p><s>statistical active learning algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we informally call a stochastic process learnable if it admits a generalization error approaching zero in probability for any concept class with finite vc-dimension (iid processes are the simplest example).</s> <s>a mixture of learnable processes need not be learnable itself, and certainly its generalization error need not decay at the same rate.</s> <s>in this paper, we argue that it is natural in predictive pac to condition not on the past observations but on the mixture component of the sample path.</s> <s>this definition not only matches what a realistic learner might demand, but also allows us to sidestep several otherwise grave problems in learning from dependent data.</s> <s>in particular, we give a novel pac generalization bound for mixtures of learnable processes with a generalization error that is not worse than that of each mixture component.</s> <s>we also provide a characterization of mixtures of absolutely regular ($\\beta$-mixing) processes, of independent interest.</s></p></d>", "label": ["<d><p><s>predictive pac learning and process decompositions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the first result for kernel regression where the procedure adapts locally at a point $x$ to both the unknown local dimension of the metric and the unknown h\\{o}lder-continuity of the regression function at $x$.</s> <s>the result holds with high probability simultaneously at all points $x$ in a metric space of unknown structure.\"</s></p></d>", "label": ["<d><p><s>adaptivity to local smoothness and dimension in kernel regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated.</s> <s>this limitation has led to a growing number of \\emph{preconditioned lasso} algorithms that pre-multiply $x$ and $y$ by matrices $p_x$, $p_y$ prior to running the standard lasso.</s> <s>a direct comparison of these and similar lasso-style algorithms to the original lasso is difficult because the performance of all of these methods depends critically on an auxiliary penalty parameter $\\lambda$.</s> <s>in this paper we propose an agnostic, theoretical framework for comparing preconditioned lasso algorithms to the lasso without having to choose $\\lambda$.</s> <s>we apply our framework to three preconditioned lasso instances and highlight when they will outperform the lasso.</s> <s>additionally, our theory offers insights into the fragilities of these algorithms to which we provide partial solutions.</s></p></d>", "label": ["<d><p><s>a comparative framework for preconditioned lasso algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of fast estimation of ordinary least squares (ols) from large amounts of data ($n \\gg p$).</s> <s>we propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation.</s> <s>all three run in the order of size of input i.e.</s> <s>o($np$) and our best method, {\\it uluru}, gives an error bound of $o(\\sqrt{p/n})$ which is independent of the amount of subsampling as long as it is above a threshold.</s> <s>we provide theoretical bounds for our algorithms in the fixed design (with randomized hadamard preconditioning) as well as sub-gaussian random design setting.</s> <s>we also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d., sub-gaussian then one can directly subsample without the expensive randomized hadamard preconditioning without loss of accuracy.</s></p></d>", "label": ["<d><p><s>new subsampling algorithms for fast least squares regression</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations ($p \\gg n$).</s> <s>the standard way to solve ridge regression in this setting works in the dual space and gives a running time of $o(n^2p)$.</s> <s>our algorithm (srht-drr) runs in time $o(np\\log(n))$ and works by preconditioning the design matrix by a randomized walsh-hadamard transform with a subsequent subsampling of features.</s> <s>we provide risk bounds for our srht-drr algorithm in the fixed design setting and show experimental results on synthetic and real datasets.</s></p></d>", "label": ["<d><p><s>faster ridge regression via the subsampled randomized hadamard transform</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents.</s> <s>although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks.</s> <s>in this paper we study the problem of sequential transfer in online learning, notably in the multi-arm bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks.</s> <s>we introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.</s></p></d>", "label": ["<d><p><s>sequential transfer in multi-armed bandit with finite set of models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions.</s> <s>we are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit than the usual distribution-free and distribution-dependent bounds for the non-bayesian stochastic bandit.</s> <s>we first show that thompson sampling attains an optimal prior-free bound in the sense that for any prior distribution its bayesian regret is bounded from above by $14 \\sqrt{n k}$.</s> <s>this result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a bayesian regret bounded from below by $\\frac{1}{20} \\sqrt{n k}$.</s> <s>we also study the case of priors for the setting of bubeck et al.</s> <s>[2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of thompson sampling is in fact uniformly bounded over time, thus showing that thompson sampling can greatly take advantage of the nice properties of these priors.</s></p></d>", "label": ["<d><p><s>prior-free and prior-dependent regret bounds for thompson sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an infinite-armed bandit problem with bernoulli rewards.</s> <s>the mean rewards are independent,   uniformly distributed over $[0,1]$.</s> <s>rewards 0 and  1 are referred to as a success and a failure, respectively.</s> <s>we propose a novel algorithm where the decision to exploit  any arm is based on two successive  targets, namely, the total number of successes until the first failure and the first $m$ failures, respectively, where $m$ is a fixed parameter.</s> <s>this two-target algorithm  achieves a long-term average regret in $\\sqrt{2n}$ for  a large parameter $m$ and a known time horizon $n$.</s> <s>this regret is optimal and  strictly less than the regret  achieved by the best known algorithms, which is in $2\\sqrt{n}$.</s> <s>the results are extended to any mean-reward distribution whose support contains 1 and to unknown  time horizons.</s> <s>numerical experiments show the performance of the algorithm for finite time horizons.</s></p></d>", "label": ["<d><p><s>two-target algorithms  for infinite-armed   bandits with bernoulli rewards</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>thompson sampling has been demonstrated in many complex bandit models, however the theoretical guarantees available for the parametric multi-armed bandit are still limited to the bernoulli case.</s> <s>here we extend them by proving asymptotic optimality of the algorithm using the jeffreys prior for $1$-dimensional exponential family bandits.</s> <s>our proof builds on previous work, but also makes extensive use of closed forms for kullback-leibler divergence and fisher information (and thus jeffreys prior) available in an exponential family.</s> <s>this allow us to give a finite time exponential concentration inequality for posterior distributions on exponential families that may be of interest in its own right.</s> <s>moreover our analysis covers some distributions for which no optimistic algorithm has yet been proposed, including heavy-tailed exponential families.</s></p></d>", "label": ["<d><p><s>thompson sampling for 1-dimensional exponential family bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>monte-carlo tree search is drawing great interest in the domain of planning under uncertainty, particularly when little or no domain knowledge is available.</s> <s>one of the central problems is the trade-off between exploration and exploitation.</s> <s>in this paper we present a novel bayesian mixture modelling and inference based thompson sampling approach to addressing this dilemma.</s> <s>the proposed dirichlet-normalgamma mcts (dng-mcts) algorithm represents the uncertainty of the accumulated reward for actions in the mcts search tree as a mixture of normal distributions and inferences on it in bayesian settings by choosing conjugate priors in the form of combinations of dirichlet and normalgamma distributions.</s> <s>thompson sampling is used to select the best action at each decision node.</s> <s>experimental results show that our proposed algorithm has achieved the state-of-the-art comparing with popular uct algorithm in the context of online planning for general markov decision processes.</s></p></d>", "label": ["<d><p><s>bayesian mixture modelling and inference based thompson sampling in monte-carlo tree search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider an unweighted k-nearest neighbor graph   on n points that have been sampled i.i.d.</s> <s>from some unknown density p on r^d.</s> <s>we prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or their distance or similarity scores.</s> <s>the key insights are that local differences in link numbers can be used to estimate some local function of p, and that integrating this function along shortest paths leads to an estimate of the underlying density.</s></p></d>", "label": ["<d><p><s>density estimation from unweighted k-nearest neighbor graphs: a roadmap</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by the desire to extend fast randomized techniques to nonlinear $l_p$ regression, we consider a class of structured regression problems.</s> <s>these problems involve vandermonde matrices which arise naturally in various statistical modeling settings, including classical polynomial fitting problems and recently developed randomized techniques for scalable kernel methods.</s> <s>we show that this structure can be exploited to further accelerate the solution of the regression problem, achieving running times that are faster than input sparsity''.</s> <s>we present empirical results confirming both the practical value of our modeling framework, as well as speedup benefits of randomized regression.\"</s></p></d>", "label": ["<d><p><s>sketching structured matrices for faster nonlinear regression</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present and study a distributed optimization algorithm by employing  a stochastic dual coordinate ascent method.</s> <s>stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems.</s> <s>it still lacks of efforts in studying them in a distributed framework.</s> <s>we make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between  computation and  communication.</s> <s>we verify  our analysis by experiments on real data sets.</s> <s>moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing svms in the same distributed framework, and observe competitive performances.</s></p></d>", "label": ["<d><p><s>trading computation for communication: distributed stochastic dual coordinate ascent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process.</s> <s>in particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow.</s> <s>if such locally adaptive smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation.</s> <s>this can lead to miscalibration of predictive intervals, which can be substantially too narrow or wide depending on the time.</s> <s>we propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix.</s> <s>this process is constructed utilizing latent dictionary functions in time, which are given nested gaussian process priors and linearly related to the observed data through a sparse mapping.</s> <s>using a differential equation representation, we bypass usual computational bottlenecks in obtaining mcmc and online algorithms for approximate bayesian inference.</s> <s>the performance is assessed in simulations and illustrated in a financial application.</s></p></d>", "label": ["<d><p><s>locally adaptive bayesian multivariate time series</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines.</s> <s>we develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series.</s> <s>our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on twitter only in a few distinct manners whereas we can collect massive amounts of twitter data.</s> <s>to operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a weighted majority voting\" classification rule that can be approximated by a nearest-neighbor classifier.</s> <s>we establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity.</s> <s>experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series.</s> <s>we then use weighted majority to forecast which news topics on twitter become trends, where we are able to detect such \"trending topics\" in advance of twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%.\"</s></p></d>", "label": ["<d><p><s>a latent source model for nonparametric time series classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many scientific data occur as sequences of multidimensional arrays called tensors.</s> <s>how can hidden, evolving trends in such data be extracted while preserving the tensor structure?</s> <s>the model that is traditionally used is the linear dynamical system (lds), which treats the observation at each time slice as a vector.</s> <s>in this paper, we propose the multilinear dynamical system (mlds) for modeling tensor time series and an expectation-maximization (em) algorithm to estimate the parameters.</s> <s>the mlds models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent, low-dimensional tensors.</s> <s>compared to the lds with an equal number of parameters, the mlds achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.</s></p></d>", "label": ["<d><p><s>multilinear dynamical systems for tensor time series</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>numerous datasets ranging from group memberships within social networks to purchase histories on e-commerce sites are represented by binary matrices.</s> <s>while this data is often either proprietary or sensitive, aggregated data, notably row and column marginals, is often viewed as much less sensitive, and may be furnished for analysis.</s> <s>here, we investigate how these data can be exploited to make inferences about the underlying matrix h. instead of assuming a generative model for h, we view the input marginals as constraints on the dataspace of possible realizations of h and compute the probability density function of particular entries h(i,j) of interest.</s> <s>we do this, for all the cells of h simultaneously, without generating realizations but rather via implicitly sampling the datasets that satisfy the input marginals.</s> <s>the end result is an efficient algorithm with running time equal to the time required by standard sampling techniques to generate a single dataset from the same dataspace.</s> <s>our experimental evaluation demonstrates the efficiency and the efficacy of our framework in multiple settings.</s></p></d>", "label": ["<d><p><s>what do row and column marginals reveal about your dataset?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries.</s> <s>we describe: effective algorithms for deciding if and entry can be reconstructed and, if so, for reconstructing and denoising it; and a priori bounds on the error of each entry, individually.</s> <s>in the noiseless case our algorithm is exact.</s> <s>for rank-one matrices, the new algorithm is fast, admits a highly-parallel implementation, and produces an error minimizing estimate that is qualitatively close to our theoretical and the state-of-the-art nuclear norm and optspace methods.</s></p></d>", "label": ["<d><p><s>error-minimizing estimates and universal entry-wise error bounds for low-rank matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most current planners assume complete domain models and focus on generating correct plans.</s> <s>unfortunately, domain modeling is a laborious and error-prone task, thus real world agents have to plan with incomplete domain models.</s> <s>while domain experts cannot guarantee completeness, often they are able to circumscribe the incompleteness of the model by providing annotations as to which parts of the domain model may be incomplete.</s> <s>in such cases, the goal should be to synthesize plans that are robust with respect to any known incompleteness of the domain.</s> <s>in this paper, we first introduce annotations expressing the knowledge of the domain incompleteness and formalize the notion of plan robustness with respect to an incomplete domain model.</s> <s>we then show an approach to compiling the problem of finding robust plans to the conformant probabilistic planning problem, and present experimental results with probabilistic-ff planner.</s></p></d>", "label": ["<d><p><s>synthesizing robust plans under incomplete domain models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search?''</s> <s>to this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees.</s> <s>we also explore another factor affecting the search performance -- margins of the partitions in these trees.</s> <s>we demonstrate, both theoretically and empirically, that large margin partitions can improve the search performance of a space-partitioning tree. \"</s></p></d>", "label": ["<d><p><s>which space partitioning tree to use for search?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the markov chain is a convenient tool to represent the dynamics of complex systems such as traffic and social systems, where probabilistic transition takes place between internal states.</s> <s>a markov chain is characterized by initial-state probabilities and a state-transition probability matrix.</s> <s>in the traditional setting, a major goal is to figure out properties of a markov chain when those probabilities are known.</s> <s>this paper tackles an inverse version of the problem: we find those probabilities from partial observations at a limited number of states.</s> <s>the observations include the frequency of visiting a state and the rate of reaching a state from another.</s> <s>practical examples of this task include traffic monitoring systems in cities, where we need to infer the traffic volume on every single link on a road network from a very limited number of observation points.</s> <s>we formulate this task as a regularized optimization problem for probability functions, which is efficiently solved using the notion of natural gradient.</s> <s>using synthetic and real-world data sets including city traffic monitoring data, we demonstrate the effectiveness of our method.</s></p></d>", "label": ["<d><p><s>solving inverse problem of markov chain with partial observations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in stochastic optimal control the distribution of the exogenous noise is typically unknown and must be inferred from limited data before dynamic programming (dp)-based solution schemes can be applied.</s> <s>if the conditional expectations in the dp recursions are estimated via kernel regression, however, the historical sample paths enter the solution procedure directly as they determine the evaluation points of the cost-to-go functions.</s> <s>the resulting data-driven dp scheme is asymptotically consistent and admits efficient computational solution when combined with parametric value function approximations.</s> <s>if training data is sparse, however, the estimated cost-to-go functions display a high variability and an optimistic bias, while the corresponding control policies perform poorly in out-of-sample tests.</s> <s>to mitigate these small sample effects, we propose a robust data-driven dp scheme, which replaces the expectations in the dp recursions with worst-case expectations over a set of distributions close to the best estimate.</s> <s>we show that the arising min-max problems in the dp recursions reduce to tractable conic programs.</s> <s>we also demonstrate that this robust algorithm dominates state-of-the-art benchmark algorithms in out-of-sample tests across several application domains.</s></p></d>", "label": ["<d><p><s>robust data-driven dynamic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the bayesian online change point detection (bocpd) algorithm provides an efficient way to do exact inference when the parameters of an underlying model may suddenly change over time.</s> <s>bocpd requires computation of the underlying model's posterior predictives, which can only be computed online in $o(1)$ time and memory for exponential family models.</s> <s>we develop variational approximations to the posterior on change point times (formulated as run lengths) for efficient inference when the underlying model is not in the exponential family, and does not have tractable posterior predictive distributions.</s> <s>in doing so, we develop improvements to online variational inference.</s> <s>we apply our methodology to a tracking problem using radar data with a signal-to-noise feature that is rice distributed.</s> <s>we also develop a variational method for inferring the parameters of the (non-exponential family) rice distribution.</s></p></d>", "label": ["<d><p><s>online variational approximations to non-exponential family change point models: with application to radar tracking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we introduce a novel method that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions.</s> <s>our method can be regarded as a natural extension of the one-class svm (ocsvm) algorithm that finds multiple parallel separating hyperplanes in a reproducing kernel hilbert space.</s> <s>we call our method q-ocsvm, as it can be used to estimate $q$ quantiles of a high-dimensional distribution.</s> <s>for this purpose, we introduce a new global convex optimization program that finds all estimated sets at once and show that it can be solved efficiently.</s> <s>we prove the correctness of our method and present empirical results that demonstrate its superiority over existing methods.</s></p></d>", "label": ["<d><p><s>q-ocsvm: a q-quantile estimator for high-dimensional distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic and-or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events.</s> <s>we present a unified formalization of stochastic and-or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars.</s> <s>starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar.</s> <s>in our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.</s></p></d>", "label": ["<d><p><s>unsupervised structure learning of stochastic and-or grammars</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>distance-based approaches to outlier detection are popular in data mining, as they do not require to model the underlying probability distribution, which is particularly challenging for high-dimensional data.</s> <s>we present an empirical comparison of various approaches to distance-based outlier detection across a large number of datasets.</s> <s>we report the surprising observation that a simple, sampling-based scheme outperforms state-of-the-art techniques in terms of both efficiency and effectiveness.</s> <s>to better understand this phenomenon, we provide a theoretical analysis why the sampling-based approach outperforms alternative methods based on k-nearest neighbor search.</s></p></d>", "label": ["<d><p><s>rapid distance-based outlier detection via sampling</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>people can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems.</s> <s>here we present a hierarchical bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image.</s> <s>we evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models.</s> <s>we also used a visual turing test\" to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing.\"</s></p></d>", "label": ["<d><p><s>one-shot learning by inverting a compositional causal process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function.</s> <s>because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing.</s> <s>in this paper, we intend to make this principle scalable.</s> <s>we introduce a stochastic majorization-minimization scheme which is able to deal with large-scale or possibly infinite data sets.</s> <s>when applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of $o(1/\\sqrt{n})$ after~$n$ iterations, and of $o(1/n)$ for strongly convex functions.</s> <s>equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems.</s> <s>we develop several efficient algorithms based on our framework.</s> <s>first, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale $\\ell_1$-logistic regression.</s> <s>second, we develop an online dc programming algorithm for non-convex sparse estimation.</s> <s>finally, we demonstrate the effectiveness of our technique for solving large-scale structured matrix factorization problems.</s></p></d>", "label": ["<d><p><s>stochastic majorization-minimization algorithms for large-scale optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>principal component analysis (pca), a well-established technique for data analysis and processing, provides a convenient form of dimensionality reduction that is effective for cleaning small gaussian noises presented in the data.</s> <s>however, the applicability of standard principal component analysis in real scenarios is limited by its sensitivity to large errors.</s> <s>in this paper, we tackle the challenge problem of recovering data corrupted with errors of high magnitude by developing a novel robust transfer principal component analysis method.</s> <s>our method is based on the assumption that useful information for the recovery of a corrupted data matrix can be gained from an uncorrupted related data matrix.</s> <s>speci?cally, we formulate the data recovery problem as a joint robust principal component analysis problem on the two data matrices, with shared common principal components across matrices and individual principal components speci?c to each data matrix.</s> <s>the formulated optimization problem is a minimization problem over a convex objective function but with non-convex rank constraints.</s> <s>we develop an ef?cient proximal projected gradient descent algorithm to solve the proposed optimization problem with convergence guarantees.</s> <s>our empirical results over image denoising tasks show the proposed method can effectively recover images with random large errors, and signi?cantly outperform both standard pca and robust pca.</s></p></d>", "label": ["<d><p><s>robust transfer principal component analysis with rank constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>robust pca methods are typically based on batch optimization and have to load all the samples into memory.</s> <s>this prevents them from efficiently processing big data.</s> <s>in this paper, we develop  an online robust principal component analysis (or-pca) that processes one sample per time instance and hence its memory cost is independent of the data size,  significantly enhancing the computation and storage efficiency.</s> <s>the proposed method is based on stochastic optimization of an equivalent reformulation of the batch rpca method.</s> <s>indeed, we show that or-pca provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust  to sparse corruption.</s> <s>moreover, or-pca can naturally be applied for tracking dynamic subspace.</s> <s>comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efficiency advantages of the or-pca over online pca and batch rpca methods.</s></p></d>", "label": ["<d><p><s>online robust pca via stochastic optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we prove the first finite-sample convergence rates for any incremental pca algorithm using sub-quadratic time and memory per iteration.</s> <s>the algorithm analyzed is oja's learning rule, an efficient and well-known scheme for estimating the top principal component.</s> <s>our analysis of this non-convex problem yields expected and high-probability convergence rates of $\\tilde{o}(1/n)$ through a novel technique.</s> <s>we relate our guarantees to existing rates for stochastic gradient descent on strongly convex functions, and extend those results.</s> <s>we also include experiments which demonstrate convergence behaviors predicted by our analysis.</s></p></d>", "label": ["<d><p><s>the fast convergence of incremental pca</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>principal geodesic analysis (pga) is a generalization of principal component analysis (pca) for dimensionality reduction of data on a riemannian manifold.</s> <s>currently pga is defined as a geometric fit to the data, rather than as a probabilistic model.</s> <s>inspired by probabilistic pca, we present a latent variable model for pga that provides a probabilistic framework for factor analysis on manifolds.</s> <s>to compute maximum likelihood estimates of the parameters in our model, we develop a monte carlo expectation maximization algorithm, where the expectation is approximated by hamiltonian monte carlo sampling of the latent variables.</s> <s>we demonstrate the ability of our method to recover the ground truth parameters in simulated sphere data, as well as its effectiveness in analyzing shape variability of a corpus callosum data set from human brain images.</s></p></d>", "label": ["<d><p><s>probabilistic principal geodesic analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the performance of standard algorithms for independent component analysis quickly deteriorates under the addition of gaussian noise.</s> <s>this is partially due to a common first step that typically consists of whitening, i.e., applying principal component analysis (pca) and rescaling the components to have identity covariance, which is not invariant under gaussian noise.</s> <s>in our paper we develop the first practical algorithm for independent component analysis that is provably invariant under gaussian noise.</s> <s>the two main contributions of this work are as follows: 1.</s> <s>we develop and implement a more efficient version of a gaussian noise invariant decorrelation (quasi-orthogonalization) algorithm using hessians of the cumulant functions.</s> <s>2.</s> <s>we propose a very simple and efficient fixed-point gi-ica (gradient iteration ica) algorithm, which is compatible with quasi-orthogonalization, as well as with the usual pca-based whitening in the noiseless case.</s> <s>the algorithm is based on a special form of gradient iteration (different from gradient descent).</s> <s>we provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants.</s> <s>we also present a number of experimental comparisons with the existing methods, showing superior results on noisy data and very competitive performance in the noiseless case.</s></p></d>", "label": ["<d><p><s>fast algorithms for gaussian noise invariant independent component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the online principal component analysis (pca) for contaminated samples (containing outliers) which are revealed sequentially to the principal components (pcs) estimator.</s> <s>due to their sensitiveness to outliers, previous online pca algorithms fail in this case and their results can be arbitrarily bad.</s> <s>here we propose the online robust pca algorithm, which is able to improve the pcs estimation upon an initial one steadily, even when faced with a constant fraction of outliers.</s> <s>we show that the final result of the proposed online rpca has an acceptable degradation from the optimum.</s> <s>actually, under mild conditions, online rpca achieves the maximal robustness with a $50\\%$ breakdown point.</s> <s>moreover, online rpca is shown to be efficient for both storage and computation, since it need not re-explore the previous samples as in traditional robust pca algorithms.</s> <s>this endows online rpca with scalability for large scale data.</s></p></d>", "label": ["<d><p><s>online pca for contaminated data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-$d$ projection matrices (the fantope).</s> <s>the convex problem can be solved efficiently using alternating direction method of multipliers (admm).</s> <s>we establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model.</s> <s>in the special case of $d=1$, our result implies the near- optimality of dspca even when the solution is not rank 1.</s> <s>we also provide a general theoretical framework for analyzing the statistical  properties of the method for arbitrary input matrices that extends the  applicability and provable guarantees to a wide array of settings.</s> <s>we  demonstrate this with an application to kendall's tau correlation matrices  and transelliptical component analysis.</s></p></d>", "label": ["<d><p><s>fantope projection and selection: a near-optimal convex relaxation of sparse pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we model a one-shot learning\" situation, where very few (scalar) observations $y_1,...,y_n$ are available.</s> <s>associated with each observation $y_i$ is a very high-dimensional vector $x_i$, which provides context for $y_i$ and enables us to predict subsequent observations, given their own context.</s> <s>one of the salient features of our analysis is that the problems studied here are easier when the dimension of $x_i$ is large; in other words, prediction becomes easier when more context is provided.</s> <s>the proposed methodology is a variant of principal component regression (pcr).</s> <s>our rigorous analysis sheds new light on pcr.</s> <s>for instance, we show that classical pcr estimators may be inconsistent in the specified setting, unless they are multiplied by a scalar $c > 1$; that is, unless the classical estimator is expanded.</s> <s>this expansion phenomenon appears to be somewhat novel and contrasts with shrinkage methods ($c < 1$), which are far more common in big data analyses. \"</s></p></d>", "label": ["<d><p><s>one-shot learning and big data with n=2</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the randomized dependence coefficient (rdc), a measure of non-linear dependence between random variables of arbitrary dimension based on the hirschfeld-gebelein-r?nyi maximum correlation coefficient.</s> <s>rdc is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of r code, included at the end of the paper.</s></p></d>", "label": ["<d><p><s>the randomized dependence coefficient</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the sparse additive model for text modeling involves the sum-of-exp computing, with consuming costs for large scales.</s> <s>moreover, the assumption of equal background across all classes/topics may be too strong.</s> <s>this paper extends to propose sparse additive model with low rank background (sam-lrb), and simple yet efficient estimation.</s> <s>particularly, by employing a double majorization bound, we approximate the log-likelihood into a quadratic lower-bound with the sum-of-exp terms absent.</s> <s>the constraints of low rank and sparsity are then simply embodied by nuclear norm and $\\ell_1$-norm regularizers.</s> <s>interestingly, we find that the optimization task in this manner can be transformed into the same form as that in robust pca.</s> <s>consequently, parameters of supervised sam-lrb can be efficiently learned using an existing algorithm for robust pca based on accelerated proximal gradient.</s> <s>besides the supervised case, we extend sam-lrb to also favor unsupervised and multifaceted scenarios.</s> <s>experiments on real world data demonstrate the effectiveness and efficiency of sam-lrb, showing state-of-the-art performances.</s></p></d>", "label": ["<d><p><s>sparse additive text models with low rank background</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in text analysis documents are represented as disorganized bags of words, models of count features are typically based on mixing a small number of topics \\cite{lda,sam}.</s> <s>recently, it has been observed that for many text corpora documents evolve into one another in a smooth way, with some features dropping and new ones being introduced.</s> <s>the counting grid \\cite{cguai} models this spatial metaphor literally: it is multidimensional grid of word distributions learned in such a way that a document's own distribution of features can be modeled as the sum of the histograms found in a window into the grid.</s> <s>the major drawback of this method is that it is essentially a mixture and all the content much be generated by a single contiguous area on the grid.</s> <s>this may be problematic especially for lower dimensional grids.</s> <s>in this paper, we overcome to this issue with the \\emph{componential counting grid} which brings the componential nature of topic models to the basic counting grid.</s> <s>we also introduce a generative kernel based on the document's grid usage and a visualization strategy useful for understanding large text corpora.</s> <s>we evaluate our approach on document classification and multimodal retrieval obtaining state of the art results on standard benchmarks.</s></p></d>", "label": ["<d><p><s>documents as multiple overlapping windows into grids of counts</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>nonnegative matrix factorization (nmf) is a popular data analysis method, the objective of which is to decompose a matrix with all nonnegative components into the product of two other nonnegative matrices.</s> <s>in this work, we describe a new simple and efficient algorithm for multi-factor nonnegative matrix factorization problem ({mfnmf}), which generalizes the original nmf problem to more than two factors.</s> <s>furthermore, we extend the mfnmf algorithm to incorporate a regularizer based on dirichlet distribution over normalized columns to encourage sparsity in the obtained factors.</s> <s>our sparse nmf algorithm affords a closed form and an intuitive interpretation, and is more efficient in comparison with previous works that use fix point iterations.</s> <s>we demonstrate the effectiveness and efficiency of our algorithms on both synthetic and real data sets.</s></p></d>", "label": ["<d><p><s>on algorithms for sparse multi-factor nmf</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations.</s> <s>however, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models.</s> <s>significant efforts have been devoted to sparsity-based model selection to decrease this cost.</s> <s>such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time.</s> <s>we address the key challenge of learning to control fine-grained feature extraction adaptively, exploiting non-homogeneity of the data.</s> <s>we propose an architecture that uses a rich feedback loop between extraction and prediction.</s> <s>the run-time control policy is learned using efficient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input.</s> <s>we demonstrate significant speedups over state-of-the-art methods on two challenging datasets.</s> <s>for articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is simultaneously 4$\\times$ faster while using only a small fraction of possible features, with similar results on an ocr task.</s></p></d>", "label": ["<d><p><s>learning adaptive value of information for structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the scalability of symbolic planning under uncertainty with factored states and actions.</s> <s>prior work has focused almost exclusively on factored states but not factored actions, and on value iteration (vi) compared to policy iteration (pi).</s> <s>our ?rst contribution is a novel method for symbolic policy backups via the application of constraints, which is used to yield a new ef?cient symbolic imple- mentation of modi?ed pi (mpi) for factored action spaces.</s> <s>while this approach improves scalability in some cases, naive handling of policy constraints comes with its own scalability issues.</s> <s>this leads to our second and main contribution, symbolic opportunistic policy iteration (opi), which is a novel convergent al- gorithm lying between vi and mpi.</s> <s>the core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update, and otherwise performs full bellman backups, thus automatically adjusting the backup per state.</s> <s>we also give a memory bounded version of this algorithm allowing a space-time tradeoff.</s> <s>empirical results show signi?cantly improved scalability over the state-of-the-art.</s></p></d>", "label": ["<d><p><s>symbolic opportunistic policy iteration for factored-action mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents four major results towards solving decentralized partially observable markov decision problems (decpomdps) culminating in an algorithm that outperforms all existing algorithms on all but one standard infinite-horizon benchmark problems.</s> <s>(1) we give an integer program that solves collaborative bayesian games (cbgs).</s> <s>the program is notable because its linear relaxation is very often integral.</s> <s>(2) we show that a decpomdp with bounded belief can be converted to a pomdp (albeit with actions exponential in the number of beliefs).</s> <s>these actions correspond to strategies of a cbg.</s> <s>(3) we present a method to transform any decpomdp into a decpomdp with bounded beliefs (the number of beliefs is a free parameter) using optimal (not lossless) belief compression.</s> <s>(4) we show that the combination of these results opens the door for new classes of decpomdp algorithms based on previous pomdp algorithms.</s> <s>we choose one such algorithm, point-based valued iteration, and modify it to produce the first tractable value iteration method for decpomdps which outperforms existing algorithms.</s></p></d>", "label": ["<d><p><s>point based value iteration with optimal belief compression for dec-pomdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study monte carlo tree search (mcts) in zero-sum extensive-form games with perfect information and simultaneous moves.</s> <s>we present a general template of mcts algorithms for these games, which can be instantiated by various selection methods.</s> <s>we formally prove that if a selection method is $\\epsilon$-hannan consistent in a matrix game and satisfies additional requirements on exploration, then the mcts algorithm eventually converges to an approximate nash equilibrium (ne) of the extensive-form game.</s> <s>we empirically evaluate this claim using regret matching and exp3 as the selection methods on randomly generated and worst case games.</s> <s>we confirm the formal result and show that additional mcts variants also converge to approximate ne on the evaluated games.</s></p></d>", "label": ["<d><p><s>convergence of monte carlo tree search in simultaneous move games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in search advertising, the search engine needs to select the most profitable advertisements to display, which can be formulated as an instance of online learning with partial feedback, also known as the stochastic multi-armed bandit (mab) problem.</s> <s>in this paper, we show that the naive application of mab algorithms to search advertising for advertisement selection will produce sample selection bias that harms the search engine by decreasing expected revenue and ?estimation of the largest mean?</s> <s>(elm) bias that harms the advertisers by increasing game-theoretic player-regret.</s> <s>we then propose simple bias-correction methods with benefits to both the search engine and the advertisers.</s></p></d>", "label": ["<d><p><s>estimation bias in multi-armed bandit algorithms for search advertising</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide several applications of optimistic mirror descent, an online learning algorithm based on the idea of predictable sequences.</s> <s>first, we recover the mirror-prox algorithm, prove an extension to holder-smooth functions, and apply the results to saddle-point type problems.</s> <s>second, we prove that a version of optimistic mirror descent (which has a close relation to the exponential weights algorithm) can be used by two strongly-uncoupled players in a finite zero-sum matrix game to converge to the minimax equilibrium at the rate of o(log t / t).</s> <s>this addresses a question of daskalakis et al, 2011.</s> <s>further, we consider a partial information version of the problem.</s> <s>we then apply the results to approximate convex programming  and show a simple algorithm for the approximate max-flow problem.</s></p></d>", "label": ["<d><p><s>optimization, learning, and games with predictable sequences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we design and analyze minimax-optimal algorithms for online linear   optimization games where the player's choice is unconstrained.</s> <s>the   player strives to minimize regret, the difference between his loss   and the loss of a post-hoc benchmark strategy.</s> <s>the standard   benchmark is the loss of the best strategy chosen from a bounded   comparator set, whereas we consider a broad range of benchmark   functions.</s> <s>we consider the problem as a sequential multi-stage   zero-sum game, and we give a thorough analysis of the minimax   behavior of the game, providing characterizations for the value of   the game, as well as both the player's and the adversary's optimal   strategy.</s> <s>we show how these objects can be computed efficiently   under certain circumstances, and by selecting an appropriate   benchmark, we construct a novel hedging strategy for an   unconstrained betting game.</s></p></d>", "label": ["<d><p><s>minimax optimal algorithms for unconstrained linear optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces the online probing\" problem: in each round, the learner is able to purchase the values of a subset of feature values.</s> <s>after the learner uses this information to come up with a prediction for the given round, he then has the option of paying for seeing the loss that he is evaluated against.</s> <s>either way, the learner pays for the imperfections of his predictions and whatever he chooses to observe, including the cost of observing the loss function for the given round and the cost of the observed features.</s> <s>we consider two variations of this problem, depending on whether the learner can observe the label for free or not.</s> <s>we provide algorithms and upper and lower bounds on the regret for both variants.</s> <s>we show that a positive cost for observing the label significantly increases the regret of the problem.\"</s></p></d>", "label": ["<d><p><s>online learning with costly features and labels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>performance guarantees for online learning algorithms typically take the form of regret bounds, which express that the cumulative loss overhead compared to the best expert in hindsight is small.</s> <s>in the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts, at the cost of modest additional overhead  compared to more complex others.</s> <s>we study which such regret trade-offs can be achieved, and how.</s> <s>we analyse regret w.r.t.</s> <s>each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss.</s> <s>we characterise the achievable and pareto optimal trade-offs, and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically.</s></p></d>", "label": ["<d><p><s>the pareto regret frontier</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback.</s> <s>we measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player's behavior.</s> <s>in a setting where losses are allowed to drift, we characterize ---in a nearly complete manner--- the power of adaptive adversaries with bounded memories and switching costs.</s> <s>in particular, we show that with switching costs, the attainable rate with bandit feedback is $t^{2/3}$.</s> <s>interestingly, this rate is significantly worse than the $\\sqrt{t}$ rate attainable with switching costs in the full-information case.</s> <s>via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force $t^{2/3}$ regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries.</s> <s>our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.</s></p></d>", "label": ["<d><p><s>online learning with switching costs and other adaptive adversaries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many applications in machine learning require optimizing unknown functions defined over a high-dimensional space from noisy samples that are expensive to obtain.</s> <s>we address this notoriously hard challenge, under the assumptions that the function varies only along some low-dimensional subspace and is smooth (i.e., it has a low norm in a reproducible kernel hilbert space).</s> <s>in particular, we present the si-bo algorithm, which leverages recent low-rank matrix recovery techniques to learn the underlying subspace of the unknown function and applies gaussian process upper confidence sampling for optimization of the function.</s> <s>we carefully calibrate the exploration?exploitation tradeoff by allocating sampling budget to subspace estimation and function optimization, and obtain the first subexponential cumulative regret bounds and convergence rates for bayesian optimization in high-dimensions under noisy observations.</s> <s>numerical results demonstrate the effectiveness of our approach in difficult scenarios.</s></p></d>", "label": ["<d><p><s>high-dimensional gaussian process bandits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>undirected graphical models, such as gaussian graphical models, ising, and multinomial/categorical graphical models, are widely used in a variety of applications for modeling distributions over a large number of variables.</s> <s>these standard instances, however, are ill-suited to modeling count data, which are increasingly ubiquitous in big-data settings such as genomic sequencing data, user-ratings data, spatial incidence data, climate studies, and site visits.</s> <s>existing classes of poisson graphical models, which arise as the joint distributions that correspond to poisson distributed node-conditional distributions, have a major drawback: they can only model negative conditional dependencies for reasons of normalizability given its infinite domain.</s> <s>in this paper, our objective is to modify the poisson graphical model distribution so that it can capture a rich dependence structure between count-valued variables.</s> <s>we begin by discussing two strategies for truncating the poisson distribution and show that only one of these leads to a valid joint distribution; even this model, however, has limitations on the types of variables and dependencies that may be modeled.</s> <s>to address this, we propose two novel variants of the poisson distribution and their corresponding joint graphical model distributions.</s> <s>these models provide a class of poisson graphical models that can capture both positive and negative conditional dependencies between count-valued variables.</s> <s>one can learn the graph structure of our model via penalized neighborhood selection, and we demonstrate the performance of our methods by learning simulated networks as well as a network from microrna-sequencing data.</s></p></d>", "label": ["<d><p><s>on poisson graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>conditional random fields, which model the distribution of a multivariate response conditioned on a set of covariates using undirected graphs, are widely used in a variety of multivariate prediction applications.</s> <s>popular instances of this class of models such as categorical-discrete crfs, ising crfs, and conditional gaussian based crfs, are not however best suited to the varied types of response variables in many applications, including count-valued responses.</s> <s>we thus introduce a ?novel subclass of crfs?, derived by imposing node-wise conditional distributions of response variables conditioned on the rest of the responses and the covariates as arising from univariate exponential families.</s> <s>this allows us to derive novel multivariate crfs given any univariate exponential distribution, including the poisson, negative binomial, and exponential distributions.</s> <s>also in particular, it addresses the common crf problem of specifying feature'' functions determining the interactions between response variables and covariates.</s> <s>we develop a class of tractable penalized $m$-estimators to learn these crf distributions from data, as well as a unified sparsistency analysis for this general class of crfs showing exact structure recovery can be achieved with high probability.\"</s></p></d>", "label": ["<d><p><s>conditional random fields via univariate exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while graphs with continuous node attributes arise in many applications, state-of-the-art graph kernels for comparing continuous-attributed graphs suffer from a high runtime complexity; for instance, the popular shortest path kernel scales as $\\mathcal{o}(n^4)$, where $n$ is the number of nodes.</s> <s>in this paper, we present a class of path kernels with computational complexity $\\mathcal{o}(n^2 (m + \\delta^2))$, where $\\delta$ is the graph diameter and $m$ the number of edges.</s> <s>due to the sparsity and small diameter of real-world graphs, these kernels scale comfortably to large graphs.</s> <s>in our experiments, the presented kernels outperform state-of-the-art kernels in terms of speed and accuracy on classification benchmark datasets.</s></p></d>", "label": ["<d><p><s>scalable kernels for graphs with continuous attributes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the detection of anomalous activity in graphs is a statistical problem that arises in many applications, such as network surveillance, disease outbreak detection, and activity monitoring in social networks.</s> <s>beyond its wide applicability, graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power.</s> <s>in this work, we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in gaussian noise.</s> <s>because this test is computationally infeasible, we provide a relaxation, called the lov\\'asz extended scan statistic (less) that uses submodularity to approximate the intractable generalized likelihood ratio.</s> <s>we demonstrate a connection between less and maximum a-posteriori inference in markov random fields, which provides us with a poly-time algorithm for less.</s> <s>using electrical network theory, we are able to control type 1 error for less and prove conditions under which less is risk consistent.</s> <s>finally, we consider specific graph models, the torus, $k$-nearest neighbor graphs, and $\\epsilon$-random graphs.</s> <s>we show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds.</s></p></d>", "label": ["<d><p><s>near-optimal anomaly detection in graphs using lovasz extended scan statistic</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we show that either explicitly or implicitly, various well-known graph-based models exhibit a common significant \\emph{harmonic} structure in its target function -- the value of a vertex is approximately the weighted average of the values of its adjacent neighbors.</s> <s>understanding of such structure and analysis of the loss defined over such structure help reveal important properties of the target function over a graph.</s> <s>in this paper, we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost.</s> <s>we use this to develop an analytical tool and analyze 5 popular models in graph-based learning: absorbing random walks, partially absorbing random walks, hitting times, pseudo-inverse of graph laplacian, and eigenvectors of the laplacian matrices.</s> <s>our analysis well explains several open questions of these models reported in the literature.</s> <s>furthermore, it provides theoretical justifications and guidelines for their practical use.</s> <s>simulations on synthetic and real datasets support our analysis.</s></p></d>", "label": ["<d><p><s>analyzing the harmonic structure in graph-based learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>gaussian graphical models (ggms) or gauss markov random fields are widely used in many applications, and the trade-off between the modeling capacity and the efficiency of learning and inference has been an important research problem.</s> <s>in this paper, we study the family of ggms with small feedback vertex sets (fvss), where an fvs is a set of nodes whose removal breaks all the cycles.</s> <s>exact inference such as computing the marginal distributions and the partition function has complexity $o(k^{2}n)$  using message-passing algorithms, where k  is the size of the fvs, and n  is the total number of nodes.</s> <s>we propose efficient structure learning algorithms for two cases: 1) all nodes are observed, which is useful in modeling social or flight networks where the fvs nodes often correspond to a small number of high-degree nodes, or hubs, while the rest of the networks is modeled by a tree.</s> <s>regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate in $o(kn^2+n^2\\log n)$  if the fvs is known or in polynomial time if the fvs is unknown but has bounded size.</s> <s>2) the fvs nodes are latent variables, where structure learning is equivalent to decomposing a inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix.</s> <s>by incorporating efficient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank correction with complexity $o(kn^{2}+n^{2}\\log n)$  per iteration.</s> <s>we also perform experiments using both synthetic data as well as real data of flight delays to demonstrate the modeling capacity with fvss of various sizes.</s> <s>we show that empirically the family of ggms of size $o(\\log n)$ strikes a good balance between the modeling capacity and the efficiency.</s></p></d>", "label": ["<d><p><s>learning gaussian graphical models with observed or latent fvss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider energy minimization for undirected graphical models, also known as map-inference problem for markov random fields.</s> <s>although combinatorial methods, which return a provably optimal integral solution of the problem, made a big progress in the past decade, they are still typically unable to cope with large-scale datasets.</s> <s>on the other hand, large scale datasets are typically defined on sparse graphs, and convex relaxation methods, such as linear programming relaxations often provide good approximations to integral solutions.</s> <s>we propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem.</s> <s>based on the information obtained from the solution of the convex relaxation, our method confines application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve big problems.</s> <s>we demonstrate the power of our approach on a computer vision energy minimization benchmark.</s></p></d>", "label": ["<d><p><s>global map-optimality by shrinking the combinatorial search area with convex relaxation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lifting attempts to speedup probabilistic inference by exploiting symmetries in the model.</s> <s>exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem.</s> <s>in the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori.</s> <s>however, there is currently no equivalent structure nor analogous complexity results for lifted inference.</s> <s>in this paper, we introduce fo-dtrees, which upgrade propositional dtrees to the first-order level.</s> <s>we show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.</s></p></d>", "label": ["<d><p><s>first-order decomposition trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discovering hierarchical regularities in data is a key problem in interacting   with large datasets, modeling cognition, and encoding knowledge.</s> <s>a previous   bayesian solution---kingman's coalescent---provides a convenient probabilistic   model for data represented as a binary tree.</s> <s>unfortunately, this is   inappropriate for data better described by bushier trees.</s> <s>we generalize an   existing belief propagation framework of kingman's coalescent to the   beta coalescent, which models a wider range of tree structures.</s> <s>because of the complex combinatorial search over possible structures, we   develop new sampling schemes using sequential monte carlo and dirichlet   process mixture models, which render inference efficient and tractable.</s> <s>we present results on both synthetic and real data that show the beta coalescent      outperforms kingman's coalescent on real datasets and is qualitatively better at    capturing data in bushy hierarchies.</s></p></d>", "label": ["<d><p><s>binary to bushy: bayesian hierarchical clustering with the beta coalescent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel mcmc sampler for dirichlet process mixture models that can be used for conjugate or non-conjugate prior distributions.</s> <s>the proposed sampler can be massively parallelized to achieve significant computational gains.</s> <s>a non-ergodic restricted gibbs iteration is mixed with split/merge proposals to produce a valid sampler.</s> <s>each regular cluster is augmented with two sub-clusters to construct likely split moves.</s> <s>unlike many previous parallel samplers, the proposed sampler accurately enforces the correct stationary distribution of the markov chain without the need for approximate models.</s> <s>empirical results illustrate that the new sampler exhibits better convergence properties than current methods.</s></p></d>", "label": ["<d><p><s>parallel sampling of dp mixture models using sub-cluster splits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>inspired by a two-level theory that unifies agenda setting and ideological  framing, we propose supervised hierarchical latent dirichlet allocation  (shlda) which jointly captures documents' multi-level topic structure and  their polar response variables.</s> <s>our model extends the nested chinese restaurant  process to discover a tree-structured topic hierarchy and uses both per-topic  hierarchical and per-word lexical regression parameters to model the response  variables.</s> <s>experiments in a political domain and on sentiment analysis tasks  show that shlda improves predictive accuracy while adding a new dimension of  insight into how topics under discussion are framed.</s></p></d>", "label": ["<d><p><s>lexical and hierarchical topic regression</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>cross language text classi?cation is an important learning task in natural language processing.</s> <s>a critical challenge of cross language learning lies in that words of different languages are in disjoint feature spaces.</s> <s>in this paper, we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents.</s> <s>speci?cally, we ?rst formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages, and then induce a cross-lingual document representation by applying latent semantic indexing on the obtained matrix.</s> <s>we use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees.</s> <s>the proposed approach is evaluated by conducting a set of experiments with cross language sentiment classi?cation tasks on amazon product reviews.</s> <s>the experimental results demonstrate that the proposed learning approach outperforms a number of comparison cross language representation learning methods, especially when the number of parallel bilingual documents is small.</s></p></d>", "label": ["<d><p><s>a novel two-step method for cross language representation learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks.</s> <s>the best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.</s> <s>we propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation.</s> <s>our approach is simpler, faster, and produces better results than the current state-of-the art method of mikolov et al.</s> <s>(2013a).</s> <s>we achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time.</s> <s>we also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.</s></p></d>", "label": ["<d><p><s>learning word embeddings efficiently with noise-contrastive estimation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>time series often have a temporal hierarchy, with information that is spread out over multiple time scales.</s> <s>common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture.</s> <s>in this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series.</s> <s>here, each layer is a recurrent network which receives the hidden state of the previous layer as input.</s> <s>this architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series.</s> <s>we show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent.</s> <s>we also offer an analysis of the different emergent time scales.</s></p></d>", "label": ["<d><p><s>training and analysing deep recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>biological tissue is often composed of cells with similar morphologies replicated throughout large volumes and many biological applications rely on the accurate identification of these cells and their locations from image data.</s> <s>here we develop a generative model that captures the regularities present in images composed of repeating elements of a few different types.</s> <s>formally, the model can be described as convolutional sparse block coding.</s> <s>for inference we use a variant of convolutional matching pursuit adapted to block-based representations.</s> <s>we extend the k-svd learning algorithm to subspaces by retaining several principal vectors from the svd decomposition instead of just one.</s> <s>good models with little cross-talk between subspaces can be obtained by learning the blocks incrementally.</s> <s>we perform extensive experiments on simulated images and the inference algorithm consistently recovers a large proportion of the cells with a small number of false positives.</s> <s>we fit the convolutional model to noisy gcamp6 two-photon images of spiking neurons and to nissl-stained slices of cortical tissue and show that it recovers cell body locations without supervision.</s> <s>the flexibility of the block-based representation is reflected in the variability of the recovered cell shapes.</s></p></d>", "label": ["<d><p><s>extracting regions of interest from biological images with convolutional sparse block coding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>imaging neuroscience links brain activation maps to behavior and cognition via correlational studies.</s> <s>due to the nature of the individual experiments, based on eliciting neural response from a small number of stimuli, this link is incomplete, and unidirectional from the causal point of view.</s> <s>to come to conclusions on the function implied  by the activation of brain regions, it is necessary to combine a wide exploration of the various brain functions and some inversion of the statistical inference.</s> <s>here we introduce a methodology for accumulating knowledge towards a bidirectional link between observed brain activity and the corresponding function.</s> <s>we rely on a large corpus of imaging studies and a predictive engine.</s> <s>technically, the challenges are to find commonality between the studies without denaturing the richness of the corpus.</s> <s>the key elements that we contribute are labeling the tasks performed with a cognitive ontology, and modeling the long tail of rare paradigms in the corpus.</s> <s>to our knowledge, our approach is the first demonstration of predicting the cognitive content of completely new brain images.</s> <s>to that end, we propose a method that predicts the experimental paradigms across different studies.</s></p></d>", "label": ["<d><p><s>mapping paradigm ontologies to and from the brain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hermitian positive definite matrices (hpd) recur throughout statistics and machine learning.</s> <s>in this paper we develop \\emph{geometric optimisation} for globally optimising certain nonconvex loss functions arising in the modelling of data via elliptically contoured distributions (ecds).</s> <s>we exploit the remarkable structure of the convex cone of positive definite matrices which allows one to uncover hidden geodesic convexity of objective functions that are nonconvex in the ordinary euclidean sense.</s> <s>going even beyond manifold convexity we show how further metric properties of hpd matrices can be exploited to globally optimise several ecd log-likelihoods that are not even geodesic convex.</s> <s>we present key results that help recognise this geometric structure, as well as obtain efficient fixed-point algorithms to optimise the corresponding objective functions.</s> <s>to our knowledge, ours are the most general results on geometric optimisation of hpd matrices known so far.</s> <s>experiments reveal the benefits of our approach---it avoids any eigenvalue computations which makes it very competitive.</s></p></d>", "label": ["<d><p><s>geometric optimisation on positive definite matrices for elliptically contoured distributions</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>recently, [valiant and valiant] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample.</s> <s>specifically, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size o(n / log n).</s> <s>we propose a novel modification of this approach and show: 1) theoretically, our estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters.</s> <s>perhaps unsurprisingly, the key step in this approach is to first use the sample to characterize the unseen\" portion of the distribution.</s> <s>this goes beyond such tools as the good-turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the \"shape\"of the unobserved portion of the distribution.</s> <s>this approach is robust, general, and theoretically principled;  we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. \"</s></p></d>", "label": ["<d><p><s>estimating the unseen: improved estimators for entropy and other properties</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper extends factorized asymptotic bayesian (fab) inference for latent feature models~(lfms).</s> <s>fab inference has not been applicable to models, including lfms, without a specific condition on the hesqsian matrix of a complete log-likelihood, which is required to derive a factorized information criterion''~(fic).</s> <s>our asymptotic analysis of the hessian matrix of lfms shows that fic of lfms has the same form as those of mixture models.</s> <s>fab/lfms have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art indian buffet processes in terms of model selection, prediction, and computational efficiency.\"</s></p></d>", "label": ["<d><p><s>factorized asymptotic bayesian inference for latent feature models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>structure learning algorithms for graphical models have focused almost exclusively on stable environments in which the underlying generative process does not change; that is, they assume that the generating model is globally stationary.</s> <s>in real-world environments, however, such changes often occur without warning or signal.</s> <s>real-world data often come from generating models that are only locally stationary.</s> <s>in this paper, we present losst, a novel, heuristic structure learning algorithm that tracks changes in graphical model structure or parameters in a dynamic, real-time manner.</s> <s>we show by simulation that the algorithm performs comparably to batch-mode learning when the generating graphical structure is globally stationary, and significantly better when it is only locally stationary.</s></p></d>", "label": ["<d><p><s>tracking time-varying graphical structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a semiparametric procedure for estimating high dimensional sparse inverse covariance matrix.</s> <s>our method, named alice, is applicable to the elliptical family.</s> <s>computationally, we develop an efficient dual inexact iterative projection (${\\rm d_2}$p) algorithm based on the alternating direction method of multipliers (admm).</s> <s>theoretically, we prove that the alice estimator achieves the parametric rate of convergence in both parameter estimation and model selection.</s> <s>moreover, alice calibrates regularizations when estimating each column of the inverse covariance matrix.</s> <s>so it not only is asymptotically tuning free, but also achieves an improved finite sample performance.</s> <s>we present numerical simulations to support our theory, and a real data example to illustrate the effectiveness of the proposed estimator.</s></p></d>", "label": ["<d><p><s>sparse inverse covariance estimation with calibration</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we address the problem of learning a sparse bayesian network  structure for continuous variables in a high-dimensional space.</s> <s>the constraint that the estimated bayesian network structure  must be a directed acyclic graph (dag) makes the problem challenging because of the huge search space of network structures.</s> <s>most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure that satisfies the dag constraint in the second stage.</s> <s>although this approach is effective in a low-dimensional setting, it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting.</s> <s>in this paper, we propose a single-stage method, called a* lasso, that recovers the optimal  sparse bayesian network structure by solving a single optimization problem with a* search algorithm that uses lasso in its scoring system.</s> <s>our approach substantially improves the computational efficiency of the well-known exact methods  based on dynamic programming.</s> <s>we also present a heuristic scheme that further improves the efficiency of a* lasso without significantly compromising the quality of solutions and   demonstrate this on benchmark bayesian networks and real data.</s></p></d>", "label": ["<d><p><s>a* lasso for learning a sparse bayesian network structure for continuous variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>penalized m-estimators are used in diverse areas of science and engineering to fit high-dimensional models with some low-dimensional structure.</s> <s>often, the penalties are \\emph{geometrically decomposable}, \\ie\\ can be expressed as a sum of (convex) support functions.</s> <s>we generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of m-estimators with such penalties.</s> <s>we then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning.</s></p></d>", "label": ["<d><p><s>on model selection consistency of penalized m-estimators: a geometric theory</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a closed-loop brain-computer interface (bci), adaptive decoders are used to learn parameters suited to decoding the user's neural response.</s> <s>feedback to the user provides information which permits the neural tuning to also adapt.</s> <s>we present an approach to model this process of co-adaptation between the encoding model of the neural signal and the decoding algorithm as a multi-agent formulation of the linear quadratic gaussian (lqg) control problem.</s> <s>in simulation we characterize how decoding performance improves as the neural encoding and adaptive decoder optimize, qualitatively resembling experimentally demonstrated closed-loop improvement.</s> <s>we then propose a novel, modified decoder update rule which is aware of the fact that the encoder is also changing and show it can improve simulated co-adaptation dynamics.</s> <s>our modeling approach offers promise for gaining insights into co-adaptation as well as improving user learning of bci control in practical settings.</s></p></d>", "label": ["<d><p><s>a multi-agent control framework for co-adaptation in brain-computer interfaces</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>movement primitives (mp) are a well-established approach for representing modular and re-usable robot movement generators.</s> <s>many state-of-the-art robot learning successes are based mps, due to their compact representation of the inherently continuous and high dimensional robot movements.</s> <s>a major goal in robot learning is to combine multiple mps as building blocks in a modular control architecture to solve complex tasks.</s> <s>to this effect, a mp representation has to allow for blending between motions, adapting to altered task variables, and co-activating multiple mps in parallel.</s> <s>we present a probabilistic formulation of the mp concept that maintains a distribution over trajectories.</s> <s>our probabilistic approach allows for the derivation of new operations which are essential for implementing all aforementioned properties in one framework.</s> <s>in order to use such a trajectory distribution for robot movement control, we analytically derive a stochastic feedback controller which reproduces the given trajectory distribution.</s> <s>we evaluate and compare our approach to existing methods on several simulated as well as real robot scenarios.</s></p></d>", "label": ["<d><p><s>probabilistic movement primitives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in order to learn effective control policies for dynamical systems, policy search methods must be able to discover successful executions of the desired task.</s> <s>while random exploration can work well in simple domains, complex and high-dimensional tasks present a serious challenge, particularly when combined with high-dimensional policies that make parameter-space exploration infeasible.</s> <s>we present a method that uses trajectory optimization as a powerful exploration strategy that guides the policy search.</s> <s>a variational decomposition of a maximum likelihood policy objective allows us to use standard trajectory optimization algorithms such as differential dynamic programming, interleaved with standard supervised learning for the policy itself.</s> <s>we demonstrate that the resulting algorithm can outperform prior methods on two challenging locomotion tasks.</s></p></d>", "label": ["<d><p><s>variational policy search via trajectory optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning good trajectories for manipulation tasks.</s> <s>this is challenging because the criterion defining a good trajectory varies with users, tasks and environments.</s> <s>in this paper, we propose a co-active online  learning framework for teaching robots the preferences of its users for object  manipulation tasks.</s> <s>the key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system.</s> <s>we argue that this  co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators.</s> <s>nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms.</s> <s>we also formulate a score function to capture the contextual information and demonstrate the generalizability of our algorithm on a variety of household tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.</s></p></d>", "label": ["<d><p><s>learning trajectory preferences for  manipulators via iterative improvement</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science.</s> <s>we investigate this behavior in the context of a multi-armed bandit task.</s> <s>we compare human behavior to a variety of models that vary in their representational and computational complexity.</s> <s>our result shows that subjects' choices, on a trial-to-trial basis, are best captured by a forgetful\" bayesian iterative learning model in combination with a partially myopic decision policy known as knowledge gradient.</s> <s>this model accounts for subjects' trial-by-trial choice better than a number of other previously proposed models, including optimal bayesian learning and risk minimization, epsilon-greedy and win-stay-lose-shift.</s> <s>it has the added benefit of being closest in performance to the optimal bayesian model than all the other heuristic models that have the same computational complexity (all are significantly less complex than the optimal model).</s> <s>these results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment.\"</s></p></d>", "label": ["<d><p><s>forgetful bayes and myopic planning: human learning and decision-making in a bandit setting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans and animals readily utilize active sensing, or the use of self-motion, to focus sensory and cognitive resources on the behaviorally most relevant stimuli and events in the environment.</s> <s>understanding the computational basis of natural active sensing is important both for advancing brain sciences and for developing more powerful artificial systems.</s> <s>recently, a goal-directed, context-sensitive, bayesian control strategy for active sensing, termed c-dac (context-dependent active controller), was proposed (ahmad & yu, 2013).</s> <s>in contrast to previously proposed algorithms for human active vision, which tend to optimize abstract statistical objectives and therefore cannot adapt to changing behavioral context or task goals, c-dac directly minimizes behavioral costs and thus, automatically adapts itself to different task conditions.</s> <s>however, c-dac is limited as a model of human active sensing, given its computational/representational requirements, especially for more complex, real-world situations.</s> <s>here, we propose a myopic approximation to c-dac, which also takes behavioral costs into account, but achieves a significant reduction in complexity by looking only one step ahead.</s> <s>we also present data from a human active visual search experiment, and compare the performance of the various models against human behavior.</s> <s>we find that c-dac and its myopic variant both achieve better fit to human data than infomax (butko & movellan, 2010), which maximizes expected cumulative future information gain.</s> <s>in summary, this work provides novel experimental results that differentiate theoretical models for human active sensing, as well as a novel active sensing algorithm that retains the context-sensitivity of the optimal controller while achieving significant computational savings.</s></p></d>", "label": ["<d><p><s>context-sensitive active sensing in humans</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning.</s> <s>bellman error basis functions (bebfs) have been shown to improve the error of policy evaluation with function approximation, with a convergence rate similar to that of value iteration.</s> <s>we propose a simple, fast and robust algorithm based on random projections, which generates bebfs for sparse feature spaces.</s> <s>we provide a finite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error.</s> <s>empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging.</s></p></d>", "label": ["<d><p><s>bellman error based feature generation using random projections on sparse spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important challenge in markov decision processes is to ensure robustness with respect to unexpected or adversarial system behavior while taking advantage of well-behaving parts of the system.</s> <s>we consider a problem setting where some unknown parts of the state space can have arbitrary transitions while other parts are purely stochastic.</s> <s>we devise an algorithm that is adaptive to potentially adversarial behavior and show that it achieves similar regret bounds as the purely stochastic case.</s></p></d>", "label": ["<d><p><s>reinforcement learning in robust markov decision processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>natural actor-critics are a popular class of policy search algorithms for finding locally optimal policies for markov decision processes.</s> <s>in this paper we address a drawback of natural actor-critics that limits their real-world applicability - their lack of safety guarantees.</s> <s>we present a principled algorithm for performing natural gradient descent over a constrained domain.</s> <s>in the context of reinforcement learning, this allows for natural actor-critic algorithms that are guaranteed to remain within a known safe region of policy space.</s> <s>while deriving our class of constrained natural actor-critic algorithms, which we call projected natural actor-critics (pnacs), we also elucidate the relationship between natural gradient descent and mirror descent.</s></p></d>", "label": ["<d><p><s>projected natural actor-critic</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most provably efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration.</s> <s>we study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (psrl).</s> <s>this algorithm proceeds in repeated episodes of known duration.</s> <s>at the start of each episode, psrl updates a prior distribution over markov decision processes and takes one sample from this posterior.</s> <s>psrl then follows the policy that is optimal for this sample during the episode.</s> <s>the algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way.</s> <s>we establish an $\\tilde{o}(\\tau s \\sqrt{at} )$ bound on the expected regret, where $t$ is time, $\\tau$ is the episode length and $s$ and $a$ are the cardinalities of the state and action spaces.</s> <s>this bound is one of the first for an algorithm not based on optimism and close to the state of the art for any reinforcement learning algorithm.</s> <s>we show through simulation that psrl significantly outperforms existing algorithms with similar regret bounds.</s></p></d>", "label": ["<d><p><s>(more) efficient reinforcement learning via posterior sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the last decade, policy gradient methods have significantly grown in popularity in the reinforcement--learning field.</s> <s>in particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems.</s> <s>policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms.</s> <s>nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters.</s> <s>step--size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection.</s> <s>in this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain.</s> <s>focusing on gaussian policies, we derive a lower bound that is second--order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples.</s> <s>the properties of the proposed approach are empirically evaluated in a linear--quadratic regulator problem.</s></p></d>", "label": ["<d><p><s>adaptive step-size for policy gradient methods</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a long term goal of interactive reinforcement learning is to incorporate non-expert human feedback to solve complex tasks.</s> <s>state-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy.</s> <s>in this paper we argue for an alternate, more effective characterization of human feedback: policy shaping.</s> <s>we introduce advise, a bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy.</s> <s>we compare advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.</s></p></d>", "label": ["<d><p><s>policy shaping: integrating human feedback with reinforcement learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods.</s> <s>as our first main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them.</s> <s>the resulting continuum adjusts the strength of the markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in td($\\lambda$)-style algorithms in policy evaluation.</s> <s>as our second main result, we show for a substantial subset of soft-greedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward any optimal policy, except in a certain pathological case.</s> <s>consequently, in the context of approximations, the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.</s></p></d>", "label": ["<d><p><s>optimistic policy iteration and natural actor-critic: a unifying view and a non-optimality result</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>pomdps provide a principled framework for planning under uncertainty, but are computationally intractable, due to the ?curse of dimensionality?</s> <s>and the ?curse of history?.</s> <s>this paper presents an online lookahead search algorithm that alleviates these difficulties by limiting the search to a set of sampled scenarios.</s> <s>the execution of all policies on the sampled scenarios is summarized using a determinized sparse partially observable tree (despot), which is a sparsely sampled belief tree.</s> <s>our algorithm, named regularized despot (r-despot), searches the despot for a policy that optimally balances the size of the policy and the accuracy on its value estimate obtained through sampling.</s> <s>we give an output-sensitive performance bound for all policies derived from the despot, and show that r-despot works well if a small optimal policy exists.</s> <s>we also give an anytime approximation to r-despot.</s> <s>experiments show strong results, compared with two of the fastest online pomdp algorithms.</s></p></d>", "label": ["<d><p><s>despot: online pomdp planning with regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tetris is a popular video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (adp) algorithms.</s> <s>a close look at the literature of this game shows that while adp algorithms, that have been (almost) entirely based on approximating the value function (value function based), have performed poorly in tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (ce) method, have achieved the best reported results.</s> <s>this makes us conjecture that tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions.</s> <s>so, in order to obtain a good performance with adp, we should use adp algorithms that search in a policy space, instead of the more traditional ones that search in a value function space.</s> <s>in this paper, we put our conjecture to test by applying such an adp algorithm, called classification-based modified policy iteration (cbmpi), to the game of tetris.</s> <s>our extensive experimental results show that for the first time an adp algorithm, namely cbmpi, obtains the best results reported in the literature for tetris in both small $10\\times 10$ and large $10\\times 20$ boards.</s> <s>although the cbmpi's results are similar to those achieved by the ce method in the large board, cbmpi uses considerably fewer (almost 1/10) samples (call to the generative model of the game) than ce.</s></p></d>", "label": ["<d><p><s>approximate dynamic programming finally performs well in the game of tetris</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider how to transfer knowledge from previous tasks to a current task in long-lived and bounded agents that must solve a sequence of mdps over a finite lifetime.</s> <s>a novel aspect of our transfer approach is that we reuse reward functions.</s> <s>while this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational  bounds of  the agent.</s> <s>specifically, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks.</s> <s>we demonstrate that our approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.</s></p></d>", "label": ["<d><p><s>reward mapping for transfer in long-lived agents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background.</s> <s>in contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem.</s> <s>specifically, by using auxiliary natural images, we train a stacked denoising autoencoder offline to learn generic image features that are more robust against variations.</s> <s>this is then followed by knowledge transfer from offline training to the online tracking process.</s> <s>online tracking involves a classification neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classification layer.</s> <s>both the feature extractor and the classifier can be further tuned to adapt to appearance changes of the moving object.</s> <s>comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is very efficient as well as more accurate.</s></p></d>", "label": ["<d><p><s>learning a deep compact image representation for visual tracking</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>motivated by recent progress in natural image statistics, we use newly available datasets with ground truth optical flow to learn the  local statistics of optical flow and rigorously compare the learned model to prior models assumed by computer vision optical flow algorithms.</s> <s>we find that a gaussian mixture model with 64 components provides a significantly better model for local flow statistics when compared to commonly used models.</s> <s>we investigate the source of the gmms success and show it is related to an explicit representation of flow boundaries.</s> <s>we also learn a model that jointly models the local intensity pattern and the local optical flow.</s> <s>in accordance with the assumptions often made in computer vision, the model learns that flow boundaries are more likely at intensity boundaries.</s> <s>however, when evaluated on a large dataset, this dependency is very weak and the benefit of conditioning flow estimation on the local intensity pattern is marginal.</s></p></d>", "label": ["<d><p><s>learning the local statistics of optical flow</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>association field models have been used to explain human contour grouping performance and to explain the mean frequency of long-range horizontal connections across cortical columns in v1.</s> <s>however, association fields essentially depend on pairwise statistics of edges in natural scenes.</s> <s>we develop a spectral test of the sufficiency of pairwise statistics and show that there is significant higher-order structure.</s> <s>an analysis using a probabilistic spectral embedding reveals curvature-dependent components to the association field, and reveals a challenge for biological learning algorithms.</s></p></d>", "label": ["<d><p><s>third-order edge statistics: contour continuation, curvature, and cortical connections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding.</s> <s>by far most approaches to unsupervised learning learning of visual features, such as sparse coding or ica, account for translations by representing the same features at different positions.</s> <s>some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition.</s> <s>all probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches.</s> <s>here, we for the first time apply a model with non-linear feature superposition and explicit position encoding.</s> <s>by avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images.</s> <s>in order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters.</s> <s>we first investigated encodings learned by the model using artificial data with mutually occluding components.</s> <s>we find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model.</s> <s>on natural image patches, the model learns component masks and features for typical image components.</s> <s>by using reverse correlation, we estimate the receptive fields associated with the model's hidden units.</s> <s>we find many gabor-like or globular receptive fields as well as fields sensitive to more complex structures.</s> <s>our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex.</s></p></d>", "label": ["<d><p><s>what are the invariant occlusive components of image patches? a probabilistic generative approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>human eye movements provide a rich source of information into the human visual processing.</s> <s>the complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood.</s> <s>this has precluded the development of reliable dynamic eye movement prediction systems.</s> <s>our work makes three contributions towards addressing this problem.</s> <s>first, we complement one of the largest and most challenging static computer vision datasets, voc 2012 actions, with human eye movement annotations collected under the task constraints of action and context recognition.</s> <s>our dataset is unique among eyetracking datasets for still images in terms of its large scale (over 1 million fixations, 9157 images), task control and action from a single image emphasis.</s> <s>second, we introduce models to automatically discover areas of interest (aoi) and introduce novel dynamic consistency metrics, based on them.</s> <s>our method can automatically determine the number and spatial support of the aois, in addition to their locations.</s> <s>based on such encodings, we show that, on unconstrained read-world stimuli, task instructions have significant influence on visual behavior.</s> <s>finally, we leverage our large scale dataset in conjunction with powerful machine learning techniques and computer vision features, to introduce novel dynamic eye movement prediction methods which learn task-sensitive reward functions from eye movement data and efficiently integrate these rewards to plan future saccades based on inverse optimal control.</s> <s>we show that the propose methodology achieves state of the art scanpath modeling results.</s></p></d>", "label": ["<d><p><s>action from still image dataset and inverse optimal control to learn task specific visual scanpaths</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video.</s> <s>as part of the proposed approach we develop a generalization of the max-path search algorithm, which allows us to efficiently search over a structured space of multiple spatio-temporal paths, while also allowing to incorporate context information into the model.</s> <s>instead of using spatial annotations, in the form of bounding boxes, to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal.</s> <s>this is achieved by incorporating gaze, along with the classification, into the structured loss within the latent svm learning framework.</s> <s>experiments on a challenging benchmark dataset, ucf-sports, show that our model is more accurate, in terms of classification, and achieves state-of-the-art results in localization.</s> <s>in addition, we show how our model can produce top-down saliency maps conditioned on the classification label and localized latent paths.</s></p></d>", "label": ["<d><p><s>action is in the eye of the beholder: eye-gaze driven model for spatio-temporal action localization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many methods have been proposed to recover the intrinsic scene properties such as shape, reflectance and illumination from a single image.</s> <s>however, most of these models have been applied on laboratory datasets.</s> <s>in this work we explore the synergy effects between intrinsic scene properties recovered from an image, and the objects and attributes present in the scene.</s> <s>we cast the problem in a joint energy minimization framework; thus our model is able to encode the strong correlations between intrinsic properties (reflectance, shape, illumination), objects (table, tv-monitor), and materials (wooden, plastic) in a given scene.</s> <s>we tested our approach on the nyu and pascal datasets, and observe both qualitative and quantitative improvements in the overall accuracy.</s></p></d>", "label": ["<d><p><s>higher order priors for joint intrinsic image, objects, and attributes estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application, perhaps particularly so for computer vision.</s> <s>however, they face a fundamental limitation: given enough data, the number of nodes in decision trees will grow exponentially with depth.</s> <s>for certain applications, for example on mobile or embedded processors, memory is a limited resource, and so the exponential growth of trees limits their depth, and thus their potential accuracy.</s> <s>this paper proposes decision jungles, revisiting the idea of ensembles of rooted decision directed acyclic graphs (dags), and shows these to be compact and powerful discriminative models for classification.</s> <s>unlike conventional decision trees that only allow one path to every node, a dag in a decision jungle allows multiple paths from the root to each leaf.</s> <s>we present and compare two new node merging algorithms that jointly optimize both the features and the structure of the dags efficiently.</s> <s>during training, node splitting and node merging are driven by the minimization of exactly the same objective function, here the weighted sum of entropies at the leaves.</s> <s>results on varied datasets show that, compared to decision forests and several other baselines, decision jungles require dramatically less memory while considerably improving generalization.</s></p></d>", "label": ["<d><p><s>decision jungles: compact and rich models for classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a common assumption in machine vision is that the training and test samples are drawn from the same distribution.</s> <s>however, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions.</s> <s>this problem is accentuated with 3d data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training.</s> <s>in this paper we present a multi-task learning algorithm for domain adaptation based on boosting.</s> <s>unlike previous approaches that learn task-specific decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks.</s> <s>we use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for specific a-priori knowledge of its global analytical form.</s> <s>this yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce.</s> <s>we evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art.</s></p></d>", "label": ["<d><p><s>non-linear domain adaptation with boosting</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>visual clutter, the perception of an image as being crowded and disordered, affects aspects of our lives ranging from object detection to aesthetics, yet relatively little effort has been made to model this important and ubiquitous percept.</s> <s>our approach models clutter as the number of proto-objects segmented from an image, with proto-objects defined as groupings of superpixels that are similar in intensity, color, and gradient orientation features.</s> <s>we introduce a novel parametric method of merging superpixels by modeling mixture of weibull distributions on similarity distance statistics, then taking the normalized number of proto-objects following partitioning as our estimate of clutter perception.</s> <s>we validated this model using a new $\\text{90}-$image dataset of realistic scenes rank ordered by human raters for clutter, and showed that our method not only predicted clutter extremely well (spearman's $\\rho = 0.81$, $p < 0.05$), but also outperformed all existing clutter perception models and even a behavioral object segmentation ground truth.</s> <s>we conclude that the number of proto-objects in an image affects clutter perception more than the number of objects or features.</s></p></d>", "label": ["<d><p><s>modeling clutter perception using parametric proto-object partitioning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work on mid-level visual representations aims to capture information at the level of complexity higher than typical visual words\", but lower than full-blown semantic objects.</s> <s>several approaches have been proposed to discover mid-level visual elements, that are both 1) representative, i.e.</s> <s>frequently occurring within a visual dataset, and 2) visually discriminative.</s> <s>however, the current approaches are rather ad hoc and difficult to analyze and evaluate.</s> <s>in this work, we pose visual element discovery as discriminative mode seeking, drawing connections to the the well-known and well-studied mean-shift algorithm.</s> <s>given a weakly-labeled image collection, our method discovers visually-coherent patch clusters that are maximally discriminative with respect to the labels.</s> <s>one advantage of our formulation is that it requires only a single pass through the data.</s> <s>we also propose the purity-coverage plot as a principled way of experimentally analyzing and evaluating different visual discovery approaches, and compare our method against prior work on the paris street view dataset.</s> <s>we also evaluate our method on the task of scene classification, demonstrating state-of-the-art performance on the mit scene-67 dataset.\"</s></p></d>", "label": ["<d><p><s>mid-level visual element discovery as discriminative mode seeking</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how does the human visual system compute the speed of a coherent motion stimulus that contains motion energy in different spatiotemporal frequency bands?</s> <s>here we propose that perceived speed is the result of optimal integration of speed information from independent spatiotemporal frequency tuned channels.</s> <s>we formalize this hypothesis with a bayesian observer model that treats the channel activity as independent cues, which are optimally combined with a prior expectation for slow speeds.</s> <s>we test the model against behavioral data from a 2afc speed discrimination task with which we measured subjects' perceived speed of drifting sinusoidal gratings with different contrasts and spatial frequencies, and of various combinations of these single gratings.</s> <s>we find that perceived speed of the combined stimuli is independent of the relative phase of the underlying grating components, and that the perceptual biases and discrimination thresholds are always smaller for the combined stimuli, supporting the cue combination hypothesis.</s> <s>the proposed bayesian model fits the data well, accounting for perceptual biases and thresholds of both simple and combined stimuli.</s> <s>fits are improved if we assume that the channel responses are subject to divisive normalization, which is in line with physiological evidence.</s> <s>our results provide an important step toward a more complete model of visual motion perception that can predict perceived speeds for stimuli of arbitrary spatial structure.</s></p></d>", "label": ["<d><p><s>optimal integration of visual speed across different spatiotemporal frequency channels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modern visual recognition systems are often limited in their ability to scale to large numbers of object categories.</s> <s>this limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows.</s> <s>one remedy is to leverage data from other sources -- such as text data -- both to train visual models and to constrain their predictions.</s> <s>in this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text.</s> <s>we demonstrate that this model matches state-of-the-art performance on the 1000-class imagenet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training.</s> <s>semantic knowledge improves such zero-shot predictions by up to 65%, achieving hit rates of up to 10% across thousands of novel labels never seen by the visual model.</s></p></d>", "label": ["<d><p><s>devise: a deep visual-semantic embedding model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms.</s> <s>current methods typically fail to find the appropriate level of generalization in a concept hierarchy for a given set of visual examples.</s> <s>recent work in cognitive science on bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly  recognized.</s> <s>we present an algorithm for learning visual concepts directly from images, using  probabilistic predictions generated by visual classifiers as the input to a bayesian generalization model.</s> <s>as no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the imagenet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children.</s> <s>we compare the performance of our system to several baseline algorithms, and show a significant advantage results from combining visual classifiers with the ability to identify an appropriate level of abstraction using bayesian generalization.</s></p></d>", "label": ["<d><p><s>visual concept learning: combining machine vision and bayesian generalization on concept hierarchies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one approach to computer object recognition and modeling the brain's ventral stream involves unsupervised learning of representations that are invariant to common transformations.</s> <s>however, applications of these ideas have usually been limited to 2d affine transformations, e.g., translation and scaling, since they are easiest to solve via convolution.</s> <s>in accord with a recent theory of transformation-invariance, we propose a model that, while capturing other common convolutional networks as special cases, can also be used with arbitrary identity-preserving transformations.</s> <s>the model's wiring can be learned from videos of transforming objects---or any other grouping of images into sets by their depicted object.</s> <s>through a series of successively more complex empirical tests, we study the invariance/discriminability properties of this model with respect to different transformations.</s> <s>first, we empirically confirm theoretical predictions for the case of 2d affine transformations.</s> <s>next, we apply the model to non-affine transformations: as expected, it performs well on face verification tasks requiring invariance to the relatively smooth transformations of 3d rotation-in-depth and changes in illumination direction.</s> <s>surprisingly, it can also tolerate clutter transformations'' which map an image of a face on one background to an image of the same face on a different background.</s> <s>motivated by these empirical findings, we tested the same model on face verification benchmark tasks from the computer vision literature: labeled faces in the wild, pubfig and a new dataset we gathered---achieving strong performance in these highly unconstrained cases as well.\"</s></p></d>", "label": ["<d><p><s>learning invariant representations and applications to face verification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks (dnns) have recently shown outstanding performance on the task of whole image classification.</s> <s>in this paper we go one step further and address the problem of object detection -- not only classifying but also precisely localizing objects of various classes using dnns.</s> <s>we present a simple and yet powerful formulation of object detection as a regression to object masks.</s> <s>we define a multi-scale inference procedure which is able to produce a high-resolution object detection at a low cost by a few network applications.</s> <s>the approach achieves state-of-the-art performance on pascal 2007 voc.</s></p></d>", "label": ["<d><p><s>deep neural networks for object detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time.</s> <s>we describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy.</s> <s>our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade.</s> <s>our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of vector quantization levels, and by choosing to rescore windows or not.</s> <s>our method can be directly plugged into any recognition system that relies on linear templates.</s> <s>we demonstrate our method to speed up the original exemplar svm detector [1] by an order of magnitude and deformable part models [2] by two orders of magnitude with no loss of accuracy.</s></p></d>", "label": ["<d><p><s>fast template evaluation with vector quantization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>category models for objects or activities typically rely on supervised learning requiring sufficiently large training sets.</s> <s>transferring knowledge from known categories to novel classes with no or only a few labels however is far less researched even though it is a common scenario.</s> <s>in this work, we extend transfer learning with semi-supervised learning to exploit unlabeled instances of (novel) categories with no or only a few labeled instances.</s> <s>our proposed approach propagated semantic transfer combines three main ingredients.</s> <s>first, we transfer information from known to novel categories by incorporating external knowledge, such as linguistic or expert-specified information, e.g., by a mid-level layer of semantic attributes.</s> <s>second, we exploit the manifold structure of novel classes.</s> <s>more specifically we adapt a graph-based learning algorithm - so far only used for semi-supervised learning - to zero-shot and few-shot learning.</s> <s>third, we improve the local neighborhood in such graph structures by replacing the raw feature-based representation with a mid-level object- or attribute-based representation.</s> <s>we evaluate our approach on three challenging datasets in two different applications, namely on animals with attributes and imagenet for image classification and on mpii composites for activity recognition.</s> <s>our approach consistently outperforms state-of-the-art transfer and semi-supervised approaches on all datasets.</s></p></d>", "label": ["<d><p><s>transfer learning in a transductive setting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential.</s> <s>however, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.)</s> <s>we propose an approach to automatically discover latent domains in image or video datasets.</s> <s>our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability.</s> <s>by maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain.</s> <s>we devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data.</s> <s>we extensively evaluate our approach on object recognition and human activity recognition tasks.</s></p></d>", "label": ["<d><p><s>reshaping visual datasets for domain adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>all the existing multi-task local learning methods are defined on homogeneous neighborhood which consists of all data points from only one task.</s> <s>in this paper, different from existing methods, we propose local learning methods for multi-task classification and regression problems based on heterogeneous neighborhood which is defined on data points from all tasks.</s> <s>specifically, we extend the k-nearest-neighbor classifier by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-specific.</s> <s>by defining a regularizer to enforce the task-specific weight matrix to approach a symmetric one, a regularized objective function is proposed and an efficient coordinate descent method is developed to solve it.</s> <s>for regression problems, we extend the kernel regression to multi-task setting in a similar way to the classification case.</s> <s>experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods.</s></p></d>", "label": ["<d><p><s>heterogeneous-neighborhood-based multi-task local learning algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a probabilistic model based on the horseshoe prior is proposed for learning dependencies in the process of identifying relevant features for prediction.</s> <s>exact inference is intractable in this model.</s> <s>however, expectation propagation offers an approximate alternative.</s> <s>because the process of estimating feature selection dependencies may suffer from over-fitting in the model proposed, additional data from a multi-task learning scenario are considered for induction.</s> <s>the same model can be used in this setting with few modifications.</s> <s>furthermore, the assumptions made are less restrictive than in other multi-task methods: the different tasks must share feature selection dependencies, but can have different relevant features and model coefficients.</s> <s>experiments with real and synthetic data show that this model performs better than other multi-task alternatives from the literature.</s> <s>the experiments also show that the model is able to induce suitable feature selection dependencies for the problems considered, only from the training data.</s></p></d>", "label": ["<d><p><s>learning feature selection dependencies in multi-task learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel formulation of multi-task learning (mtl) called parametric task learning (ptl) that can systematically handle infinitely many tasks parameterized by a continuous parameter.</s> <s>our key finding is that, for a certain class of ptl problems, the path of optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter.</s> <s>based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks efficiently.</s> <s>we show that our ptl formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression, and demonstrate the usefulness of the proposed method experimentally in these scenarios.</s></p></d>", "label": ["<d><p><s>parametric task learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a boosting method, directboost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, directboost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense.</s> <s>experimental results on a collection of machine-learning benchmark datasets show that directboost gives consistently better results than adaboost, logitboost, lpboost with column generation and brownboost, and is noise tolerant when it maximizes an n'th order bottom sample margin.</s></p></d>", "label": ["<d><p><s>direct 0-1 loss minimization and margin maximization with boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose to train an ensemble with the help of a reservoir in which the learning algorithm can store a limited number of samples.</s> <s>this novel approach lies in the area between offline and online ensemble approaches and can be seen either as a restriction of the former or an enhancement of the latter.</s> <s>we identify some basic strategies that can be used to populate this reservoir and present our main contribution, dubbed greedy edge expectation maximization (geem), that maintains the reservoir content in the case of boosting by viewing the samples through their projections into the weak classifier response space.</s> <s>we propose an efficient algorithmic implementation which makes it tractable in practice, and demonstrate its efficiency experimentally on several compute-vision data-sets, on which it outperforms both online and offline methods in a memory constrained setting.</s></p></d>", "label": ["<d><p><s>reservoir boosting : between online and offline ensemble learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we go beyond the notion of pairwise similarity and look into  search problems with $k$-way similarity functions.</s> <s>in this paper, we focus on problems related to  \\emph{3-way jaccard} similarity: $\\mathcal{r}^{3way}= \\frac{|s_1 \\cap s_2 \\cap s_3|}{|s_1 \\cup s_2 \\cup s_3|}$, $s_1, s_2, s_3 \\in \\mathcal{c}$, where $\\mathcal{c}$ is a size $n$ collection of sets (or binary vectors).</s> <s>we show that approximate $\\mathcal{r}^{3way}$ similarity search problems admit  fast algorithms with  provable guarantees, analogous to the pairwise case.</s> <s>our analysis and speedup guarantees naturally extend to $k$-way resemblance.</s> <s>in the process, we extend traditional framework of \\emph{locality sensitive hashing (lsh)} to handle higher order similarities, which could be of independent theoretical interest.</s> <s>the applicability of $\\mathcal{r}^{3way}$ search is shown on the google sets\" application.</s> <s>in addition, we demonstrate the advantage of $\\mathcal{r}^{3way}$ resemblance over the pairwise case in improving retrieval quality.\"</s></p></d>", "label": ["<d><p><s>beyond pairwise: provably fast algorithms for approximate </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>up-propagation is an algorithm for inverting and learning neural network generative models sensory input is processed by inverting a model that generates patterns from hidden variables using topdown connections the inversion process is iterative utilizing a negative feedback loop that depends on an error signal propagated by bottomup connections the error signal is also used to learn the generative model from examples the algorithm is benchmarked against principal component analysis in experiments on images of handwritten digits.</s></p></d>", "label": ["<d><p><s>learning generative models with the up propagation algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we have constructed an inexpensive video based motorized tracking system that learns to track a head.</s> <s>it uses real time graphical user inputs or an auxiliary infrared detector as supervisory signals to train a convolutional neural network.</s> <s>the inputs to the neural network consist of normalized luminance and chrominance images and motion information from frame differences.</s> <s>subsampled images are also used to provide scale invariance.</s> <s>during the online training phases the neural network rapidly adjusts the input weights depending up on the reliability of the different channels in the surrounding environment.</s> <s>this quick adaptation allows the system to robustly track a head even when other objects are moving within a cluttered background.</s></p></d>", "label": ["<d><p><s>a neural network based head tracking system</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances.</s> <s>recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list.</s> <s>most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances.</s> <s>we propose a highly efficient approach, titled toppush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances.</s> <s>we present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach.</s> <s>empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.</s></p></d>", "label": ["<d><p><s>top rank optimization in linear time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a seriation algorithm for ranking a set of n items given pairwise comparisons between these items.</s> <s>intuitively, the algorithm assigns similar rankings to items that compare similarly with all others.</s> <s>it does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking.</s> <s>we first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order.</s> <s>we then show that ranking reconstruction is still exact even when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than other scoring methods.</s> <s>an additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems.</s> <s>experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.</s></p></d>", "label": ["<d><p><s>serialrank: spectral ranking using seriation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our understanding of the neural computations that underlie the ability of animals to choose among options has advanced through a synthesis of computational modeling, brain imaging and behavioral choice experiments.</s> <s>yet, there remains a gulf between theories of preference learning and accounts of the real, economic choices that humans face in daily life, choices that are usually between some amount of money and an item.</s> <s>in this paper, we develop a theory of magnitude-sensitive preference learning that permits an agent to rationally infer its preferences for items compared with money options of different magnitudes.</s> <s>we show how this theory yields classical and anomalous supply-demand curves and predicts choices for a large panel of risky lotteries.</s> <s>accurate replications of such phenomena without recourse to utility functions suggest that the theory proposed is both psychologically realistic and econometrically viable.</s></p></d>", "label": ["<d><p><s>magnitude-sensitive preference formation`</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by generating personalized recommendations using ordinal (or preference) data, we study the question of learning a mixture of multinomial logit (mnl) model, a parameterized class of distributions over permutations, from partial ordinal or preference data (e.g.</s> <s>pair-wise comparisons).</s> <s>despite its long standing importance across disciplines including social choice, operations research and revenue management, little is known about this question.</s> <s>in case of single mnl models (no mixture), computationally and statistically tractable learning from pair-wise comparisons is feasible.</s> <s>however, even learning mixture of two mnl model is infeasible in general.</s> <s>given this state of affairs, we seek conditions under which it is feasible to learn the mixture model in both computationally and statistically efficient manner.</s> <s>to that end, we present a sufficient condition as well as an efficient algorithm for learning mixed mnl models from partial preferences/comparisons data.</s> <s>in particular, a mixture of $r$ mnl components over $n$ objects can be learnt using samples whose size scales polynomially in $n$ and $r$ (concretely, $n^3 r^{3.5} \\log^4 n$, with $r \\ll n^{2/7}$ when the model parameters are sufficiently {\\em incoherent}).</s> <s>the algorithm has two phases: first, learn the pair-wise marginals for each component using tensor decomposition; second, learn the model parameters for each component using rankcentrality introduced by negahban et al.</s> <s>in the process of proving these results, we obtain a generalization of existing analysis for tensor decomposition to a more realistic regime where only partial information about each sample is available.</s></p></d>", "label": ["<d><p><s>learning mixed multinomial logit model from ordinal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>let $p$ be an unknown and arbitrary probability distribution over $[0 ,1)$.</s> <s>we consider the problem of \\emph{density estimation}, in which a learning algorithm is given i.i.d.</s> <s>draws from $p$ and must (with high probability) output a hypothesis distribution that is close to $p$.</s> <s>the main contribution of this paper is a highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a hypothesis distribution with a piecewise constant probability density function.</s> <s>in more detail, for any $k$ and $\\eps$, we give an algorithm that makes $\\tilde{o}(k/\\eps^2)$ draws from $p$, runs in $\\tilde{o}(k/\\eps^2)$ time, and outputs a hypothesis distribution $h$ that is piecewise constant with $o(k \\log^2(1/\\eps))$ pieces.</s> <s>with high probability the hypothesis $h$ satisfies $\\dtv(p,h) \\leq c \\cdot \\opt_k(p) + \\eps$, where $\\dtv$ denotes the total variation distance (statistical distance), $c$ is a universal constant, and $\\opt_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution.</s> <s>the sample size and running time of our algorithm are both optimal up to logarithmic factors.</s> <s>the ``approximation factor'' $c$ that is present in our result is inherent in the problem, as we prove that no algorithm with sample size bounded in terms of $k$ and $\\eps$ can achieve $c < 2$ regardless of what kind of hypothesis distribution it uses.</s></p></d>", "label": ["<d><p><s>near-optimal density estimation in near-linear time using variable-width histograms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation.</s> <s>one of the most promising directions for unsupervised learning may lie in deep learning methods, given their success in supervised learning.</s> <s>however, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale.</s> <s>as a result there are some easier, more scalable shallow methods, such as the gaussian mixture model and the student-t mixture model, that remain surprisingly competitive.</s> <s>in this paper we propose a new scalable deep generative model for images, called the deep gaussian mixture model, that is a straightforward but powerful generalization of gmms to multiple layers.</s> <s>the parametrization of a deep gmm allows it to efficiently capture products of variations in natural images.</s> <s>we propose a new em-based algorithm that scales well to large datasets, and we show that both the expectation and the maximization steps can easily be distributed over multiple machines.</s> <s>in our density estimation experiments we show that deeper gmm architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.</s></p></d>", "label": ["<d><p><s>factoring variations in natural images with deep gaussian mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while robust parameter estimation has been well studied in parametric density estimation, there has been little investigation into robust density estimation in the nonparametric setting.</s> <s>we present a robust version of the popular kernel density estimator (kde).</s> <s>as with other estimators, a robust version of the kde is useful since sample contamination is a common issue with datasets.</s> <s>what ``robustness'' means for a nonparametric density estimate is not straightforward and is a topic we explore in this paper.</s> <s>to construct a robust kde we scale the traditional kde and project it to its nearest weighted kde in the $l^2$ norm.</s> <s>because the squared $l^2$ norm penalizes point-wise errors superlinearly this causes the weighted kde to allocate more weight to high density regions.</s> <s>we demonstrate the robustness of the spkde with numerical experiments and a consistency result which shows that asymptotically the spkde recovers the uncontaminated density under sufficient conditions on the contamination.</s></p></d>", "label": ["<d><p><s>robust kernel density estimation by scaling and projection in hilbert space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>distributed learning of probabilistic models from multiple data repositories with minimum communication is increasingly important.</s> <s>we study a simple communication-efficient learning framework that first calculates the local maximum likelihood estimates (mle) based on the data subsets, and then combines the local mles to achieve the best possible approximation to the global mle, based on the whole dataset jointly.</s> <s>we study the statistical properties of this framework, showing that the loss of efficiency compared to the global setting relates to how much the underlying distribution families deviate from full exponential families, drawing connection to the theory of information loss by fisher, rao and efron.</s> <s>we show that the full-exponential-family-ness\" represents the lower bound of the error rate of arbitrary combinations of local mles, and is achieved by a kl-divergence-based combination method but not by a more common linear combination method.</s> <s>we also study the empirical properties of the kl and linear combination methods, showing that the kl method significantly outperforms linear combination in practical settings with issues such as model misspecification, non-convexity, and heterogeneous data partitions.\"</s></p></d>", "label": ["<d><p><s>distributed estimation, information loss and exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the integration of partially redundant information from multiple sensors is a standard computational problem for agents interacting with the world.</s> <s>in man and other primates, integration has been shown psychophysically to be nearly optimal in the sense of error minimization.</s> <s>an influential generalization of this notion of optimality is that populations of multisensory neurons should retain all the information from their unisensory afferents about the underlying, common stimulus [1].</s> <s>more recently, it was shown empirically that a neural network trained to perform latent-variable density estimation, with the activities of the unisensory neurons as observed data, satisfies the information-preservation criterion, even though the model architecture was not designed to match the true generative process for the data [2].</s> <s>we prove here an analytical connection between these seemingly different tasks, density estimation and sensory integration; that the former implies the latter for the model used in [2]; but that this does not appear to be true for all models.</s></p></d>", "label": ["<d><p><s>sensory integration and density estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases.</s> <s>in this paper, we provide an efficient and robust table completion tool, based on a bayesian nonparametric latent feature model.</s> <s>in particular, we propose a general observation model for the indian buffet process (ibp) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations.</s> <s>then, we propose an inference algorithm that scales linearly with the number of observations.</s> <s>finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard ibp and the bayesian probabilistic matrix factorization with gaussian observations.</s></p></d>", "label": ["<d><p><s>general table completion using a bayesian nonparametric model</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets.</s> <s>however, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: we expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time.</s> <s>in this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model.</s> <s>we demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.</s></p></d>", "label": ["<d><p><s>dependent nonparametric trees for dynamic hierarchical clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together.</s> <s>we refer to this form of dependency as ?region sparsity?.</s> <s>classical sparse regression methods, such as the lasso and automatic relevance determination (ard), model parameters as independent a priori, and therefore do not exploit such dependencies.</s> <s>here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting.</s> <s>our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed gaussian process to model the dependencies between the prior variances of regression weights.</s> <s>we combine this with a structured model of the prior variances of fourier coefficients, which eliminates unnecessary high frequencies.</s> <s>the resulting prior encourages weights to be region-sparse in two different bases simultaneously.</s> <s>we develop efficient approximate inference methods and show substantial improvements over comparable methods (e.g., group lasso and smooth rvm) for both simulated and real datasets from brain imaging.</s></p></d>", "label": ["<d><p><s>sparse bayesian structure learning with ?dependent relevance determination? priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics.</s> <s>random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks.</s> <s>the most popular random forest variants (such as breiman's random forest and extremely randomized trees) operate on batches of training data.</s> <s>online methods are now in greater demand.</s> <s>existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance.</s> <s>in this work, we use mondrian processes (roy and teh, 2009) to construct ensembles of random decision trees we call mondrian forests.</s> <s>mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online mondrian forests is the same as that of batch mondrian forests.</s> <s>mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.</s></p></d>", "label": ["<d><p><s>mondrian forests: efficient online random forests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a sampling technique for hierarchical dirichlet process models.</s> <s>the parallel algorithm builds upon [chang & fisher 2013] by proposing large split and merge moves based on learned sub-clusters.</s> <s>the additional global split and merge moves drastically improve convergence in the experimental results.</s> <s>furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.</s></p></d>", "label": ["<d><p><s>parallel sampling of hdps using sub-cluster splits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources.</s> <s>multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios.</s> <s>in this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data.</s> <s>we demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients.</s> <s>our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.</s></p></d>", "label": ["<d><p><s>localized data fusion for kernel k-means clustering with application to cancer biology</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized fredholm integral equation.</s> <s>our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call fredholm kernels.</s> <s>we proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that fredholm kernels can effectively utilize unlabeled data under the noise assumption.</s> <s>we demonstrate that methods based on fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"</s></p></d>", "label": ["<d><p><s>learning with fredholm kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems.</s> <s>have we tried hard enough for kernel methods?</s> <s>in this paper, we propose an approach that scales up kernel methods using a novel concept called ``doubly stochastic functional gradients''.</s> <s>based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient---one using random training points and another using random features associated with the kernel---and performing descent steps with this noisy functional gradient.</s> <s>our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting.</s> <s>we demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel hilbert space in rate o(1/t), and achieves a generalization bound of o(1/\\sqrt{t}).</s> <s>our approach can readily scale kernel methods up to the regimes which are dominated by neural nets.</s> <s>we show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from molecularspace, 8 million handwritten digits from mnist, and 1 million photos from imagenet using convolution features.</s></p></d>", "label": ["<d><p><s>scalable kernel methods via doubly stochastic gradients</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of estimating the kernel mean in a reproducing kernel hilbert space (rkhs) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel pca matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in rkhss.</s> <s>previous work [1] has shown that shrinkage can help in constructing ?better?</s> <s>estimators of the kernel mean than the empirical estimator.</s> <s>the present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions.</s> <s>using the kernel pca basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions.</s> <s>our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework.</s> <s>the proposed estimators are simple to implement and perform well in practice.</s></p></d>", "label": ["<d><p><s>kernel mean estimation via spectral filtering</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms.</s> <s>however, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation.</s> <s>we propose the first {\\em fast} oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel {\\em without} explicitly mapping the data to the high-dimensional space.</s> <s>in particular, we propose an embedding for mappings induced by the polynomial kernel.</s> <s>using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel pca of the data, as well as doing approximate kernel principal component regression.</s></p></d>", "label": ["<d><p><s>subspace embeddings for the polynomial kernel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most standard algorithms for prediction with expert advice depend on a parameter called the learning rate.</s> <s>this learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting.</s> <s>for the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning.</s> <s>but in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate.</s> <s>to close the gap between theory and practice we introduce an approach to learn the learning rate.</s> <s>up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available.</s> <s>our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.</s></p></d>", "label": ["<d><p><s>learning the learning rate for prediction with expert advice</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates.</s> <s>using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur.</s> <s>we first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays.</s> <s>we then analyze adaptiverevision, an algorithm that is efficiently implementable and achieves comparable guarantees.</s> <s>the key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps.</s> <s>experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods.</s></p></d>", "label": ["<d><p><s>delay-tolerant algorithms for asynchronous distributed online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider online prediction problems where the loss between the prediction and the outcome is measured by the squared euclidean distance and its generalization, the squared mahalanobis distance.</s> <s>we derive the minimax solutions for the case where the prediction and action spaces are the simplex (this setup is sometimes called the brier game) and the $\\ell_2$ ball (this setup is related to gaussian density estimation).</s> <s>we show that in both cases the value of each sub-game is a quadratic function of a simple statistic of the state, with coefficients that can be efficiently computed using an explicit recurrence relation.</s> <s>the resulting deterministic minimax strategy and randomized maximin strategy are linear functions of the statistic.</s></p></d>", "label": ["<d><p><s>efficient minimax strategies for square loss games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study online combinatorial decision problems, where one must make sequential decisions in some combinatorial space without knowing in advance the cost of decisions on each trial; the goal is to minimize the total regret over some sequence of trials relative to the best fixed decision in hindsight.</s> <s>such problems have been studied mostly in settings where decisions are represented by boolean vectors and costs are linear in this representation.</s> <s>here we study a general setting where costs may be linear in any suitable low-dimensional vector representation of elements of the decision space.</s> <s>we give a general algorithm for such problems that we call low-dimensional online mirror descent (ldomd); the algorithm generalizes both the component hedge algorithm of koolen et al.</s> <s>(2010), and a recent algorithm of suehiro et al.</s> <s>(2012).</s> <s>our study offers a unification and generalization of previous work, and emphasizes the role of the convex polytope arising from the vector representation of the decision space; while boolean representations lead to 0-1 polytopes, more general vector representations lead to more general polytopes.</s> <s>we study several examples of both types of polytopes.</s> <s>finally, we demonstrate the benefit of having a general framework for such problems via an application to an online transportation problem; the associated transportation polytopes generalize the birkhoff polytope of doubly stochastic matrices, and the resulting algorithm generalizes the permelearn algorithm of helmbold and warmuth (2009).</s></p></d>", "label": ["<d><p><s>online decision-making in general combinatorial spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning to optimize an unknown markov decision process (mdp).</s> <s>we show that, if the mdp can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system.</s> <s>we characterize this dependence explicitly as $\\tilde{o}(\\sqrt{d_k d_e t})$ where $t$ is time elapsed, $d_k$ is the kolmogorov dimension and $d_e$ is the \\emph{eluder dimension}.</s> <s>these represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings.</s> <s>moreover, we present a simple and computationally efficient algorithm \\emph{posterior sampling for reinforcement learning} (psrl) that satisfies these bounds.</s></p></d>", "label": ["<d><p><s>model-based reinforcement learning and the eluder dimension</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion.</s> <s>conditional value-at-risk (cvar) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research.</s> <s>in this paper, we consider the mean-cvar optimization problem in mdps.</s> <s>we first derive a formula for computing the gradient of this risk-sensitive objective function.</s> <s>we then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction.</s> <s>we establish the convergence of our algorithms to locally risk-sensitive optimal policies.</s> <s>finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.</s></p></d>", "label": ["<d><p><s>algorithms for cvar optimization in mdps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in multi-task reinforcement learning (mtrl), the objective is to simultaneously learn multiple tasks and exploit their similarity to improve the performance w.r.t.\\ single-task learning.</s> <s>in this paper we investigate the case when all the tasks can be accurately represented in a linear approximation space using the same small subset of the original (large) set of features.</s> <s>this is equivalent to assuming that the weight vectors of the task value functions are \\textit{jointly sparse}, i.e., the set of their non-zero components is small and it is shared across tasks.</s> <s>building on existing results in multi-task regression, we develop two multi-task extensions of the fitted $q$-iteration algorithm.</s> <s>while the first algorithm assumes that the tasks are jointly sparse in the given representation, the second one learns a transformation of the features in the attempt of finding a more sparse representation.</s> <s>for both algorithms we provide a sample complexity analysis and numerical simulations.</s></p></d>", "label": ["<d><p><s>sparse multi-task reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called probabilistic differential dynamic programming (pddp).</s> <s>pddp takes into account uncertainty explicitly for dynamics models using gaussian processes (gps).</s> <s>based on the second-order local approximation of the value function, pddp performs dynamic programming around a nominal trajectory in gaussian belief spaces.</s> <s>different from typical gradient-based policy search methods, pddp does not require a policy parameterization and learns a locally optimal, time-varying control policy.</s> <s>we demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks.</s> <s>compared with the classical ddp and a state-of-the-art gp-based policy search method, pddp offers a superior combination of data-efficiency, learning speed, and applicability.</s></p></d>", "label": ["<d><p><s>probabilistic differential dynamic programming</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>importance sampling is an essential component of off-policy model-free reinforcement learning algorithms.</s> <s>however, its most effective variant, \\emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms.</s> <s>in this paper, we take two steps toward bridging this gap.</s> <s>first, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling.</s> <s>second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy lstd(lambda).</s> <s>we show empirically that our new wis-lstd(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy lstd(lambda) (yu 2010, bertsekas & yu 2009).</s></p></d>", "label": ["<d><p><s>weighted importance sampling for off-policy learning with linear function approximation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper presents a representation theory for permutation-valued functions, which in their general form can also be called listwise ranking functions.</s> <s>pointwise ranking functions assign a score to each object independently, without taking into account the other objects under consideration; whereas listwise loss functions evaluate the set of scores assigned to all objects as a whole.</s> <s>in many supervised learning to rank tasks, it might be of interest to use listwise ranking functions instead; in particular, the bayes optimal ranking functions might themselves be listwise, especially if the loss function is listwise.</s> <s>a key caveat to using listwise ranking functions has been the lack of an appropriate representation theory for such functions.</s> <s>we show that a natural symmetricity assumption that we call exchangeability allows us to explicitly characterize the set of such exchangeable listwise ranking functions.</s> <s>our analysis draws from the theories of tensor analysis, functional analysis and de finetti theorems.</s> <s>we also present experiments using a novel reranking method motivated by our representation theory.</s></p></d>", "label": ["<d><p><s>a representation theory for ranking functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many important distributions are high dimensional, and often they can be modeled as gaussian mixtures.</s> <s>we derive the first sample-efficient polynomial-time estimator for high-dimensional spherical gaussian mixtures.</s> <s>based on intuitive spectral reasoning, it approximates mixtures of $k$ spherical gaussians in $d$-dimensions to within$\\ell_1$ distance $\\epsilon$ using $\\mathcal{o}({dk^9(\\log^2 d)}/{\\epsilon^4})$ samples and $\\mathcal{o}_{k,\\epsilon}(d^3\\log^5 d)$ computation time.</s> <s>conversely, we show that any estimator requires $\\omega\\bigl({dk}/{\\epsilon^2}\\bigr)$ samples, hence the algorithm's sample complexity is nearly optimal in the dimension.</s> <s>the implied time-complexity factor \\mathcal{o}_{k,\\epsilon}$ is exponential in $k$, but much smaller than previously known.</s> <s>we also construct a simple estimator for one-dimensional gaussian mixtures that uses $\\tilde\\mathcal{o}(k /\\epsilon^2)$ samples and $\\tilde\\mathcal{o}((k/\\epsilon)^{3k+1})$ computation time.</s></p></d>", "label": ["<d><p><s>near-optimal-sample estimators for spherical gaussian mixtures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide statistical and computational analysis of sparse principal component analysis (pca) in high dimensions.</s> <s>the sparse pca problem is highly nonconvex in nature.</s> <s>consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain.</s> <s>meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence.</s> <s>on the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees.</s> <s>in this paper, we propose a two-stage sparse pca procedure that attains the optimal principal subspace estimator in polynomial time.</s> <s>the main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem.</s> <s>however, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction.</s> <s>to obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse pca with early stopping.</s> <s>under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure.</s> <s>computationally, our procedure converges at the rate of $1/\\sqrt{t}$ within the initialization stage, and at a geometric rate within the main stage.</s> <s>statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level $s^*$, dimension $d$ and sample size $n$.</s> <s>our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.</s></p></d>", "label": ["<d><p><s>tighten after relax: minimax-optimal sparse pca in polynomial time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting.</s> <s>in particular, we examine the consistency (both asymptotic and finitary) of the optimal nitzan-paroush weighted majority and related rules.</s> <s>in the case of known expert competence levels, we give sharp error estimates for the optimal rule.</s> <s>when the competence levels are unknown, they must be empirically estimated.</s> <s>we provide frequentist and bayesian analyses for this situation.</s> <s>some of our proof techniques are non-standard and may be of independent interest.</s> <s>the bounds we derive are nearly optimal, and several challenging open problems are posed.</s> <s>experimental results are provided to illustrate the theory.</s></p></d>", "label": ["<d><p><s>consistency of weighted majority votes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a central result in statistical theory is pinsker's theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation.</s> <s>in this paper, we present an extension to pinsker's theorem where estimation is carried out under storage or communication constraints.</s> <s>in particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level.</s> <s>we give sharp upper and lower bounds for the case of a euclidean ball, which establishes the pareto-optimal minimax tradeoff between storage and risk in this setting.</s></p></d>", "label": ["<d><p><s>quantized estimation of gaussian sequence models in euclidean balls</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision.</s> <s>however, minimizing submodular functions poses a number of algorithmic challenges.</s> <s>recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of simple\" submodular functions.</s> <s>empirically, this algorithm performs extremely well, but no theoretical analysis was given.</s> <s>in this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence.</s> <s>our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.\"</s></p></d>", "label": ["<d><p><s>on the convergence rate of decomposable submodular function minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of minimizing block-separable convex functions subject to linear constraints.</s> <s>while the alternating direction method of multipliers (admm) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of admm to multiple blocks is still unclear.</s> <s>in this paper, we propose a parallel randomized block coordinate method named parallel direction method of multipliers (pdmm) to solve the optimization problems with multi-block linear constraints.</s> <s>pdmm randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent.</s> <s>we establish the global convergence and the iteration complexity for pdmm with constant step size.</s> <s>we also show that pdmm can do randomized block coordinate descent on overlapping blocks.</s> <s>experimental results show that pdmm performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.</s></p></d>", "label": ["<d><p><s>parallel direction method of multipliers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>state of the art statistical estimators for high-dimensional problems take the form of regularized, and hence non-smooth, convex programs.</s> <s>a key facet of thesestatistical estimation problems is that these are typically not strongly convex under a high-dimensional sampling regime when the hessian matrix becomes rank-deficient.</s> <s>under vanilla convexity however, proximal optimization methods attain only a sublinear rate.</s> <s>in this paper, we investigate a novel variant of strong convexity, which we call constant nullspace strong convexity (cnsc), where we require that the objective function be strongly convex only over a constant subspace.</s> <s>as we show, the cnsc condition is naturally satisfied by high-dimensional statistical estimators.</s> <s>we then analyze the behavior of proximal methods under this cnsc condition: we show global linear convergence of proximal gradient and local quadratic convergence of proximal newton method, when the regularization function comprising the statistical estimator is decomposable.</s> <s>we corroborate our theory via numerical experiments, and show a qualitative difference in the convergence rates of the proximal algorithms when the loss function does satisfy the cnsc condition.</s></p></d>", "label": ["<d><p><s>constant nullspace strong convexity and fast convergence of proximal methods under high-dimensional settings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we introduce a new fast incremental gradient method saga, in the spirit of sag, sdca, miso and svrg.</s> <s>saga improves on the theory behind sag and svrg, with better theoretical convergence rates, and support for composite objectives where a proximal operator is used on the regulariser.</s> <s>unlike sdca, saga supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem.</s> <s>we give experimental results showing the effectiveness of our method.</s></p></d>", "label": ["<d><p><s>saga: a fast incremental gradient method with support for non-strongly convex composite objectives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization.</s> <s>as the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly.</s> <s>this work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.</s></p></d>", "label": ["<d><p><s>time--data tradeoffs by aggressive smoothing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider the forward--backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a lipschitz continuous gradient and the other being partly smooth relatively to an active manifold $\\mathcal{m}$.</s> <s>we propose a generic framework in which we show that the forward--backward (i) correctly identifies the active manifold $\\mathcal{m}$ in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely.</s> <s>this gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the lasso, the group lasso, the fused lasso and the nuclear norm regularization to name a few.</s> <s>these results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.</s></p></d>", "label": ["<d><p><s>local linear convergence of forward--backward under partial smoothness</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we focus on the problem of maximum a posteriori (map) inference in markov random fields with binary variables and pairwise interactions.</s> <s>for this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation, followed by randomized rounding.</s> <s>we develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases.</s> <s>in practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark map inference tasks.</s></p></d>", "label": ["<d><p><s>simple map inference via low-rank relaxations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate.</s> <s>however, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs.</s> <s>here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential.</s> <s>our work makes two important contributions.</s> <s>first, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (glm) for neural spike trains.</s> <s>we show that the classic glm is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite ?push-pull?</s> <s>fashion.</s> <s>our model can therefore be viewed as a direct extension of the glm in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant.</s> <s>second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the glm.</s> <s>most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains.</s> <s>we validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells.</s> <s>we show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.</s></p></d>", "label": ["<d><p><s>inferring synaptic conductances from spike trains with a biophysically inspired point process model</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits.</s> <s>an important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics.</s> <s>here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity.</s> <s>this model captures the temporal dynamics, effective network connectivity in large population recordings, and correlations due to shared stimulus drive as well as common noise.</s> <s>moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a relatively large number of idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional latent dynamical model.</s> <s>we introduce a fast estimation method using online expectation maximization with laplace approximations.</s> <s>inference scales linearly in both population size and recording duration.</s> <s>we apply this model to multi-channel recordings from primary visual cortex and show that it accounts for a large number of individual neural receptive fields using a small number of nonlinear inputs and a low-dimensional dynamical model.</s></p></d>", "label": ["<d><p><s>low-dimensional models of neural population activity in sensory cortical circuits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many signals, such as spike trains recorded in multi-channel electrophysiological recordings, may be represented as the sparse sum of translated and scaled copies of waveforms whose timing and amplitudes are of interest.</s> <s>from the aggregate signal, one may seek to estimate the identities, amplitudes, and translations of the waveforms that compose the signal.</s> <s>here we present a fast method for recovering these identities, amplitudes, and translations.</s> <s>the method involves greedily selecting component waveforms and then refining estimates of their amplitudes and translations, moving iteratively between these steps in a process analogous to the well-known orthogonal matching pursuit (omp) algorithm.</s> <s>our approach for modeling translations borrows from continuous basis pursuit (cbp), which we extend in several ways: by selecting a subspace that optimally captures translated copies of the waveforms, replacing the convex optimization problem with a greedy approach, and moving to the fourier domain to more precisely estimate time shifts.</s> <s>we test the resulting method, which we call continuous orthogonal matching pursuit (comp), on simulated and neural data, where it shows gains over cbp in both speed and accuracy.</s></p></d>", "label": ["<d><p><s>inferring sparse representations of continuous signals with continuous orthogonal matching pursuit</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making.</s> <s>there is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a markov chain monte carlo algorithm.</s> <s>however, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior).</s> <s>we explore this bottleneck in a simple, linear-gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics.</s> <s>the well-known langevin sampling (ls) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework.</s> <s>however, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by ls yields critically slow mixing when the posterior is high-dimensional.</s> <s>next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than ls, while being far more biologically plausible.</s> <s>in these networks, strong -- but transient -- selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior.</s> <s>intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.</s></p></d>", "label": ["<d><p><s>fast sampling-based inference in balanced neuronal networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the idea that animals might use information-driven planning to explore an unknown environment and build an internal model of it has been proposed for quite some time.</s> <s>recent work has demonstrated that agents using this principle can efficiently learn models of probabilistic environments with discrete, bounded state spaces.</s> <s>however, animals and robots are commonly confronted with unbounded environments.</s> <s>to address this more challenging situation, we study information-based learning strategies of agents in unbounded state spaces using non-parametric bayesian models.</s> <s>specifically, we demonstrate that the chinese restaurant process (crp) model is able to solve this problem and that an empirical bayes version is able to efficiently explore bounded and unbounded worlds by relying on little prior information.</s></p></d>", "label": ["<d><p><s>information-based learning by agents in unbounded state spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it is well-known that neural networks are computationally hard to train.</s> <s>on the other hand, in practice, modern day neural networks are trained efficiently using sgd and a variety of tricks that include different activation functions (e.g.</s> <s>relu), over-specification (i.e., train networks which are larger than needed), and regularization.</s> <s>in this paper we revisit the computational complexity of training neural networks from a modern perspective.</s> <s>we provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training neural networks.</s></p></d>", "label": ["<d><p><s>on the computational efficiency of training neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>attentional neural network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture.</s> <s>the top-down influence is especially effective when dealing with high noise or difficult segmentation problems.</s> <s>our system is modular and extensible.</s> <s>it is also easy to train and cheap to run, and yet can accommodate complex behaviors.</s> <s>we obtain classification accuracy better than or competitive with state of art results on the mnist variation dataset, and successfully disentangle overlaid digits with high success rates.</s> <s>we view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.</s></p></d>", "label": ["<d><p><s>attentional neural network: feature selection using cognitive feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multilayer neural networks (mnns) are commonly trained using gradient descent-based methods, such as backpropagation (bp).</s> <s>inference in probabilistic graphical models is often done using variational bayes methods, such as expectation propagation (ep).</s> <s>we show how an ep based approach can also be used to train deterministic mnns.</s> <s>specifically, we approximate the posterior of the weights given the data using a ?mean-field?</s> <s>factorized distribution, in an online setting.</s> <s>using online ep and the central limit theorem we find an analytical approximation to the bayes update of this posterior, as well as the resulting bayes estimates of the weights and outputs.</s> <s>despite a different origin, the resulting algorithm, expectation backpropagation (ebp), is very similar to bp in form and efficiency.</s> <s>however, it has several additional advantages: (1) training is parameter-free, given initial conditions (prior) and the mnn architecture.</s> <s>this is useful for large-scale problems, where parameter tuning is a major challenge.</s> <s>(2) the weights can be restricted to have discrete values.</s> <s>this is especially useful for implementing trained mnns in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude.</s> <s>we test the ebp algorithm numerically in eight binary text classification tasks.</s> <s>in all tasks, ebp outperforms: (1) standard bp with the optimal constant learning rate (2) previously reported state of the art.</s> <s>interestingly, ebp-trained mnns with binary weights usually perform better than mnns with continuous (real) weights - if we average the mnn output using the inferred posterior.</s></p></d>", "label": ["<d><p><s>expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cross-language learning allows us to use training data from one language to build models for a different language.</s> <s>many approaches to bilingual learning require that we have word-level alignment of sentences from parallel corpora.</s> <s>in this work we explore the use of autoencoder-based methods for cross-language learning of vectorial word representations that are aligned between two languages, while not relying on word-level alignments.</s> <s>we show that by simply learning to reconstruct the bag-of-words representations of aligned sentences, within and between languages, we can in fact learn high-quality representations and do without word alignments.</s> <s>we empirically investigate the success of our approach on the problem of cross-language text classification, where a classifier trained on a given language (e.g., english) must learn to generalize to a different language (e.g., german).</s> <s>in experiments on 3 language pairs, we show that our approach achieves state-of-the-art performance, outperforming a method exploiting word alignments and a strong machine translation baseline.</s></p></d>", "label": ["<d><p><s>an autoencoder approach to learning bilingual word representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a pre-training technique for recurrent neural networks based on linear autoencoder networks for sequences, i.e.</s> <s>linear dynamical systems modelling the target sequences.</s> <s>we start by giving a closed form solution for the definition of the optimal weights of a linear autoencoder given a training set of sequences.</s> <s>this solution, however, is computationally very demanding, so we suggest a procedure to get an approximate solution for a given number of hidden units.</s> <s>the weights obtained for the linear autoencoder are then used as initial weights for the input-to-hidden connections of a recurrent neural network, which is then trained on the desired task.</s> <s>using four well known datasets of sequences of polyphonic music, we show that the proposed pre-training approach is highly effective, since it allows to largely improve the state of the art results on all the considered datasets.</s></p></d>", "label": ["<d><p><s>pre-training of recurrent neural networks via linear autoencoders</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>electroencephalography (eeg) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves.</s> <s>we apply convolutional neural networks (cnns) to analyze and classify eeg data recorded within a rhythm perception study in kigali, rwanda which comprises 12 east african and 12 western rhythmic stimuli ?</s> <s>each presented in a loop for 32 seconds to 13 participants.</s> <s>we investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures.</s> <s>using cnns, we are able to recognize individual rhythms from the eeg with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel.</s> <s>aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.</s></p></d>", "label": ["<d><p><s>using convolutional neural networks to recognize rhythm ?stimuli from electroencephalography recordings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a two-layer spiking network capable of performing approximate inference and learning for a hidden markov model.</s> <s>the lower layer sensory neurons detect noisy measurements of hidden world states.</s> <s>the higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons.</s> <s>we show how such a neuronal network with synaptic plasticity can implement a form of bayesian inference similar to monte carlo methods such as particle filtering.</s> <s>each spike in the population of inference neurons represents a sample of a particular hidden world state.</s> <s>the spiking activity across the neural population approximates the posterior distribution of hidden state.</s> <s>the model provides a functional explanation for the poisson-like noise commonly observed in cortical responses.</s> <s>uncertainties in spike times provide the necessary variability for sampling during inference.</s> <s>unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown.</s> <s>we demonstrate how this network can sequentially learn the hidden markov model using a spike-timing dependent hebbian learning rule and achieve power-law convergence rates.</s></p></d>", "label": ["<d><p><s>neurons as monte carlo samplers: bayesian ?inference and learning in spiking networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry.</s> <s>the computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood.</s> <s>until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale.</s> <s>however, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models.</s> <s>here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons.</s> <s>we treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-bayesian generalized linear model (glm).</s> <s>in addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the glm and of the learning rules.</s> <s>using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (stdp) rule, where nonlinear effects play a substantial role.</s> <s>on synthetic data generated from the biophysical simulator neuron, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.</s></p></d>", "label": ["<d><p><s>a framework for studying synaptic plasticity with neural spike train data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recursive neural networks have recently obtained state of the art performance on several natural language processing tasks.</s> <s>however, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context.</s> <s>this is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word android is positive in the sentence android beats ios.</s> <s>we introduce global belief recursive neural networks (gb-rnns) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference.</s> <s>this allows phrase level predictions and representations to give feedback to words.</s> <s>we show the effectiveness of this model on the task of contextual sentiment analysis.</s> <s>we also show that dropout can improve rnn training and that a combination of unsupervised and supervised word vector representations performs better than either alone.</s> <s>the feedbackward step improves f1 performance by 3% over the standard rnn on this task, obtains state-of-the-art performance on the semeval 2013 challenge and can accurately predict the sentiment of specific entities.</s></p></d>", "label": ["<d><p><s>global belief recursive neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>traditional convolutional neural networks (cnn) are stationary and feedforward.</s> <s>they neither change their parameters during evaluation nor use feedback from higher to lower layers.</s> <s>real brains, however, do.</s> <s>so does our deep attention selective network (dasnet) architecture.</s> <s>dasnet's feedback structure can dynamically alter its convolutional filter sensitivities during classification.</s> <s>it harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters.</s> <s>feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (snes).</s> <s>on the cifar-10 and cifar-100 datasets, dasnet outperforms the previous state-of-the-art model on unaugmented datasets.</s></p></d>", "label": ["<d><p><s>deep networks with internal selective attention through feedback connections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>training of the neural autoregressive density estimator (nade) can be viewed as doing one step of probabilistic inference on missing values in data.</s> <s>we propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in $k$ steps rather than to learn to reconstruct in a single inference step.</s> <s>the proposed model is an unsupervised building block for deep learning that combines the desirable properties of nade and multi-predictive training: (1) its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for boltzmann machines.</s> <s>the proposed nade-k is competitive with the state-of-the-art in density estimation on the two datasets tested.</s></p></d>", "label": ["<d><p><s>iterative neural autoregressive distribution estimator nade-k</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we extend generative stochastic networks to supervised learning of representations.</s> <s>in particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter lambda.</s> <s>we use a new variant of network training involving noise injection, i.e.</s> <s>walkback training, to jointly optimize multiple network layers.</s> <s>neither additional regularization constraints, such as l1, l2 norms or dropout variants, nor pooling- or convolutional layers were added.</s> <s>nevertheless, we are able to obtain state-of-the-art performance on the mnist dataset, without using permutation invariant digits and outperform baseline models on sub-variants of the mnist and rectangles dataset significantly.</s></p></d>", "label": ["<d><p><s>general stochastic networks for classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks.</s> <s>nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities.</s> <s>in this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal.</s> <s>rather than learning with maximum likelihood, we train the model to minimize the variation of information.</s> <s>we provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data.</s> <s>we apply our method to restricted boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training.</s> <s>in addition, we extend to deep networks with recurrent encoding structure to finetune the whole network.</s> <s>in experiments, we demonstrate the state-of-the-art visual recognition performance on mir-flickr database and pascal voc 2007 database with and without text features.</s></p></d>", "label": ["<d><p><s>improved multimodal deep learning with variation of information</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we extend the multinomial logit model to represent some of the empirical phenomena that are frequently observed in the choices made by humans.</s> <s>these phenomena include the similarity effect, the attraction effect, and the compromise effect.</s> <s>we formally quantify the strength of these phenomena that can be represented by our choice model, which illuminates the flexibility of our choice model.</s> <s>we then show that our choice model can be represented as a restricted boltzmann machine and that its parameters can be learned effectively from data.</s> <s>our numerical experiments with real data of human choices suggest that we can train our choice model in such a way that it represents the typical phenomena of choice.</s></p></d>", "label": ["<d><p><s>restricted boltzmann machines modeling human choice</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a model for bidirectional retrieval of images and sentences through a deep, multi-modal embedding of visual and natural language data.</s> <s>unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space.</s> <s>we then introduce a structured max-margin objective that allows our model to explicitly associate these fragments across modalities.</s> <s>extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments improves performance on image-sentence retrieval tasks.</s> <s>additionally, our model provides interpretable predictions for the image-sentence retrieval task since the inferred inter-modal alignment of fragments is explicit.</s></p></d>", "label": ["<d><p><s>deep fragment embeddings for bidirectional image sentence mapping</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling.</s> <s>it uses a novel recursive neural network architecture for context propagation, referred to as rcpn.</s> <s>it first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image.</s> <s>then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature.</s> <s>therefore, the information from every location in the image is propagated to every other location.</s> <s>experimental results on stanford background and sift flow datasets show that the proposed method outperforms previous approaches.</s> <s>it is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a gpu for pixel-wise labeling of a 256x256 image starting from raw rgb pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.</s></p></d>", "label": ["<d><p><s>recursive context propagation network for semantic scene labeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a framework for defining high-order image models that can be used in a variety of applications.</s> <s>the approach involves modeling local patterns in a multiscale representation of an image.</s> <s>local properties of a coarsened image reflect non-local properties of the original image.</s> <s>in the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel.</s> <s>with the multiscale representation we capture the frequency of patterns observed at different scales of resolution.</s> <s>this framework leads to expressive priors that depend on a relatively small number of parameters.</s> <s>for inference and learning we use an mcmc method for block sampling with very large blocks.</s> <s>we evaluate the approach with two example applications.</s> <s>one involves contour detection.</s> <s>the other involves binary segmentation.</s></p></d>", "label": ["<d><p><s>multiscale fields of patterns</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the prominence of weakly labeled data gives rise to a growing demand for object detection methods that can cope with minimal supervision.</s> <s>we propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class.</s> <s>we formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples.</s> <s>together, these lead to state-of-the-art weakly-supervised detection results on the challenging pascal voc dataset.</s></p></d>", "label": ["<d><p><s>weakly-supervised discovery of visual pattern configurations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deriving from the gradient vector of a generative model of local features, fisher vector coding (fvc) has been identified as an effective coding method for image classification.</s> <s>most, if not all, fvc implementations employ the gaussian mixture model (gmm) to characterize the generation process of local features.</s> <s>this choice has shown to be sufficient for traditional low dimensional local features, e.g., sift; and typically, good performance can be achieved with only a few hundred gaussian distributions.</s> <s>however, the same number of gaussians is insufficient to model the feature space spanned by higher dimensional local features, which have become popular recently.</s> <s>in order to improve the modeling capacity for high dimensional features, it turns out to be inefficient and computationally impractical to simply increase the number of gaussians.</s> <s>in this paper, we propose a model in which each local feature is drawn from a gaussian distribution whose mean vector is sampled from a subspace.</s> <s>with certain approximation, this model can be converted to a sparse coding procedure and the learning/inference problems can be readily solved by standard sparse coding methods.</s> <s>by calculating the gradient vector of the proposed model, we derive a new fisher vector encoding strategy, termed sparse coding based fisher vector coding (scfvc).</s> <s>moreover, we adopt the recently developed deep convolutional neural network (cnn) descriptor as a high dimensional local feature and implement image classification with the proposed scfvc.</s> <s>our experimental evaluations demonstrate that our method not only significantly outperforms the traditional gmm based fisher vector encoding but also achieves the state-of-the-art performance in generic object recognition, indoor scene, and fine-grained image classification problems.</s></p></d>", "label": ["<d><p><s>encoding high dimensional local features by sparse coding based fisher vectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hierarchical feed-forward networks have been successfully applied in object recognition.</s> <s>at each level of the hierarchy, features are extracted and encoded, followed by a pooling step.</s> <s>within this processing pipeline, the common trend is to learn the feature coding templates, often referred as codebook entries, filters, or over-complete basis.</s> <s>recently, an approach that apparently does not use templates has been shown to obtain very promising results.</s> <s>this is the second-order pooling (o2p).</s> <s>in this paper, we analyze o2p as a coding-pooling scheme.</s> <s>we find that at testing phase, o2p automatically adapts the feature coding templates to the input features, rather than using templates learned during the training phase.</s> <s>from this finding, we are able to bring common concepts of coding-pooling schemes to o2p, such as feature quantization.</s> <s>this allows for significant accuracy improvements of o2p in standard benchmarks of image classification, namely caltech101 and voc07.</s></p></d>", "label": ["<d><p><s>self-adaptable templates for feature coding</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in many situations we have some measurement of confidence on ``positiveness for a binary label.</s> <s>the ``positiveness\" is a continuous value whose range is a bounded interval.</s> <s>it quantifies the affiliation of each training data to the positive class.</s> <s>we propose a novel learning algorithm called \\emph{expectation loss svm} (e-svm) that is devoted to the problems where only the ``positiveness\" instead of a binary label of each training sample is available.</s> <s>our e-svm algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved.</s> <s>in experiments, we show that the e-svm algorithm can effectively address the segment proposal classification task under both strong supervision (e.g.</s> <s>the pixel-level annotations are available) and the weak supervision (e.g.</s> <s>only bounding-box annotations are available), and outperforms the alternative approaches.</s> <s>besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection.</s> <s>our method achieves the state-of-the-art object detection performance on pascal voc 2007 dataset.\"</s></p></d>", "label": ["<d><p><s>learning from weakly supervised data by the expectation loss svm (e-svm) algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lazy local learning methods train a classifier on the fly\" at test time, using only a subset of the training instances that are most relevant to the novel test example.</s> <s>the goal is to tailor the classifier to the properties of the data surrounding the test example.</s> <s>existing methods assume that the instances most useful for building the local model are strictly those closest to the test example.</s> <s>however, this fails to account for the fact that the success of the resulting classifier depends on the full distribution of selected training instances.</s> <s>rather than simply gather the test example's nearest neighbors, we propose to predict the subset of training data that is jointly relevant to training its local model.</s> <s>we develop an approach to discover patterns between queries and their \"good\" neighborhoods using large-scale multi-label classification with compressed sensing.</s> <s>given a novel test point, we estimate both the composition and size of the training subset likely to yield an accurate local model.</s> <s>we demonstrate the approach on image classification tasks on sun and apascal and show it outperforms traditional global and local approaches.\"</s></p></d>", "label": ["<d><p><s>predicting useful neighborhoods for lazy local learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes.</s> <s>contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination.</s> <s>by exploiting such a unified model for semantics, we enforce each category to be generated as a sparse combination of a supercategory + attributes, with an additional exclusive regularization to learn discriminative composition.</s> <s>the proposed reconstructive regularization guides the discriminative learning process to learn a better generalizing model, as well as generates compact semantic description of each category, which enables humans to analyze what has been learned.</s></p></d>", "label": ["<d><p><s>a unified semantic embedding: relating taxonomies and attributes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes.</s> <s>for example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images.</s> <s>in practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right.</s> <s>we propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions.</s> <s>by leveraging statistics about each attribute?s error tendencies, our method obtains more robust discriminative models for the unseen classes.</s> <s>we further devise extensions to handle the few-shot scenario and unreliable attribute descriptions.</s> <s>on three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.</s></p></d>", "label": ["<d><p><s>zero-shot recognition with unreliable attributes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements.</s> <s>more precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (image dependent pairwise relations).</s> <s>these spatial relationships are represented by a mixture model.</s> <s>we use deep convolutional neural networks (dcnns) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches.</s> <s>hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of dcnns.</s> <s>our method significantly outperforms the state of the art methods on the lsp and flic datasets and also performs very well on the buffy dataset without any training.</s></p></d>", "label": ["<d><p><s>articulated pose estimation by a graphical model with image dependent pairwise relations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a framework for post model selection inference, via marginal screening, in linear regression.</s> <s>at the core of this framework is a result that characterizes the exact distribution of linear functions of the response $y$, conditional on the model being selected (``condition on selection framework).</s> <s>this allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure.</s> <s>in contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix $x$.</s> <s>furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets.</s> <s>although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable.</s> <s>we show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit and marginal screening+lasso.\"</s></p></d>", "label": ["<d><p><s>exact post model selection inference for marginal screening</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the use of m-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard l_0 constraints.</s> <s>of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (iht)) methods is known to offer the fastest and most scalable solutions.</s> <s>however, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models.</s> <s>in this work we bridge this gap by providing the first analysis for iht-style methods in the high dimensional statistical setting.</s> <s>our bounds are tight and match known minimax lower bounds.</s> <s>our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as htp, cosamp, sp) in the high dimensional regression setting.</s> <s>finally, we extend our analysis to the problem of low-rank matrix recovery.</s></p></d>", "label": ["<d><p><s>on iterative hard thresholding methods for high-dimensional m-estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the l1-regularized logistic regression (or sparse logistic regression) is a widely used method for simultaneous classification and feature selection.</s> <s>although many recent efforts have been devoted to its efficient implementation, its application to high dimensional data still poses significant challenges.</s> <s>in this paper, we present a fast and effective sparse logistic regression screening rule (slores) to identify the zero components in the solution vector, which may lead to a substantial reduction in the number of features to be entered to the optimization.</s> <s>an appealing feature of slores is that the data set needs to be scanned only once to run the screening and its computational cost is negligible compared to that of solving the sparse logistic regression problem.</s> <s>moreover, slores is independent of solvers for sparse logistic regression, thus slores can be integrated with any existing solver to improve the efficiency.</s> <s>we have evaluated slores using high-dimensional data sets from different applications.</s> <s>extensive experimental results demonstrate that slores outperforms the existing state-of-the-art screening rules and the efficiency of solving sparse logistic regression is improved by one magnitude in general.</s></p></d>", "label": ["<d><p><s>a safe screening rule for sparse logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a generalized dantzig selector (gds) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation.</s> <s>we investigate both computational and statistical aspects of the gds.</s> <s>based on conjugate proximal operator, a flexible inexact admm framework is designed for solving gds.</s> <s>thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on gaussian widths of the unit norm ball and the error set.</s> <s>further, we consider a non-trivial example of the gds using k-support norm.</s> <s>we derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting.</s> <s>for statistical analysis, we provide upper bounds for the gaussian widths needed in the gds analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm.</s> <s>the experimental results confirm our theoretical analysis.</s></p></d>", "label": ["<d><p><s>generalized dantzig selector: application to the k-support norm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes.</s> <s>our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel.</s> <s>each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task.</s> <s>we develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified.</s> <s>superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions.</s> <s>we present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy.</s> <s>moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.</s></p></d>", "label": ["<d><p><s>parallel feature selection inspired by group testing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the $k$-support norm has successfully been applied to sparse vector prediction problems.</s> <s>we observe that it belongs to a wider class of norms, which we call the box-norms.</s> <s>within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the $k$-support norm.</s> <s>we extend the norms from the vector to the matrix setting and we introduce the spectral $k$-support norm.</s> <s>we study its properties and show that it is closely related to the multitask learning cluster norm.</s> <s>we apply the norms to real and synthetic matrix completion datasets.</s> <s>our findings indicate that spectral $k$-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.</s></p></d>", "label": ["<d><p><s>spectral k-support norm regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the beta-negative binomial process (bnbp), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix.</s> <s>as the marginal probability distribution of the bnbp that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the bnbp has to truncate the number of atoms of the beta process.</s> <s>this paper introduces an exchangeable partition probability function to explicitly describe how the bnbp clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups.</s> <s>a fully collapsed gibbs sampler is developed for the bnbp, leading to a novel nonparametric bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.</s></p></d>", "label": ["<d><p><s>beta-negative binomial process and exchangeable ?random partitions for mixed-membership modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dirichlet process mixture of gaussians (dpmg) has been used in the literature for clustering and density estimation problems.</s> <s>however, many real-world data exhibit cluster distributions that cannot be captured by a single gaussian.</s> <s>modeling such data sets by dpmg creates several extraneous clusters even when clusters are relatively well-defined.</s> <s>herein, we present the infinite mixture of infinite gaussian mixtures (i2gmm) for more flexible modeling of data sets with skewed and multi-modal cluster distributions.</s> <s>instead of using a single gaussian for each cluster as in the standard dpmg model, the generative model of i2gmm uses a single dpmg for each cluster.</s> <s>the individual dpmgs are linked together through centering of their base distributions at the atoms of a higher level dp prior.</s> <s>inference is performed by a collapsed gibbs sampler that also enables partial parallelization.</s> <s>experimental results on several artificial and real-world data sets suggest the proposed i2gmm model can predict clusters more accurately than existing variational bayes and gibbs sampler versions of dpmg.</s></p></d>", "label": ["<d><p><s>the infinite mixture of infinite gaussian mixtures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a fast algorithm for the admixture of poisson mrfs (apm) topic model and propose a novel metric to directly evaluate this model.</s> <s>the apm topic model recently introduced by inouye et al.</s> <s>(2014) is the first topic model that allows for word dependencies within each topic unlike in previous topic models like lda that assume independence between words within a topic.</s> <s>research in both the semantic coherence of a topic models (mimno et al.</s> <s>2011, newman et al.</s> <s>2010) and measures of model fitness (mimno & blei 2011) provide strong support that explicitly modeling word dependencies---as in apm---could be both semantically meaningful and essential for appropriately modeling real text data.</s> <s>though apm shows significant promise for providing a better topic model, apm has a high computational complexity because $o(p^2)$ parameters must be estimated where $p$ is the number of words (inouye et al.</s> <s>could only provide results for datasets with $p = 200$).</s> <s>in light of this, we develop a parallel alternating newton-like algorithm for training the apm model that can handle $p = 10^4$ as an important step towards scaling to large datasets.</s> <s>in addition, inouye et al.</s> <s>only provided tentative and inconclusive results on the utility of apm.</s> <s>thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e.</s> <s>how much one word brings to mind\" another word (boyd-graber et al.</s> <s>2006)).</s> <s>we provide compelling quantitative and qualitative results on the bnc corpus that demonstrate the superiority of apm over previous topic models for identifying semantically meaningful word dependencies.</s> <s>(matlab code available at: http://bigdata.ices.utexas.edu/software/apm/)\"</s></p></d>", "label": ["<d><p><s>capturing semantically meaningful word dependencies with an admixture of poisson mrfs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (1) discovering topic prevalence over time, and (2) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct.</s> <s>the high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are well accommodated through an underlying dynamic sparse factor model.</s> <s>the framework naturally admits heavy-tailed innovations, capable of inferring abrupt temporal jumps in the importance of topics.</s> <s>posterior inference is performed through straightforward gibbs sampling, based on the forward-filtering backward-sampling algorithm.</s> <s>moreover, an efficient data subsampling scheme is leveraged to speed up inference on massive datasets.</s> <s>the modeling framework is illustrated on two real datasets: the us state of the union address and the jstor collection from science.</s></p></d>", "label": ["<d><p><s>dynamic rank factor model for text streams</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>topic models, such as latent dirichlet allocation (lda), posit that documents are drawn from admixtures of distributions over words, known as topics.</s> <s>the inference problem of recovering topics from such a collection of documents drawn from admixtures, is np-hard.</s> <s>making a strong assumption called separability, [4] gave the first provable algorithm for inference.</s> <s>for the widely used lda model, [6] gave a provable algorithm using clever tensor-methods.</s> <s>but [4, 6] do not learn topic vectors with bounded $l_1$ error (a natural measure for probability vectors).</s> <s>our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as svd, which provably solves the inference problem for the model with bounded $l_1$ error.</s> <s>a topic in lda and other models is essentially characterized by a group of co-occurring words.</s> <s>motivated by this, we introduce topic specific catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually.</s> <s>a major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (svd) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from dominant admixtures.</s> <s>dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others.</s> <s>apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on $w_0$, the lowest probability that a topic is dominant, and is better than [4].</s> <s>empirical evidence shows that on several real world corpora, both catchwords and dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].</s></p></d>", "label": ["<d><p><s>a provable svd-based algorithm for learning topics in dominant admixture corpus</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while topic models can discover patterns of word usage in large corpora, it is difficult to meld this unsupervised structure with noisy, human-provided labels, especially when the label space is large.</s> <s>in this paper, we present a model-label to hierarchy (l2h)-that can induce a hierarchy of user-generated labels and the topics associated with those labels from a set of multi-labeled documents.</s> <s>the model is robust enough to account for missing labels from untrained, disparate annotators and provide an interpretable summary of an otherwise unwieldy label set.</s> <s>we show empirically the effectiveness of l2h in predicting held-out words and labels for unseen documents.</s></p></d>", "label": ["<d><p><s>learning a concept hierarchy from multi-labeled documents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points.</s> <s>the success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used.</s> <s>in this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification.</s> <s>this pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions.</s> <s>we consider two nonparametric classifiers in this framework, i.e.</s> <s>the nearest neighbor classifier and the plug-in classifier.</s> <s>modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering.</s> <s>under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity.</s> <s>we also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for low density separation, a widely used criteria for semi-supervised learning and clustering.</s> <s>based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.</s></p></d>", "label": ["<d><p><s>on a theory of nonparametric pairwise similarity for clustering: connecting clustering to classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present max-margin bayesian clustering (bmc), a general and robust framework that incorporates the max-margin criterion into bayesian clustering models, as well as two concrete models of bmc to demonstrate its flexibility and effectiveness in dealing with different clustering tasks.</s> <s>the dirichlet process max-margin gaussian mixture is a nonparametric bayesian clustering model that relaxes the underlying gaussian assumption of dirichlet process gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data.</s> <s>we further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion.</s> <s>extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.</s></p></d>", "label": ["<d><p><s>robust bayesian max-margin clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we model the joint clustering and outlier detection problem using an extension of the facility location formulation.</s> <s>the advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable.</s> <s>we provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence.</s> <s>extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.</s></p></d>", "label": ["<d><p><s>on integrated clustering and outlier detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we present theoretical analysis of son~--~a convex optimization procedure for clustering using a sum-of-norms (son) regularization recently proposed in \\cite{icml2011hocking_419,son, lindsten650707, pelckmans2005convex}.</s> <s>in particular, we show if the samples are drawn from two cubes, each being one cluster, then son can provably identify the cluster membership provided that the distance between the two cubes is larger than a threshold which (linearly) depends on the size of the cube and the ratio of numbers of samples in each cluster.</s> <s>to the best of our knowledge, this paper is the first to provide a rigorous analysis to understand why and when son works.</s> <s>we believe this may provide important insights to develop novel convex optimization based algorithms for clustering.</s></p></d>", "label": ["<d><p><s>convex optimization procedure for clustering: theoretical revisit</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces.</s> <s>to this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces.</s> <s>as the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as k-means, many model-specific methods have been proposed.</s> <s>in this paper, we provide new simple and efficient algorithms for this problem.</s> <s>our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity be- tween subspaces.</s> <s>these conditions are weaker than those considered in the standard statistical literature.</s> <s>experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory.</s> <s>we also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.</s></p></d>", "label": ["<d><p><s>greedy subspace clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of finding clusters in an unweighted graph, when the graph is partially observed.</s> <s>we analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization.</s> <s>for the commonly used stochastic block model, we obtain \\emph{explicit} bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs.</s> <s>we corroborate our theoretical findings through extensive simulations.</s> <s>we also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the amazon mechanical turk, and observe significant performance improvement over traditional methods such as k-means.</s></p></d>", "label": ["<d><p><s>graph clustering with missing data: convex algorithms and analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modeling data as being sampled from a union of independent subspaces has been widely applied to a number of real world applications.</s> <s>however, dimensionality reduction approaches that theoretically preserve this independence assumption have not been well studied.</s> <s>our key contribution is to show that $2k$ projection vectors are sufficient for the independence preservation of any $k$ class data sampled from a union of independent subspaces.</s> <s>it is this non-trivial observation that we use for designing our dimensionality reduction technique.</s> <s>in this paper, we propose a novel dimensionality reduction algorithm that theoretically preserves this structure for a given dataset.</s> <s>we support our theoretical analysis with empirical results on both synthetic and real world data achieving \\textit{state-of-the-art} results compared to popular dimensionality reduction techniques.</s></p></d>", "label": ["<d><p><s>dimensionality reduction with subspace structure preservation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large-scale clustering of data points in metric spaces is an important problem in mining big data sets.</s> <s>for many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the ``balanced clustering'' problem.</s> <s>although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem.</s> <s>in the present paper we develop a general framework based on ``mapping coresets'' to tackle this issue.</s> <s>for a wide range of clustering objective functions such as k-center, k-median, and k-means, our techniques give distributed algorithms for balanced clustering that match the best known single machine approximation ratios.</s></p></d>", "label": ["<d><p><s>distributed balanced clustering via mapping coresets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning.</s> <s>for incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls.</s> <s>this paper aims to study nonconvex hull learning which has rarely been investigated in the literature.</s> <s>in order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points.</s> <s>employing this measure, we present a greedy algorithmic framework, dubbed zeta hulls, to perform structured column sampling.</s> <s>the process of pursuing a zeta hull involves the computation of matrix inverse.</s> <s>to accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique.</s> <s>extensive experimental results show that data representation learned by zeta hulls can achieve state-of-the-art accuracy in text and image classification tasks.</s></p></d>", "label": ["<d><p><s>zeta hull pursuits: learning nonconvex data hulls</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present the bayesian case model (bcm), a general framework for bayesian case-based reasoning (cbr) and prototype classification and clustering.</s> <s>bcm brings the intuitive power of cbr to a bayesian generative framework.</s> <s>the bcm learns prototypes, the ``quintessential observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features.</s> <s>simultaneously, bcm pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes.</s> <s>the prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy.</s> <s>human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by bcm, compared to those given by prior art.\"</s></p></d>", "label": ["<d><p><s>the bayesian case model: a generative approach for case-based reasoning and prototype classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral graph partitioning methods have received significant attention from both practitioners and theorists in computer science.</s> <s>some notable studies have been carried out regarding the behavior of these methods for infinitely large sample size (von luxburg et al., 2008; rohe et al., 2011), which provide sufficient confidence to practitioners about the effectiveness of these methods.</s> <s>on the other hand, recent developments in computer vision have led to a plethora of applications, where the model deals with multi-way affinity relations and can be posed as uniform hyper-graphs.</s> <s>in this paper, we view these models as random m-uniform hypergraphs and establish the consistency of spectral algorithm in this general setting.</s> <s>we develop a planted partition model or stochastic blockmodel for such problems using higher order tensors, present a spectral technique suited for the purpose and study its large sample behavior.</s> <s>the analysis reveals that the algorithm is consistent for m-uniform hypergraphs for larger values of m, and also the rate of convergence improves for increasing m. our result provides the first theoretical evidence that establishes the importance of m-way affinities.</s></p></d>", "label": ["<d><p><s>consistency of spectral partitioning of uniform hypergraphs under planted partition model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov chain monte carlo (mcmc) algorithms are simple and extremely powerful techniques to sample from almost arbitrary distributions.</s> <s>the flaw in practice is that it can take a large and/or unknown amount of time to converge to the stationary distribution.</s> <s>this paper gives sufficient conditions to guarantee that univariate gibbs sampling on markov random fields (mrfs) will be fast mixing, in a precise sense.</s> <s>further, an algorithm is given to project onto this set of fast-mixing parameters in the euclidean norm.</s> <s>following recent work, we give an example use of this to project in various divergence measures, comparing of univariate marginals obtained by sampling after projection to common variational methods and gibbs sampling on the original parameters.</s></p></d>", "label": ["<d><p><s>projecting markov random field parameters for fast mixing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we combine the ideas behind trees and gaussian graphical models to form a new nonparametric family of graphical models.</s> <s>our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees.</s> <s>the tree edges are chosen to connect variables that most violate joint gaussianity.</s> <s>the non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic.</s> <s>a nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso.</s> <s>the result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points.</s> <s>theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"</s></p></d>", "label": ["<d><p><s>blossom tree graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models.</s> <s>it introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent.</s></p></d>", "label": ["<d><p><s>distributed parameter estimation in probabilistic graphical models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings.</s> <s>our approach builds on observing the precise manner in which the classical graphical model mle ``breaks down'' under high-dimensional settings.</s> <s>our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure.</s> <s>we provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the $\\ell_1$-regularized mles that are much more difficult to compute.</s> <s>we corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and gaussian graphical models.</s></p></d>", "label": ["<d><p><s>elementary estimators for graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d.</s> <s>samples.</s> <s>our first result is an unconditional computational lower bound of $\\omega (p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of statistical algorithms recently introduced by feldman et al.</s> <s>the construction is related to the notoriously difficult learning parities with noise problem in computational learning theory.</s> <s>our lower bound shows that the $\\widetilde o(p^{d+2})$ runtime required by bresler, mossel, and sly's exhaustive-search algorithm cannot be significantly improved without restricting the class of models.</s> <s>aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., most recent papers on structure learning assume that the model has the correlation decay property.</s> <s>indeed, focusing on ferromagnetic ising models, bento and montanari showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold.</s> <s>our second set of results gives a class of repelling (antiferromagnetic) models that have the \\emph{opposite} behavior: very strong repelling allows efficient learning in time $\\widetilde o(p^2)$.</s> <s>we provide an algorithm whose performance interpolates between $\\widetilde o(p^2)$ and $\\widetilde o(p^{d+2})$ depending on the strength of the repulsion.</s></p></d>", "label": ["<d><p><s>structure learning of antiferromagnetic ising models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we address the problem of learning the structure of gaussian chain graph models in a high-dimensional space.</s> <s>chain graph models are generalizations of undirected and directed graphical models that contain a mixed set of directed and undirected edges.</s> <s>while the problem of sparse structure learning has been studied extensively for gaussian graphical models and more recently for conditional gaussian graphical models (cggms), there has been little previous work on the structure recovery of gaussian chain graph models.</s> <s>we consider linear regression models and a re-parameterization of the linear regression models using cggms as building blocks of chain graph models.</s> <s>we argue that when the goal is to recover model structures, there are many advantages of using cggms as chain component models over linear regression models, including convexity of the optimization problem, computational efficiency, recovery of structured sparsity, and ability to leverage the model structure for semi-supervised learning.</s> <s>we demonstrate our approach on simulated and genomic datasets.</s></p></d>", "label": ["<d><p><s>on sparse gaussian chain graph models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>owing to several applications in large scale learning and vision problems, fast submodular function minimization (sfm) has become a critical problem.</s> <s>theoretically, unconstrained sfm can be performed in polynomial time (iwata and orlin 2009), however these algorithms are not practical.</s> <s>in 1976, wolfe proposed an algorithm to find the minimum euclidean norm point in a polytope, and in 1980, fujishige showed how wolfe's algorithm can be used for sfm.</s> <s>for general submodular functions, the fujishige-wolfe minimum norm algorithm seems to have the best empirical performance.</s> <s>despite its good practical performance, theoretically very little is known about wolfe's minimum norm algorithm -- to our knowledge the only result is an exponential time analysis due to wolfe himself.</s> <s>in this paper we give a maiden convergence analysis of wolfe's algorithm.</s> <s>we prove that in t iterations, wolfe's algorithm returns a o(1/t)-approximate solution to the min-norm point.</s> <s>we also prove a robust version of fujishige's theorem which shows that an o(1/n^2)-approximate solution to the min-norm point problem implies exact submodular minimization.</s> <s>as a corollary, we get the first pseudo-polynomial time guarantee for the fujishige-wolfe minimum norm algorithm for submodular function minimization.</s> <s>in particular, we show that the min-norm point algorithm solves sfm in o(n^7f^2)-time, where $f$ is an upper bound on the maximum change a single element can cause in the function value.</s></p></d>", "label": ["<d><p><s>provable submodular minimization using wolfe's algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we derive a second-order ordinary differential equation (ode), which is the limit of nesterov?s accelerated gradient method.</s> <s>this ode exhibits approximate equivalence to nesterov?s scheme and thus can serve as a tool for analysis.</s> <s>we show that the continuous time ode allows for a better understanding of nesterov?s scheme.</s> <s>as a byproduct, we obtain a family of schemes with similar convergence rates.</s> <s>the ode interpretation also suggests restarting nesterov?s scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.</s></p></d>", "label": ["<d><p><s>a differential equation for modeling nesterov?s accelerated gradient method: theory and insights</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs.</s> <s>however, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning.</s> <s>in this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters.</s> <s>we extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches.</s> <s>we define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning.</s> <s>we show that our formulation outperforms structural svm baselines in two tasks: multiclass document classification and part-of-speech tagging.</s></p></d>", "label": ["<d><p><s>learning distributed representations for structured output prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel information-theoretic approach for bayesian optimization called predictive entropy search (pes).</s> <s>at each iteration, pes selects the next evaluation point that maximizes the expected information gained with respect to the global maximum.</s> <s>pes codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution.</s> <s>this reformulation allows pes to obtain approximations that are both more accurate and efficient than other alternatives such as entropy search (es).</s> <s>furthermore, pes can easily perform a fully bayesian treatment of the model hyperparameters while es cannot.</s> <s>we evaluate pes in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics.</s> <s>we show that the increased accuracy of pes leads to significant gains in optimization performance.</s></p></d>", "label": ["<d><p><s>predictive entropy search for efficient global optimization of black-box functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to cope with the high level of ambiguity faced in domains such as computer vision or natural language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals.</s> <s>in structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.)</s> <s>is exponentially large.</s> <s>we study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and high-order potentials (hops) studied for graphical models.</s> <s>specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed hops.</s> <s>we discuss benefits, tradeoffs, and show that our constructions lead to significantly better proposals.</s></p></d>", "label": ["<d><p><s>submodular meets structured: finding diverse subsets in exponentially-large structured item sets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call noisy power method.</s> <s>our result characterizes the convergence behavior of the algorithm when a large amount noise is introduced after each matrix-vector multiplication.</s> <s>the noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (pca), and privacy-preserving spectral analysis.</s> <s>our general analysis subsumes several existing ad-hoc convergence bounds and resolves a number of open problems in multiple applications.</s> <s>a recent work of mitliagkas et al.~(nips 2013) gives a space-efficient algorithm for pca in a streaming model where samples are drawn from a spiked covariance model.</s> <s>we give a simpler and more general analysis that applies to arbitrary distributions.</s> <s>moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime.</s> <s>as a second application, we provide an algorithm for differentially private principal component analysis that runs in nearly linear time in the input sparsity and achieves nearly tight worst-case error bounds.</s> <s>complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix.</s> <s>this result resolves the main problem left open by hardt and roth (stoc 2013) and leads to strong average-case improvements over the optimal worst-case bound.</s></p></d>", "label": ["<d><p><s>the noisy power method: a meta algorithm with applications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse-group lasso (sgl) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms.</s> <s>however, in large-scale applications, the complexity of the regularizers entails great computational challenges.</s> <s>in this paper, we propose a novel two-layer feature reduction method (tlfre) for sgl via a decomposition of its dual feasible set.</s> <s>the two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization.</s> <s>existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer.</s> <s>to our best knowledge, tlfre is the first one that is capable of dealing with multiple sparsity-inducing regularizers.</s> <s>moreover, tlfre has a very low computational cost and can be integrated with any existing solvers.</s> <s>experiments on both synthetic and real data sets show that tlfre improves the efficiency of sgl by orders of magnitude.</s></p></d>", "label": ["<d><p><s>two-layer feature reduction for sparse-group lasso via decomposition of convex sets</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for massive data sets, efficient computation commonly relies on distributed algorithms that store and process subsets of the data on different machines, minimizing communication costs.</s> <s>our focus is on regression and classification problems involving many features.</s> <s>a variety of distributed algorithms have been proposed in this context, but challenges arise in defining an algorithm with low communication, theoretical guarantees and excellent practical performance in general settings.</s> <s>we propose a median selection subset aggregation estimator (message) algorithm, which attempts to solve these problems.</s> <s>the algorithm applies feature selection in parallel for each subset using lasso or another method, calculates the `median' feature inclusion index, estimates coefficients for the selected features in parallel for each subset, and then averages these estimates.</s> <s>the algorithm is simple, involves very minimal communication, scales efficiently in both sample and feature size, and has theoretical guarantees.</s> <s>in particular, we show model selection consistency and coefficient estimation efficiency.</s> <s>extensive experiments show excellent performance in variable selection, estimation, prediction, and computation time relative to usual competitors.</s></p></d>", "label": ["<d><p><s>median selection subset aggregation for parallel inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present the first provably sublinear time hashing algorithm for approximate \\emph{maximum inner product search} (mips).</s> <s>searching with (un-normalized) inner product as the underlying similarity measure is a known difficult problem and finding hashing schemes for mips was considered hard.</s> <s>while the existing locality sensitive hashing (lsh) framework is insufficient for solving mips, in this paper we extend the lsh framework to allow asymmetric hashing schemes.</s> <s>our proposal is based on a key observation that the problem of finding maximum inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search in classical settings.</s> <s>this key observation makes efficient sublinear hashing scheme for mips possible.</s> <s>under the extended asymmetric lsh (alsh) framework, this paper provides an example of explicit construction of provably fast hashing scheme for mips.</s> <s>our proposed algorithm is simple and easy to implement.</s> <s>the proposed hashing scheme leads to significant computational savings over the two popular conventional lsh schemes: (i) sign random projection (srp) and (ii) hashing based on $p$-stable distributions for $l_2$ norm (l2lsh), in the collaborative filtering task of item recommendations on netflix and movielens (10m) datasets.</s></p></d>", "label": ["<d><p><s>asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>despite the prevalence of collaborative filtering in recommendation systems, there has been little theoretical development on why and how well it works, especially in the ``online'' setting, where items are recommended to users over time.</s> <s>we address this theoretical gap by introducing a model for online recommendation systems, cast item recommendation under the model as a learning problem, and analyze the performance of a cosine-similarity collaborative filtering method.</s> <s>in our model, each of $n$ users either likes or dislikes each of $m$ items.</s> <s>we assume there to be $k$ types of users, and all the users of a given type share a common string of probabilities determining the chance of liking each item.</s> <s>at each time step, we recommend an item to each user, where a key distinction from related bandit literature is that once a user consumes an item (e.g., watches a movie), then that item cannot be recommended to the same user again.</s> <s>the goal is to maximize the number of likable items recommended to users over time.</s> <s>our main result establishes that after nearly $\\log(km)$ initial learning time steps, a simple collaborative filtering algorithm achieves essentially optimal performance without knowing $k$.</s> <s>the algorithm has an exploitation step that uses cosine similarity and two types of exploration steps, one to explore the space of items (standard in the literature) and the other to explore similarity between users (novel to this work).</s></p></d>", "label": ["<d><p><s>a latent source model for online collaborative filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a general framework for graph clustering where a label is observed to each pair of nodes.</s> <s>this allows a very rich encoding of various types of pairwise interactions between nodes.</s> <s>we propose a new tractable approach to this problem based on maximum likelihood estimator and convex optimization.</s> <s>we analyze our algorithm under a general generative model, and provide both necessary and sufficient conditions for successful recovery of the underlying clusters.</s> <s>our theoretical results cover and subsume a wide range of existing graph clustering results including planted partition, weighted clustering and partially observed graphs.</s> <s>furthermore, the result is applicable to novel settings including time-varying graphs such that new insights can be gained on solving these problems.</s> <s>our theoretical findings are further supported by empirical results on both synthetic and real data.</s></p></d>", "label": ["<d><p><s>clustering from labels and time-varying graphs</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases.</s> <s>in particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency.</s> <s>however, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases.</s> <s>we argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes.</s> <s>this paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space.</s> <s>we cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes.</s> <s>a tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods.</s> <s>extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes.</s></p></d>", "label": ["<d><p><s>discrete graph hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>l-bfgs has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s.</s> <s>with an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize l-bfgs effectively in a distributed system.</s> <s>in this paper, we study the problem of parallelizing the l-bfgs algorithm in large clusters of tens of thousands of shared-nothing commodity machines.</s> <s>first, we show that a naive implementation of l-bfgs using map-reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact.</s> <s>second, we propose a new l-bfgs algorithm, called vector-free l-bfgs, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism.</s> <s>the algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets.</s> <s>we prove the mathematical equivalence of the new vector-free l-bfgs and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.</s></p></d>", "label": ["<d><p><s>large-scale l-bfgs using mapreduce</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the recently established rpca method provides a convenient way to restore low-rank matrices from grossly corrupted observations.</s> <s>while elegant in theory and powerful in reality, rpca is not an ultimate solution to the low-rank matrix recovery problem.</s> <s>indeed, its performance may not be perfect even when data are strictly low-rank.</s> <s>this is because rpca ignores clustering structures of the data which are ubiquitous in applications.</s> <s>as the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of rpca degrades.</s> <s>we show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by low-rank representation (lrr)~\\cite{tpami_2013_lrr}, provided that the dictionary in lrr is configured appropriately.</s> <s>more precisely, we mathematically prove that if the dictionary itself is low-rank then lrr is immune to the coherence parameter which increases with the underlying cluster number.</s> <s>this provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments.</s> <s>experiments on randomly generated matrices and real motion sequences verify our claims.</s> <s>see the full paper at arxiv:1404.4032.</s></p></d>", "label": ["<d><p><s>recovery of coherent data via low-rank dictionary pursuit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable.</s> <s>in this paper, we show how to make these algorithms scalable for data matrices that have many more rows than columns, so-called tall-and-skinny matrices.\"</s> <s>one key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the nmf problem.</s> <s>our final methods need to read the data matrix only once and are suitable for streaming, multi-core, and mapreduce architectures.</s> <s>we demonstrate the efficacy of these algorithms on terabyte-sized matrices from scientific computing and bioinformatics.\"</s></p></d>", "label": ["<d><p><s>scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies.</s> <s>however, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear.</s> <s>previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way.</s> <s>specifically, some models violate dale's law (i.e.</s> <s>allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation.</s> <s>we propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints.</s> <s>we directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics.</s> <s>the resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices.</s> <s>our results constitute a step forward in our understanding of the neural substrate of memory.</s></p></d>", "label": ["<d><p><s>analog memories in a balanced rate-based network of e-i neurons</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a biologically-plausible learning rule that provably converges to the class means of general mixture models.</s> <s>this rule generalizes the classical bcm neural rule within a tensor framework, substantially increasing the generality of the learning problem it solves.</s> <s>it achieves this by incorporating triplets of samples from the mixtures, which provides a novel information processing interpretation to spike-timing-dependent plasticity.</s> <s>we provide both proofs of convergence, and a close fit to experimental data on stdp.</s></p></d>", "label": ["<d><p><s>feedforward learning of mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales.</s> <s>to understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs.</s> <s>here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity.</s> <s>population states are modelled using a hidden markov decision tree with state-dependent tuning parameters and a generalised linear observation model.</s> <s>we present a variational bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings.</s> <s>on simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters.</s> <s>using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model.</s> <s>finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.</s></p></d>", "label": ["<d><p><s>a bayesian model for identifying hierarchically organised states in neural population activity</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>high-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models.</s> <s>such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells.</s> <s>here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons.</s> <s>the model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies.</s> <s>in the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by expectation maximization (em).</s> <s>we also address the crucial problem of initializing parameters for em by extending a sparse subspace clustering algorithm to integer-valued spike count observations.</s> <s>we illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.</s></p></d>", "label": ["<d><p><s>clustered factor analysis of multineuronal spike data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hippocampal place fields have been shown to reflect behaviorally relevant aspects of space.</s> <s>for instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment.</s> <s>we hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning.</s> <s>in particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribution.</s> <s>under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the transition policy.</s> <s>furthermore, we demonstrate that this representation of space can support efficient reinforcement learning.</s> <s>we also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries.</s> <s>when applied recursively, this segmentation can be used to discover a hierarchical decomposition of space.</s> <s>thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.</s></p></d>", "label": ["<d><p><s>design principles of the hippocampal cognitive map</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fluorescent calcium imaging provides a potentially powerful tool for inferring connectivity in neural circuits with up to thousands of neurons.</s> <s>however, a key challenge in using calcium imaging for connectivity detection is that current systems often have a temporal response and frame rate that can be orders of magnitude slower than the underlying neural spiking process.</s> <s>bayesian inference based on expectation-maximization (em) have been proposed to overcome these limitations, but they are often computationally demanding since the e-step in the em procedure typically involves state estimation in a high-dimensional nonlinear dynamical system.</s> <s>in this work, we propose a computationally fast method for the state estimation based on a hybrid of loopy belief propagation and approximate message passing (amp).</s> <s>the key insight is that a neural system as viewed through calcium imaging can be factorized into simple scalar dynamical systems for each neuron with linear interconnections between the neurons.</s> <s>using the structure, the updates in the proposed hybrid amp methodology can be computed by a set of one-dimensional state estimation procedures and linear transforms with the connectivity matrix.</s> <s>this yields a computationally scalable method for inferring connectivity of large neural circuits.</s> <s>simulations of the method on realistic neural networks demonstrate good accuracy with computation times that are potentially significantly faster than current approaches based on markov chain monte carlo methods.</s></p></d>", "label": ["<d><p><s>scalable inference for neuronal connectivity from calcium imaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a unified formulation and algorithm to find an extremely sparse representation for calcium image sequences in terms of cell locations, cell shapes, spike timings and impulse responses.</s> <s>solution of a single optimization problem yields cell segmentations and activity estimates that are on par with the state of the art, without the need for heuristic pre- or postprocessing.</s> <s>experiments on real and synthetic data demonstrate the viability of the proposed method.</s></p></d>", "label": ["<d><p><s>sparse space-time deconvolution for calcium image analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has been long argued that, because of inherent ambiguity and noise, the brain needs to represent uncertainty in the form of probability distributions.</s> <s>the neural encoding of such distributions remains however highly controversial.</s> <s>here we present a novel circuit model for representing multidimensional real-valued distributions using a spike based spatio-temporal code.</s> <s>our model combines the computational advantages of the currently competing models for probabilistic codes and exhibits realistic neural responses along a variety of classic measures.</s> <s>furthermore, the model highlights the challenges associated with interpreting neural activity in relation to behavioral uncertainty and points to alternative population-level approaches for the experimental validation of distributed representations.</s></p></d>", "label": ["<d><p><s>spatio-temporal representations of uncertainty in spiking neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a framework for unsupervised learning of structured predictors with overlapping, global features.</s> <s>each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (crf).</s> <s>then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the crf.</s> <s>the autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used.</s> <s>we illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning.</s> <s>finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.</s></p></d>", "label": ["<d><p><s>conditional random field autoencoders for unsupervised structured prediction</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex.</s> <s>inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention.</s> <s>the attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling.</s> <s>by ignoring scene background clutter, the generative model can concentrate its resources on the object of interest.</s> <s>a convolutional neural net is employed to provide good initializations during posterior inference which uses hamiltonian monte carlo.</s> <s>upon learning images of faces, our model can robustly attend to the face region of novel test subjects.</s> <s>more importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.</s></p></d>", "label": ["<d><p><s>learning generative models with visual attention</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks (dnns) are powerful models that have achieved excellent performance on difficult learning tasks.</s> <s>although dnns work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.</s> <s>in this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.</s> <s>our method uses a multilayered long short-term memory (lstm) to map the input sequence to a vector of a fixed dimensionality, and then another deep lstm to decode the target sequence from the vector.</s> <s>our main result is that on an english to french translation task from the wmt-14 dataset, the translations produced by the lstm achieve a bleu score of 34.8 on the entire test set, where the lstm's bleu score was penalized on out-of-vocabulary words.</s> <s>additionally, the lstm did not have difficulty on long sentences.</s> <s>for comparison, a phrase-based smt system achieves a bleu score of 33.3 on the same dataset.</s> <s>when we used the lstm to rerank the 1000 hypotheses produced by the aforementioned smt system, its bleu score increases to 36.5, which is close to the previous state of the art.</s> <s>the lstm also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice.</s> <s>finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the lstm's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</s></p></d>", "label": ["<d><p><s>sequence to sequence learning with neural networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to gabor filters and color blobs.</s> <s>such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks.</s> <s>features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively.</s> <s>in this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results.</s> <s>transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected.</s> <s>in an example network trained on imagenet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network.</s> <s>we also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features.</s> <s>a final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.</s></p></d>", "label": ["<d><p><s>how transferable are features in deep neural networks?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important goal in visual recognition is to devise image representations that are invariant to particular transformations.</s> <s>in this paper, we address this goal with a new type of convolutional neural network (cnn) whose invariance is encoded by a reproducing kernel.</s> <s>unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data.</s> <s>such an approach enjoys several benefits over classical ones.</s> <s>first, by teaching cnns to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting.</s> <s>second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance.</s> <s>we evaluate our methodology on visual recognition tasks where cnns have proven to perform well, e.g., digit recognition with the mnist dataset, and the more challenging cifar-10 and stl-10 datasets, where our accuracy is competitive with the state of the art.</s></p></d>", "label": ["<d><p><s>convolutional kernel networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition.</s> <s>whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like imagenet and the rise of convolutional neural networks (cnns) for learning high-level features, performance at scene recognition has not attained the same level of success.</s> <s>this may be because current deep features trained from imagenet are not competitive enough for such tasks.</s> <s>here, we introduce a new scene-centric database called places with over 7 million labeled pictures of scenes.</s> <s>we propose new methods to compare the density and diversity of image datasets and show that places is as dense as other scene datasets and has more diversity.</s> <s>using cnn, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets.</s> <s>a visualization of the cnn layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.</s></p></d>", "label": ["<d><p><s>learning deep features for scene recognition using places database</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities.</s> <s>we introduce an attribute grammar framework for representing symbolic expressions.</s> <s>given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity.</s> <s>however, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions.</s> <s>consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search.</s> <s>the first of these is a simple n-gram model, the other being a recursive neural-network.</s> <s>we show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.</s></p></d>", "label": ["<d><p><s>learning to discover efficient mathematical identities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>particle colliders enable us to probe the fundamental nature of matter by observing exotic particles produced by high-energy collisions.</s> <s>because the experimental measurements from these collisions are necessarily incomplete and imprecise, machine learning algorithms play a major role in the analysis of experimental data.</s> <s>the high-energy physics community typically relies on standardized machine learning software packages for this analysis, and devotes substantial effort towards improving statistical power by hand crafting high-level features derived from the raw collider measurements.</s> <s>in this paper, we train artificial neural networks to detect the decay of the higgs boson to tau leptons on a dataset of 82 million simulated collision events.</s> <s>we demonstrate that deep neural network architectures are particularly well-suited for this task with the ability to automatically discover high-level features from the data and increase discovery significance.</s></p></d>", "label": ["<d><p><s>searching for higgs boson decay modes with deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis.</s> <s>we revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones.</s> <s>generative approaches have thus far been either inflexible, inefficient or non-scalable.</s> <s>we show that deep generative models and approximate bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.</s></p></d>", "label": ["<d><p><s>semi-supervised learning with deep generative models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we investigate architectures of discriminatively trained deep convolutional networks (convnets) for action recognition in video.</s> <s>the challenge is to capture the complementary information on appearance from still frames and motion between frames.</s> <s>we also aim to generalise the best performing hand-crafted features within a data-driven learning framework.</s> <s>our contribution is three-fold.</s> <s>first, we propose a two-stream convnet architecture which incorporates spatial and temporal networks.</s> <s>second, we demonstrate that a convnet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data.</s> <s>finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both.</s> <s>our architecture is trained and evaluated on the standard video actions benchmarks of ucf-101 and hmdb-51, where it is competitive with the state of the art.</s> <s>it also exceeds by a large margin previous attempts to use deep nets for video classification.</s></p></d>", "label": ["<d><p><s>two-stream convolutional networks for action recognition in videos</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>metric labeling is a special case of energy minimization for pairwise markov random fields.</s> <s>the energy function consists of arbitrary unary potentials, and pairwise potentials that are proportional to a given metric distance function over the label set.</s> <s>popular methods for solving metric labeling include (i) move-making algorithms, which iteratively solve a minimum st-cut problem; and (ii) the linear programming (lp) relaxation based approach.</s> <s>in order to convert the fractional solution of the lp relaxation to an integer solution, several randomized rounding procedures have been developed in the literature.</s> <s>we consider a large class of parallel rounding procedures, and design move-making algorithms that closely mimic them.</s> <s>we prove that the multiplicative bound of a move-making algorithm exactly matches the approximation factor of the corresponding rounding procedure for any arbitrary distance function.</s> <s>our analysis includes all known results for move-making algorithms as special cases.</s></p></d>", "label": ["<d><p><s>rounding-based moves for metric labeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we improve a recent gurantee of bach and moulines on the linear convergence of sgd for smooth and strongly convex objectives, reducing a quadratic dependence on the strong convexity to a linear dependence.</s> <s>furthermore, we show how reweighting the sampling distribution (i.e.</s> <s>importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence on average smoothness, dominating previous results, and more broadly discus how importance sampling for sgd can improve convergence also in other scenarios.</s> <s>our results are based on a connection we make between sgd and the randomized kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods.</s></p></d>", "label": ["<d><p><s>stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an accelerated randomized proximal coordinate gradient (apcg) method, for solving a broad class of composite convex optimization problems.</s> <s>in particular, our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods.</s> <s>we show how to apply the apcg method to solve the dual of the regularized empirical risk minimization (erm) problem, and devise efficient implementations that can avoid full-dimensional vector operations.</s> <s>for ill-conditioned erm problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (sdca) method.</s></p></d>", "label": ["<d><p><s>an accelerated proximal coordinate gradient method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy.</s> <s>the proposed approach, refereed as inference by learning or ibyl, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers.</s> <s>we thoroughly experiment with classic computer vision related mrf problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the mrf.</s> <s>we make our code available on-line.</s></p></d>", "label": ["<d><p><s>inference by learning: speeding-up graphical model optimization via a coarse-to-fine cascade of pruning classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the task of reconstructing a matrix given a sample of observed entries is known as the \\emph{matrix completion problem}.</s> <s>such a consideration arises in a wide variety of problems, including recommender systems, collaborative filtering, dimensionality reduction, image processing, quantum physics or multi-class classification to name a few.</s> <s>most works have focused on recovering an unknown real-valued low-rank matrix from randomly sub-sampling its entries.</s> <s>here, we investigate the case where the observations take a finite numbers of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification.</s> <s>we also consider a general sampling scheme (non-necessarily uniform) over the matrix entries.</s> <s>the performance of a nuclear-norm penalized estimator is analyzed theoretically.</s> <s>more precisely, we derive bounds for the kullback-leibler divergence between the true and estimated distributions.</s> <s>in practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tackle potentially high dimensional settings.</s></p></d>", "label": ["<d><p><s>probabilistic low-rank matrix completion on finite alphabets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences.</s> <s>in this paper, we explore a two-tiered notion of privacy where there is a small set of ``public'' users who are willing to share their preferences openly, and a large set of ``private'' users who require privacy guarantees.</s> <s>we show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy.</s> <s>moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability.</s> <s>we demonstrate gains from controlled access to private user preferences.</s></p></d>", "label": ["<d><p><s>controlling privacy in recommender systems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop collaborative topic poisson factorization (ctpf), a generative model of articles and reader preferences.</s> <s>ctpf can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest.</s> <s>in detail, ctpf models both reader behavior and article texts with poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers.</s> <s>this provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership.</s> <s>further, we exploit stochastic variational inference to model massive real-world datasets.</s> <s>for example, we can fit cptf to the full arxiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day.</s> <s>we demonstrate empirically that our model outperforms several baselines, including the previous state-of-the-art approach.</s></p></d>", "label": ["<d><p><s>content-based recommendations with poisson factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the problem of rank aggregation under the plackett-luce model.</s> <s>the goal is to infer a global ranking and related scores of the items, based on partial rankings provided by multiple users over multiple subsets of items.</s> <s>a question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error.</s> <s>without any assumptions on how the items are assigned to users, we derive an oracle lower bound and the cram\\'er-rao lower bound of the estimation error.</s> <s>we prove an upper bound on the estimation error achieved by the maximum likelihood estimator, and show that both the upper bound and the cram\\'er-rao lower bound inversely depend on the spectral gap of the laplacian of an appropriately defined comparison graph.</s> <s>since random comparison graphs are known to have large spectral gaps, this suggests the use of random assignments when we have the control.</s> <s>precisely, the matching oracle lower bound and the upper bound on the estimation error imply that the maximum likelihood estimator together with a random assignment is minimax-optimal up to a logarithmic factor.</s> <s>we further analyze a popular rank-breaking scheme that decompose partial rankings into pairwise comparisons.</s> <s>we show that even if one applies the mismatched maximum likelihood estimator that assumes independence (on pairwise comparisons that are now dependent due to rank-breaking), minimax optimal performance is still achieved up to a logarithmic factor.</s></p></d>", "label": ["<d><p><s>minimax-optimal inference from partial rankings</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the accuracy of information retrieval systems is often measured using average precision (ap).</s> <s>given a set of positive (relevant) and negative (non-relevant) samples, the parameters of a retrieval system can be estimated using the ap-svm framework, which minimizes a regularized convex upper bound on the empirical ap loss.</s> <s>however, the high computational complexity of loss-augmented inference, which is required for learning an ap-svm, prohibits its use with large training datasets.</s> <s>to alleviate this deficiency, we propose three complementary approaches.</s> <s>the first approach guarantees an asymptotic decrease in the computational complexity of loss-augmented inference by exploiting the problem structure.</s> <s>the second approach takes advantage of the fact that we do not require a full ranking during loss-augmented inference.</s> <s>this helps us to avoid the expensive step of sorting the negative samples according to their individual scores.</s> <s>the third approach approximates the ap loss over all samples by the ap loss over difficult samples (for example, those that are incorrectly classified by a binary svm), while ensuring the correct classification of the remaining samples.</s> <s>using the pascal voc action classification and object detection datasets, we show that our approaches provide significant speed-ups during training without degrading the test accuracy of ap-svm.</s></p></d>", "label": ["<d><p><s>efficient optimization for average precision svm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose robirank, a ranking algorithm that is motivated by observing a close connection between evaluation metrics for learning to rank and loss functions for robust classification.</s> <s>the algorithm shows a very competitive performance on standard benchmark datasets against other representative algorithms in the literature.</s> <s>further, in large scale problems where explicit feature vectors and scores are not given, our algorithm can be efficiently parallelized across a large number of machines; for a task that requires 386,133 x 49,824,519 pairwise interactions between items to be ranked, our algorithm finds solutions that are of dramatically higher quality than that can be found by a state-of-the-art competitor algorithm, given the same amount of wall-clock time for computation.</s></p></d>", "label": ["<d><p><s>ranking via robust binary classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we derive theoretical bounds for the long-term influence of a node in an independent cascade model (icm).</s> <s>we relate these bounds to the spectral radius of a particular matrix and show that the behavior is sub-critical when this spectral radius is lower than 1.</s> <s>more specifically, we point out that, in general networks, the sub-critical regime behaves in o(sqrt(n)) where n is the size of the network, and that this upper bound is met for star-shaped networks.</s> <s>we apply our results to epidemiology and percolation on arbitrary networks, and derive a bound for the critical value beyond which a giant connected component arises.</s> <s>finally, we show empirically the tightness of our bounds for a large family of networks.</s></p></d>", "label": ["<d><p><s>tight bounds for influence in diffusion networks and application to bond percolation and epidemiology</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>events in an online social network can be categorized roughly into endogenous events, where users just respond to the actions of their neighbors within the network, or exogenous events, where users take actions due to drives external to the network.</s> <s>how much external drive should be provided to each user, such that the network activity can be steered towards a target state?</s> <s>in this paper, we model social events using multivariate hawkes processes, which can capture both endogenous and exogenous event intensities, and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity.</s> <s>exploiting this connection, we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a desired activity level.</s> <s>we experimented with event data gathered from twitter, and show that our method can steer the activity of the network more accurately than alternatives.</s></p></d>", "label": ["<d><p><s>shaping social activity by incentivizing users</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>coverage functions are an important class of discrete functions that capture laws of diminishing returns.</s> <s>in this paper, we propose a new problem of learning time-varying coverage functions which arise naturally from applications in social network analysis, machine learning, and algorithmic game theory.</s> <s>we develop a novel parametrization of the time-varying coverage function by illustrating the connections with counting processes.</s> <s>we present an efficient algorithm to learn the parameters by maximum likelihood estimation, and provide a rigorous theoretic analysis of its sample complexity.</s> <s>empirical experiments from information diffusion in social network analysis demonstrate that with few assumptions about the underlying diffusion process, our method performs significantly better than existing approaches on both synthetic and real world data.</s></p></d>", "label": ["<d><p><s>learning time-varying coverage functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modern applications in sensitive domains such as biometrics and medicine frequently require the use of non-decomposable loss functions such as precision@k, f-measure etc.</s> <s>compared to point loss functions such as hinge-loss, these offer much more fine grained control over prediction, but at the same time present novel challenges in terms of algorithm design and analysis.</s> <s>in this work we initiate a study of online learning techniques for such non-decomposable loss functions with an aim to enable incremental learning as well as design scalable solvers for batch problems.</s> <s>to this end, we propose an online learning framework for such loss functions.</s> <s>our model enjoys several nice properties, chief amongst them being the existence of efficient online learning algorithms with sublinear regret and online to batch conversion bounds.</s> <s>our model is a provable extension of existing online learning models for point loss functions.</s> <s>we instantiate two popular losses, prec @k and pauc, in our model and prove sublinear regret bounds for both of them.</s> <s>our proofs require a novel structural lemma over ranked lists which may be of independent interest.</s> <s>we then develop scalable stochastic gradient descent solvers for non-decomposable loss functions.</s> <s>we show that for a large family of loss functions satisfying a certain uniform convergence property (that includes prec @k, pauc, and f-measure), our methods provably converge to the empirical risk minimizer.</s> <s>such uniform convergence results were not known for these losses and we establish these using novel proof techniques.</s> <s>we then use extensive experimentation on real life and benchmark datasets to establish that our method can be orders of magnitude faster than a recently proposed cutting plane method.</s></p></d>", "label": ["<d><p><s>online and stochastic gradient methods for non-decomposable loss functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of online planning in a markov decision process with discounted rewards for any given initial state.</s> <s>we consider the pac sample complexity problem of computing, with probability $1-\\delta$, an $\\epsilon$-optimal action using the smallest possible number of calls to the generative model (which provides reward and next-state samples).</s> <s>we design an algorithm, called stop (for stochastic-optimistic planning), based on the optimism in the face of uncertainty\" principle.</s> <s>stop can be used in the general setting, requires only a generative model, and enjoys a complexity bound that only depends on the local structure of the mdp.\"</s></p></d>", "label": ["<d><p><s>optimistic planning in markov decision processes using a generative model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player?s action history.</s> <s>we prove a series of new results for conditional swap regret minimization.</s> <s>we present algorithms for minimizing conditional swap regret with bounded conditioning history.</s> <s>we further extend these results to the case where conditional swaps are considered only for a subset of actions.</s> <s>we also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium.</s> <s>finally, we extend our results to the multi-armed bandit scenario.</s></p></d>", "label": ["<d><p><s>conditional swap regret and conditional correlated equilibrium</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes.</s> <s>in every round, the learner suffers some loss and receives some feedback based on the action and the outcome.</s> <s>the goal of the learner is to minimize her cumulative loss.</s> <s>applications range from dynamic pricing to label-efficient prediction to dueling bandits.</s> <s>in this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes.</s> <s>we propose bpm, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution.</s> <s>we show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents.</s> <s>as demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time.</s></p></d>", "label": ["<d><p><s>efficient partial monitoring with prior information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a model by choosing the maximum entropy distribution from the set of models satisfying certain smoothness and independence criteria; we show that inference on this model generalizes local kernel estimation to the context of bayesian inference on stochastic processes.</s> <s>our model enables bayesian inference in contexts when standard techniques like gaussian process inference are too expensive to apply.</s> <s>exact inference on our model is possible for any likelihood function from the exponential family.</s> <s>inference is then highly efficient, requiring only o(log n) time and o(n) space at run time.</s> <s>we demonstrate our algorithm on several problems and show quantifiable improvement in both speed and performance relative to models based on the gaussian process.</s></p></d>", "label": ["<d><p><s>nonparametric bayesian inference on multivariate exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the ability to automatically discover patterns and perform extrapolation is an essential quality of intelligent systems.</s> <s>kernel methods, such as gaussian processes, have great potential for pattern extrapolation, since the kernel flexibly and interpretably controls the generalisation properties of these methods.</s> <s>however, automatically extrapolating large scale multidimensional patterns is in general difficult, and developing gaussian process models for this purpose involves several challenges.</s> <s>a vast majority of kernels, and kernel learning methods, currently only succeed in smoothing and interpolation.</s> <s>this difficulty is compounded by the fact that gaussian processes are typically only tractable for small datasets, and scaling an expressive kernel learning approach poses different challenges than scaling a standard gaussian process model.</s> <s>one faces additional computational constraints, and the need to retain significant model structure for expressing the rich information available in a large dataset.</s> <s>in this paper, we propose a gaussian process approach for large scale multidimensional pattern extrapolation.</s> <s>we recover sophisticated out of class kernels, perform texture extrapolation, inpainting, and video extrapolation, and long range forecasting of land surface temperatures, all on large multidimensional datasets, including a problem with 383,400 training points.</s> <s>the proposed method significantly outperforms alternative scalable and flexible gaussian process methods, in speed and accuracy.</s> <s>moreover, we show that a distinct combination of expressive kernels, a fully non-parametric representation, and scalable inference which exploits existing model structure, are critical for large scale multidimensional pattern extrapolation.</s></p></d>", "label": ["<d><p><s>fast kernel learning for multidimensional pattern extrapolation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time.</s> <s>here, we show that privileged information can naturally be treated as noise in the latent function of a gaussian process classifier (gpc).</s> <s>that is, in contrast to the standard gpc setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the gpc probit likelihood function.</s> <s>extensive experiments on public datasets show that the proposed gpc method using privileged noise, called gpc+, improves over a standard gpc without privileged knowledge, and also over the current state-of-the-art svm-based method, svm+.</s> <s>moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.</s></p></d>", "label": ["<d><p><s>mind the nuisance: gaussian process classification using privileged noise</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an automated variational method for approximate inference in gaussian process (gp) models whose posteriors are often intractable.</s> <s>using a mixture of gaussians as the variational distribution, we show that (i) the variational objective and its gradients can be approximated efficiently via sampling from univariate gaussian distributions and (ii) the gradients of the gp hyperparameters can be obtained analytically regardless of the model likelihood.</s> <s>we further propose two instances of the variational distribution whose covariance matrices can be parametrized linearly in the number of observations.</s> <s>these results allow gradient-based optimization to be done efficiently in a black-box manner.</s> <s>our approach is thoroughly verified on 5 models using 6 benchmark datasets, performing as well as the exact or hard-coded implementations while running orders of magnitude faster than the alternative mcmc sampling approaches.</s> <s>our method can be a valuable tool for practitioners and researchers to investigate new models with minimal effort in deriving model-specific inference algorithms.</s></p></d>", "label": ["<d><p><s>automated variational inference for gaussian process models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>state-space models have been successfully used for more than fifty years in different areas of science and engineering.</s> <s>we present a procedure for efficient variational bayesian learning of nonlinear state-space models based on sparse gaussian processes.</s> <s>the result of learning is a tractable posterior over nonlinear dynamical systems.</s> <s>in comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting.</s> <s>our main algorithm uses a hybrid inference approach combining variational bayes and sequential monte carlo.</s> <s>we also present stochastic variational inference and online learning approaches for fast learning with long time series.</s></p></d>", "label": ["<d><p><s>variational gaussian process state-space models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the prediction of time-changing variances is an important task in the modeling of financial data.</s> <s>standard econometric models are often limited as they assume rigid functional relationships for the evolution of the variance.</s> <s>moreover, functional parameters are usually learned by maximum likelihood, which can lead to overfitting.</s> <s>to address these problems we introduce gp-vol, a novel non-parametric model for time-changing variances based on gaussian processes.</s> <s>this new model can capture highly flexible functional relationships for the variances.</s> <s>furthermore, we introduce a new online algorithm for fast inference in gp-vol.</s> <s>this method is much faster than current offline inference procedures and it avoids overfitting problems by following a fully bayesian approach.</s> <s>experiments with financial data show that gp-vol performs significantly better than current standard alternatives.</s></p></d>", "label": ["<d><p><s>gaussian process volatility model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bandit convex optimization (bco) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning.</s> <s>while the special case of linear cost functions is well understood, a gap on the attainable regret for bco with nonlinear losses remains an important open question.</s> <s>in this paper we take a step towards understanding the best attainable regret bounds for bco: we give an efficient and near-optimal regret algorithm for bco with strongly-convex and smooth loss functions.</s> <s>in contrast to previous works on bco that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.</s></p></d>", "label": ["<d><p><s>bandit convex optimization: towards tight bounds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a multi-armed bandit (mab) problem a gambler needs to choose at each round of play one of k arms, each characterized by an unknown reward distribution.</s> <s>reward realizations are only observed when an arm is selected, and the gambler's objective is to maximize his cumulative expected earnings over some given horizon of play t. to do this, the gambler needs to acquire information about arms (exploration) while simultaneously optimizing immediate rewards (exploitation); the price paid due to this trade off is often referred to as the regret, and the main question is how small can this price be as a function of the horizon length t. this problem has been studied extensively when the reward distributions do not change over time; an assumption that supports a sharp characterization of the regret, yet is often violated in practical settings.</s> <s>in this paper, we focus on a mab formulation which allows for a broad range of temporal uncertainties in the rewards, while still maintaining mathematical tractability.</s> <s>we fully characterize the (regret) complexity of this class of mab problems by establishing a direct link between the extent of allowable reward variation\" and the minimal achievable regret, and by establishing a connection between the adversarial and the stochastic mab frameworks.\"</s></p></d>", "label": ["<d><p><s>stochastic multi-armed-bandit problem with non-stationary rewards</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many areas of medicine, security, and life sciences, we want to allocate limited resources to different sources in order to detect extreme values.</s> <s>in this paper, we study an efficient way to allocate these resources sequentially under limited feedback.</s> <s>while sequential design of experiments is well studied in bandit theory, the most commonly optimized property is the regret with respect to the maximum mean reward.</s> <s>however, in other problems such as network intrusion detection, we are interested in detecting the most extreme value output by the sources.</s> <s>therefore, in our work we study extreme regret which measures the efficiency of an algorithm compared to the oracle policy selecting the source with the heaviest tail.</s> <s>we propose the extremehunter algorithm, provide its analysis, and evaluate it empirically on synthetic and real-world experiments.</s></p></d>", "label": ["<d><p><s>extreme bandits</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper we consider the problem of learning online what is the information to consider when making sequential decisions.</s> <s>we formalize this as a contextual multi-armed bandit problem where a high dimensional ($d$-dimensional) context vector arrives to a learner which needs to select an action to maximize its expected reward at each time step.</s> <s>each dimension of the context vector is called a type.</s> <s>we assume that there exists an unknown relation between actions and types, called the relevance relation, such that the reward of an action only depends on the contexts of the relevant types.</s> <s>when the relation is a function, i.e., the reward of an action only depends on the context of a single type, and the expected reward of an action is lipschitz continuous in the context of its relevant type, we propose an algorithm that achieves $\\tilde{o}(t^{\\gamma})$ regret with a high probability, where $\\gamma=2/(1+\\sqrt{2})$.</s> <s>our algorithm achieves this by learning the unknown relevance relation, whereas prior contextual bandit algorithms that do not exploit the existence of a relevance relation will have $\\tilde{o}(t^{(d+1)/(d+2)})$ regret.</s> <s>our algorithm alternates between exploring and exploiting, it does not require reward observations in exploitations, and it guarantees with a high probability that actions with suboptimality greater than $\\epsilon$ are never selected in exploitations.</s> <s>our proposed method can be applied to a variety of learning applications including medical diagnosis, recommender systems, popularity prediction from social networks, network security etc., where at each instance of time vast amounts of different types of information are available to the decision maker, but the effect of an action depends only on a single type.</s></p></d>", "label": ["<d><p><s>discovering, learning and exploiting relevance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most work on sequential learning assumes a fixed set of actions that are available all the time.</s> <s>however, in practice, actions can consist of picking subsets of readings from sensors that may break from time to time, road segments that can be blocked or goods that are out of stock.</s> <s>in this paper we study learning algorithms that are able to deal with stochastic availability of such unreliable composite actions.</s> <s>we propose and analyze algorithms based on the follow-the-perturbed-leader prediction method for several learning settings differing in the feedback provided to the learner.</s> <s>our algorithms rely on a novel loss estimation technique that we call counting asleep times.</s> <s>we deliver regret bounds for our algorithms for the previously studied full information and (semi-)bandit settings, as well as a natural middle point between the two that we call the restricted information setting.</s> <s>a special consequence of our results is a significant improvement of the best known performance guarantees achieved by an efficient algorithm for the sleeping bandit problem with stochastic availability.</s> <s>finally, we evaluate our algorithms empirically and show their improvement over the known approaches.</s></p></d>", "label": ["<d><p><s>online combinatorial optimization with stochastic decision sets and adversarial losses</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show that the usual score function for conditional markov networks can be written as the expectation over the scores of their spanning trees.</s> <s>we also show that a small random sample of these output trees can attain a significant fraction of the margin obtained by the complete graph and we provide conditions under which we can perform tractable inference.</s> <s>the experimental results confirm that practical learning is scalable to realistic datasets using this approach.</s></p></d>", "label": ["<d><p><s>multilabel structured output learning with random spanning trees of max-margin markov networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose to learn a mahalanobis distance to perform alignment of multivariate time series.</s> <s>the learning examples for this task are time series for which the true alignment is known.</s> <s>we cast the alignment problem as a structured prediction task, and propose realistic losses between alignments for which the optimization is tractable.</s> <s>we provide experiments on real data in the audio-to-audio context, where we show that the learning of a similarity measure leads to improvements in the performance of the alignment task.</s> <s>we also propose to use this metric learning framework to perform feature selection and, from basic audio features, build a combination of these with better alignment performance.</s></p></d>", "label": ["<d><p><s>metric learning for temporal sequence alignment</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the class of optimization problems arising from computationally intensive l1-regularized m-estimators, where the function or gradient values are very expensive to compute.</s> <s>a particular instance of interest is the l1-regularized mle for learning conditional random fields (crfs), which are a popular class of statistical models for varied structured prediction problems such as sequence labeling, alignment, and classification with label taxonomy.</s> <s>l1-regularized mles for crfs are particularly expensive to optimize since computing the gradient values requires an expensive inference step.</s> <s>in this work, we propose the use of a carefully constructed proximal quasi-newton algorithm for such computationally intensive m-estimation problems, where we employ an aggressive active set selection technique.</s> <s>in a key contribution of the paper, we show that our proximal quasi-newton algorithm is provably super-linearly convergent, even in the absence of strong convexity, by leveraging a restricted variant of strong convexity.</s> <s>in our experiments, the proposed algorithm converges considerably faster than current state-of-the-art on the problems of sequence labeling and hierarchical classification.</s></p></d>", "label": ["<d><p><s>proximal quasi-newton for computationally intensive l1-regularized m-estimators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error.</s> <s>we describe an efficient algorithm for exact loss augmented inference,and a fast gradient descent algorithm for learning in this model.</s> <s>the objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points.</s> <s>our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed.</s> <s>in experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.</s></p></d>", "label": ["<d><p><s>discriminative metric learning by neighborhood gerrymandering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many machine learning approaches are characterized by information constraints on how they interact with the training data.</s> <s>these include memory and sequential access constraints (e.g.</s> <s>fast first-order methods to solve stochastic optimization problems); communication constraints (e.g.</s> <s>distributed learning); partial access to the underlying data (e.g.</s> <s>missing features and multi-armed bandits) and more.</s> <s>however, currently we have little understanding how such information constraints fundamentally affect our performance, independent of the learning problem semantics.</s> <s>for example, are there learning problems where any algorithm which has small memory footprint (or can use any bounded number of bits from each example, or has certain communication constraints) will perform worse than what is possible without such constraints?</s> <s>in this paper, we describe how a single set of results implies positive answers to the above, for several different settings.</s></p></d>", "label": ["<d><p><s>fundamental limits of online and distributed algorithms for statistical learning and estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present two related contributions of independent interest: (1) high-probability finite sample rates for $k$-nn density estimation, and (2) practical mode estimators -- based on $k$-nn -- which attain minimax-optimal rates under surprisingly general distributional conditions.</s></p></d>", "label": ["<d><p><s>optimal rates for k-nn density and mode estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>existing research \\cite{reg} suggests that embedding graphs on a unit sphere can be beneficial in learning labels on the vertices of a graph.</s> <s>however the choice of optimal embedding remains an open issue.</s> <s>\\emph{orthonormal representation} of graphs, a class of embeddings over the unit sphere, was introduced by lov\\'asz \\cite{lovasz_shannon}.</s> <s>in this paper, we show that there exists orthonormal representations which are statistically consistent over a large class of graphs, including power law and random graphs.</s> <s>this result is achieved by extending the notion of consistency designed in the inductive setting to graph transduction.</s> <s>as part of the analysis, we explicitly derive relationships between the rademacher complexity measure and structural properties of graphs, such as the chromatic number.</s> <s>we further show the fraction of vertices of a graph $g$, on $n$ nodes, that need to be labelled for the learning algorithm to be consistent, also known as labelled sample complexity, is $ \\omega\\left(\\frac{\\vartheta(g)}{n}\\right)^{\\frac{1}{4}}$ where $\\vartheta(g)$ is the famous lov\\'asz~$\\vartheta$ function of the graph.</s> <s>this, for the first time, relates labelled sample complexity to graph connectivity properties, such as the density of graphs.</s> <s>in the multiview setting, whenever individual views are expressed by a graph, it is a well known heuristic that a convex combination of laplacians \\cite{lap_mv1} tend to improve accuracy.</s> <s>the analysis presented here easily extends to multiple graph transduction, and helps develop a sound statistical understanding of the heuristic, previously unavailable.</s></p></d>", "label": ["<d><p><s>learning on graphs using orthonormal representation is statistically consistent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the brain uses population codes to form distributed, noise-tolerant representations of sensory and motor variables.</s> <s>recent work has examined the theoretical optimality of such codes in order to gain insight into the principles governing population codes found in the brain.</s> <s>however, the majority of the population coding literature considers either conditionally independent neurons or neurons with noise governed by a stimulus-independent covariance matrix.</s> <s>here we analyze population coding under a simple alternative model in which latent input noise\" corrupts the stimulus before it is encoded by the population.</s> <s>this provides a convenient and tractable description for irreducible uncertainty that cannot be overcome by adding neurons, and induces stimulus-dependent correlations that mimic certain aspects of the correlations observed in real populations.</s> <s>we examine prior-dependent, bayesian optimal coding in such populations using exact analyses of cases in which the posterior is approximately gaussian.</s> <s>these analyses extend previous results on independent poisson population codes and yield an analytic expression for squared loss and a tight upper bound for mutual information.</s> <s>we show that, for homogeneous populations that tile the input domain, optimal tuning curve width depends on the prior, the loss function, the resource constraint, and the amount of input noise.</s> <s>this framework provides a practical testbed for examining issues of optimality, noise, correlation, and coding fidelity in realistic neural populations.\"</s></p></d>", "label": ["<d><p><s>optimal prior-dependent neural population codes under shared input noise</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations.</s> <s>many behaviors leading to decision making and action selection in a closed loop setting are naturally phrased within a control theoretic framework.</s> <s>within the framework of optimal control theory, one is usually given a cost function which is minimized by selecting a control law based on the observations.</s> <s>while in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimizing the sensors themselves.</s> <s>however, this sensory adaptation is geared towards control rather than perception, as is often assumed.</s> <s>in this work we show that sensory adaptation for control differs from sensory adaptation for perception, even for simple control setups.</s> <s>this implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed.</s></p></d>", "label": ["<d><p><s>optimal neural codes for control and estimation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a basic problem in the design of privacy-preserving algorithms is the \\emph{private maximization problem}: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy.</s> <s>this problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine learning.</s> <s>previous algorithms for this problem are either range-dependent---i.e., their utility diminishes with the size of the universe---or only apply to very restricted function classes.</s> <s>this work provides the first general purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy.</s> <s>its applicability is demonstrated on two fundamental tasks in data mining and machine learning.</s></p></d>", "label": ["<d><p><s>the large margin mechanism for differentially private maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>local differential privacy has recently surfaced as a strong measure of privacy in contexts where personal information remains private even from data analysts.</s> <s>working in a setting where the data providers and data analysts want to maximize the utility of statistical inferences performed on the released data, we study the fundamental tradeoff between local differential privacy and information theoretic utility functions.</s> <s>we introduce a family of extremal privatization mechanisms, which we call staircase mechanisms, and prove that it contains the optimal privatization mechanism that maximizes utility.</s> <s>we further show that for all information theoretic utility functions studied in this paper, maximizing utility is equivalent to solving a linear program, the outcome of which is the optimal staircase mechanism.</s> <s>however, solving this linear program can be computationally expensive since it has a number of variables that is exponential in the data size.</s> <s>to account for this, we show that two simple staircase mechanisms, the binary and randomized response mechanisms, are universally optimal in the high and low privacy regimes, respectively, and well approximate the intermediate regime.</s></p></d>", "label": ["<d><p><s>extremal mechanisms for local differential privacy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks.</s> <s>unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\em adversarial} workers with no specific assumptions on their labeling strategy.</s> <s>our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems.</s> <s>our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker.</s> <s>we provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm.</s> <s>finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.</s></p></d>", "label": ["<d><p><s>reputation-based worker filtering in crowdsourcing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a predictor that is deployed in a live production system may perturb the features it uses to make predictions.</s> <s>such a feedback loop can occur, for example, when a model that predicts a certain type of behavior ends up causing the behavior it predicts, thus creating a self-fulfilling prophecy.</s> <s>in this paper we analyze predictor feedback detection as a causal inference problem, and introduce a local randomization scheme that can be used to detect non-linear feedback in real-world problems.</s> <s>we conduct a pilot study for our proposed methodology using a predictive system currently deployed as a part of a search engine.</s></p></d>", "label": ["<d><p><s>feedback detection for live predictors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a technique for significantly speeding up alternating least squares (als) and gradient descent (gd), two widely used algorithms for tensor factorization.</s> <s>by exploiting properties of the khatri-rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms.</s> <s>our algorithm, dfacto, only requires two matrix-vector products and is easy to parallelize.</s> <s>dfacto is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets.</s> <s>for instance, dfacto only takes 480 seconds on 4 machines to perform one iteration of the als algorithm and 1,143 seconds to perform one iteration of the gd algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.</s></p></d>", "label": ["<d><p><s>dfacto: distributed factorization of tensors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>with the emergence of big graphs in a variety of real applications like social networks, machine learning based on distributed graph-computing~(dgc) frameworks has attracted much attention from big data machine learning community.</s> <s>in dgc frameworks, the graph partitioning~(gp) strategy plays a key role to affect the performance, including the workload balance and communication cost.</s> <s>typically, the degree distributions of natural graphs from real applications follow skewed power laws, which makes gp a challenging task.</s> <s>recently, many methods have been proposed to solve the gp problem.</s> <s>however, the existing gp methods cannot achieve satisfactory performance for applications with power-law graphs.</s> <s>in this paper, we propose a novel vertex-cut method, called \\emph{degree-based hashing}~(dbh), for gp.</s> <s>dbh makes effective use of the skewed degree distributions for gp.</s> <s>we theoretically prove that dbh can achieve lower communication cost than existing methods and can simultaneously guarantee good workload balance.</s> <s>furthermore, empirical results on several large power-law graphs also show that dbh can outperform the state of the art.</s></p></d>", "label": ["<d><p><s>distributed power-law graph computing: theoretical and empirical analysis</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>can we effectively learn a nonlinear representation in time comparable to linear learning?</s> <s>we describe a new algorithm that explicitly and adaptively expands higher-order interaction features over base linear representations.</s> <s>the algorithm is designed for extreme computational efficiency, and an extensive experimental study shows that its computation/prediction tradeoff ability compares very favorably against strong baselines.</s></p></d>", "label": ["<d><p><s>scalable non-linear learning with adaptive polynomial expansions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general framework for regularization based on group majorization.</s> <s>in this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope).</s> <s>common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group.</s> <s>we derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms.</s> <s>finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.</s></p></d>", "label": ["<d><p><s>orbit regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms.</s> <s>in high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of ledoit/wolf are applied.</s> <s>in the standard setting, i.i.d.</s> <s>data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias.</s> <s>recent work by sancetta has extended the shrinkage framework beyond i.i.d.</s> <s>data.</s> <s>we contribute in this work by showing that the sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes.</s> <s>we propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an eeg-based brain-computer-interfacing experiment.</s></p></d>", "label": ["<d><p><s>covariance shrinkage for autocorrelated data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics.</s> <s>we first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements.</s> <s>detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem.</s> <s>instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data.</s> <s>further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery.</s> <s>our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.</s></p></d>", "label": ["<d><p><s>a dual algorithm for olfactory computation in the locust brain</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>max-norm regularizer has been extensively studied in the last decade as it promotes an effective low rank estimation of the underlying data.</s> <s>however, max-norm regularized problems are typically formulated and solved in a batch manner, which prevents it from processing big data due to possible memory bottleneck.</s> <s>in this paper, we propose an online algorithm for solving max-norm regularized problems that is scalable to large problems.</s> <s>particularly, we consider the matrix decomposition problem as an example, although our analysis can also be applied in other problems such as matrix completion.</s> <s>the key technique in our algorithm is to reformulate the max-norm into a matrix factorization form, consisting of a basis component and a coefficients one.</s> <s>in this way, we can solve the optimal basis and coefficients alternatively.</s> <s>we prove that the basis produced by our algorithm converges to a stationary point asymptotically.</s> <s>experiments demonstrate encouraging results for the effectiveness and robustness of our algorithm.</s> <s>see the full paper at arxiv:1406.3190.</s></p></d>", "label": ["<d><p><s>online optimization for max-norm regularization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of recovering the sparsest vector in a subspace $ \\mathcal{s} \\in \\mathbb{r}^p $ with $ \\text{dim}(\\mathcal{s})=n$.</s> <s>this problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse pca, and other problems in signal processing and machine learning.</s> <s>simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $1/ \\sqrt{n}$.</s> <s>in contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $\\omega(1)$.</s> <s>to our knowledge, this is the first practical algorithm to achieve this linear scaling.</s> <s>this result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace.</s> <s>empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.</s></p></d>", "label": ["<d><p><s>finding a sparse vector in a subspace: linear sparsity using alternating directions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with compressive sensing of signals drawn from a gaussian mixture model (gmm) with sparse precision matrices.</s> <s>previous work has shown: (i) a signal drawn from a given gmm can be perfectly reconstructed from r noise-free measurements if the (dominant) rank of each covariance matrix is less than r; (ii) a sparse gaussian graphical model can be efficiently estimated from fully-observed training signals using graphical lasso.</s> <s>this paper addresses a problem more challenging than both (i) and (ii), by assuming that the gmm is unknown and each signal is only partially observed through incomplete linear measurements.</s> <s>under these challenging assumptions, we develop a hierarchical bayesian method to simultaneously estimate the gmm and recover the signals using solely the incomplete measurements and a bayesian shrinkage prior that promotes sparsity of the gaussian precision matrices.</s> <s>in addition, we provide theoretical performance bounds to relate the reconstruction error to the number of signals for which measurements are available, the sparsity level of precision matrices, and the ?incompleteness?</s> <s>of measurements.</s> <s>the proposed method is demonstrated extensively on compressive sensing of imagery and video, and the results with simulated and hardware-acquired real measurements show significant performance improvement over state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>compressive sensing of signals from a gmm with sparse precision matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the goals of neuroscience is to identify neural networks that correlate with important behaviors, environments, or genotypes.</s> <s>this work proposes a strategy for identifying neural networks characterized by time- and frequency-dependent connectivity patterns, using convolutional dictionary learning that links spike-train data to local field potentials (lfps) across multiple areas of the brain.</s> <s>analytical contributions are: (i) modeling dynamic relationships between lfps and spikes; (ii) describing the relationships between spikes and lfps, by analyzing the ability to predict lfp data from one region based on spiking information from across the brain; and (iii) development of a clustering methodology that allows inference of similarities in neurons from multiple regions.</s> <s>results are based on data sets in which spike and lfp data are recorded simultaneously from up to 16 brain regions in a mouse.</s></p></d>", "label": ["<d><p><s>on the relations of lfps & neural spike trains</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>persistent activity refers to the phenomenon that cortical neurons keep firing even after the stimulus triggering the initial neuronal responses is moved.</s> <s>persistent activity is widely believed to be the substrate for a neural system retaining a memory trace of the stimulus information.</s> <s>in a conventional view, persistent activity is regarded as an attractor of the network dynamics, but it faces a challenge of how to be closed properly.</s> <s>here, in contrast to the view of attractor, we consider that the stimulus information is encoded in a marginally unstable state of the network which decays very slowly and exhibits persistent firing for a prolonged duration.</s> <s>we propose a simple yet effective mechanism to achieve this goal, which utilizes the property of short-term plasticity (stp) of neuronal synapses.</s> <s>stp has two forms, short-term depression (std) and short-term facilitation (stf), which have opposite effects on retaining neuronal responses.</s> <s>we find that by properly combining stf and std, a neural system can hold persistent activity of graded lifetime, and that persistent activity fades away naturally without relying on an external drive.</s> <s>the implications of these results on neural information representation are discussed.</s></p></d>", "label": ["<d><p><s>a synaptical story of persistent activity with graded lifetime in a neural system</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in sparse principal component analysis we are given noisy observations of a low-rank matrix of dimension $n\\times p$ and seek to reconstruct it under additional sparsity assumptions.</s> <s>in particular, we assume here that the principal components $\\bv_1,\\dots,\\bv_r$ have at most $k_1, \\cdots, k_q$ non-zero entries respectively, and study the high-dimensional regime in which $p$ is of the same order as $n$.</s> <s>in an influential paper, johnstone and lu \\cite{johnstone2004sparse} introduced a simple algorithm that estimates the support of the principal vectors $\\bv_1,\\dots,\\bv_r$ by the largest entries in the diagonal of the empirical covariance.</s> <s>this method can be shown to succeed with high probability if $k_q \\le c_1\\sqrt{n/\\log p}$, and to fail with high probability if $k_q\\ge c_2 \\sqrt{n/\\log p}$ for two constants $0 < c_1,c_2 < \\infty$.</s> <s>despite a considerable amount of work over the last ten years, no practical algorithm exists with provably better support recovery guarantees.</s> <s>here we analyze a covariance thresholding algorithm that was recently proposed by krauthgamer, nadler and vilenchik \\cite{krauthgamerspca}.</s> <s>we confirm empirical evidence presented by these authors and rigorously prove that the algorithm succeeds with high probability for $k$ of order $\\sqrt{n}$.</s> <s>recent conditional lower bounds \\cite{berthet2013computational} suggest that it might be impossible to do significantly better.</s> <s>the key technical component of our analysis develops new bounds on the norm of kernel random matrices, in regimes that were not considered before.</s></p></d>", "label": ["<d><p><s>sparse pca via covariance thresholding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study low-rank approximation in the streaming model in which the rows of an $n \\times d$ matrix $a$ are presented one at a time in an arbitrary order.</s> <s>at the end of the stream, the streaming algorithm should output a $k \\times d$ matrix $r$ so that $\\|a-ar^{\\dagger}r\\|_f^2 \\leq (1+\\eps)\\|a-a_k\\|_f^2$, where $a_k$ is the best rank-$k$ approximation to $a$.</s> <s>a deterministic streaming algorithm of liberty (kdd, 2013), with an improved analysis of ghashami and phillips (soda, 2014), provides such a streaming algorithm using $o(dk/\\epsilon)$ words of space.</s> <s>a natural question is if smaller space is possible.</s> <s>we give an almost matching lower bound of $\\omega(dk/\\epsilon)$ bits of space, even for randomized algorithms which succeed only with constant probability.</s> <s>our lower bound matches the upper bound of ghashami and phillips up to the word size, improving on a simple $\\omega(dk)$ space lower bound.</s></p></d>", "label": ["<d><p><s>low rank approximation lower bounds in row-update streams</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known.</s> <s>the formulation counts sparse pca with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications.</s> <s>we compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm, trace norm and their combinations.</s> <s>even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.</s></p></d>", "label": ["<d><p><s>tight convex relaxations for sparse matrix factorization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we study the statistical performance of robust tensor decomposition with gross corruption.</s> <s>the observations are noisy realization of the superposition of a low-rank tensor $\\mathcal{w}^*$ and an entrywise sparse corruption tensor $\\mathcal{v}^*$.</s> <s>unlike conventional noise with bounded variance in previous convex tensor decomposition analysis, the magnitude of the gross corruption can be arbitrary large.</s> <s>we show that under certain conditions, the true low-rank tensor as well as the sparse corruption tensor can be recovered simultaneously.</s> <s>our theory yields nonasymptotic frobenius-norm estimation error bounds for each tensor separately.</s> <s>we show through numerical experiments that our theory can precisely predict the scaling behavior in practice.</s></p></d>", "label": ["<d><p><s>robust tensor decomposition with gross corruption</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>patch-based methods have been widely used for noise reduction in recent years.</s> <s>in this paper, we propose a general statistical aggregation method which combines image patches denoised with several commonly-used algorithms.</s> <s>we show that weakly denoised versions of the input image obtained with standard methods, can serve to compute an efficient patch-based aggregated estimd aggregation (ewa) estimator.</s> <s>the resulting approach (pewa) is based on a mcmc sampling and has a nice statistical foundation while producing denoising results that are comparable to the current state-of-the-art.</s> <s>we demonstrate the performance of the denoising algorithm on real images and we compare the results to several competitive methods.</s></p></d>", "label": ["<d><p><s>pewa: patch-based exponentially weighted aggregation for image denoising</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision.</s> <s>we combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework.</s> <s>our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them.</s> <s>the system is directly trained from question-answer pairs.</s> <s>we establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.</s></p></d>", "label": ["<d><p><s>a multi-world approach to question answering about real-world scenes based on uncertain input</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>matching local visual features is a crucial problem in computer vision and its accuracy greatly depends on the choice of similarity measure.</s> <s>as it is generally very difficult to design by hand a similarity or a kernel perfectly adapted to the data of interest, learning it automatically with as few assumptions as possible is preferable.</s> <s>however, available techniques for kernel learning suffer from several limitations, such as restrictive parametrization or scalability.</s> <s>in this paper, we introduce a simple and flexible family of non-linear kernels which we refer to as quantized kernels (qk).</s> <s>qks are arbitrary kernels in the index space of a data quantizer, i.e., piecewise constant similarities in the original feature space.</s> <s>quantization allows to compress features and keep the learning tractable.</s> <s>as a result, we obtain state-of-the-art matching performance on a standard benchmark dataset with just a few bits to represent each feature dimension.</s> <s>qks also have explicit non-linear, low-dimensional feature mappings that grant access to euclidean geometry for uncompressed features.</s></p></d>", "label": ["<d><p><s>quantized kernel learning for feature matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>video summarization is a challenging problem with great application potential.</s> <s>whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem.</s> <s>our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality.</s> <s>to this end, we propose the sequential determinantal point process (seqdpp), a probabilistic model for diverse sequential subset selection.</s> <s>our novel seqdpp heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard dpp, which treats video frames as randomly permutable items.</s> <s>meanwhile, seqdpp retains the power of modeling diverse subsets, essential for summarization.</s> <s>our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard dpp model.</s></p></d>", "label": ["<d><p><s>diverse sequential subset selection for supervised video summarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>extracting 3d shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (nrsfm), has so far been studied only on synthetic datasets and controlled environments.</s> <s>typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed.</s> <s>in order to integrate nrsfm into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings.</s> <s>furthermore, nrsfm needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation ``leaking'', optical flow ``bleeding'' etc.</s> <s>in this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and nrsfm for 3d reconstruction of objects in videos.</s> <s>for each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3d shape under different rank bounds of the trajectory matrix.</s> <s>we show that dense 3d shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera.</s> <s>we achieve competitive results on a public nrsfm benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches.</s> <s>we further test our approach on popular video segmentation datasets.</s> <s>to the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in youtube or hollywood movies, without object-specific priors.</s></p></d>", "label": ["<d><p><s>grouping-based low-rank trajectory completion and 3d reconstruction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of image collection summarization by learning mixtures of submodular functions.</s> <s>we argue that submodularity is very natural to this problem, and we show that a number of previously used scoring functions are submodular ?</s> <s>a property not explicitly mentioned in these publications.</s> <s>we provide classes of submodular functions capturing the necessary properties of summaries, namely coverage, likelihood, and diversity.</s> <s>to learn mixtures of these submodular functions as scoring functions, we formulate summarization as a supervised learning problem using large-margin structured prediction.</s> <s>furthermore, we introduce a novel evaluation metric, which we call v-rouge, for automatic summary scoring.</s> <s>while a similar metric called rouge has been successfully applied to document summarization [14], no such metric was known for quantifying the quality of image collection summaries.</s> <s>we provide a new dataset consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using mechanical turk.</s> <s>we also extensively compare our method with previously explored methods for this problem and show that our learning approach outperforms all competitors on this new dataset.</s> <s>this paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new dataset of image collections and human summaries.</s></p></d>", "label": ["<d><p><s>learning mixtures of submodular functions for image collection summarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences.</s> <s>in this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision.</s> <s>the deep identification-verification features (deepid2) are learned with carefully designed deep convolutional networks.</s> <s>the face identification task increases the inter-personal variations by drawing deepid2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling deepid2 features extracted from the same identity together, both of which are essential to face recognition.</s> <s>the learned deepid2 features can be well generalized to new identities unseen in the training data.</s> <s>on the challenging lfw dataset, 99.15% face verification accuracy is achieved.</s> <s>compared with the best previous deep learning result on lfw, the error rate has been significantly reduced by 67%.</s></p></d>", "label": ["<d><p><s>deep learning face representation by joint identification-verification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation.</s> <s>this applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples.</s> <s>such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects.</s> <s>likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation.</s> <s>by assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the fourier domain that can eliminate most redundancies.</s> <s>it can leverage off-the-shelf solvers with no modification (e.g.</s> <s>libsvm), and train several pose classifiers simultaneously at no extra cost.</s> <s>our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.</s></p></d>", "label": ["<d><p><s>fast training of pose detectors in the fourier domain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories.</s> <s>recently, deep convolutional neural networks (cnns) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2m+ labeled classification images.</s> <s>unfortunately, only a small fraction of those labels are available for the detection task.</s> <s>it is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes.</s> <s>in this paper, we propose large scale detection through adaptation (lsda), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors.</s> <s>our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data.</s> <s>evaluation on the imagenet lsvrc-2013 detection challenge demonstrates the efficacy of our approach.</s> <s>this algorithm enables us to produce a >7.6k detector by using available classification data from leaf nodes in the imagenet tree.</s> <s>we additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6k detector).</s> <s>models and software are available at</s></p></d>", "label": ["<d><p><s>lsda: large scale detection through adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>even with the advent of more sophisticated, data-hungry methods, boosted decision trees remain extraordinarily successful for fast rigid object detection, achieving top accuracy on numerous datasets.</s> <s>while effective, most boosted detectors use decision trees with orthogonal (single feature) splits, and the topology of the resulting decision boundary may not be well matched to the natural topology of the data.</s> <s>given highly correlated data, decision trees with oblique (multiple feature) splits can be effective.</s> <s>use of oblique splits, however, comes at considerable computational expense.</s> <s>inspired by recent work on discriminative decorrelation of hog features, we instead propose an efficient feature transform that removes correlations in local neighborhoods.</s> <s>the result is an overcomplete but locally decorrelated representation ideally suited for use with orthogonal decision trees.</s> <s>in fact, orthogonal trees with our locally decorrelated features outperform oblique trees trained over the original features at a fraction of the computational cost.</s> <s>the overall improvement in accuracy is dramatic: on the caltech pedestrian dataset, we reduce false positives nearly tenfold over the previous state-of-the-art.</s></p></d>", "label": ["<d><p><s>local decorrelation for improved pedestrian detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>convolutional neural nets (convnets) trained from massive labeled datasets have substantially improved the state-of-the-art in image classification and object detection.</s> <s>however, visual understanding requires establishing correspondence on a finer level than object category.</s> <s>given their large pooling regions and training from whole-image labels, it is not clear that convnets derive their success from an accurate correspondence model which could be used for precise localization.</s> <s>in this paper, we study the effectiveness of convnet activation features for tasks requiring correspondence.</s> <s>we present evidence that convnet features localize at a much finer scale than their receptive field sizes, that they can be used to perform intraclass aligment as well as conventional hand-engineered features, and that they outperform conventional features in keypoint prediction on objects from pascal voc 2011.</s></p></d>", "label": ["<d><p><s>do convnets learn correspondence?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the combination of modern reinforcement learning and deep learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection.</s> <s>the arcade learning environment (ale) provides a set of atari games that represent a useful benchmark set of such applications.</s> <s>a recent breakthrough in combining model-free reinforcement learning with deep learning, called dqn, achieves the best real-time agents thus far.</s> <s>planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play.</s> <s>our main goal in this work is to build a better real-time atari game playing agent than dqn.</s> <s>the central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play.</s> <s>we proposed new agents based on this idea and show that they outperform dqn.</s></p></d>", "label": ["<d><p><s>deep learning for real-time atari game play using offline monte-carlo tree search planning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have.</s> <s>deep networks are able to sequentially map portions of each layer's input-space to the same output.</s> <s>in this way, deep models compute functions that react equally to complicated patterns of different inputs.</s> <s>the compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth.</s> <s>this paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions.</s> <s>in particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks.</s> <s>we improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.</s></p></d>", "label": ["<d><p><s>on the number of linear regions of deep neural networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model g that captures the data distribution, and a discriminative model d that estimates the probability that a sample came from the training data rather than g. the training procedure for g is to maximize the probability of d making a mistake.</s> <s>this framework corresponds to a minimax two-player game.</s> <s>in the space of arbitrary functions g and d, a unique solution exists, with g recovering the training data distribution and d equal to 1/2 everywhere.</s> <s>in the case where g and d are defined by multilayer perceptrons, the entire system can be trained with backpropagation.</s> <s>there is no need for any markov chains or unrolled approximate inference networks during either training or generation of samples.</s> <s>experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.</s></p></d>", "label": ["<d><p><s>generative adversarial nets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation.</s> <s>these sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity.</s> <s>convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups.</s> <s>as a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity.</s> <s>in this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups.</s> <s>symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension.</s> <s>like convnets, they are trained with backpropagation.</s> <s>the composition of feature transformations through the layers of a symnet provides a new approach to deep learning.</s> <s>experiments on norb and mnist-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.</s></p></d>", "label": ["<d><p><s>deep symmetry networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings.</s> <s>attributes can correspond to a wide variety of concepts, such as document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors.</s> <s>we describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence.</s> <s>this leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes.</s> <s>we perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution.</s> <s>we also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.</s></p></d>", "label": ["<d><p><s>a multiplicative model for learning distributed text-based attribute representations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>let $f: \\{-1,1\\}^n \\rightarrow \\mathbb{r}$ be a polynomial with at most $s$ non-zero real coefficients.</s> <s>we give an algorithm for exactly reconstructing $f$ given random examples from the uniform distribution on $\\{-1,1\\}^n$ that runs in time polynomial in $n$ and $2^{s}$ and succeeds if the function satisfies the \\textit{unique sign property}: there is one output value which corresponds to a unique set of values of the participating parities.</s> <s>this sufficient condition is satisfied when every coefficient of $f$ is perturbed by a small random noise, or satisfied with high probability when $s$ parity functions are chosen randomly or when all the coefficients are positive.</s> <s>learning sparse polynomials over the boolean domain in time polynomial in $n$ and $2^{s}$ is considered notoriously hard in the worst-case.</s> <s>our result shows that the problem is tractable for almost all sparse polynomials.</s> <s>then, we show an application of this result to hypergraph sketching which is the problem of learning a sparse (both in the number of hyperedges and the size of the hyperedges) hypergraph from uniformly drawn random cuts.</s> <s>we also provide experimental results on a real world dataset.</s></p></d>", "label": ["<d><p><s>sparse polynomial learning and graph sketching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the residual bootstrap (rb) method in the context of high-dimensional linear regression.</s> <s>specifically, we analyze the distributional approximation of linear contrasts $c^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, where $\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator.</s> <s>when regression coefficients are estimated via least squares, classical results show that rb consistently approximates the laws of contrasts, provided that $p\\ll n$, where the design matrix is of size $n\\times p$.</s> <s>up to now, relatively little work has considered how additional structure in the linear model may extend the validity of rb to the setting where $p/n\\asymp 1$.</s> <s>in this setting, we propose a version of rb that resamples residuals obtained from ridge regression.</s> <s>our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile.</s> <s>under a few extra technical assumptions, we derive a simple criterion for ensuring that rb consistently approximates the law of a given contrast.</s> <s>we then specialize this result to study confidence intervals for mean response values $x_i^{\\top} \\beta$, where $x_i^{\\top}$ is the $i$th row of the design.</s> <s>more precisely, we show that conditionally on a gaussian design with near low-rank structure, rb \\emph{simultaneously} approximates all of the laws $x_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$.</s> <s>this result is also notable as it imposes no sparsity assumptions on $\\beta$.</s> <s>furthermore, since our consistency results are formulated in terms of the mallows (kantorovich) metric, the existence of a limiting distribution is not required.</s></p></d>", "label": ["<d><p><s>a residual bootstrap for high-dimensional regression with near low-rank designs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>subsampling methods have been recently proposed to speed up least squares estimation in large scale settings.</s> <s>however, these algorithms are typically not robust to outliers or corruptions in the observed covariates.</s> <s>the concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper.</s> <s>this property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error.</s> <s>under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.</s></p></d>", "label": ["<d><p><s>fast and robust least squares estimation in corrupted linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications.</s> <s>existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive.</s> <s>we propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables.</s> <s>we demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee.</s> <s>we conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.</s></p></d>", "label": ["<d><p><s>fast multivariate spatio-temporal analysis via low rank tensor learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new provable method for robust pca, where the task is to recover a low-rank matrix, which is corrupted with sparse perturbations.</s> <s>our method consists of simple alternating projections onto the set of low rank and sparse matrices with intermediate de-noising steps.</s> <s>we prove correct recovery of the low rank and sparse components under tight recovery conditions, which match those for the state-of-art convex relaxation techniques.</s> <s>our method is extremely simple to implement and has low computational complexity.</s> <s>for a $m \\times n$ input matrix (say m \\geq n), our method has o(r^2 mn\\log(1/\\epsilon)) running time, where $r$ is the rank of the low-rank component and $\\epsilon$ is the accuracy.</s> <s>in contrast, the convex relaxation methods have a running time o(mn^2/\\epsilon), which is not scalable to large problem instances.</s> <s>our running time nearly matches that of the usual pca (i.e.</s> <s>non robust), which is o(rmn\\log (1/\\epsilon)).</s> <s>thus, we achieve ``best of both the worlds'', viz low computational complexity and provable recovery for robust pca.</s> <s>our analysis represents one of the few instances of global convergence guarantees for non-convex methods.</s></p></d>", "label": ["<d><p><s>non-convex robust pca</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the dawid-skene estimator has been widely used for inferring the true labels from the noisy labels provided by non-expert crowdsourcing workers.</s> <s>however, since the estimator maximizes a non-convex log-likelihood function, it is hard to theoretically justify its performance.</s> <s>in this paper, we propose a two-stage efficient algorithm for multi-class crowd labeling problems.</s> <s>the first stage uses the spectral method to obtain an initial estimate of parameters.</s> <s>then the second stage refines the estimation by optimizing the objective function of the dawid-skene estimator via the em algorithm.</s> <s>we show that our algorithm achieves the optimal convergence rate up to a logarithmic factor.</s> <s>we conduct extensive experiments on synthetic and real datasets.</s> <s>experimental results demonstrate that the proposed algorithm is comparable to the most accurate empirical approach, while outperforming several other recently proposed methods.</s></p></d>", "label": ["<d><p><s>spectral methods meet em: a provably optimal algorithm for crowdsourcing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a new probabilistic model for transcribing piano music from audio to a symbolic form.</s> <s>our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data.</s> <s>as a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony.</s> <s>in order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording specific spectral profiles and temporal envelopes in an unsupervised fashion.</s> <s>our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset f1 on real piano audio.</s></p></d>", "label": ["<d><p><s>unsupervised transcription of piano music</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the {\\em combinatorial pure exploration (cpe)} problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a \\emph{decision class}, which is a collection of subsets of arms with certain combinatorial structures such as size-$k$ subsets, matchings, spanning trees or paths, etc.</s> <s>the cpe problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure.</s> <s>in this paper, we provide a series of results for the general cpe problem.</s> <s>we present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings.</s> <s>we prove problem-dependent upper bounds of our algorithms.</s> <s>our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool.</s> <s>we also establish a general problem-dependent lower bound for the cpe problem.</s> <s>our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes.</s> <s>in addition, applying our results back to the problems of top-$k$ arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.</s></p></d>", "label": ["<d><p><s>combinatorial pure exploration of multi-armed bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>empirical risk minimization (erm) is a fundamental learning rule for statistical learning problems where the data is generated according to some unknown distribution $\\mathsf{p}$ and returns a hypothesis $f$ chosen from a fixed class $\\mathcal{f}$ with small loss $\\ell$.</s> <s>in the parametric setting, depending upon $(\\ell, \\mathcal{f},\\mathsf{p})$ erm can have slow $(1/\\sqrt{n})$ or fast $(1/n)$ rates of convergence of the excess risk as a function of the sample size $n$.</s> <s>there exist several results that give sufficient conditions for fast rates in terms of joint properties of $\\ell$, $\\mathcal{f}$, and $\\mathsf{p}$, such as the margin condition and the bernstein condition.</s> <s>in the non-statistical prediction with expert advice setting, there is an analogous slow and fast rate phenomenon, and it is entirely characterized in terms of the mixability of the loss $\\ell$ (there being no role there for $\\mathcal{f}$ or $\\mathsf{p}$).</s> <s>the notion of stochastic mixability builds a bridge between these two models of learning, reducing to classical mixability in a special case.</s> <s>the present paper presents a direct proof of fast rates for erm in terms of stochastic mixability of $(\\ell,\\mathcal{f}, \\mathsf{p})$, and in so doing provides new insight into the fast-rates phenomenon.</s> <s>the proof exploits an old result of kemperman on the solution to the general moment problem.</s> <s>we also show a partial converse that suggests a characterization of fast rates for erm in terms of stochastic mixability is possible.</s></p></d>", "label": ["<d><p><s>from stochastic mixability to fast rates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study agnostic active learning, where the goal is to learn a classifier in a pre-specified hypothesis class interactively with as few label queries as possible, while making no assumptions on the true function generating the labels.</s> <s>the main algorithms for this problem are {\\em{disagreement-based active learning}}, which has a high label requirement, and {\\em{margin-based active learning}}, which only applies to fairly restricted settings.</s> <s>a major challenge is to find an algorithm which achieves better label complexity, is consistent in an agnostic setting, and applies to general classification problems.</s> <s>in this paper, we provide such an algorithm.</s> <s>our solution is based on two novel contributions -- a reduction from consistent active learning to confidence-rated prediction with guaranteed error, and a novel confidence-rated predictor.</s></p></d>", "label": ["<d><p><s>beyond disagreement-based agnostic active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of online optimization, where a learner chooses a decision from a given decision set and suffers some loss associated with the decision and the state of the environment.</s> <s>the learner's objective is to minimize its cumulative regret against the best fixed decision in hindsight.</s> <s>over the past few decades numerous variants have been considered, with many algorithms designed to achieve sub-linear regret in the worst case.</s> <s>however, this level of robustness comes at a cost.</s> <s>proposed algorithms are often over-conservative, failing to adapt to the actual complexity of the loss sequence which is often far from the worst case.</s> <s>in this paper we introduce a general algorithm that, provided with a safe learning algorithm and an opportunistic benchmark, can effectively combine good worst-case guarantees with much improved performance on easy data.</s> <s>we derive general theoretical bounds on the regret of the proposed algorithm and discuss its implementation in a wide range of applications, notably in the problem of learning with shifting experts (a recent colt open problem).</s> <s>finally, we provide numerical simulations in the setting of prediction with expert advice with comparisons to the state of the art.</s></p></d>", "label": ["<d><p><s>exploiting easy data in online optimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this work concerns learning probabilistic models for ranking data in a heterogeneous population.</s> <s>the specific problem we study is learning the parameters of a {\\em mallows mixture model}.</s> <s>despite being widely studied, current heuristics for this problem do not have theoretical guarantees and can get stuck in bad local optima.</s> <s>we present the first polynomial time algorithm which provably learns the parameters of a mixture of two mallows models.</s> <s>a key component of our algorithm is a novel use of tensor decomposition techniques to learn the top-$k$ prefix in both the rankings.</s> <s>before this work, even the question of {\\em identifiability} in the case of a mixture of two mallows models was unresolved.</s></p></d>", "label": ["<d><p><s>learning mixtures of ranking models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we study revenue optimization learning algorithms for posted-price auctions with strategic buyers.</s> <s>we analyze a very broad family of monotone regret minimization algorithms for this problem, which includes the previous best known algorithm, and show that no algorithm in that family admits a strategic regret more favorable than $\\omega(\\sqrt{t})$.</s> <s>we then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $o(\\log t)$, an exponential improvement upon the previous best algorithm.</s> <s>our new algorithm admits a natural analysis and simpler proofs, and the ideas behind its design are general.</s> <s>we also report the results of empirical evaluations comparing our algorithm with the previous best algorithm and show a consistent exponential improvement in several different scenarios.</s></p></d>", "label": ["<d><p><s>optimal regret minimization in posted-price auctions with strategic buyers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the behavior of nearest neighbor classification in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions.</s> <s>these are more general than existing bounds, and enable us, as a by-product, to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known.</s> <s>we illustrate our upper and lower bounds by introducing a new smoothness class customized for nearest neighbor classification.</s> <s>we find, for instance, that under the tsybakov margin condition the convergence rate of nearest neighbor matches recently established lower bounds for nonparametric classification.</s></p></d>", "label": ["<d><p><s>rates of convergence for nearest neighbor classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>some of the simplest loss functions considered in machine learning are the square loss, the logistic loss and the hinge loss.</s> <s>the most common family of algorithms, including gradient descent (gd) with and without weight decay, always predict with a linear combination of the past instances.</s> <s>we give a random construction for sets of examples where the target linear weight vector is trivial to learn but any algorithm from the above family is drastically sub-optimal.</s> <s>our lower bound on the latter algorithms holds even if the algorithms are enhanced with an arbitrary kernel function.</s> <s>this type of result was known for the square loss.</s> <s>however, we develop new techniques that let us prove such hardness results for any loss function satisfying some minimal requirements on the loss function (including the three listed above).</s> <s>we also show that algorithms that regularize with the squared euclidean distance are easily confused by random features.</s> <s>finally, we conclude by discussing related open problems regarding feed forward neural networks.</s> <s>we conjecture that our hardness results hold for any training algorithm that is based on the squared euclidean distance regularization (i.e.</s> <s>back-propagation with the weight decay heuristic).</s></p></d>", "label": ["<d><p><s>the limits of squared euclidean distance regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in reinforcement learning (rl), state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$.</s> <s>in many problems, a good approximation of $p$ is not needed.</s> <s>for instance, if from one state-action pair $(s,a)$, one can only transit to states with the same value, learning $p(\\cdot|s,a)$ accurately is irrelevant (only its support matters).</s> <s>this paper aims at capturing such behavior by defining a novel hardness measure for markov decision processes (mdps) we call the {\\em distribution-norm}.</s> <s>the distribution-norm w.r.t.~a measure $\\nu$ is defined on zero $\\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\\nu$.</s> <s>we first provide a concentration inequality for the dual of the distribution-norm.</s> <s>this allows us to replace the generic but loose $||\\cdot||_1$ concentration inequalities used in most previous analysis of rl algorithms, to benefit from this new hardness measure.</s> <s>we then show that several common rl benchmarks have low hardness when measured using the new norm.</s> <s>the distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of mdps.</s></p></d>", "label": ["<d><p><s>how hard is my mdp?\" the distribution-norm to the rescue\"</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we explore the connection between dimensionality and communication cost in distributed learning problems.</s> <s>specifically we study the problem of estimating the mean $\\vectheta$ of an unknown $d$ dimensional gaussian distribution in the distributed setting.</s> <s>in this problem, the samples from the unknown distribution are distributed among $m$ different machines.</s> <s>the goal is to estimate the mean $\\vectheta$ at the optimal minimax rate while communicating as few bits as possible.</s> <s>we show that in this setting, the communication cost scales linearly in the number of dimensions i.e.</s> <s>one needs to deal with different dimensions individually.</s> <s>applying this result to previous lower bounds for one dimension in the interactive setting \\cite{zdjw13} and to our improved bounds for the simultaneous setting, we prove new lower bounds of $\\omega(md/\\log(m))$ and $\\omega(md)$ for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively.</s> <s>to complement, we also demonstrate an interactive protocol achieving the minimax squared loss with $o(md)$ bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor.</s> <s>given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters.</s> <s>specifically, when the parameter is promised to be $s$-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a $d/s$ factor of communication.</s> <s>we conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.</s></p></d>", "label": ["<d><p><s>on communication cost of distributed statistical estimation and dimensionality</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large markov decision processes (mdps) are usually solved using approximate dynamic programming (adp) methods such as approximate value iteration (avi) or approximate policy iteration (api).</s> <s>the main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using difference of convex functions (dc) programming.</s> <s>to do so, we study the minimization of a norm of the optimal bellman residual (obr) $t^*q-q$, where $t^*$ is the so-called optimal bellman operator.</s> <s>controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the obr is consistant in the vapnik sense.</s> <s>finally, we frame this optimization problem as a dc program.</s> <s>that allows envisioning using the large related literature on dc programming to address the reinforcement leaning (rl) problem.</s></p></d>", "label": ["<d><p><s>difference of convex functions programming for reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems.</s> <s>these trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization.</s> <s>our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous.</s> <s>we show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques.</s> <s>we present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.</s></p></d>", "label": ["<d><p><s>learning neural network policies with guided policy search under unknown dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>any reinforcement learning algorithm that applies to all markov decision processes (mdps) will suffer $\\omega(\\sqrt{sat})$ regret on some mdp, where $t$ is the elapsed time and $s$ and $a$ are the cardinalities of the state and action spaces.</s> <s>this implies $t = \\omega(sa)$ time to guarantee a near-optimal policy.</s> <s>in many settings of practical interest, due to the curse of dimensionality, $s$ and $a$ can be so enormous that this learning time is unacceptable.</s> <s>we establish that, if the system is known to be a \\emph{factored} mdp, it is possible to achieve regret that scales polynomially in the number of \\emph{parameters} encoding the factored mdp, which may be exponentially smaller than $s$ or $a$.</s> <s>we provide two algorithms that satisfy near-optimal regret bounds in this context: posterior sampling reinforcement learning (psrl) and an upper confidence bound algorithm (ucrl-factored).</s></p></d>", "label": ["<d><p><s>near-optimal reinforcement learning in factored mdps</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the challenging practical problem of optimizing the power production of a complex of hydroelectric power plants, which involves control over three continuous action variables, uncertainty in the amount of water inflows and a variety of constraints that need to be satisfied.</s> <s>we propose a policy-search-based approach coupled with predictive modelling to address this problem.</s> <s>this approach has some key advantages compared to other alternatives, such as dynamic programming: the policy representation and search algorithm can conveniently incorporate domain knowledge; the resulting policies are easy to interpret, and the algorithm is naturally parallelizable.</s> <s>our algorithm obtains a policy which outperforms the solution found by dynamic programming both quantitatively and qualitatively.</s></p></d>", "label": ["<d><p><s>optimizing energy production using policy search and predictive state representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe how to use robust markov decision processes for value function approximation with state aggregation.</s> <s>the robustness serves to reduce the sensitivity to the approximation error of sub-optimal policies in comparison to classical methods such as fitted value iteration.</s> <s>this results in reducing the bounds on the gamma-discounted infinite horizon performance loss by a factor of 1/(1-gamma) while preserving polynomial-time computational complexity.</s> <s>our experimental results show that using the robust representation can significantly improve the solution quality with minimal additional computational cost.</s></p></d>", "label": ["<d><p><s>raam: the benefits of robustness in approximating aggregated mdps in reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor factorizations have become popular methods for learning from multi-relational data.</s> <s>in this context, the rank of a factorization is an important parameter that determines runtime as well as generalization ability.</s> <s>to determine conditions under which factorization is an efficient approach for learning from relational data, we derive upper and lower bounds on the rank required to recover adjacency tensors.</s> <s>based on our findings, we propose a novel additive tensor factorization model for learning from latent and observable patterns in multi-relational data and present a scalable algorithm for computing the factorization.</s> <s>experimentally, we show that the proposed approach does not only improve the predictive performance over pure latent variable methods but that it also reduces the required rank --- and therefore runtime and memory complexity --- significantly.</s></p></d>", "label": ["<d><p><s>reducing the rank in relational factorization models by including observable patterns</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of drawing samples from a discrete distribution can be converted into a discrete optimization problem.</s> <s>in this work, we show how sampling from a continuous distribution can be converted into an optimization problem over continuous space.</s> <s>central to the method is a stochastic process recently described in mathematical statistics that we call the gumbel process.</s> <s>we present a new construction of the gumbel process and a* sampling, a practical generic sampling algorithm that searches for the maximum of a gumbel process using a* search.</s> <s>we analyze the correctness and convergence time of a* sampling and demonstrate empirically that it makes more efficient use of bound and likelihood evaluations than the most closely related adaptive rejection sampling-based algorithms.</s></p></d>", "label": ["<d><p><s>a* sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new sequential monte carlo algorithm we call the particle cascade.</s> <s>the particle cascade is an asynchronous, anytime alternative to traditional sequential monte carlo algorithms that is amenable to parallel and distributed implementations.</s> <s>it uses no barrier synchronizations which leads to improved particle throughput and memory efficiency.</s> <s>it is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget.</s> <s>we prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.</s></p></d>", "label": ["<d><p><s>asynchronous anytime sequential monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>runge-kutta methods are the classic family of solvers for ordinary differential equations (odes), and the basis for the state of the art.</s> <s>like most numerical methods, they return point estimates.</s> <s>we construct a family of probabilistic numerical methods that instead return a gauss-markov process defining a probability distribution over the ode solution.</s> <s>in contrast to prior work, we construct this family such that posterior means match the outputs of the runge-kutta family exactly, thus inheriting their proven good properties.</s> <s>remaining degrees of freedom not identified by the match to runge-kutta are chosen such that the posterior probability measure fits the observed structure of the ode.</s> <s>our results shed light on the structure of runge-kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.</s></p></d>", "label": ["<d><p><s>probabilistic ode solvers with runge-kutta means</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed.</s> <s>this bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails.</s> <s>it applies to a large group of kernel tests based on v-statistics, which are degenerate under the null hypothesis, and non-degenerate elsewhere.</s> <s>to illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series.</s> <s>in experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the gibbs sampler.</s> <s>the code is available at https://github.com/kacperchwialkowski/wildbootstrap.</s></p></d>", "label": ["<d><p><s>a wild bootstrap for degenerate kernel tests</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in learning with label proportions (llp), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known.</s> <s>this setting has broad practical relevance, in particular for privacy preserving data processing.</s> <s>we first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels.</s> <s>we provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds.</s> <s>then, we present an iterative learning algorithm that uses this as initialization.</s> <s>we ground this algorithm in rademacher-style generalization bounds that fit the llp setting, introducing a generalization of rademacher complexity and a label proportion complexity measure.</s> <s>this latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk.</s> <s>experiments are provided on fourteen domains, whose size ranges up to 300k observations.</s> <s>they display that our algorithms are scalable and tend to consistently outperform the state of the art in llp.</s> <s>moreover, in many cases, our algorithms compete with or are just percents of auc away from the oracle that learns knowing all labels.</s> <s>on the largest domains, half a dozen proportions can suffice, i.e.</s> <s>roughly 40k times less than the total number of labels.</s></p></d>", "label": ["<d><p><s>(almost) no label no cry</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives, false positives, true negatives and false negatives.</s> <s>despite significant interest from theoretical and applied communities, little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases.</s> <s>we consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities.</s> <s>this family includes many well known binary classification metrics such as classification accuracy, am measure, f-measure and the jaccard similarity coefficient as special cases.</s> <s>our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class, with a performance metric-dependent threshold.</s> <s>the optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities, but alternative techniques are required for the general case.</s> <s>we propose two algorithms for estimating the optimal classifiers, and prove their statistical consistency.</s> <s>both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection, thus are simple to implement in practice.</s> <s>the first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection.</s> <s>the second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers.</s> <s>we present empirical comparisons between these algorithms on benchmark datasets.</s></p></d>", "label": ["<d><p><s>consistent binary classification with generalized performance metrics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present two new methods for inference in gaussian process (gp) models with general nonlinear likelihoods.</s> <s>inference is based on a variational framework where a gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a taylor series expansion or statistical linearization.</s> <s>we show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented kalman filters respectively, hence we refer to our algorithms as extended and unscented gps.</s> <s>the unscented gp treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models.</s> <s>we evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.</s></p></d>", "label": ["<d><p><s>extended and unscented gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel sampling algorithm for markov chain monte carlo-based bayesian inference for factorial hidden markov models.</s> <s>this algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time.</s> <s>the sampling approach overcomes limitations with common conditional gibbs samplers that use asymmetric updates and become easily trapped in local modes.</s> <s>instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing.</s> <s>we illustrate the application of the approach with simulated and a real data example.</s></p></d>", "label": ["<d><p><s>hamming ball auxiliary sampling for factorial hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces a novel mathematical and computational framework, namely {\\it log-hilbert-schmidt metric} between positive definite operators on a hilbert space.</s> <s>this is a generalization of the log-euclidean metric on the riemannian manifold of positive definite matrices to the infinite-dimensional setting.</s> <s>the general framework is applied in particular to compute distances between covariance operators on a reproducing kernel hilbert space (rkhs), for which we obtain explicit formulas via the corresponding gram matrices.</s> <s>empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional rkhs covariance operator.</s> <s>on several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the log-euclidean metric, stein and jeffreys divergences, achieving new state of the art results.</s></p></d>", "label": ["<d><p><s>log-hilbert-schmidt metric between positive definite operators on hilbert spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions.</s> <s>due to its asymptotic properties, sample-reweighted loss minimization is a commonly employed technique to deal with this difference.</s> <s>however, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias.</s> <s>we develop a framework for robustly learning a probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation.</s> <s>our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated.</s> <s>we demonstrate the behavior and effectiveness of our approach on synthetic and uci binary classification tasks.</s></p></d>", "label": ["<d><p><s>robust classification under sample selection bias</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>gaussian process regression can be accelerated by constructing a small pseudo-dataset to summarise the observed data.</s> <s>this idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained.</s> <s>this presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size.</s> <s>in this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints.</s> <s>this is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a kullback-leibler (kl) minimisation.</s> <s>inference and learning can then be performed efficiently using the gaussian belief propagation algorithm.</s> <s>we demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets.</s> <s>we trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.</s></p></d>", "label": ["<d><p><s>tree-structured gaussian process approximations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the objective is to return the arm with the largest reward.</s> <s>we characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget.</s> <s>in particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms.</s> <s>we analyze the proposed strategies and compare their empirical performance.</s> <s>finally, as a by-product of our analysis, we point out the connection to the $g$-optimality criterion used in optimal experimental design.</s></p></d>", "label": ["<d><p><s>best-arm identification in linear bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a new type of k-armed bandit problem where the expected return of one arm may depend on the returns of other arms.</s> <s>we present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret.</s> <s>we also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.</s></p></d>", "label": ["<d><p><s>bounded regret for finite-armed structured bandits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider online learning problems under a a partial observability model capturing situations where the information conveyed to the learner is between full information and bandit feedback.</s> <s>in the simplest variant, we assume that in addition to its own loss, the learner also gets to observe losses of some other actions.</s> <s>the revealed losses depend on the learner's action and a directed observation system chosen by the environment.</s> <s>for this setting, we propose the first algorithm that enjoys near-optimal regret guarantees without having to know the observation system before selecting its actions.</s> <s>along similar lines, we also define a new partial information setting that models online combinatorial optimization problems where the feedback received by the learner is between semi-bandit and full feedback.</s> <s>as the predictions of our first algorithm cannot be always computed efficiently in this setting, we propose another algorithm with similar properties and with the benefit of always being computationally efficient, at the price of a slightly more complicated tuning mechanism.</s> <s>both algorithms rely on a novel exploration strategy called implicit exploration, which is shown to be more efficient both computationally and information-theoretically than previously studied exploration strategies for the problem.</s></p></d>", "label": ["<d><p><s>efficient learning by implicit exploration in bandit problems with side observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose information-directed sampling -- a new algorithm for online optimization problems in which a decision-maker must balance between exploration and exploitation while learning from partial feedback.</s> <s>each action is sampled in a manner that minimizes the ratio between the square of expected single-period regret and a measure of information gain: the mutual information between the optimal action and the next observation.</s> <s>we establish an expected regret bound for information-directed sampling that applies across a very general class of models and scales with the entropy of the optimal action distribution.</s> <s>for the widely studied bernoulli and linear bandit models, we demonstrate simulation performance surpassing popular approaches, including upper confidence bound algorithms, thompson sampling, and knowledge gradient.</s> <s>further, we present simple analytic examples illustrating that information-directed sampling can dramatically outperform upper confidence bound algorithms and thompson sampling due to the way it measures information gain.</s></p></d>", "label": ["<d><p><s>learning to optimize via information-directed sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint.</s> <s>we propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial gaussian process on the spike and slab probabilities.</s> <s>thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions.</s> <s>furthermore, we provide a bayesian inference scheme for the proposed model based on the expectation propagation framework.</s> <s>using numerical experiments on synthetic data, we demonstrate the benefits of the model.</s></p></d>", "label": ["<d><p><s>bayesian inference for structured spike and slab priors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>analysis of estimation error and associated structured statistical recovery based on norm regularized regression, e.g., lasso, needs to consider four aspects: the norm, the loss function, the design matrix, and the noise vector.</s> <s>this paper presents generalizations of such estimation error analysis on all four aspects, compared to the existing literature.</s> <s>we characterize the restricted error set, establish relations between error sets for the constrained and regularized problems, and present an estimation error bound applicable to {\\em any} norm.</s> <s>precise characterizations of the bound are presented for a variety of noise vectors, design matrices, including sub-gaussian, anisotropic, and dependent samples, and loss functions, including least squares and generalized linear models.</s> <s>gaussian widths, as a measure of size of suitable sets, and associated tools play a key role in our generalized analysis.</s></p></d>", "label": ["<d><p><s>estimation with norm regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning sparse additive models, i.e., functions of the form: $f(\\vecx) = \\sum_{l \\in s} \\phi_{l}(x_l)$, $\\vecx \\in \\matr^d$ from point queries of $f$.</s> <s>here $s$ is an unknown subset of coordinate variables with $\\abs{s} = k \\ll d$.</s> <s>assuming $\\phi_l$'s to be smooth, we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \\textit{uniform approximation} to each unknown $\\phi_l$.</s> <s>we provide a rigorous theoretical analysis of our scheme along with sample complexity bounds.</s> <s>our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions, from point queries corrupted with arbitrary bounded noise.</s> <s>lastly we theoretically analyze the impact of noise -- either arbitrary but bounded, or stochastic -- on the performance of our algorithm.</s></p></d>", "label": ["<d><p><s>efficient sampling for learning sparse additive models in high dimensions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of recovering a symmetric, positive semidefinite (spsd) matrix from a subset of its entries, possibly corrupted by noise.</s> <s>in contrast to previous matrix recovery work, we drop the assumption of a random sampling of entries in favor of a deterministic sampling of principal submatrices of the matrix.</s> <s>we develop a set of sufficient conditions for the recovery of a spsd matrix from a set of its principal submatrices, present necessity results based on this set of conditions and develop an algorithm that can exactly recover a matrix when these conditions are met.</s> <s>the proposed algorithm is naturally generalized to the problem of noisy matrix recovery, and we provide a worst-case bound on reconstruction error for this scenario.</s> <s>finally, we demonstrate the algorithm's utility on noiseless and noisy simulated datasets.</s></p></d>", "label": ["<d><p><s>deterministic symmetric positive semidefinite matrix completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new active learning algorithm for parametric linear regression with random design.</s> <s>we provide finite sample convergence guarantees for general distributions in the misspecified model.</s> <s>this is the first active learner for this setting that provably can improve over passive learning.</s> <s>unlike other learning settings (such as classification), in regression the passive learning rate of o(1/epsilon) cannot in general be improved upon.</s> <s>nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases.</s> <s>for a given distribution, achieving the optimal risk requires prior knowledge of the distribution.</s> <s>following the stratification technique advocated in monte-carlo function integration, our active learner approaches a the optimal risk using piecewise constant approximations.</s></p></d>", "label": ["<d><p><s>active regression by stratification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework.</s> <s>different online learning settings (hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games.</s> <s>the original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function.</s> <s>with different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties.</s> <s>moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates.</s> <s>finally, we translate our new hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.</s></p></d>", "label": ["<d><p><s>a drifting-games analysis for online learning and applications to boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an inference method for gaussian graphical models when only pairwise distances of n objects are observed.</s> <s>formally, this is a problem of estimating an n x n covariance matrix from the mahalanobis distances dmh(xi, xj), where object xi lives in a latent feature space.</s> <s>we solve the problem in fully bayesian fashion by integrating over the matrix-normal likelihood and a matrix-gamma prior; the resulting matrix-t posterior enables network recovery even under strongly correlated features.</s> <s>hereby, we generalize tiwnet, which assumes euclidean distances with strict feature independence.</s> <s>in spite of the greatly increased flexibility, our model neither loses statistical power nor entails more computational cost.</s> <s>we argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments, which is successfully demonstrated for a network of biological pathways in cancer patients.</s></p></d>", "label": ["<d><p><s>distance-based network recovery under feature correlation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a technique for decomposing the parameter learning problem in bayesian networks into independent learning problems.</s> <s>our technique applies to incomplete datasets and exploits variables that are either hidden or observed in the given dataset.</s> <s>we show empirically that the proposed technique can lead to orders-of-magnitude savings in learning time.</s> <s>we explain, analytically and empirically, the reasons behind our reported savings, and compare the proposed technique to related ones that are sometimes used by inference algorithms.</s></p></d>", "label": ["<d><p><s>decomposing parameter estimation problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the sensitivity of a map configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters.</s> <s>these perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed.</s> <s>our main contribution is an exact algorithm that can check whether the map configuration is robust with respect to given perturbations.</s> <s>its complexity is essentially the same as that of obtaining the map configuration itself, so it can be promptly used with minimal effort.</s> <s>we use our algorithm to identify the largest global perturbation that does not induce a change in the map configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets.</s> <s>a strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.</s></p></d>", "label": ["<d><p><s>global sensitivity analysis for map inference in graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc.</s> <s>.</s> <s>however, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection.</s> <s>as we show, time-series graphical models based on vector auto-regressive processes are inef?cient in capturing such multi-scale structure.</s> <s>in this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems.</s> <s>we also propose algorithms to ef?ciently estimate the interaction structure from data.</s> <s>we demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.</s></p></d>", "label": ["<d><p><s>multi-scale graphical models for spatio-temporal processes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider a setting in which low-power distributed sensors are each making highly noisy measurements of some unknown target function.</s> <s>a center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate.</s> <s>the question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-de?ned game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries.</s> <s>we prove positive (and negative) results on the denoising power of several natural dynamics, and also show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.</s></p></d>", "label": ["<d><p><s>active learning and best-response dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of low-rank tensor factorization in the presence of missing data.</s> <s>we ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition?</s> <s>we propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors.</s> <s>we show that under certain standard assumptions, our method can recover a three-mode $n\\times n\\times n$ dimensional rank-$r$ tensor exactly from $o(n^{3/2} r^5 \\log^4 n)$ randomly sampled entries.</s> <s>in the process of proving this result, we solve two challenging sub-problems for tensors with missing data.</s> <s>first, in analyzing the initialization step, we prove a generalization of a celebrated result by szemer\\'edie et al.</s> <s>on the spectrum of random graphs.</s> <s>next, we prove global convergence of alternating minimization with a good initialization.</s> <s>simulations suggest that the dependence of the sample size on dimensionality $n$ is indeed tight.</s></p></d>", "label": ["<d><p><s>provable tensor factorization with missing data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>low-rank tensor estimation has been frequently applied in many real-world problems.</s> <s>despite successful applications, existing schatten 1-norm minimization (snm) methods may become very slow or even not applicable for large-scale problems.</s> <s>to address this difficulty, we therefore propose an efficient and scalable core tensor schatten 1-norm minimization method for simultaneous tensor decomposition and completion, with a much lower computational complexity.</s> <s>we first induce the equivalence relation of schatten 1-norm of a low-rank tensor and its core tensor.</s> <s>then the schatten 1-norm of the core tensor is used to replace that of the whole tensor, which leads to a much smaller-scale matrix snm problem.</s> <s>finally, an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee.</s> <s>extensive experimental results show that our method is usually more accurate than the state-of-the-art methods, and is orders of magnitude faster.</s></p></d>", "label": ["<d><p><s>generalized higher-order orthogonal iteration for tensor decomposition and completion</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze skip-gram with negative-sampling (sgns), a word embedding method introduced by mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (pmi) of the respective word and context pairs, shifted by a global constant.</s> <s>we find that another embedding method, nce, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context.</s> <s>we show that using a sparse shifted positive pmi word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks.</s> <s>when dense low-dimensional vectors are preferred, exact factorization with svd can achieve solutions that are at least as good as sgns's solutions for word similarity tasks.</s> <s>on analogy questions sgns remains superior to svd.</s> <s>we conjecture that this stems from the weighted nature of sgns's factorization.</s></p></d>", "label": ["<d><p><s>neural word embedding as implicit matrix factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov logic networks (mlns) are weighted first-order logic templates for generating large (ground) markov networks.</s> <s>lifted inference algorithms for them bring the power of logical inference to probabilistic inference.</s> <s>these algorithms operate as much as possible at the compact first-order level, grounding or propositionalizing the mln only as necessary.</s> <s>as a result, lifted inference algorithms can be much more scalable than propositional algorithms that operate directly on the much larger ground network.</s> <s>unfortunately, existing lifted inference algorithms suffer from two interrelated problems, which severely affects their scalability in practice.</s> <s>first, for most real-world mlns having complex structure, they are unable to exploit symmetries and end up grounding most atoms (the grounding problem).</s> <s>second, they suffer from the evidence problem, which arises because evidence breaks symmetries, severely diminishing the power of lifted inference.</s> <s>in this paper, we address both problems by presenting a scalable, lifted importance sampling-based approach that never grounds the full mln.</s> <s>specifically, we show how to scale up the two main steps in importance sampling: sampling from the proposal distribution and weight computation.</s> <s>scalable sampling is achieved by using an informed, easy-to-sample proposal distribution derived from a compressed mln-representation.</s> <s>fast weight computation is achieved by only visiting a small subset of the sampled groundings of each formula instead of all of its possible groundings.</s> <s>we show that our new algorithm yields an asymptotically unbiased estimate.</s> <s>our experiments on several mlns clearly demonstrate the promise of our approach.</s></p></d>", "label": ["<d><p><s>scaling-up importance sampling for markov logic networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we propose a sparse random feature algorithm, which learns a sparse non-linear predictor by minimizing an $\\ell_1$-regularized objective function over the hilbert space induced from kernel function.</s> <s>by interpreting the algorithm as randomized coordinate descent in the infinite-dimensional space, we show the proposed approach converges to a solution comparable within $\\eps$-precision to exact kernel method by drawing $o(1/\\eps)$ number of random features, contrasted to the $o(1/\\eps^2)$-type convergence achieved by monte-carlo analysis in current random feature literature.</s> <s>in our experiments, the sparse random feature algorithm obtains sparse solution that requires less memory and prediction time while maintains comparable performance on tasks of regression and classification.</s> <s>in the meantime, as an approximate solver for infinite-dimensional $\\ell_1$-regularized problem, the randomized approach converges to better solution than boosting approach when the greedy step of boosting cannot be performed exactly.</s></p></d>", "label": ["<d><p><s>sparse random feature algorithm as coordinate descent in hilbert space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many classification problems, the input is represented as a set of features, e.g., the bag-of-words (bow) representation of documents.</s> <s>support vector machines (svms) are widely used tools for such classification problems.</s> <s>the performance of the svms is generally determined by whether kernel values between data points can be defined properly.</s> <s>however, svms for bow representations have a major weakness in that the co-occurrence of different but semantically similar words cannot be reflected in the kernel calculation.</s> <s>to overcome the weakness, we propose a kernel-based discriminative classifier for bow data, which we call the latent support measure machine (latent smm).</s> <s>with the latent smm, a latent vector is associated with each vocabulary term, and each document is represented as a distribution of the latent vectors for words appearing in the document.</s> <s>to represent the distributions efficiently, we use the kernel embeddings of distributions that hold high order moment information about distributions.</s> <s>then the latent smm finds a separating hyperplane that maximizes the margins between distributions of different classes while estimating latent vectors for words to improve the classification performance.</s> <s>in the experiments, we show that the latent smm achieves state-of-the-art accuracy for bow text classification, is robust with respect to its own hyper-parameters, and is useful to visualize words.</s></p></d>", "label": ["<d><p><s>latent support measure machines for bag-of-words data classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel machines such as kernel svm and kernel ridge regression usually construct high quality models; however, their use in real-world applications remains limited due to the high prediction cost.</s> <s>in this paper, we present two novel insights for improving the prediction efficiency of kernel machines.</s> <s>first, we show that by adding ?pseudo landmark points?</s> <s>to the classical nystr?om kernel approximation in an elegant way, we can significantly reduce the prediction error without much additional prediction cost.</s> <s>second, we provide a new theoretical analysis on bounding the error of the solution computed by using nystr?om kernel approximation method, and show that the error is related to the weighted kmeans objective function where the weights are given by the model computed from the original kernel.</s> <s>this theoretical insight suggests a new landmark point selection technique for the situation where we have knowledge of the original model.</s> <s>based on these two insights, we provide a divide-and-conquer framework for improving the prediction speed.</s> <s>first, we divide the whole problem into smaller local subproblems to reduce the problem size.</s> <s>in the second phase, we develop a kernel approximation based fast prediction approach within each subproblem.</s> <s>we apply our algorithm to real world large-scale classification and regression datasets, and show that the proposed algorithm is consistently and significantly better than other competitors.</s> <s>for example, on the covertype classification problem, in terms of prediction time, our algorithm achieves more than 10000 times speedup over the full kernel svm, and a two-fold speedup over the state-of-the-art ldkl approach, while obtaining much higher prediction accuracy than ldkl (95.2% vs. 89.53%).</s></p></d>", "label": ["<d><p><s>fast prediction for large-scale kernel machines</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the global markov property for gaussian graphical models ensures graph separation implies conditional independence.</s> <s>specifically if a node set $s$ graph separates nodes $u$ and $v$ then $x_u$ is conditionally independent of $x_v$ given $x_s$.</s> <s>the opposite direction need not be true, that is, $x_u \\perp x_v \\mid x_s$ need not imply $s$ is a node separator of $u$ and $v$.</s> <s>when it does, the relation $x_u \\perp x_v \\mid x_s$ is called faithful.</s> <s>in this paper we provide a characterization of faithful relations and then provide an algorithm to test faithfulness based only on knowledge of other conditional relations of the form $x_i \\perp x_j \\mid x_s$.</s></p></d>", "label": ["<d><p><s>testing unfaithful gaussian graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than markov chain monte carlo (mcmc) benchmarks.</s> <s>the central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute marginal likelihood or a partition function).</s> <s>mcmc has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics.</s> <s>bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs.</s> <s>we introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations.</s> <s>our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple monte carlo and annealed importance sampling on both synthetic and real-world examples.</s></p></d>", "label": ["<d><p><s>sampling for inference in probabilistic models with fast bayesian quadrature</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision.</s> <s>in this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models.</s> <s>moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models.</s> <s>on the timit phoneme recognition and cifar-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.</s></p></d>", "label": ["<d><p><s>do deep nets really need to be deep?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many fundamental image-related problems involve deconvolution operators.</s> <s>real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few.</s> <s>instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation.</s> <s>we note directly applying existing deep neural networks does not produce reasonable results.</s> <s>our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts.</s> <s>our network contains two submodules, both trained in a supervised manner with proper initialization.</s> <s>they yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.</s></p></d>", "label": ["<d><p><s>deep convolutional neural network for image deconvolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces.</s> <s>gradient descent or quasi-newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum.</s> <s>here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest.</s> <s>such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum.</s> <s>motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-newton methods.</s> <s>we apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.</s></p></d>", "label": ["<d><p><s>identifying and attacking the saddle point problem in high-dimensional non-convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process.</s> <s>e.g., dropout (hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network.</s> <s>we examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space.</s> <s>we present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it.</s> <s>in the fully-supervised setting, our regularizer matches the performance of dropout.</s> <s>but, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results.</s> <s>we provide a case study in which we transform the recursive neural tensor network of (socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.</s></p></d>", "label": ["<d><p><s>learning with pseudo-ensembles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of ising models, given i.i.d.</s> <s>samples.</s> <s>while there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class.</s> <s>in contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds.</s> <s>presence of these structural properties makes the graph class hard to learn.</s> <s>we derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously.</s> <s>we also extend our framework to the random graph setting and derive corollaries for erdos-renyi graphs in a certain dense setting.</s></p></d>", "label": ["<d><p><s>on the information theoretic limits of learning ising models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a multimodal retrieval procedure based on latent feature models.</s> <s>the procedure consists of a nonparametric bayesian framework for learning underlying semantically meaningful abstract features in a multimodal dataset, a probabilistic retrieval model that allows cross-modal queries and an extension model for relevance feedback.</s> <s>experiments on two multimodal datasets, pascal-sentence and sun-attribute, demonstrate the effectiveness of the proposed retrieval procedure in comparison to the state-of-the-art algorithms for learning binary codes.</s></p></d>", "label": ["<d><p><s>a probabilistic framework for multimodal retrieval using integrative indian buffet process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics.</s> <s>while several divergence estimators exist, relatively few have known convergence properties.</s> <s>in particular, even for those estimators whose mse convergence rates are known, the asymptotic distributions are unknown.</s> <s>we establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples.</s> <s>this estimator has mse convergence rate of o(1/t), is simple to implement, and performs well in high dimensions.</s> <s>this theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples.</s> <s>we experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.</s></p></d>", "label": ["<d><p><s>multivariate f-divergence estimation with confidence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many machine learning problems can be reduced to the maximization of submodular functions.</s> <s>although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions.</s> <s>the optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by buchbinder et al.</s> <s>and follows a strongly serial double-greedy logic and program analysis.</s> <s>in this work, we propose two methods to parallelize the double-greedy algorithm.</s> <s>the first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee.</s> <s>the second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism.</s> <s>as a consequence we explore the trade off space between guaranteed performance and objective optimality.</s> <s>we implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.</s></p></d>", "label": ["<d><p><s>parallel double greedy submodular maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>submodular optimization has found many applications in machine learning and beyond.</s> <s>we carry out the first systematic investigation of inference in probabilistic models defined through submodular functions, generalizing regular pairwise mrfs and determinantal point processes.</s> <s>in particular, we present l-field, a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients.</s> <s>we obtain both lower and upper bounds on the log-partition function, which enables us to compute probability intervals for marginals, conditionals and marginal likelihoods.</s> <s>we also obtain fully factorized approximate posteriors, at the same computational cost as ordinary submodular optimization.</s> <s>our framework results in convex problems for optimizing over differentials of submodular functions, which we show how to optimally solve.</s> <s>we provide theoretical guarantees of the approximation quality with respect to the curvature of the function.</s> <s>we further establish natural relations between our variational approach and the classical mean-field method.</s> <s>lastly, we empirically demonstrate the accuracy of our inference scheme on several submodular models.</s></p></d>", "label": ["<d><p><s>from map to marginals: variational inference in bayesian submodular models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the problem of stochastic network design in bidirected trees.</s> <s>in this problem, an underlying phenomenon (e.g., a behavior, rumor, or disease) starts at multiple sources in a tree and spreads in both directions along its edges.</s> <s>actions can be taken to increase the probability of propagation on edges, and the goal is to maximize the total amount of spread away from all sources.</s> <s>our main result is a rounded dynamic programming approach that leads to a fully polynomial-time approximation scheme (fptas), that is, an algorithm that can find (1??</s> <s>)-optimal solutions for any problem instance in time polynomial in the input size and 1/?.</s> <s>our algorithm outperforms competing approaches on a motivating problem from computational sustainability to remove barriers in river networks to restore the health of aquatic ecosystems.</s></p></d>", "label": ["<d><p><s>stochastic network design in bidirected trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a model-based excessive gap technique to analyze first-order primal- dual methods for constrained convex minimization.</s> <s>as a result, we construct first- order primal-dual methods with optimal convergence rates on the primal objec- tive residual and the primal feasibility gap of their iterates separately.</s> <s>through a dual smoothing and prox-center selection strategy, our framework subsumes the augmented lagrangian, alternating direction, and dual fast-gradient methods as special cases, where our rates apply.</s></p></d>", "label": ["<d><p><s>constrained convex minimization via model-based excessive gap</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>branch-and-bound is a widely used method in combinatorial optimization, including mixed integer programming, structured prediction and map inference.</s> <s>while most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree.</s> <s>we address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch-and-bound.</s> <s>our strategies are learned by imitation learning.</s> <s>we apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (mip).</s> <s>we compare our method with one of the fastest open-source solvers, scip; and a very efficient commercial solver, gurobi.</s> <s>we demonstrate that our approach achieves better solutions faster on four mip libraries.</s></p></d>", "label": ["<d><p><s>learning to search in branch and bound algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep learning has been a long standing pursuit in machine learning, which until recently was hampered by unreliable training methods before the discovery of improved heuristics for embedded layer training.</s> <s>a complementary research strategy is to develop alternative modeling architectures that admit efficient training methods while expanding the range of representable structures toward deep models.</s> <s>in this paper, we develop a new architecture for nested nonlinearities that allows arbitrarily deep compositions to be trained to global optimality.</s> <s>the approach admits both parametric and nonparametric forms through the use of normalized kernels to represent each latent layer.</s> <s>the outcome is a fully convex formulation that is able to capture compositions of trainable nonlinear layers to arbitrary depth.</s></p></d>", "label": ["<d><p><s>convex deep learning via normalized kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing.</s> <s>in this problem, the inverse of a covariance matrix of a multivariate normal distribution is estimated, assuming that it is sparse.</s> <s>an $\\ell_1$ regularized log-determinant optimization problem is typically solved to approximate such matrices.</s> <s>because of memory limitations, most existing algorithms are unable to handle large scale instances of this problem.</s> <s>in this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets.</s> <s>our method treats the sought matrix block-by-block using quadratic approximations, and we show that this approach has advantages over existing methods in several aspects.</s> <s>numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods, especially for large-scale problems.</s></p></d>", "label": ["<d><p><s>a block-coordinate descent approach for large-scale sparse inverse covariance estimation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>lifted inference algorithms for probabilistic first-order logic frameworks such as markov logic networks (mlns) have received significant attention in recent years.</s> <s>these algorithms use so called lifting rules to identify symmetries in the first-order representation and reduce the inference problem over a large probabilistic model to an inference problem over a much smaller model.</s> <s>in this paper, we present two new lifting rules, which enable fast map inference in a large class of mlns.</s> <s>our first rule uses the concept of single occurrence equivalence class of logical variables, which we define in the paper.</s> <s>the rule states that the map assignment over an mln can be recovered from a much smaller mln, in which each logical variable in each single occurrence equivalence class is replaced by a constant (i.e., an object in the domain of the variable).</s> <s>our second rule states that we can safely remove a subset of formulas from the mln if all equivalence classes of variables in the remaining mln are single occurrence and all formulas in the subset are tautology (i.e., evaluate to true) at extremes (i.e., assignments with identical truth value for groundings of a predicate).</s> <s>we prove that our two new rules are sound and demonstrate via a detailed experimental evaluation that our approach is superior in terms of scalability and map solution quality to the state of the art approaches.</s></p></d>", "label": ["<d><p><s>new rules for domain independent lifted map inference</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper, we present a new approach for lifted map inference in markov logic networks (mlns).</s> <s>the key idea in our approach is to compactly encode the map inference problem as an integer polynomial program (ipp) by schematically applying three lifted inference steps to the mln: lifted decomposition, lifted conditioning, and partial grounding.</s> <s>our ipp encoding is lifted in the sense that an integer assignment to a variable in the ipp may represent a truth-assignment to multiple indistinguishable ground atoms in the mln.</s> <s>we show how to solve the ipp by first converting it to an integer linear program (ilp) and then solving the latter using state-of-the-art ilp techniques.</s> <s>experiments on several benchmark mlns show that our new algorithm is substantially superior to ground inference and existing methods in terms of computational efficiency and solution quality.</s></p></d>", "label": ["<d><p><s>an integer polynomial programming based framework for lifted map inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the jacobi metric introduced in mathematical physics can be used to analyze hamiltonian monte carlo (hmc).</s> <s>in a geometrical setting, each step of hmc corresponds to a geodesic on a riemannian manifold with a jacobi metric.</s> <s>our calculation of the sectional curvature of this hmc manifold allows us to see that it is positive in cases such as sampling from a high dimensional multivariate gaussian.</s> <s>we show that positive curvature can be used to prove theoretical concentration results for hmc markov chains.</s></p></d>", "label": ["<d><p><s>positive curvature and hamiltonian monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayes-adaptive planning offers a principled solution to the exploration-exploitation trade-off under model uncertainty.</s> <s>it finds the optimal policy in belief space, which explicitly accounts for the expected effect on future rewards of reductions in uncertainty.</s> <s>however, the bayes-adaptive solution is typically intractable in domains with large or continuous state spaces.</s> <s>we present a tractable method for approximating the bayes-adaptive solution by combining simulation-based search with a novel value function approximation technique that generalises over belief space.</s> <s>our method outperforms prior approaches in both discrete bandit tasks and simple continuous navigation and control tasks.</s></p></d>", "label": ["<d><p><s>bayes-adaptive simulation-based search with value function approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks.</s> <s>this paper proposes a theoretical explanation for this phenomenon: we show that, under a generative poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization.</s> <s>dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set.</s> <s>we also show that, under similar conditions, dropout preserves the bayes decision boundary and should therefore induce minimal bias in high dimensions.</s></p></d>", "label": ["<d><p><s>altitude training: strong bounds for single-layer dropout</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability.</s> <s>while various methods have been proposed to speed up their convergence, the model selection phase is often ignored.</s> <s>in fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach.</s> <s>in this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation.</s> <s>the algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way.</s> <s>optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.</s></p></d>", "label": ["<d><p><s>simultaneous model selection and optimization through parameter-free stochastic learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study consistency properties of algorithms for non-decomposable performance measures that cannot be expressed as a sum of losses on individual data points, such as the f-measure used in text retrieval and several other performance measures used in class imbalanced settings.</s> <s>while there has been much work on designing algorithms for such performance measures, there is limited understanding of the theoretical properties of these algorithms.</s> <s>recently, ye et al.</s> <s>(2012) showed consistency results for two algorithms that optimize the f-measure, but their results apply only to an idealized setting, where precise knowledge of the underlying probability distribution (in the form of the `true' posterior class probability) is available to a learning algorithm.</s> <s>in this work, we consider plug-in algorithms that learn a classifier by applying an empirically determined threshold to a suitable `estimate' of the class probability, and provide a general methodology to show consistency of these methods for any non-decomposable measure that can be expressed as a continuous function of true positive rate (tpr) and true negative rate (tnr), and for which the bayes optimal classifier is the class probability function thresholded suitably.</s> <s>we use this template to derive consistency results for plug-in algorithms for the f-measure and for the geometric mean of tpr and precision; to our knowledge, these are the first such results for these measures.</s> <s>in addition, for continuous distributions, we show consistency of plug-in algorithms for any performance measure that is a continuous and monotonically increasing function of tpr and tnr.</s> <s>experimental results confirm our theoretical findings.</s></p></d>", "label": ["<d><p><s>on the statistical consistency of plug-in classifiers for non-decomposable performance measures</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we analyse a plug-in estimator for a large class of integral functionals of one or more continuous probability densities.</s> <s>this class includes important families of entropy, divergence, mutual information, and their conditional versions.</s> <s>for densities on the d-dimensional unit cube [0,1]^d that lie in a beta-holder smoothness class, we prove our estimator converges at the rate o(n^(1/(beta+d))).</s> <s>furthermore, we prove that the estimator obeys an exponential concentration inequality about its mean, whereas most previous related results have bounded only expected error of estimators.</s> <s>finally, we demonstrate our bounds to the case of conditional renyi mutual information.</s></p></d>", "label": ["<d><p><s>exponential concentration of a density functional estimator</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>diffusion-weighted magnetic resonance imaging (dwi) and fiber tractography are the only methods to measure the structure of the white matter in the living human brain.</s> <s>the diffusion signal has been modelled as the combined contribution from many individual fascicles of nerve fibers passing through each location in the white matter.</s> <s>typically, this is done via basis pursuit, but estimation of the exact directions is limited due to discretization.</s> <s>the difficulties inherent in modeling dwi data are shared by many other problems involving fitting non-parametric mixture models.</s> <s>ekanadaham et al.</s> <s>proposed an approach, continuous basis pursuit, to overcome discretization error in the 1-dimensional case (e.g., spike-sorting).</s> <s>here, we propose a more general algorithm that fits mixture models of any dimensionality without discretization.</s> <s>our algorithm uses the principles of l2-boost, together with refitting of the weights and pruning of the parameters.</s> <s>the addition of these steps to l2-boost both accelerates the algorithm and assures its accuracy.</s> <s>we refer to the resulting algorithm as elastic basis pursuit, or ebp, since it expands and contracts the active set of kernels as needed.</s> <s>we show that in contrast to existing approaches to fitting mixtures, our boosting framework (1) enables the selection of the optimal bias-variance tradeoff along the solution path, and (2) scales with high-dimensional problems.</s> <s>in simulations of dwi, we find that ebp yields better parameter estimates than a non-negative least squares (nnls) approach, or the standard model used in dwi, the tensor model, which serves as the basis for diffusion tensor imaging (dti).</s> <s>we demonstrate the utility of the method in dwi data acquired in parts of the brain containing crossings of multiple fascicles of nerve fibers.</s></p></d>", "label": ["<d><p><s>deconvolution of high dimensional mixtures via boosting, with application to diffusion-weighted mri of human brain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new bayesian formulation is developed for nonlinear support vector machines (svms), based on a gaussian process and with the svm hinge loss expressed as a scaled mixture of normals.</s> <s>we then integrate the bayesian svm into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier.</s> <s>inference is performed with expectation conditional maximization (ecm) and markov chain monte carlo (mcmc).</s> <s>an extensive set of experiments demonstrate the utility of using a nonlinear bayesian svm within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability</s></p></d>", "label": ["<d><p><s>bayesian nonlinear support vector machines and discriminative factor modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a theoretical analysis of f-measures for binary, multiclass and multilabel classification.</s> <s>these performance measures are non-linear, but in many scenarios they are pseudo-linear functions of the per-class false negative/false positive rate.</s> <s>based on this observation, we present a general reduction of f-measure maximization to cost-sensitive classification with unknown costs.</s> <s>we then propose an algorithm with provable guarantees to obtain an approximately optimal classifier for the f-measure by solving a series of cost-sensitive classification problems.</s> <s>the strength of our analysis is to be valid on any dataset and any class of classifiers, extending the existing theoretical results on f-measures, which are asymptotic in nature.</s> <s>we present numerical experiments to illustrate the relative importance of cost asymmetry and thresholding when learning linear classifiers on various f-measure optimization tasks.</s></p></d>", "label": ["<d><p><s>optimizing f-measures by cost-sensitive classification</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications.</s> <s>in this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data.</s> <s>we then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss.</s> <s>we next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset).</s> <s>finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than $2\\sqrt{2}$ times the fully supervised case.</s> <s>these theoretical findings are also validated through experiments.</s></p></d>", "label": ["<d><p><s>analysis of learning from positive and unlabeled data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection.</s> <s>the core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them.</s> <s>we investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries.</s> <s>in particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words).</s> <s>we offer a simple heuristic method for making learning more robust to feature cross-substitution attacks.</s> <s>we then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model.</s> <s>our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion.</s> <s>we show that our algorithmic approach significantly outperforms state-of-the-art alternatives.</s></p></d>", "label": ["<d><p><s>feature cross-substitution in adversarial classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the convex polytope machine (cpm), a novel non-linear learning algorithm for large-scale binary classification tasks.</s> <s>the cpm finds a large margin convex polytope separator which encloses one class.</s> <s>we develop a stochastic gradient descent based algorithm that is amenable to massive datasets, and augment it with a heuristic procedure to avoid sub-optimal local minima.</s> <s>our experimental evaluations of the cpm on large-scale datasets from distinct domains (mnist handwritten digit recognition, text topic, and web security) demonstrate that the cpm trains models faster, sometimes several orders of magnitude, than state-of-the-art similar approaches and kernel-svm methods while achieving comparable or better classification performance.</s> <s>our empirical results suggest that, unlike prior similar approaches, we do not need to control the number of sub-classifiers (sides of the polytope) to avoid overfitting.</s></p></d>", "label": ["<d><p><s>large-margin convex polytope machine</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>by exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area.</s> <s>using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms.</s></p></d>", "label": ["<d><p><s>a boosting framework on grounds of online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an algorithm for learning fast multiclass object detection cascades is introduced.</s> <s>it produces multi-resolution (mres) cascades, whose early stages are binary target vs. non-target detectors that eliminate false positives, late stages multiclass classifiers that finely discriminate target classes, and middle stages have intermediate numbers of classes, determined in a data-driven manner.</s> <s>this mres structure is achieved with a new structurally biased boosting algorithm (sbboost).</s> <s>sbbost extends previous multiclass boosting approaches, whose boosting mechanisms are shown to implement two complementary data-driven biases: 1) the standard bias towards examples difficult to classify, and 2) a bias towards difficult classes.</s> <s>it is shown that structural biases can be implemented by generalizing this class-based bias, so as to encourage the desired mres structure.</s> <s>this is accomplished through a generalized definition of multiclass margin, which includes a set of bias parameters.</s> <s>sbboost is a boosting algorithm for maximization of this margin.</s> <s>it can also be interpreted as standard multiclass boosting algorithm augmented with margin thresholds or a cost-sensitive boosting algorithm with costs defined by the bias parameters.</s> <s>a stage adaptive bias policy is then introduced to determine bias parameters in a data driven manner.</s> <s>this is shown to produce mres cascades that have high detection rate and are computationally efficient.</s> <s>experiments on multiclass object detection show improved performance over previous solutions.</s></p></d>", "label": ["<d><p><s>multi-resolution cascades for multiclass object detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present new ensemble learning algorithms for multi-class classification.</s> <s>our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees.</s> <s>we give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family.</s> <s>these bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the rademacher complexities based on the ensemble?s mixture weights.</s> <s>we introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the h-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of adaboost and logistic regression and their l1-regularized counterparts.</s></p></d>", "label": ["<d><p><s>multi-class deep boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider logistic regression with arbitrary outliers in the covariate matrix.</s> <s>we propose a new robust logistic regression algorithm, called rolr, that estimates the parameter through a simple linear programming procedure.</s> <s>we prove that rolr is robust to a constant fraction of adversarial outliers.</s> <s>to the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees.</s> <s>besides regression, we apply rolr to solving binary classification problems where a fraction of training samples are corrupted.</s></p></d>", "label": ["<d><p><s>robust logistic regression and classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the indian buffet process is a versatile statistical tool for modeling distributions over binary matrices.</s> <s>we provide an efficient spectral algorithm as an alternative to costly variational bayes and sampling-based algorithms.</s> <s>we derive a novel tensorial characterization of the moments of the indian buffet process proper and for two of its applications.</s> <s>we give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees.</s> <s>our algorithm provides superior accuracy and cheaper computation than comparable variational bayesian approach on a number of reference problems.</s></p></d>", "label": ["<d><p><s>spectral methods for indian buffet process inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>supervised topic models simultaneously model the latent topic structure of large collections of documents and a response variable associated with each document.</s> <s>existing inference methods are based on either variational approximation or monte carlo sampling.</s> <s>this paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent dirichlet allocation (slda) models.</s> <s>the spectral-slda algorithm is provably correct and computationally efficient.</s> <s>we prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of slda.</s> <s>thorough experiments on a diverse range of synthetic and real-world datasets verify the theory and demonstrate the practical effectiveness of the algorithm.</s></p></d>", "label": ["<d><p><s>spectral methods for supervised topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a learning approach for the mixture of hidden markov models (mhmm) based on the method of moments (mom).</s> <s>computational advantages of mom make mhmm learning amenable for large data sets.</s> <s>it is not possible to directly learn an mhmm with existing learning approaches, mainly due to a permutation ambiguity in the estimation process.</s> <s>we show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise.</s> <s>we demonstrate the validity of our approach on synthetic and real data.</s></p></d>", "label": ["<d><p><s>spectral learning of mixture of hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing the $k$ dominant eigenvalues and eigenvectors of massive graphs is a key operation in numerous machine learning applications; however, popular solvers suffer from slow convergence, especially when $k$ is reasonably large.</s> <s>in this paper, we propose and analyze a novel multi-scale spectral decomposition method (mseigs), which first clusters the graph into smaller clusters whose spectral decomposition can be computed efficiently and independently.</s> <s>we show theoretically as well as empirically that the union of all cluster's subspaces has significant overlap with the dominant subspace of the original graph, provided that the graph is clustered appropriately.</s> <s>thus, eigenvectors of the clusters serve as good initializations to a block lanczos algorithm that is used to compute spectral decomposition of the original graph.</s> <s>we further use hierarchical clustering to speed up the computation and adopt a fast early termination strategy to compute quality approximations.</s> <s>our method outperforms widely used solvers in terms of convergence speed and approximation quality.</s> <s>furthermore, our method is naturally parallelizable and exhibits significant speedups in shared-memory parallel settings.</s> <s>for example, on a graph with more than 82 million nodes and 3.6 billion edges, mseigs takes less than 3 hours on a single-core machine while randomized svd takes more than 6 hours, to obtain a similar approximation of the top-50 eigenvectors.</s> <s>using 16 cores, we can reduce this time to less than 40 minutes.</s></p></d>", "label": ["<d><p><s>multi-scale spectral decomposition of massive graphs</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g.</s> <s>the adjacency or the laplacian.</s> <s>recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model.</s> <s>here, we propose to use instead a simpler object, a symmetric real matrix known as the bethe hessian operator, or deformed laplacian.</s> <s>we show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.</s></p></d>", "label": ["<d><p><s>spectral clustering of graphs with the bethe hessian</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consistently matching keypoints across images, and the related problem of finding clusters of nearby images, are critical components of various tasks in computer vision, including structure from motion (sfm).</s> <s>unfortunately, occlusion and large repetitive structures tend to mislead most currently used matching algorithms, leading to characteristic pathologies in the final output.</s> <s>in this paper we introduce a new method, permutations diffusion maps (pdm), to solve the matching problem, as well as a related new affinity measure, derived using ideas from harmonic analysis on the symmetric group.</s> <s>we show that just by using it as a preprocessing step to existing sfm pipelines, pdm can greatly improve reconstruction quality on difficult datasets.</s></p></d>", "label": ["<d><p><s>permutation diffusion maps (pdm) with application to the image association problem in computer vision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many single-channel signal decomposition techniques rely on a low-rank factorization of a time-frequency transform.</s> <s>in particular, nonnegative matrix factorization (nmf) of the spectrogram -- the (power) magnitude of the short-time fourier transform (stft) -- has been considered in many audio applications.</s> <s>in this setting, nmf with the itakura-saito divergence was shown to underly a generative gaussian composite model (gcm) of the stft, a step forward from more empirical approaches based on ad-hoc transform and divergence specifications.</s> <s>still, the gcm is not yet a generative model of the raw signal itself, but only of its stft.</s> <s>the work presented in this paper fills in this ultimate gap by proposing a novel signal synthesis model with low-rank time-frequency structure.</s> <s>in particular, our new approach opens doors to multi-resolution representations, that were not possible in the traditional nmf setting.</s> <s>we describe two expectation-maximization algorithms for estimation in the new model and report audio signal processing results with music decomposition and speech enhancement.</s></p></d>", "label": ["<d><p><s>low-rank time-frequency synthesis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans are able to segregate auditory objects in a complex acoustic scene, through an interplay of bottom-up feature extraction and top-down selective attention in the brain.</s> <s>the detailed mechanism underlying this process is largely unknown and the ability to mimic this procedure is an important problem in artificial intelligence and computational neuroscience.</s> <s>we consider the problem of decoding the attentional state of a listener in a competing-speaker environment from magnetoencephalographic (meg) recordings from the human brain.</s> <s>we develop a behaviorally inspired state-space model to account for the modulation of the meg with respect to attentional state of the listener.</s> <s>we construct a decoder based on the maximum a posteriori (map) estimate of the state parameters via the expectation-maximization (em) algorithm.</s> <s>the resulting decoder is able to track the attentional modulation of the listener with multi-second resolution using only the envelopes of the two speech streams as covariates.</s> <s>we present simulation studies as well as application to real meg data from two human subjects.</s> <s>our results reveal that the proposed decoder provides substantial gains in terms of temporal resolution, complexity, and decoding accuracy.</s></p></d>", "label": ["<d><p><s>a state-space model for decoding auditory attentional modulation from meg in a competing-speaker environment</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map.</s> <s>in contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full svd; nor (b) resort to augmented lagrangian techniques; nor (c) solve linear systems per iteration.</s> <s>instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost.</s> <s>numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.</s></p></d>", "label": ["<d><p><s>efficient structured matrix rank minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several problems such as network intrusion, community detection, and disease outbreak can be described by observations attributed to nodes or edges of a graph.</s> <s>in these applications presence of intrusion, community or disease outbreak is characterized by novel observations on some unknown connected subgraph.</s> <s>these problems can be formulated in terms of optimization of suitable objectives on connected subgraphs, a problem which is generally computationally difficult.</s> <s>we overcome the combinatorics of connectivity by embedding connected subgraphs into linear matrix inequalities (lmi).</s> <s>computationally efficient tests are then realized by optimizing convex objective functions subject to these lmi constraints.</s> <s>we prove, by means of a novel euclidean embedding argument, that our tests are minimax optimal for exponential family of distributions on 1-d and 2-d lattices.</s> <s>we show that internal conductance of the connected subgraph family plays a fundamental role in characterizing detectability.</s></p></d>", "label": ["<d><p><s>efficient minimax signal detection on graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources.</s> <s>we introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (sacs).</s> <s>sacs encourage the total signal for each of the unknown sources to be close to a specified value.</s> <s>this is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect.</s> <s>we incorporate sacs into an additive factorial hidden markov model (afhmm) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed.</s> <s>a convex quadratic program for approximate inference is employed for recovering those source signals.</s> <s>on a real-world energy disaggregation data set, we show that the use of sacs dramatically improves the original afhmm, and significantly improves over a recent state-of-the art approach.</s></p></d>", "label": ["<d><p><s>signal aggregate constraints in additive factorial hmms, with application to energy disaggregation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study an online learning setting where the player is temporarily deprived of feedback each time it switches to a different action.</s> <s>such model of \\emph{adaptive feedback} naturally occurs in scenarios where the environment reacts to the player's actions and requires some time to recover and stabilize after the algorithm switches actions.</s> <s>this motivates a variant of the multi-armed bandit problem, which we call the \\emph{blinded multi-armed bandit}, in which no feedback is given to the algorithm whenever it switches arms.</s> <s>we develop efficient online learning algorithms for this problem and prove that they guarantee the same asymptotic regret as the optimal algorithms for the standard multi-armed bandit problem.</s> <s>this result stands in stark contrast to another recent result, which states that adding a switching cost to the standard multi-armed bandit makes it substantially harder to learn, and provides a direct comparison of how feedback and loss contribute to the difficulty of an online learning problem.</s> <s>we also extend our results to the general prediction framework of bandit linear optimization, again attaining near-optimal regret bounds.</s></p></d>", "label": ["<d><p><s>the blinded bandit: learning with adaptive feedback</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees.</s> <s>we complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal.</s> <s>our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds.</s> <s>some encouraging empirical results are also presented.</s></p></d>", "label": ["<d><p><s>near-optimal sample compression for nearest neighbors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it was recently proved using graph covers (ruozzi, 2012) that the bethe partition function is upper bounded by the true partition function for a binary pairwise model that is attractive.</s> <s>here we provide a new, arguably simpler proof from first principles.</s> <s>we make use of the idea of clamping a variable to a particular value.</s> <s>for an attractive model, we show that summing over the bethe partition functions for each sub-model obtained after clamping any variable can only raise (and hence improve) the approximation.</s> <s>in fact, we derive a stronger result that may have other useful implications.</s> <s>repeatedly clamping until we obtain a model with no cycles, where the bethe approximation is exact, yields the result.</s> <s>we also provide a related lower bound on a broad class of approximate partition functions of general pairwise multi-label models that depends only on the topology.</s> <s>we demonstrate that clamping a few wisely chosen variables can be of practical value by dramatically reducing approximation error.</s></p></d>", "label": ["<d><p><s>clamping variables and approximate inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work presents novel algorithms for learning bayesian networks of bounded treewidth.</s> <s>both exact and approximate methods are developed.</s> <s>the exact method combines mixed integer linear programming formulations for structure learning and treewidth computation.</s> <s>the approximate method consists in sampling k-trees (maximal graphs of treewidth k), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that k-tree.</s> <s>the approaches are empirically compared to each other and to state-of-the-art methods on a collection of public data sets with up to 100 variables.</s></p></d>", "label": ["<d><p><s>advances in learning bayesian networks of bounded treewidth</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>implementing inference procedures for each new probabilistic model is time-consuming and error-prone.</s> <s>probabilistic programming addresses this problem by allowing a user to specify the model and then automatically generating the inference procedure.</s> <s>to make this practical it is important to generate high performance inference code.</s> <s>in turn, on modern architectures, high performance requires parallel execution.</s> <s>in this paper we present augur, a probabilistic modeling language and compiler for bayesian networks designed to make effective use of data-parallel architectures such as gpus.</s> <s>we show that the compiler can generate data-parallel inference code scalable to thousands of gpu cores by making use of the conditional independence relationships in the bayesian network.</s></p></d>", "label": ["<d><p><s>augur: data-parallel probabilistic modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>computing the partition function (i.e., the normalizing constant) of a given pairwise binary graphical model is np-hard in general.</s> <s>as a result, the partition function is typically estimated by approximate inference algorithms such as belief propagation (bp) and tree-reweighted belief propagation (trbp).</s> <s>the former provides reasonable estimates in practice but has convergence issues.</s> <s>the later has better convergence properties but typically provides poorer estimates.</s> <s>in this work, we propose a novel scheme that has better convergence properties than bp and provably provides better partition function estimates in many instances than trbp.</s> <s>in particular, given an arbitrary pairwise binary graphical model, we construct a specific ``attractive'' 2-cover.</s> <s>we explore the properties of this special cover and show that it can be used to construct an algorithm with the desired properties.</s></p></d>", "label": ["<d><p><s>making pairwise binary graphical models attractive</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the following problem: given samples from a high dimensional discrete distribution, we want to estimate the leading $(\\delta,\\rho)$-modes of the underlying distributions.</s> <s>a point is defined to be a $(\\delta,\\rho)$-mode if it is a local optimum of the density within a $\\delta$-neighborhood under metric $\\rho$.</s> <s>as we increase the ``scale'' parameter $\\delta$, the neighborhood size increases and the total number of modes monotonically decreases.</s> <s>the sequence of the $(\\delta,\\rho)$-modes reveal intrinsic topographical information of the underlying distributions.</s> <s>though the mode finding problem is generally intractable in high dimensions, this paper unveils that, if the distribution can be approximated well by a tree graphical model, mode characterization is significantly easier.</s> <s>an efficient algorithm with provable theoretical guarantees is proposed and is applied to applications like data analysis and multiple predictions.</s></p></d>", "label": ["<d><p><s>mode estimation for high dimensional discrete tree graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a model where the rate of an inhomogeneous poisson process is modified by a chinese restaurant process.</s> <s>applying a mcmc sampler to this model allows us to do posterior bayesian inference about the number of states in poisson-like data.</s> <s>our sampler is shown to get accurate results for synthetic data and we apply it to v1 neuron spike data to find discrete firing rate states depending on the orientation of a stimulus.</s></p></d>", "label": ["<d><p><s>poisson process jumping between an unknown number of rates: application to neural spike data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we examine the number of controlled experiments required to discover a causal graph.</s> <s>hauser and buhlmann showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph.</s> <s>their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments.</s> <s>we show that significant improvements are possible with the aid of randomization ?</s> <s>in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most o(log log n) experiments in expectation.</s> <s>this bound cannot be improved; we show it is tight for some causal graphs.</s> <s>we then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a erd?s-r?nyi model then the expected number of experiments to discover the causal graph is constant.</s> <s>finally, we present computer simulations to complement our theoretic results.</s> <s>our work exploits a structural characterization of essential graphs by andersson et al.</s> <s>their characterization is based upon a set of orientation forcing operations.</s> <s>our results show a distinction between which forcing operations are most important in worst-case and average-case settings.</s></p></d>", "label": ["<d><p><s>randomized experimental design for causal graph discovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the problem of $mz$-transportability, that is, transferring causal knowledge collected in several heterogeneous domains to a target domain in which only passive observations and limited experimental data can be collected.</s> <s>the paper first establishes a necessary and sufficient condition for deciding the feasibility of $mz$-transportability, i.e., whether causal effects in the target domain are estimable from the information available.</s> <s>it further proves that a previously established algorithm for computing transport formula is in fact complete, that is, failure of the algorithm implies non-existence of a transport formula.</s> <s>finally, the paper shows that the do-calculus is complete for the $mz$-transportability class.</s></p></d>", "label": ["<d><p><s>transportability from multiple environments with limited experiments: completeness results</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents.</s> <s>in our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss.</s> <s>this suggests a general framework for the design and analysis of new social choice mechanisms.</s> <s>we compare bayesian estimators, which minimize bayesian expected loss, for the mallows model and the condorcet model respectively, and the kemeny rule.</s> <s>we consider various normative properties, in addition to computational complexity and asymptotic behavior.</s> <s>in particular, we show that the bayesian estimator for the condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the condorcet model for some ground truth parameter.</s></p></d>", "label": ["<d><p><s>a statistical decision-theoretic framework for social choice</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>performing interventions is a major challenge in economic policy-making.</s> <s>we propose \\emph{causal strategic inference} as a framework for conducting interventions and apply it to large, networked microfinance economies.</s> <s>the basic solution platform consists of modeling a microfinance market as a networked economy, learning the parameters of the model from the real-world microfinance data, and designing algorithms for various computational problems in question.</s> <s>we adopt nash equilibrium as the solution concept for our model.</s> <s>for a special case of our model, we show that an equilibrium point always exists and that the equilibrium interest rates are unique.</s> <s>for the general case, we give a constructive proof of the existence of an equilibrium point.</s> <s>our empirical study is based on the microfinance data from bangladesh and bolivia, which we use to first learn our models.</s> <s>we show that causal strategic inference can assist policy-makers by evaluating the outcomes of various types of interventions, such as removing a loss-making bank from the market, imposing an interest rate cap, and subsidizing banks.</s></p></d>", "label": ["<d><p><s>causal strategic inference in networked microfinance economies</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>predicting depth is an essential component in understanding the 3d geometry of a scene.</s> <s>while for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues.</s> <s>moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale.</s> <s>in this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally.</s> <s>we also apply a scale-invariant error to help measure depth relations rather than scale.</s> <s>by leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both nyu depth and kitti, and matches detailed depth boundaries without the need for superpixelation.</s></p></d>", "label": ["<d><p><s>depth map prediction from a single image using a multi-scale deep network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>previous theoretical and experimental work on optimal decision-making was restricted to the artificial setting of a reliability of the momentary sensory evidence that remained constant within single trials.</s> <s>the work presented here describes the computation and characterization of optimal decision-making in the more realistic case of an evidence reliability that varies across time even within a trial.</s> <s>it shows that, in this case, the optimal behavior is determined by a bound in the decision maker's belief that depends only on the current, but not the past, reliability.</s> <s>we furthermore demonstrate that simpler heuristics fail to match the optimal performance for certain characteristics of the process that determines the time-course of this reliability, causing a drop in reward rate by more than 50%.</s></p></d>", "label": ["<d><p><s>optimal decision-making with time-varying evidence reliability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli.</s> <s>recent work finds that people?s category judgments are guided by a small set of examples that are retrieved from memory at decision time.</s> <s>this limited and stochastic retrieval places limits on human performance for probabilistic classification decisions.</s> <s>in light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items.</s> <s>one shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion.</s> <s>in this contribution, we take a first principles approach to constructing idealized training sets.</s> <s>we apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are).</s> <s>as predicted, we find that the machine teacher recommends idealized training sets.</s> <s>we also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model.</s> <s>as predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective.</s> <s>our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.</s></p></d>", "label": ["<d><p><s>optimal teaching for limited-capacity human learners</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels.</s> <s>we present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution.</s> <s>like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size.</s> <s>while the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies.</s> <s>we evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.</s></p></d>", "label": ["<d><p><s>recurrent models of visual attention</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning in recurrent neural networks has been a topic fraught with difficulties and problems.</s> <s>we here report substantial progress in the unsupervised learning of recurrent networks that can keep track of an input signal.</s> <s>specifically, we show how these networks can learn to efficiently represent their present and past inputs, based on local learning rules only.</s> <s>our results are based on several key insights.</s> <s>first, we develop a local learning rule for the recurrent weights whose main aim is to drive the network into a regime where, on average, feedforward signal inputs are canceled by recurrent inputs.</s> <s>we show that this learning rule minimizes a cost function.</s> <s>second, we develop a local learning rule for the feedforward weights that, based on networks in which recurrent inputs already predict feedforward inputs, further minimizes the cost.</s> <s>third, we show how the learning rules can be modified such that the network can directly encode non-whitened inputs.</s> <s>fourth, we show that these learning rules can also be applied to a network that feeds a time-delayed version of the network output back into itself.</s> <s>as a consequence, the network starts to efficiently represent both its signal inputs and their history.</s> <s>we develop our main theory for linear networks, but then sketch how the learning rules could be transferred to balanced, spiking networks.</s></p></d>", "label": ["<d><p><s>unsupervised learning of an efficient short-term memory network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks.</s> <s>these models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and internet-scale clusters problematic.</s> <s>the computation is dominated by the convolution operations in the lower layers of the model.</s> <s>we exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation.</s> <s>using large state-of-the-art models, we demonstrate speedups of convolutional layers on both cpu and gpu by a factor of 2?, while keeping the accuracy within 1% of the original model.</s></p></d>", "label": ["<d><p><s>exploiting linear structure within convolutional networks for efficient evaluation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the classification of high-dimensional data defined on graphs is particularly difficult when the graph geometry is unknown.</s> <s>we introduce a haar scattering transform on graphs, which computes invariant signal descriptors.</s> <s>it is implemented with a deep cascade of additions, subtractions and absolute values, which iteratively compute orthogonal haar wavelet transforms.</s> <s>multiscale neighborhoods of unknown graphs are estimated by minimizing an average total variation, with a pair matching algorithm of polynomial complexity.</s> <s>supervised classification with dimension reduction is tested on data bases of scrambled images, and for signals sampled on unknown irregular grids on a sphere.</s></p></d>", "label": ["<d><p><s>unsupervised deep haar scattering on graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>various factors, such as identities, views (poses), and illuminations, are coupled in face images.</s> <s>disentangling the identity and view representations is a major challenge in face recognition.</s> <s>existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy.</s> <s>this is different from the behavior of human brain.</s> <s>intriguingly, even without accessing 3d data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2d image, making face perception in the brain robust to view changes.</s> <s>in this sense, human brain has learned and encoded 3d face models from 2d images.</s> <s>to take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (mvp), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2d face image.</s> <s>the identity features of mvp achieve superior performance on the multipie dataset.</s> <s>mvp is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.</s></p></d>", "label": ["<d><p><s>multi-view perceptron: a deep model for learning face identity and view representations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations.</s> <s>we present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance.</s> <s>in particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner.</s> <s>the first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks.</s> <s>an em-type method is then studied for the joint optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an mcmc-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables.</s> <s>extensive experiments demonstrate that our joint learning framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g., 1000 times faster than competing approaches).</s></p></d>", "label": ["<d><p><s>deep joint task learning for generic object extraction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training.</s> <s>in this paper we present an approach for training a convolutional neural network using only unlabeled data.</s> <s>we train the network to discriminate between a set of surrogate classes.</s> <s>each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch.</s> <s>we find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition.</s> <s>the feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (stl-10, cifar-10, caltech-101).</s></p></d>", "label": ["<d><p><s>discriminative unsupervised feature learning with convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1.</s> <s>to this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time.</s> <s>we also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the ?syntax?</s> <s>of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.</s></p></d>", "label": ["<d><p><s>modeling deep temporal dependencies with recurrent grammar cells\"\"</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>semantic matching is of central importance to many natural language tasks \\cite{bordes2014semantic,retrievalqa}.</s> <s>a successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them.</s> <s>as a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech.</s> <s>the proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels.</s> <s>our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages.</s> <s>the empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.</s></p></d>", "label": ["<d><p><s>convolutional neural network architectures for matching natural language sentences</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>recursive neural networks comprise a class of architecture that can operate on structured input.</s> <s>they have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations.</s> <s>even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks.</s> <s>in this work we introduce a new architecture --- a deep recursive neural network (deep rnn) --- constructed by stacking multiple recursive layers.</s> <s>we evaluate the proposed model on the task of fine-grained sentiment classification.</s> <s>our results show that deep rnns outperform associated shallow counterparts that employ the same number of parameters.</s> <s>furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative rnn variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results.</s> <s>we provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.</s></p></d>", "label": ["<d><p><s>deep recursive neural networks for compositionality in language</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient.</s> <s>the human mind faces the same challenge.</s> <s>therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa.</s> <s>here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection.</s> <s>we apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment.</s> <s>we find that people quickly learn to adaptively choose between cognitive strategies.</s> <s>people's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection.</s> <s>rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.</s></p></d>", "label": ["<d><p><s>algorithm selection by rational metareasoning as a model of human strategy selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian observer models are very effective in describing human performance in perceptual tasks, so much so that they are trusted to faithfully recover hidden mental representations of priors, likelihoods, or loss functions from the data.</s> <s>however, the intrinsic degeneracy of the bayesian framework, as multiple combinations of elements can yield empirically indistinguishable results, prompts the question of model identifiability.</s> <s>we propose a novel framework for a systematic testing of the identifiability of a significant class of bayesian observer models, with practical applications for improving experimental design.</s> <s>we examine the theoretical identifiability of the inferred internal representations in two case studies.</s> <s>first, we show which experimental designs work better to remove the underlying degeneracy in a time interval estimation task.</s> <s>second, we find that the reconstructed representations in a speed perception task under a slow-speed prior are fairly robust.</s></p></d>", "label": ["<d><p><s>a framework for testing identifiability of bayesian models of perception</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to master a discipline such as algebra or physics, students must acquire a set of cognitive skills.</s> <s>traditionally, educators and domain experts manually determine what these skills are and then select practice exercises to hone a particular skill.</s> <s>we propose a technique that uses student performance data to automatically discover the skills needed in a discipline.</s> <s>the technique assigns a latent skill to each exercise such that a student's expected accuracy on a sequence of same-skill exercises improves monotonically with practice.</s> <s>rather than discarding the skills identified by experts, our technique incorporates a nonparametric prior over the exercise-skill assignments that is based on the expert-provided skills and a weighted chinese restaurant process.</s> <s>we test our technique on datasets from five different intelligent tutoring systems designed for students ranging in age from middle school through college.</s> <s>we obtain two surprising results.</s> <s>first, in three of the five datasets, the skills inferred by our technique support significantly improved predictions of student performance over the expert-provided skills.</s> <s>second, the expert-provided skills have little value: our technique predicts student performance nearly as well when it ignores the domain expertise as when it attempts to leverage it.</s> <s>we discuss explanations for these surprising results and also the relationship of our skill-discovery technique to alternative approaches.</s></p></d>", "label": ["<d><p><s>automatic discovery of cognitive skills to improve the prediction of student learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to keep up with the big data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in markov random fields.</s> <s>despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected.</s> <s>in this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations.</s> <s>it only updates the high-order factors when passing messages across machines.</s> <s>we demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.</s></p></d>", "label": ["<d><p><s>message passing inference for large scale graphical models with high order potentials</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic variational inference (svi) uses stochastic optimization to scale up bayesian computation to massive data.</s> <s>we present an alternative perspective on svi as approximate parallel coordinate ascent.</s> <s>svi trades-off bias and variance to step close to the unknown true coordinate optimum given by batch variational bayes (vb).</s> <s>we define a model to automate this process.</s> <s>the model infers the location of the next vb optimum from a sequence of noisy realizations.</s> <s>as a consequence of this construction, we update the variational parameters using bayes rule, rather than a hand-crafted optimization schedule.</s> <s>when our model is a kalman filter this procedure can recover the original svi algorithm and svi with adaptive steps.</s> <s>we may also encode additional assumptions in the model, such as heavy-tailed noise.</s> <s>by doing so, our algorithm outperforms the original svi schedule and a state-of-the-art adaptive svi algorithm in two diverse domains.</s></p></d>", "label": ["<d><p><s>a filtering approach to stochastic variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic variational inference (svi) lets us scale up bayesian computation to massive data.</s> <s>it uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients.</s> <s>as with most traditional stochastic optimization methods, svi takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients.</s> <s>in this paper, we explore the idea of following biased stochastic gradients in svi.</s> <s>our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms.</s> <s>we will demonstrate the many advantages of this technique.</s> <s>first, its computational cost is the same as for svi and storage requirements only multiply by a constant factor.</s> <s>second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient.</s> <s>we test our method on latent dirichlet allocation with three large corpora.</s></p></d>", "label": ["<d><p><s>smoothed gradients for stochastic variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent dirichlet allocation (lda) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics.</s> <s>in this paper, we theoretically investigate variational bayesian (vb) learning in lda.</s> <s>more specifically, we analytically derive the leading term of the vb free energy under an asymptotic setup, and show that there exist transition thresholds in dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes.</s> <s>then we further theoretically reveal the notable phenomenon that vb tends to induce weaker sparsity than map in the lda model, which is opposed to other models.</s> <s>we experimentally demonstrate the practical validity of our asymptotic theory on real-world last.fm music data.</s></p></d>", "label": ["<d><p><s>analysis of variational bayesian latent dirichlet allocation: weaker sparsity than map</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational gaussian (vg) inference methods that optimize a lower bound to the marginal likelihood are a popular approach for bayesian inference.</s> <s>these methods are fast and easy to use, while being reasonably accurate.</s> <s>a difficulty remains in computation of the lower bound when the latent dimensionality $l$ is large.</s> <s>even though the lower bound is concave for many models, its computation requires optimization over $o(l^2)$ variational parameters.</s> <s>efficient reparameterization schemes can reduce the number of parameters, but give inaccurate solutions or destroy concavity leading to slow convergence.</s> <s>we propose decoupled variational inference that brings the best of both worlds together.</s> <s>first, it maximizes a lagrangian of the lower bound reducing the number of parameters to $o(n)$, where $n$ is the number of data examples.</s> <s>the reparameterization obtained is unique and recovers maxima of the lower-bound even when the bound is not concave.</s> <s>second, our method maximizes the lower bound using a sequence of convex problems, each of which is parallellizable over data examples and computes gradient efficiently.</s> <s>overall, our approach avoids all direct computations of the covariance, only requiring its linear projections.</s> <s>theoretically, our method converges at the same rate as existing methods in the case of concave lower bounds, while remaining convergent at a reasonable rate for the non-concave case.</s></p></d>", "label": ["<d><p><s>decoupled variational gaussian inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational inference algorithms have proven successful for bayesian analysis in large data settings, with recent advances using stochastic variational inference (svi).</s> <s>however, such methods have largely been studied in independent or exchangeable data settings.</s> <s>we develop an svi algorithm to learn the parameters of hidden markov models (hmms) in a time-dependent data setting.</s> <s>the challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations.</s> <s>we propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects.</s> <s>we demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.</s></p></d>", "label": ["<d><p><s>stochastic variational inference for hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a structured prediction algorithm for object localization based on support vector machines (svms) using privileged information.</s> <s>privileged information provides useful high-level knowledge for image understanding and facilitates learning a reliable model even with a small number of training examples.</s> <s>in our setting, we assume that such information is available only at training time since it may be difficult to obtain from visual data accurately without human supervision.</s> <s>our goal is to improve performance by incorporating privileged information into ordinary learning framework and adjusting model parameters for better generalization.</s> <s>we tackle object localization problem based on a novel structural svm using privileged information, where an alternating loss-augmented inference procedure is employed to handle the term in the objective function corresponding to privileged information.</s> <s>we apply the proposed algorithm to the caltech-ucsd birds 200-2011 dataset, and obtain encouraging results suggesting further investigation into the benefit of privileged information in structured prediction.</s></p></d>", "label": ["<d><p><s>object localization based on structural svm using privileged information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials.</s> <s>motivated by this property, we exploit the concave-convex procedure to perform inference on continuous markov random fields with polynomial potentials.</s> <s>in particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programming.</s> <s>we demonstrate the effectiveness of our approach in the context of 3d reconstruction, shape from shading and image denoising, and show that our approach significantly outperforms existing approaches in terms of efficiency as well as the quality of the retrieved solution.</s></p></d>", "label": ["<d><p><s>efficient inference of continuous markov random fields with polynomial potentials</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while there are many studies on weight regularization, the study on structure regularization is rare.</s> <s>many existing systems on structured prediction focus on increasing the level of structural dependencies within the model.</s> <s>however, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction.</s> <s>to control structure-based overfitting, we propose a structure regularization framework via \\emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power.</s> <s>we show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy.</s> <s>as a by-product, the proposed method can also substantially accelerate the training speed.</s> <s>the method and the theoretical results can apply to general graphical models with arbitrary structures.</s> <s>experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.</s></p></d>", "label": ["<d><p><s>structure regularization for structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a determinantal point process (dpp) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix.</s> <s>to fit a dpp to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data.</s> <s>however, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be np-hard.</s> <s>thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of dpps with fixed kernel matrices.</s> <s>in this work we propose a novel algorithm for learning the full kernel matrix.</s> <s>by changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure.</s> <s>we test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.</s></p></d>", "label": ["<d><p><s>expectation-maximization for learning determinantal point processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos.</s> <s>in this work, we encode actions based on attributes that describes actions as high-level concepts: \\textit{e.g.</s> <s>}, jump forward and motion in the air.</s> <s>we base our analysis on two types of action attributes.</s> <s>one type of action attributes is generated by humans.</s> <s>the second type is data-driven attributes, which is learned from data using dictionary learning methods.</s> <s>attribute-based representation may exhibit high variance due to noisy and redundant attributes.</s> <s>we propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set.</s> <s>three attribute selection criteria are proposed and formulated as a submodular optimization problem.</s> <s>a greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum.</s> <s>experimental results on the olympic sports and ucf101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.</s></p></d>", "label": ["<d><p><s>submodular attribute selection for action recognition in video</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure.</s> <s>we present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring.</s> <s>this new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data.</s> <s>the proposed formulation also allows to handle non-uniform blur with a straightforward extension.</s> <s>experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method.</s> <s>one surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.</s></p></d>", "label": ["<d><p><s>scale adaptive blind deblurring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the generic viewpoint assumption (gva) states that the position of the viewer or the light in a scene is not special.</s> <s>thus, any estimated parameters from an observation should be stable under small perturbations such as object, viewpoint or light positions.</s> <s>the gva has been analyzed and quantified in previous works, but has not been put to practical use in actual vision tasks.</s> <s>in this paper, we show how to utilize the gva to estimate shape and illumination from a single shading image, without the use of other priors.</s> <s>we propose a novel linearized spherical harmonics (sh) shading model which enables us to obtain a computationally efficient form of the gva term.</s> <s>together with a data term, we build a model whose unknowns are shape and sh illumination.</s> <s>the model parameters are estimated using the alternating direction method of multipliers embedded in a multi-scale estimation framework.</s> <s>in this prior-free framework, we obtain competitive shape and illumination estimation results under a variety of models and lighting conditions, requiring fewer assumptions than competing methods.</s></p></d>", "label": ["<d><p><s>shape and illumination from shading using the generic viewpoint assumption</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>self-paced learning (spl) is a recently proposed learning regime inspired by the learning process of humans and animals that gradually incorporates easy to more complex samples into training.</s> <s>existing methods are limited in that they ignore an important aspect in learning: diversity.</s> <s>to incorporate this information, we propose an approach called self-paced learning with diversity (spld) which formalizes the preference for both easy and diverse samples into a general regularizer.</s> <s>this regularization term is independent of the learning objective, and thus can be easily generalized into various learning tasks.</s> <s>albeit non-convex, the optimization of the variables included in this spld regularization term for sample selection can be globally solved in linearithmic time.</s> <s>we demonstrate that our method significantly outperforms the conventional spl on three real-world datasets.</s> <s>specifically, spld achieves the best map so far reported in literature on the hollywood2 and olympic sports datasets.</s></p></d>", "label": ["<d><p><s>self-paced learning with diversity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning the canonical parameters specifying an undirected graphical model (markov random field) from the mean parameters.</s> <s>for graphical models representing a minimal exponential family, the canonical parameters are uniquely determined by the mean parameters, so the problem is feasible in principle.</s> <s>the goal of this paper is to investigate the computational feasibility of this statistical task.</s> <s>our main result shows that parameter estimation is in general intractable: no algorithm can learn the canonical parameters of a generic pair-wise binary graphical model from the mean parameters in time bounded by a polynomial in the number of variables (unless rp = np).</s> <s>indeed, such a result has been believed to be true (see the monograph by wainwright and jordan) but no proof was known.</s> <s>our proof gives a polynomial time reduction from approximating the partition function of the hard-core model, known to be hard, to learning approximate parameters.</s> <s>our reduction entails showing that the marginal polytope boundary has an inherent repulsive property, which validates an optimization procedure over the polytope that does not use any knowledge of its structure (as required by the ellipsoid method and others).</s></p></d>", "label": ["<d><p><s>hardness of parameter estimation in graphical models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a new framework for how to use sequential monte carlo (smc) algorithms for inference in probabilistic graphical models (pgm).</s> <s>via a sequential decomposition of the pgm we find a sequence of auxiliary distributions defined on a monotonically increasing sequence of probability spaces.</s> <s>by targeting these auxiliary distributions using smc we are able to approximate the full joint distribution defined by the pgm.</s> <s>one of the key merits of the smc sampler is that it provides an unbiased estimate of the partition function of the model.</s> <s>we also show how it can be used within a particle markov chain monte carlo framework in order to construct high-dimensional block-sampling algorithms for general pgms.</s></p></d>", "label": ["<d><p><s>sequential monte carlo for graphical models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we analyze a reweighted version of the kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph.</s> <s>we establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the kikuchi expansion, and show that a reweighted version of the sum product algorithm applied to the kikuchi region graph will produce global optima of the kikuchi approximation whenever the algorithm converges.</s> <s>when the region graph has two layers, corresponding to a bethe approximation, we show that our sufficient conditions for concavity are also necessary.</s> <s>finally, we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph.</s> <s>we conclude with simulations that demonstrate the advantages of the reweighted kikuchi approach.</s></p></d>", "label": ["<d><p><s>concavity of reweighted kikuchi approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel probabilistic tracking algorithm that incorporates combinatorial data association constraints and model-based track management using variational bayes.</s> <s>we use a bethe entropy approximation to incorporate data association constraints that are often ignored in previous probabilistic tracking algorithms.</s> <s>noteworthy aspects of our method include a model-based mechanism to replace heuristic logic typically used to initiate and destroy tracks, and an assignment posterior with linear computation cost in window length as opposed to the exponential scaling of previous map-based approaches.</s> <s>we demonstrate the applicability of our method on radar tracking and computer vision problems.</s></p></d>", "label": ["<d><p><s>a complete variational tracker</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a new hybrid architecture that consists of a deep convolutional network and a markov random field.</s> <s>we show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images.</s> <s>the architecture can exploit structural domain constraints such as geometric relationships between body joint locations.</s> <s>we show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.</s></p></d>", "label": ["<d><p><s>joint training of a convolutional network and a graphical model for human pose estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we reduce a broad class of machine learning problems, usually addressed by em or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set.</s> <s>these $k$ ``anchors'' lead to a global solution and a more interpretable model that can even outperform em and sampling on generalization error.</s> <s>to find the $k$ anchors, we propose a novel divide-and-conquer learning scheme ``dca'' that distributes the problem to $\\mathcal o(k\\log k)$ same-type sub-problems on different low-d random hyperplanes, each can be solved by any solver.</s> <s>for the 2d sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries.</s> <s>dca also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning.</s> <s>we apply our method to gmm, hmm, lda, nmf and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets.</s></p></d>", "label": ["<d><p><s>divide-and-conquer learning by anchoring a conical hull</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we address the problem of deciding whether a causal or probabilistic query is estimable from data corrupted by missing entries, given a model of missingness process.</s> <s>we extend the results of mohan et al, 2013 by presenting more general conditions for recovering probabilistic queries of the form p(y|x) and p(y,x) as well as causal queries of the form p(y|do(x)).</s> <s>we show that causal queries may be recoverable even when the factors in their identifying estimands are not recoverable.</s> <s>specifically, we derive graphical conditions for recovering causal effects of the form p(y|do(x)) when y and its missingness mechanism are not d-separable.</s> <s>finally, we apply our results to problems of attrition and characterize the recovery of causal effects from data corrupted by attrition.</s></p></d>", "label": ["<d><p><s>graphical models for recovering probabilistic and causal queries from missing data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sparse high dimensional graphical model selection is a popular topic in contemporary machine learning.</s> <s>to this end, various useful approaches have been proposed in the context of $\\ell_1$ penalized estimation in the gaussian framework.</s> <s>though many of these approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the gaussian functional form.</s> <s>to address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (concord) has been recently proposed.</s> <s>this method uses cyclic coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the gaussian approach.</s> <s>in direct contrast to the parallel work in the gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization.</s> <s>in this paper, we address this crucial gap by proposing two proximal gradient methods (concord-ista and concord-fista) for performing $\\ell_1$-regularized inverse covariance matrix estimation in the pseudo-likelihood framework.</s> <s>we present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous pay offs for $\\ell_1$-penalized partial correlation graph estimation outside the gaussian setting, thus yielding the fastest and most scalable approach for such problems.</s> <s>we undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.</s></p></d>", "label": ["<d><p><s>optimization methods for sparse pseudo-likelihood graphical model selection</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present an algorithm for finding a chordal markov network that maximizes any given decomposable scoring function.</s> <s>the algorithm is based on a recursive characterization of clique trees, and it runs in o(4^n) time for n vertices.</s> <s>on an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (corander et al., nips 2013).</s> <s>within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size.</s> <s>we also study the performance of a recent integer linear programming algorithm (bartlett and cussens, uai 2013).</s> <s>our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.</s></p></d>", "label": ["<d><p><s>learning chordal markov networks by dynamic programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we develop a family of algorithms for optimizing superposition-structured?</s> <s>or ?dirty?</s> <s>statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization.</s> <s>most of the current approaches are first-order methods, including proximal gradient or alternating direction method of multipliers (admm).</s> <s>we propose a new family of second-order methods where we approximate the loss function using quadratic approximation.</s> <s>the superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization.</s> <s>we propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm.</s> <s>empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer.</s> <s>for these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"</s></p></d>", "label": ["<d><p><s>quic & dirty: a quadratic approximation approach for dirty statistical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a new exponential family probabilistic model for permutations that can capture hierarchical structure, and that has the well known mallows and generalized mallows models as subclasses.</s> <s>we describe how one can do parameter estimation and propose an approach to structure search for this class of models.</s> <s>we provide experimental evidence that this added flexibility both improves predictive performance and enables a deeper understanding of collections of permutations.</s></p></d>", "label": ["<d><p><s>recursive inversion models for permutations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective.</s> <s>intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information.</s> <s>the method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems.</s> <s>we demonstrate that correlation explanation (corex) automatically discovers meaningful structure for data from diverse sources including personality tests, dna, and human language.</s></p></d>", "label": ["<d><p><s>discovering structure in high-dimensional data through correlation explanation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>life-logging video streams, financial time series, and twitter tweets are a few examples of high-dimensional signals over practically unbounded time.</s> <s>we consider the problem of computing optimal segmentation of such signals by k-piecewise linear function, using only one pass over the data by maintaining a coreset for the signal.</s> <s>the coreset enables fast further analysis such as automatic summarization and analysis of such signals.</s> <s>a coreset (core-set) is a compact representation of the data seen so far, which approximates the data well for a specific task -- in our case, segmentation of the stream.</s> <s>we show that, perhaps surprisingly, the segmentation problem admits coresets of cardinality only linear in the number of segments k, independently of both the dimension d of the signal, and its number n of points.</s> <s>more precisely, we construct a representation of size o(klog n /eps^2) that provides a (1+eps)-approximation for the sum of squared distances to any given k-piecewise linear function.</s> <s>moreover, such coresets can be constructed in a parallel streaming approach.</s> <s>our results rely on a novel eduction of statistical estimations to problems in computational geometry.</s> <s>we empirically evaluate our algorithms on very large synthetic and real data sets from gps, video and financial domains, using 255 machines in amazon cloud.</s></p></d>", "label": ["<d><p><s>coresets for k-segmentation of streaming data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of hierarchical clustering is to construct a cluster tree, which can be viewed as the modal structure of a density.</s> <s>for this purpose, we use a convex optimization program that can efficiently estimate a family of hierarchical dense sets in high-dimensional distributions.</s> <s>we further extend existing graph-based methods to approximate the cluster tree of a distribution.</s> <s>by avoiding direct density estimation, our method is able to handle high-dimensional data more efficiently than existing density-based approaches.</s> <s>we present empirical results that demonstrate the superiority of our method over existing ones.</s></p></d>", "label": ["<d><p><s>approximating hierarchical mv-sets for hierarchical clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods.</s> <s>existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut.</s> <s>in this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice.</s> <s>for the optimization of our tight continuous relaxation we propose a new algorithm for the hard sum-of-ratios minimization problem which achieves monotonic descent.</s> <s>extensive comparisons show that our method beats all existing approaches for ratio cut and other balanced k-cut criteria.</s></p></d>", "label": ["<d><p><s>tight continuous relaxation of the balanced k-cut problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e.</s> <s>disjoint clusters, so that there is higher density within clusters than across clusters.</s> <s>both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve.</s> <s>we are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store.</s> <s>the data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting.</s> <s>for this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately.</s> <s>the first algorithm is {\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size.</s> <s>the second algorithm is {\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information.</s> <s>this algorithm requires a memory growing sub-linearly with the network size.</s> <s>to construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.</s></p></d>", "label": ["<d><p><s>streaming, memory limited algorithms for community detection</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the computational complexity of computing nash equilibria in generalized interdependent-security (ids) games.</s> <s>like traditional ids games, originally introduced by economists and risk-assessment experts heal and kunreuther about a decade ago, generalized ids games model agents?</s> <s>voluntary investment decisions when facing potential direct risk and transfer risk exposure from other agents.</s> <s>a distinct feature of generalized ids games, however, is that full investment can reduce transfer risk.</s> <s>as a result, depending on the transfer-risk reduction level, generalized ids games may exhibit strategic complementarity (sc) or strategic substitutability (ss).</s> <s>we consider three variants of generalized ids games in which players exhibit only sc, only ss, and both sc+ss.</s> <s>we show that determining whether there is a pure-strategy nash equilibrium (psne) in sc+ss-type games is np-complete, while computing a single psne in sc-type games takes worst-case polynomial time.</s> <s>as for the problem of computing all mixed-strategy nash equilibria (msne) efficiently, we produce a partial characterization.</s> <s>whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that kearns and ortiz originally studied in the context of traditional ids games in their nips 2003 paper, we can compute all msne that satisfy some ordering constraints in polynomial time in all three game variants.</s> <s>yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the pure-nash-extension problem, also originally introduced by kearns and ortiz, and that it is np complete for all three variants.</s> <s>finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized ids games has on msne by solving several randomly-generated instances of sc+ss-type games with graph structures taken from several real-world datasets.</s></p></d>", "label": ["<d><p><s>computing nash equilibria in generalized interdependent security games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>game-theoretic algorithms for physical security have made an impressive real-world impact.</s> <s>these algorithms compute an optimal strategy for the defender to commit to in a stackelberg game, where the attacker observes the defender's strategy and best-responds.</s> <s>in order to build the game model, though, the payoffs of potential attackers for various outcomes must be estimated; inaccurate estimates can lead to significant inefficiencies.</s> <s>we design an algorithm that optimizes the defender's strategy with no prior information, by observing the attacker's responses to randomized deployments of resources and learning his priorities.</s> <s>in contrast to previous work, our algorithm requires a number of queries that is polynomial in the representation of the game.</s></p></d>", "label": ["<d><p><s>learning optimal commitment to overcome insecurity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the power of voting among diverse, randomized software agents.</s> <s>with teams of computer go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning.</s> <s>this model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models).</s> <s>we analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows.</s> <s>our experiments, which pit teams of computer go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse.</s></p></d>", "label": ["<d><p><s>diverse randomized agents vote to win</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we define a fairness solution criterion for multi-agent decision-making problems, where agents have local interests.</s> <s>this new criterion aims to maximize the worst performance of agents with consideration on the overall performance.</s> <s>we develop a simple linear programming approach and a more scalable game-theoretic approach for computing an optimal fairness policy.</s> <s>this game-theoretic approach formulates this fairness optimization as a two-player, zero-sum game and employs an iterative algorithm for finding a nash equilibrium, corresponding to an optimal fairness policy.</s> <s>we scale up this approach by exploiting problem structure and value function approximation.</s> <s>our experiments on resource allocation problems show that this fairness criterion provides a more favorable solution than the utilitarian criterion, and that our game-theoretic approach is significantly faster than linear programming.</s></p></d>", "label": ["<d><p><s>fairness in multi-agent sequential decision-making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction.</s> <s>we consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future.</s> <s>we further assume a buyer?s valuation of a good is a function of a context vector that describes the good being sold.</s> <s>we give the first algorithm attaining sublinear (o(t^{2/3})) regret in the contextual setting against a surplus-maximizing buyer.</s> <s>we also extend this result to repeated second-price auctions with multiple buyers.</s></p></d>", "label": ["<d><p><s>repeated contextual auctions with strategic buyers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning models of options for real-time abstract planning, in the setting where reward functions can be specified at any time and their expected returns must be efficiently computed.</s> <s>we introduce a new model for an option that is independent of any reward function, called the {\\it universal option model (uom)}.</s> <s>we prove that the uom of an option can construct a traditional option model given a reward function, and the option-conditional return is computed directly by a single dot-product of the uom with the reward function.</s> <s>we extend the uom to linear function approximation, and we show it gives the td solution of option returns and value functions of policies over options.</s> <s>we provide a stochastic approximation algorithm for incrementally learning uoms from data and prove its consistency.</s> <s>we demonstrate our method in two domains.</s> <s>the first domain is document recommendation, where each user query defines a new reward function and a document's relevance is the expected return of a simulated random-walk through the document's references.</s> <s>the second domain is a real-time strategy game, where the controller must select the best game unit to accomplish dynamically-specified tasks.</s> <s>our experiments show that uoms are substantially more efficient in evaluating option returns and policies than previously known methods.</s></p></d>", "label": ["<d><p><s>universal option models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sampling from hierarchical bayesian models is often difficult for mcmc methods, because of the strong correlations between the model parameters and the hyperparameters.</s> <s>recent riemannian manifold hamiltonian monte carlo (rmhmc) methods have significant potential advantages in this setting, but are computationally expensive.</s> <s>we introduce a new rmhmc method, which we call semi-separable hamiltonian monte carlo, which uses a specially designed mass matrix that allows the joint hamiltonian over model parameters and hyperparameters to decompose into two simpler hamiltonians.</s> <s>this structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm.</s> <s>the resulting method can mix faster than simpler gibbs sampling while being simpler and more efficient than previous instances of rmhmc.</s></p></d>", "label": ["<d><p><s>semi-separable hamiltonian monte carlo for inference in bayesian hierarchical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dynamics-based sampling methods, such as hybrid monte carlo (hmc) and langevin dynamics (ld), are commonly used to sample target distributions.</s> <s>recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets.</s> <s>an outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization.</s> <s>to remedy this problem, we show that one can leverage a small number of additional variables in order to stabilize momentum fluctuations induced by the unknown noise.</s> <s>our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory.</s></p></d>", "label": ["<d><p><s>bayesian sampling using stochastic gradient thermostats</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>gaussian processes (gps) are a powerful tool for probabilistic inference over functions.</s> <s>they have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters.</s> <s>however the scalability of these models to big datasets remains an active topic of research.</s> <s>we introduce a novel re-parametrisation of variational inference for sparse gp regression and latent variable models that allows for an efficient distributed algorithm.</s> <s>this is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a map-reduce setting.</s> <s>we show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes.</s> <s>we further demonstrate the utility in scaling gaussian processes to big data.</s> <s>we show that gp performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on mnist).</s> <s>the results show that gps perform better than many common models often used for big data.</s></p></d>", "label": ["<d><p><s>distributed variational inference in sparse gaussian process regression and latent variable models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>locally weighted regression (lwr) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data.</s> <s>as an interesting feature, lwr can regress on non-stationary functions, a beneficial property, for instance, in control problems.</s> <s>however, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results.</s> <s>gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required.</s> <s>in this paper, we suggest a path from gaussian (process) regression to locally weighted regression, where we retain the best of both approaches.</s> <s>using a localizing function basis and approximate inference techniques, we build a gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to lwr.</s> <s>empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.</s></p></d>", "label": ["<d><p><s>incremental local gaussian regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient.</s> <s>the problem is extremely challenging and general inference remains computationally expensive.</s> <s>we seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations.</s> <s>motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time.</s> <s>through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.</s></p></d>", "label": ["<d><p><s>just-in-time learning for fast and flexible inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a distributed markov chain monte carlo (mcmc) inference algorithm for large scale bayesian posterior simulation.</s> <s>we assume that the dataset is partitioned and stored across nodes of a cluster.</s> <s>our procedure involves an independent mcmc posterior sampler at each node based on its local partition of the data.</s> <s>moment statistics of the local posteriors are collected from each sampler and propagated across the cluster using expectation propagation message passing with low communication costs.</s> <s>the moment sharing scheme improves posterior estimation quality by enforcing agreement among the samplers.</s> <s>we demonstrate the speed and inference quality of our method with empirical studies on bayesian logistic regression and sparse linear regression with a spike-and-slab prior.</s></p></d>", "label": ["<d><p><s>distributed bayesian posterior sampling via moment sharing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper describes a third-generation parameter server framework for distributed machine learning.</s> <s>this framework offers two relaxations to balance system performance and algorithm efficiency.</s> <s>we propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees.</s> <s>we present an in-depth analysis of two large scale machine learning problems ranging from $\\ell_1$-regularized logistic regression on cpus to reconstruction ica on gpus, using 636tb of real data with hundreds of billions of samples and dimensions.</s> <s>we demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.</s></p></d>", "label": ["<d><p><s>communication efficient distributed machine learning with the parameter server</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness.</s> <s>a sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ml algorithms, where parameters of an ml program are partitioned to different workers and undergone concurrent iterative updates.</s> <s>we argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis.</s> <s>in this paper, we develop a system for model-parallelism, strads, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ml programs.</s> <s>strads enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ml.</s> <s>we demonstrate the efficacy of model-parallel algorithms implemented on strads versus popular implementations for topic modeling, matrix factorization, and lasso.</s></p></d>", "label": ["<d><p><s>on model parallelization and scheduling strategies for distributed machine learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>communication remains the most significant bottleneck in the performance of distributed optimization algorithms for large-scale machine learning.</s> <s>in this paper, we propose a communication-efficient framework, cocoa, that uses local computation in a primal-dual setting to dramatically reduce the amount of necessary communication.</s> <s>we provide a strong convergence rate analysis for this class of algorithms, as well as experiments on real-world distributed datasets with implementations in spark.</s> <s>in our experiments, we find that as compared to state-of-the-art mini-batch versions of sgd and sdca algorithms, cocoa converges to the same .001-accurate solution quality on average 25?</s> <s>as quickly.</s></p></d>", "label": ["<d><p><s>communication-efficient distributed dual coordinate ascent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discriminative dictionary learning (dl) has been widely studied in various pattern classification problems.</s> <s>most of the existing dl methods aim to learn a synthesis dictionary to represent the input signal while enforcing the representation coefficients and/or representation residual to be discriminative.</s> <s>however, the $\\ell_0$ or $\\ell_1$-norm sparsity constraint on the representation coefficients adopted in many dl methods makes the training and testing phases time consuming.</s> <s>we propose a new discriminative dl framework, namely projective dictionary pair learning (dpl), which learns a synthesis dictionary and an analysis dictionary jointly to achieve the goal of signal representation and discrimination.</s> <s>compared with conventional dl methods, the proposed dpl method can not only greatly reduce the time complexity in the training and testing phases, but also lead to very competitive accuracies in a variety of visual classification tasks.</s></p></d>", "label": ["<d><p><s>projective dictionary pair learning for pattern classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs.</s> <s>we investigate the viability of a similar idea within message passing -- for integral solutions -- in the context of two combinatorial problems: 1) for traveling salesman problem (tsp), we propose a factor-graph based on held-karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form.</s> <s>2) for graph-partitioning (a.k.a.</s> <s>community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques.</s> <s>in both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances.</s> <s>in particular for tsp we are able to find near-optimal solutions in the time that empirically grows with $n^3$, demonstrating that augmentation is practical and efficient.</s></p></d>", "label": ["<d><p><s>augmentative message passing for traveling salesman problem and graph partitioning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>one of the most fundamental problems in causal inference is the estimation of a causal effect when variables are confounded.</s> <s>this is difficult in an observational study because one has no direct evidence that all confounders have been adjusted for.</s> <s>we introduce a novel approach for estimating causal effects that exploits observational conditional independencies to suggest ``weak'' paths in a unknown causal graph.</s> <s>the widely used faithfulness condition of spirtes et al.</s> <s>is relaxed to allow for varying degrees of ``path cancellations'' that will imply conditional independencies but do not rule out the existence of confounding causal paths.</s> <s>the outcome is a posterior distribution over bounds on the average causal effect via a linear programming approach and bayesian inference.</s> <s>we claim this approach should be used in regular practice to complement other default tools in observational studies.</s></p></d>", "label": ["<d><p><s>causal inference through a witness protection program</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>biclustering is the analog of clustering on a bipartite graph.</s> <s>existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa.</s> <s>we propose a biclustering algorithm that maximizes a global objective function using message passing.</s> <s>our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties.</s> <s>because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice.</s> <s>moreover, expectation-maximization can be used to learn the model parameters if they are unknown.</s> <s>in simulations, we find that our method outperforms two of the best existing biclustering algorithms, isa and las, when the planted clusters overlap.</s> <s>applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.</s></p></d>", "label": ["<d><p><s>biclustering using message passing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a scoring and classification procedure based on the pac-bayesian approach and the auc (area under curve) criterion.</s> <s>we focus initially on the class of linear score functions.</s> <s>we derive pac-bayesian non-asymptotic bounds for two types of prior for the score parameters: a gaussian prior, and a spike-and-slab prior; the latter makes it possible to perform feature selection.</s> <s>one important advantage of our approach is that it is amenable to powerful bayesian computational tools.</s> <s>we derive in particular a sequential monte carlo algorithm, as an efficient method which may be used as a gold standard, and an expectation-propagation algorithm, as a much faster but approximate method.</s> <s>we also extend our method to a class of non-linear score functions, essentially leading to a nonparametric procedure, by considering a gaussian process prior.</s></p></d>", "label": ["<d><p><s>pac-bayesian auc classification and scoring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>region-specific linear models are widely used in practical applications because of their non-linear but highly interpretable model representations.</s> <s>one of the key challenges in their use is non-convexity in simultaneous optimization of regions and region-specific models.</s> <s>this paper proposes novel convex region-specific linear models, which we refer to as partition-wise linear models.</s> <s>our key ideas are 1) assigning linear models not to regions but to partitions (region-specifiers) and representing region-specific linear models by linear combinations of partition-specific models, and 2) optimizing regions via partition selection from a large number of given partition candidates by means of convex structured regularizations.</s> <s>in addition to providing initialization-free globally-optimal solutions, our convex formulation makes it possible to derive a generalization bound and to use such advanced optimization techniques as proximal methods and decomposition of the proximal maps for sparsity-inducing regularizations.</s> <s>experimental results demonstrate that our partition-wise linear models perform better than or are at least competitive with state-of-the-art region-specific or locally linear models.</s></p></d>", "label": ["<d><p><s>partition-wise linear models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the class of shuffle ideals is a fundamental sub-family of regular languages.</s> <s>the shuffle ideal generated by a string set $u$ is the collection of all strings containing some string $u \\in u$ as a (not necessarily contiguous) subsequence.</s> <s>in spite of its apparent simplicity, the problem of learning a shuffle ideal from given data is known to be computationally intractable.</s> <s>in this paper, we study the pac learnability of shuffle ideals and present positive results on this learning problem under element-wise independent and identical distributions and markovian distributions in the statistical query model.</s> <s>a constrained generalization to learning shuffle ideals under product distributions is also provided.</s> <s>in the empirical direction, we propose a heuristic algorithm for learning shuffle ideals from given labeled strings under general unrestricted distributions.</s> <s>experiments demonstrate the advantage for both efficiency and accuracy of our algorithm.</s></p></d>", "label": ["<d><p><s>learning shuffle ideals under restricted distributions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this work, we study the problem of transductive pairwise classification from pairwise similarities~\\footnote{the pairwise similarities are usually derived from some side information instead of the underlying class labels.}.</s> <s>the goal of transductive pairwise classification from pairwise similarities is to infer the pairwise class relationships, to which we refer as pairwise labels, between all examples given a subset of class relationships for a small set of examples, to which we refer as labeled examples.</s> <s>we propose a very simple yet effective algorithm that consists of two simple steps: the first step is to complete the sub-matrix corresponding to the labeled examples and the second step is to reconstruct the label matrix from the completed sub-matrix and the provided similarity matrix.</s> <s>our analysis exhibits that under several mild preconditions we can recover the label matrix with a small error, if the top eigen-space that corresponds to the largest eigenvalues of the similarity matrix covers well the column space of label matrix and is subject to a low coherence, and the number of observed pairwise labels is sufficiently enough.</s> <s>we demonstrate the effectiveness of the proposed algorithm by several experiments.</s></p></d>", "label": ["<d><p><s>extracting certainty from uncertainty: transductive pairwise classification from pairwise similarities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data.</s> <s>in this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect.</s> <s>we find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method.</s> <s>furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.</s></p></d>", "label": ["<d><p><s>incremental clustering: the case for extra clusters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables.</s> <s>a popular approach to solve this problem is the block coordinate descent (bcd) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed.</s> <s>with the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the bcd method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm.</s> <s>in this work, we propose an inexact parallel bcd approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function.</s> <s>we investigate the convergence of this parallel bcd method for both randomized and cyclic variable selection rules.</s> <s>we analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions.</s> <s>the numerical experiments suggest that for a special case of lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.</s></p></d>", "label": ["<d><p><s>parallel successive convex approximation for nonsmooth nonconvex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>proximal gradient descent (pgd) and stochastic proximal gradient descent (spgd) are popular methods for solving regularized risk minimization problems in machine learning and statistics.</s> <s>in this paper, we propose and analyze an accelerated variant of these methods in the mini-batch setting.</s> <s>this method incorporates two acceleration techniques: one is nesterov's acceleration method, and the other is a variance reduction for the stochastic gradient.</s> <s>accelerated proximal gradient descent (apg) and proximal stochastic variance reduction gradient (prox-svrg) are in a trade-off relationship.</s> <s>we show that our method, with the appropriate mini-batch size, achieves lower overall complexity than both apg and prox-svrg.</s></p></d>", "label": ["<d><p><s>stochastic proximal gradient descent with acceleration techniques</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the birkhoff polytope (the convex hull of the set of permutation matrices), which is represented using $\\theta(n^2)$ variables and constraints, is frequently invoked in formulating relaxations of optimization problems over permutations.</s> <s>using a recent construction of goemans (2010), we show that when optimizing over the convex hull of the permutation vectors (the permutahedron), we can reduce the number of variables and constraints to $\\theta(n \\log n)$ in theory and $\\theta(n \\log^2 n)$ in practice.</s> <s>we modify the recent convex formulation of the 2-sum problem introduced by fogel et al.</s> <s>(2013) to use this polytope, and demonstrate how we can attain results of similar quality in significantly less computational time for large $n$.</s> <s>to our knowledge, this is the first usage of goemans' compact formulation of the permutahedron in a convex optimization problem.</s> <s>we also introduce a simpler regularization scheme for this convex formulation of the 2-sum problem that yields good empirical results.</s></p></d>", "label": ["<d><p><s>beyond the birkhoff polytope: convex relaxations for vector permutation problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the mirror descent algorithm (mda) generalizes gradient descent by using a bregman divergence to replace squared euclidean distance.</s> <s>in this paper, we similarly generalize the alternating direction method of multipliers (admm) to bregman admm (badmm), which allows the choice of different bregman divergences to exploit the structure of problems.</s> <s>badmm provides a unified framework for admm and its variants, including generalized admm, inexact admm and bethe admm.</s> <s>we establish the global convergence and the $o(1/t)$ iteration complexity for badmm.</s> <s>in some cases, badmm can be faster than admm by a factor of $o(n/\\ln n)$ where $n$ is the dimensionality.</s> <s>in solving the linear program of mass transportation problem, badmm leads to massive parallelism and can easily run on gpu.</s> <s>badmm is several times faster than highly optimized commercial software gurobi.</s></p></d>", "label": ["<d><p><s>bregman alternating direction method of multipliers</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we consider a multi-step version of the stochastic admm method with efficient guarantees for high-dimensional problems.</s> <s>we first analyze the simple setting, where the optimization problem consists of a loss function and a single regularizer (e.g.</s> <s>sparse optimization), and then extend to the multi-block setting with multiple regularizers and multiple variables (e.g.</s> <s>matrix decomposition into sparse and low rank components).</s> <s>for the sparse optimization problem, our method achieves the minimax rate of $o(s\\log d/t)$ for $s$-sparse problems in $d$ dimensions in $t$ steps, and is thus, unimprovable by any method up to constant factors.</s> <s>for the matrix decomposition problem with a general loss function, we analyze the multi-step admm with multiple blocks.</s> <s>we establish $o(1/t)$ rate and efficient scaling as the size of matrix grows.</s> <s>for natural noise models (e.g.</s> <s>independent noise), our convergence rate is minimax-optimal.</s> <s>thus, we establish tight convergence guarantees for multi-block admm in high dimensions.</s> <s>experiments show that for both sparse optimization and matrix decomposition problems, our algorithm outperforms the state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>multi-step stochastic admm in high dimensions: applications to sparse optimization and matrix decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider regularized empirical risk minimization problems.</s> <s>in particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function.</s> <s>when the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (rbcd) manner.</s> <s>existing rbcd methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration.</s> <s>thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained.</s> <s>however, such a ``batch setting may be computationally expensive in practice.</s> <s>in this paper, we propose a mini-batch randomized block coordinate descent (mrbcd) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration.</s> <s>we further accelerate the mrbcd method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators.</s> <s>theoretically, we show that for strongly convex functions, the mrbcd method attains lower overall iteration complexity than existing rbcd methods.</s> <s>as an application, we further trim the mrbcd method to solve the regularized sparse learning problems.</s> <s>our numerical experiments shows that the mrbcd method naturally exploits the sparsity structure and achieves better computational performance than existing methods.\"</s></p></d>", "label": ["<d><p><s>accelerated mini-batch randomized block coordinate descent method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the estimation of the $k$-dimensional sparse principal subspace of covariance matrix $\\sigma$ in the high-dimensional setting.</s> <s>we aim to recover the oracle principal subspace solution, i.e., the principal subspace estimator obtained assuming the true support is known a priori.</s> <s>to this end, we propose a family of estimators based on the semidefinite relaxation of sparse pca with novel regularizations.</s> <s>in particular, under a weak assumption on the magnitude of the population projection matrix, one estimator within this family exactly recovers the true support with high probability, has exact rank-$k$, and attains a $\\sqrt{s/n}$ statistical rate of convergence with $s$ being the subspace sparsity level and $n$ the sample size.</s> <s>compared to existing support recovery results for sparse pca, our approach does not hinge on the spiked covariance model or the limited correlation condition.</s> <s>as a complement to the first estimator that enjoys the oracle property, we prove that, another estimator within the family achieves a sharper statistical rate of convergence than the standard semidefinite relaxation of sparse pca, even when the previous assumption on the magnitude of the projection matrix is violated.</s> <s>we validate the theoretical results by numerical experiments on synthetic datasets.</s></p></d>", "label": ["<d><p><s>sparse pca with oracle property</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the principal component analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model.</s> <s>on the one hand, we use information theory, and recent results in probability theory to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources.</s> <s>it turns out that this is possible as soon as the signal-to-noise ratio beta becomes larger than c\\sqrt{k log k} (and in particular beta can remain bounded has the problem dimensions increase).</s> <s>on the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models.</s> <s>we show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds.</s> <s>this is possibly related to a fundamental limitation of computationally tractable estimators for this problem.</s> <s>for moderate dimensions, we propose an hybrid approach that uses unfolding together with power iteration, and show that it outperforms significantly baseline methods.</s> <s>finally, we consider the case in which additional side information is available about the unknown signal.</s> <s>we characterize the amount of side information that allow the iterative algorithms to converge to a good estimate.</s></p></d>", "label": ["<d><p><s>a statistical model for tensor pca</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>canonical correlation analysis (cca) is a widely used statistical tool with both well established theory and favorable performance for a wide range of machine learning problems.</s> <s>however, computing cca for huge datasets can be very slow since it involves implementing qr decomposition or singular value decomposition of huge matrices.</s> <s>in this paper we introduce l-cca, an iterative algorithm which can compute cca fast on huge sparse datasets.</s> <s>theory on both the asymptotic convergence and finite time accuracy of l-cca are established.</s> <s>the experiments also show that l-cca outperform other fast cca approximation schemes on two real datasets.</s></p></d>", "label": ["<d><p><s>large scale canonical correlation analysis with iterative least squares</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimating a vector from noisy quadratic observations is a task that arises naturally in many contexts, from dimensionality reduction, to synchronization and phase retrieval problems.</s> <s>it is often the case that additional information is available about the unknown vector (for instance, sparsity, sign or magnitude of its entries).</s> <s>many authors propose non-convex quadratic optimization problems that aim at exploiting optimally this information.</s> <s>however, solving these problems is typically np-hard.</s> <s>we consider a simple model for noisy quadratic observation of an unknown vector $\\bvz$.</s> <s>the unknown vector is constrained to belong to a cone $\\cone \\ni \\bvz$.</s> <s>while optimal estimation appears to be intractable for the general problems in this class, we provide evidence that it is tractable when $\\cone$ is a convex cone with an efficient projection.</s> <s>this is surprising, since the corresponding optimization problem is non-convex and --from a worst case perspective-- often np hard.</s> <s>we characterize the resulting minimax risk in terms of the statistical dimension of the cone $\\delta(\\cone)$.</s> <s>this quantity is already known to control the risk of estimation from gaussian observations and random linear measurements.</s> <s>it is rather surprising that the same quantity plays a role in the estimation risk from quadratic measurements.</s></p></d>", "label": ["<d><p><s>cone-constrained principal component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the distributed computing setting in which there are multiple servers, each holding a set of points, who wish to compute functions on the union of their point sets.</s> <s>a key task in this setting is principal component analysis (pca), in which the servers would like to compute a low dimensional subspace capturing as much of the variance of the union of their point sets as possible.</s> <s>given a procedure for approximate pca, one can use it to approximately solve problems such as $k$-means clustering and low rank approximation.</s> <s>the essential properties of an approximate distributed pca algorithm are its communication cost and computational efficiency for a given desired accuracy in downstream applications.</s> <s>we give new algorithms and analyses for distributed pca which lead to improved communication and computational costs for $k$-means clustering and related problems.</s> <s>our empirical study on real world data shows a speedup of orders of magnitude, preserving communication with only a negligible degradation in solution quality.</s> <s>some of these techniques we develop, such as input-sparsity subspace embeddings with high correctness probability with a dimension and sparsity independent of the error probability, may be of independent interest.</s></p></d>", "label": ["<d><p><s>improved distributed principal component analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a generalized unsupervised manifold alignment (guma) method to build the connections between different but correlated datasets without any known correspondences.</s> <s>based on the assumption that datasets of the same theme usually have similar manifold structures, guma is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space.</s> <s>the main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsupervised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases.</s> <s>experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.</s></p></d>", "label": ["<d><p><s>generalized unsupervised manifold alignment</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a general framework for constructing prior distributions with structured variables.</s> <s>the prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest.</s> <s>in cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain.</s> <s>we further analyze the special case of sparse structure.</s> <s>while the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints.</s> <s>as a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value.</s> <s>our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data.</s> <s>for this task, we employ the gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support.</s> <s>experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.</s></p></d>", "label": ["<d><p><s>on prior distributions and approximate inference for structured variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neuronal encoding models range from the detailed biophysically-based hodgkin huxley model, to the statistical linear time invariant model specifying firing rates in terms of the extrinsic signal.</s> <s>decoding the former becomes intractable, while the latter does not adequately capture the nonlinearities present in the neuronal encoding system.</s> <s>for use in practical applications, we wish to record the output of neurons, namely spikes, and decode this signal fast in order to drive a machine, for example a prosthetic device.</s> <s>here, we introduce a causal, real-time decoder of the biophysically-based integrate and fire encoding neuron model.</s> <s>we show that the upper bound of the real-time reconstruction error decreases polynomially in time, and that the l2 norm of the error is bounded by a constant that depends on the density of the spikes, as well as the bandwidth and the decay of the input signal.</s> <s>we numerically validate the effect of these parameters on the reconstruction error.</s></p></d>", "label": ["<d><p><s>real-time decoding of an integrate and fire encoder</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to extract motion information, the brain needs to compensate for time delays that are ubiquitous in neural signal transmission and processing.</s> <s>here we propose a simple yet effective mechanism to implement anticipative tracking in neural systems.</s> <s>the proposed mechanism utilizes the property of spike-frequency adaptation (sfa), a feature widely observed in neuronal responses.</s> <s>we employ continuous attractor neural networks (canns) as the model to describe the tracking behaviors in neural systems.</s> <s>incorporating sfa, a cann exhibits intrinsic mobility, manifested by the ability of the cann to hold self-sustained travelling waves.</s> <s>in tracking a moving stimulus, the interplay between the external drive and the intrinsic mobility of the network determines the tracking performance.</s> <s>interestingly, we find that the regime of anticipation effectively coincides with the regime where the intrinsic speed of the travelling wave exceeds that of the external drive.</s> <s>depending on the sfa amplitudes, the network can achieve either perfect tracking, with zero-lag to the input, or perfect anticipative tracking, with a constant leading time to the input.</s> <s>our model successfully reproduces experimentally observed anticipative tracking behaviors, and sheds light on our understanding of how the brain processes motion information in a timely manner.</s></p></d>", "label": ["<d><p><s>spike frequency adaptation implements anticipative tracking in continuous attractor neural networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the local field potential (lfp) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions.</s> <s>it is believed that these regions may jointly constitute a ``brain state,'' relating to cognition and behavior.</s> <s>an infinite hidden markov model (ihmm) is proposed to model the evolution of brain states, based on electrophysiological lfp data measured at multiple brain regions.</s> <s>a brain state influences the spectral content of each region in the measured lfp.</s> <s>a new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the lfps are characterized in terms of gaussian processes (gps).</s> <s>the lfps are modeled as a mixture of gps, with state- and region-dependent mixture weights, and with the spectral content of the data encoded in gp spectral mixture covariance kernels.</s> <s>the model is able to estimate the number of brain states and the number of mixture components in the mixture of gps.</s> <s>a new variational bayesian split-merge algorithm is employed for inference.</s> <s>the model infers state changes as a function of external covariates in two novel electrophysiological datasets, using lfp data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.</s></p></d>", "label": ["<d><p><s>analysis of brain states from multi-region lfp time-series</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory).</s> <s>there is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons.</s> <s>rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how the latent variables interact.</s> <s>specifically, we propose extensions to probabilistic canonical correlation analysis (pcca) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed group latent auto-regressive analysis, glara).</s> <s>we then applied these methods to populations of neurons recorded simultaneously in visual areas v1 and v2, and found that glara provides a better description of the recordings than pcca.</s> <s>this work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.</s></p></d>", "label": ["<d><p><s>extracting latent structure from multiple interacting neural populations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of inferring direct neural network connections from calcium imaging time series.</s> <s>inverse covariance estimation has proven to be a fast and accurate method for learning macro- and micro-scale network connectivity in the brain and in a recent kaggle connectomics competition inverse covariance was the main component of several top ten solutions, including our own and the winning team's algorithm.</s> <s>however, the accuracy of inverse covariance estimation is highly sensitive to signal preprocessing of the calcium fluorescence time series.</s> <s>furthermore, brute force optimization methods such as grid search and coordinate ascent over signal processing parameters is a time intensive process, where learning may take several days and parameters that optimize one network may not generalize to networks with different size and parameters.</s> <s>in this paper we show how inverse covariance estimation can be dramatically improved using a simple convolution filter prior to applying sample covariance.</s> <s>furthermore, these signal processing parameters can be learned quickly using a supervised optimization algorithm.</s> <s>in particular, we maximize a binomial log-likelihood loss function with respect to a convolution filter of the time series and the inverse covariance regularization parameter.</s> <s>our proposed algorithm is relatively fast on networks the size of those in the competition (1000 neurons), producing auc scores with similar accuracy to the winning solution in training time under 2 hours on a cpu.</s> <s>prediction on new networks of the same size is carried out in less than 15 minutes, the time it takes to read in the data and write out the solution.</s></p></d>", "label": ["<d><p><s>learning convolution filters for inverse covariance estimation of neural network connectivity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate a general framework of multiplicative multitask feature learning which decomposes each task's model parameters into a multiplication of two components.</s> <s>one of the components is used across all tasks and the other component is task-specific.</s> <s>several previous methods have been proposed as special cases of our framework.</s> <s>we study the theoretical properties of this framework when different regularization conditions are applied to the two decomposed components.</s> <s>we prove that this framework is mathematically equivalent to the widely used multitask feature learning methods that are based on a joint regularization of all model parameters, but with a more general form of regularizers.</s> <s>further, an analytical formula is derived for the across-task component as related to the task-specific component for all these regularizers, leading to a better understanding of the shrinkage effect.</s> <s>study of this framework motivates new multitask learning algorithms.</s> <s>we propose two new learning formulations by varying the parameters in the proposed framework.</s> <s>empirical studies have revealed the relative advantages of the two new formulations by comparing with the state of the art, which provides instructive insights into the feature learning problem with multiple tasks.</s></p></d>", "label": ["<d><p><s>on multiplicative multitask feature learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time).</s> <s>the weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks.</s> <s>two types of convex relaxations have recently been proposed for the tensor multilinear rank.</s> <s>however, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous.</s> <s>we propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms.</s> <s>the results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning.</s> <s>both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.</s></p></d>", "label": ["<d><p><s>multitask learning meets tensor factorization: task imputation via convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new multi-task framework, in which $k$ online learners are sharing a single annotator with limited bandwidth.</s> <s>on each round, each of the $k$ learners receives an input, and makes a prediction about the label of that input.</s> <s>then, a shared (stochastic) mechanism decides which of the $k$ inputs will be annotated.</s> <s>the learner that receives the feedback (label) may update its prediction rule, and we proceed to the next round.</s> <s>we develop an online algorithm for multi-task binary classification that learns in this setting, and bound its performance in the worst-case setting.</s> <s>additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allowed to decouple exploration and exploitation.</s> <s>empirical study with ocr data, vowel prediction (vj project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially makes more (accuracy) for the same labour of the annotator.</s></p></d>", "label": ["<d><p><s>learning multiple tasks in parallel with a shared annotator</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a new method named calibrated multivariate regression (cmr) for fitting high dimensional multivariate regression models.</s> <s>compared to existing methods, cmr calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance.</s> <s>computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity $o(1/\\epsilon)$, where $\\epsilon$ is a pre-specified numerical accuracy.</s> <s>theoretically, we prove that cmr achieves the optimal rate of convergence in parameter estimation.</s> <s>we illustrate the usefulness of cmr by thorough numerical simulations and show that cmr consistently outperforms other high dimensional multivariate regression methods.</s> <s>we also apply cmr on a brain activity prediction problem and find that cmr is as competitive as the handcrafted model created by human experts.</s></p></d>", "label": ["<d><p><s>multivariate regression with calibration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>group lasso is widely used to enforce the structural sparsity, which achieves the sparsity at inter-group level.</s> <s>in this paper, we propose a new formulation called ``exclusive group lasso'', which brings out sparsity at intra-group level in the context of feature selection.</s> <s>the proposed exclusive group lasso is applicable on any feature structures, regardless of their overlapping or non-overlapping structures.</s> <s>we give analysis on the properties of exclusive group lasso, and propose an effective iteratively re-weighted algorithm to solve the corresponding optimization problem with rigorous convergence analysis.</s> <s>we show applications of exclusive group lasso for uncorrelated feature selection.</s> <s>extensive experiments on both synthetic and real-world datasets indicate the good performance of proposed methods.</s></p></d>", "label": ["<d><p><s>exclusive feature learning on arbitrary structures via </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first.</s> <s>previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks.</s> <s>recent work on covariate shift focuses on matching the marginal distributions on observations $x$ across domains.</s> <s>similarly, work on target/conditional shift focuses on matching marginal distributions on labels $y$ and adjusting conditional distributions $p(x|y)$, such that $p(x)$ can be matched across domains.</s> <s>however, covariate shift assumes that the support of test $p(x)$ is contained in the support of training $p(x)$, i.e., the training set is richer than the test set.</s> <s>target/conditional shift makes a similar assumption for $p(y)$.</s> <s>moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available.</s> <s>also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth.</s> <s>in this paper, we consider a general case where both the support and the model change across domains.</s> <s>we transform both $x$ and $y$ by a location-scale shift to achieve transfer between tasks.</s> <s>since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.</s></p></d>", "label": ["<d><p><s>flexible transfer learning under support and model shift</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition.</s> <s>samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion.</s> <s>within the model, textures are represented by the correlations between feature maps in several layers of the network.</s> <s>we show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit.</s> <s>the model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.</s></p></d>", "label": ["<d><p><s>texture synthesis using convolutional neural networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>scene labeling is a challenging computer vision task.</s> <s>it requires the use of both local discriminative features and global context information.</s> <s>we adopt a deep recurrent convolutional neural network (rcnn) for this task, which is originally proposed for object recognition.</s> <s>different from traditional convolutional neural networks (cnn), this model has intra-layer recurrent connections in the convolutional layers.</s> <s>therefore each convolutional layer becomes a two-dimensional recurrent neural network.</s> <s>the units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods.</s> <s>while recurrent iterations proceed, the region of context captured by each unit expands.</s> <s>in this way, feature extraction and context modulation are seamlessly integrated, which is different from typical methods that entail separate modules for the two steps.</s> <s>to further utilize the context, a multi-scale rcnn is proposed.</s> <s>over two benchmark datasets, standford background and sift flow, the model outperforms many state-of-the-art models in accuracy and efficiency.</s></p></d>", "label": ["<d><p><s>convolutional neural networks with intra-layer recurrent connections for scene labeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>syntactic constituency parsing is a fundamental problem in naturallanguage processing which has been the subject of intensive researchand engineering for decades.</s> <s>as a result, the most accurate parsersare domain specific, complex, and inefficient.</s> <s>in this paper we showthat the domain agnostic attention-enhanced sequence-to-sequence modelachieves state-of-the-art results on the most widely used syntacticconstituency parsing dataset, when trained on a large synthetic corpusthat was annotated using existing parsers.</s> <s>it also matches theperformance of standard parsers when trained on a smallhuman-annotated dataset, which shows that this model is highlydata-efficient, in contrast to sequence-to-sequence models without theattention mechanism.</s> <s>our parser is also fast, processing over ahundred sentences per second with an unoptimized cpu implementation.</s></p></d>", "label": ["<d><p><s>grammar as a foreign language</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>efforts to automate the reconstruction of neural circuits from 3d electron microscopic (em) brain images are critical for the field of connectomics.</s> <s>an important computation for reconstruction is the detection of neuronal boundaries.</s> <s>images acquired by serial section em, a leading 3d em technique, are highly anisotropic, with inferior quality along the third dimension.</s> <s>for such images, the 2d max-pooling convolutional network has set the standard for performance at boundary detection.</s> <s>here we achieve a substantial gain in accuracy through three innovations.</s> <s>following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection.</s> <s>second, we incorporate 3d as well as 2d filters, to enable computations that use 3d context.</s> <s>finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map.</s> <s>backpropagation training is accelerated by znn, a new implementation of 3d convolutional networks that uses multicore cpu parallelism for speed.</s> <s>our hybrid 2d-3d architecture could be more generally applicable to other types of anisotropic 3d images, including video, and our recursive framework for any image labeling problem.</s></p></d>", "label": ["<d><p><s>recursive training of 2d-3d convolutional networks for neuronal boundary prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels.</s> <s>recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models.</s> <s>we here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure.</s> <s>our model scales to images of arbitrary size and its likelihood is computationally tractable.</s> <s>we find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.</s></p></d>", "label": ["<d><p><s>generative image modeling using spatial lstms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>state-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.</s> <s>advances like sppnet and fast r-cnn have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck.</s> <s>in this work, we introduce a region proposal network (rpn) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.</s> <s>an rpn is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position.</s> <s>rpns are trained end-to-end to generate high-quality region proposals, which are used by fast r-cnn for detection.</s> <s>with a simple alternating optimization, rpn and fast r-cnn can be trained to share convolutional features.</s> <s>for the very deep vgg-16 model, our detection system has a frame rate of 5fps (including all steps) on a gpu, while achieving state-of-the-art object detection accuracy on pascal voc 2007 (73.2% map) and 2012 (70.4% map) using 300 proposals per image.</s> <s>code is available at https://github.com/shaoqingren/faster_rcnn.</s></p></d>", "label": ["<d><p><s>faster r-cnn: towards real-time object detection with region proposal networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important problem for both graphics and vision is to synthesize novel views of a 3d object from a single image.</s> <s>this is in particular challenging due to the partial observability inherent in projecting a 3d object onto the image space, and the ill-posedness of inferring object shape and pose.</s> <s>however, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data.</s> <s>in this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image.</s> <s>the recurrent structure allows our model to capture long- term dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the multi-pie dataset and for a dataset of 3d chair models, and also show its ability of disentangling latent data factors without using object class labels.</s></p></d>", "label": ["<d><p><s>weakly-supervised disentangling with recurrent transformations for 3d view synthesis</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this work aims to address the problem of image-based question-answering (qa) with new models and datasets.</s> <s>in our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images.</s> <s>our model performs 1.8 times better than the only published results on an existing image qa dataset.</s> <s>we also present a question generation algorithm that converts image descriptions, which are widely available, into qa form.</s> <s>we used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers.</s> <s>a suite of baseline results on this new dataset are also presented.</s></p></d>", "label": ["<d><p><s>exploring models and data for image question answering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we present the mqa model, which is able to answer questions about the content of an image.</s> <s>the answer can be a sentence, a phrase or a single word.</s> <s>our model contains four components: a long short-term memory (lstm) to extract the question representation, a convolutional neural network (cnn) to extract the visual representation, an lstm for storing the linguistic context in an answer, and a fusing component to combine the information from the first three components and generate the answer.</s> <s>we construct a freestyle multilingual image question answering (fm-iqa) dataset to train and evaluate our mqa model.</s> <s>it contains over 150,000 images and 310,000 freestyle chinese question-answer pairs and their english translations.</s> <s>the quality of the generated answers of our mqa model on this dataset is evaluated by human judges through a turing test.</s> <s>specifically, we mix the answers provided by humans and our model.</s> <s>the human judges need to distinguish our model from the human.</s> <s>they will also provide a score (i.e.</s> <s>0, 1, 2, the larger the better) indicating the quality of the answer.</s> <s>we propose strategies to monitor the quality of this evaluation process.</s> <s>the experiments show that in 64.7% of cases, the human judges cannot distinguish our model from humans.</s> <s>the average score is 1.454 (1.918 for human).</s> <s>the details of this work, including the fm-iqa dataset, can be found on the project page: \\url{http://idl.baidu.com/fm-iqa.html}.</s></p></d>", "label": ["<d><p><s>are you talking to a machine? dataset and methods for multilingual image question</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>convolutional neural networks (cnns) can be shifted across 2d images or 3d videos to segment them.</s> <s>they have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background.</s> <s>in contrast, multi-dimensional recurrent nns (md-rnns) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the rnn is a long short-term memory (lstm).</s> <s>despite these theoretical advantages, however, unlike cnns, previous md-lstm variants were hard to parallelise on gpus.</s> <s>here we re-arrange the traditional cuboid order of computations in md-lstm in pyramidal fashion.</s> <s>the resulting pyramid-lstm is easy to parallelise, especially for 3d data such as stacks of brain slice images.</s> <s>pyramid-lstm achieved best known pixel-wise brain image segmentation results on mrbrains13 (and competitive results on em-isbi12).</s></p></d>", "label": ["<d><p><s>parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>simple decision heuristics are models of human and animal behavior that use few pieces of information---perhaps only a single piece of information---and integrate the pieces in simple ways, for example, by considering them sequentially, one at a time, or by giving them equal weight.</s> <s>it is unknown how quickly these heuristics can be learned from experience.</s> <s>we show, analytically and empirically, that only a few training samples lead to substantial progress in learning.</s> <s>we focus on three families of heuristics: single-cue decision making, lexicographic decision making, and tallying.</s> <s>our empirical analysis is the most extensive to date, employing 63 natural data sets on diverse subjects.</s></p></d>", "label": ["<d><p><s>learning from small samples: an analysis of simple decision heuristics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of this paper is to generate high-quality 3d object proposals in the context of autonomous driving.</s> <s>our method exploits stereo imagery to  place proposals in the form of 3d bounding boxes.</s> <s>we formulate the problem as minimizing an energy function encoding object size priors,  ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground.</s> <s>our experiments  show significant performance gains over existing rgb and rgb-d object proposal methods  on the challenging kitti benchmark.</s> <s>combined with convolutional neural net (cnn) scoring, our approach outperforms all existing results on all three kitti object classes.</s></p></d>", "label": ["<d><p><s>3d object proposals for accurate object class detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to infer a multilayer representation of high-dimensional count vectors, we propose the poisson gamma belief network (pgbn) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer.</s> <s>the pgbn's hidden layers are jointly trained with an upward-downward gibbs sampler, each iteration of which upward samples dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer.</s> <s>the gamma-negative binomial process combined with a layer-wise training strategy allows the pgbn to infer the width of each layer given a fixed budget on the width of the first layer.</s> <s>the pgbn with a single hidden layer reduces to poisson factor analysis.</s> <s>example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the pgbn, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over poisson factor analysis, given the same limit on the width of the first layer.</s></p></d>", "label": ["<d><p><s>the poisson gamma belief network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets.</s> <s>existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks.</s> <s>however, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations.</s> <s>we therefore propose to blend representation modelling and task classification into a unified statistical learning problem.</s> <s>a multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder.</s> <s>we show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.</s></p></d>", "label": ["<d><p><s>semi-supervised factored logistic regression for high-dimensional neuroimaging data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks (dnn) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models.</s> <s>in the past, gpus enabled these breakthroughs because of their greater computational speed.</s> <s>in the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices.</s> <s>as a result, there is much interest in research and development of dedicated hardware for deep learning (dl).</s> <s>binary weights, i.e., weights which are constrained to only two possible values (e.g.</s> <s>-1 or 1), would bring great benefits to specialized dl hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks.</s> <s>we introduce binaryconnect, a method which consists in training a dnn with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated.</s> <s>like other dropout schemes, we show that binaryconnect acts as regularizer and we obtain near state-of-the-art results with binaryconnect on the permutation-invariant mnist, cifar-10 and svhn.</s></p></d>", "label": ["<d><p><s>binaryconnect: training deep neural networks with binary weights during propagations</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>recently, strong results have been demonstrated by deep recurrent neural networks on natural language transduction problems.</s> <s>in this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation.</s> <s>these experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as stacks, queues, and deques.</s> <s>we show that these architectures exhibit superior generalisation performance to deep rnns and are often able to learn the underlying generating algorithms in our transduction experiments.</s></p></d>", "label": ["<d><p><s>learning to transduce with unbounded memory</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>discrete fourier transforms provide a significant speedup in the computation of convolutions in deep learning.</s> <s>in this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (cnns).we employ spectral representations to introduce a number of innovations to cnn design.</s> <s>first, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain.</s> <s>this approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality.</s> <s>this representation also enables a new form of stochastic regularization by randomized modification of resolution.</s> <s>we show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.</s> <s>finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters.</s> <s>while this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization.</s> <s>we observe on a variety of popular cnn configurations that this leads to significantly faster convergence during training.</s></p></d>", "label": ["<d><p><s>spectral representations for convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the dynamics of simple decisions are well understood and modeled as a class of random walk models (e.g.</s> <s>laming, 1968; ratcliff, 1978; busemeyer and townsend, 1993; usher and mcclelland, 2001; bogacz et al., 2006).</s> <s>however, most real-life decisions include a rich and dynamically-changing influence of additional information we call context.</s> <s>in this work, we describe a computational theory of decision making under dynamically shifting context.</s> <s>we show how the model generalizes the dominant existing model of fixed-context decision making (ratcliff, 1978) and can be built up from a weighted combination of fixed-context decisions evolving simultaneously.</s> <s>we also show how the model generalizes re- cent work on the control of attention in the flanker task (yu et al., 2009).</s> <s>finally, we show how the model recovers qualitative data patterns in another task of longstanding psychological interest, the ax continuous performance test (servan-schreiber et al., 1996), using the same model parameters.</s></p></d>", "label": ["<d><p><s>a theory of decision making under dynamic context</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bidirectional recurrent neural networks (rnn) are trained to predict both in the positive and negative time directions simultaneously.</s> <s>they have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult.</s> <s>recently, two different frameworks, gsn and nade, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible.</s> <s>as far as we know, neither gsn or nade have been studied in the context of time series before.as an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics.</s> <s>although unidirectional rnns have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial.</s> <s>we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently.</s> <s>our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn.</s> <s>we also provide results on music data for which the bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods.</s></p></d>", "label": ["<d><p><s>bidirectional recurrent neural networks as generative models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many neural circuits are composed of numerous distinct cell types that perform different operations on their inputs, and send their outputs to distinct targets.</s> <s>therefore, a key step in understanding neural systems is to reliably distinguish  cell types.</s> <s>an important example is the retina, for which present-day techniques for identifying cell types are accurate, but very labor-intensive.</s> <s>here, we develop automated classifiers for functional identification of retinal ganglion cells, the output neurons of the retina, based solely on recorded voltage patterns on a large scale array.</s> <s>we use per-cell classifiers based on features extracted from electrophysiological images (spatiotemporal voltage waveforms) and interspike intervals (autocorrelations).</s> <s>these classifiers achieve high performance in distinguishing between the major ganglion cell classes of the primate retina, but fail in achieving the same accuracy in predicting cell polarities (on vs. off).</s> <s>we then show how to use indicators of functional coupling within populations of ganglion cells (cross-correlation) to infer cell polarities with a matrix completion algorithm.</s> <s>this can result in accurate, fully automated methods for cell type classification.</s></p></d>", "label": ["<d><p><s>recognizing retinal ganglion cells in the dark</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (rnn) by combining the elements of the variational autoencoder.</s> <s>we argue that through the use of high-level latent random variables, the variational rnn (vrnn) can model the kind of variability observed in highly structured sequential data such as natural speech.</s> <s>we empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset.</s> <s>our results show the important roles that latent random variables can play in the rnn dynamics.</s></p></d>", "label": ["<d><p><s>a recurrent latent variable model for sequential data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.in this paper we explore the benefit of using recurrent neural networks to model student learning.this family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.we show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.these results suggest a promising new line of research for knowledge tracing.</s></p></d>", "label": ["<d><p><s>deep knowledge tracing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep dynamic generative models are developed to learn sequential dependencies in time-series data.</s> <s>the multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (tsbns), defined as a sequential stack of sigmoid belief networks (sbns).</s> <s>each sbn has a contextual hidden state, inherited from the previous sbns in the sequence, and is used to regulate its hidden bias.</s> <s>scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior.</s> <s>this recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood.</s> <s>experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.</s></p></d>", "label": ["<d><p><s>deep temporal sigmoid belief networks for sequence modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly.</s> <s>this paper argues it is dangerous to think ofthese quick wins as coming for free.</s> <s>using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ml systems.</s> <s>we explore several ml-specific risk factors toaccount for in system design.</s> <s>these include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.</s></p></d>", "label": ["<d><p><s>hidden technical debt in machine learning systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an exploratory approach to statistical model criticism using maximum mean discrepancy (mmd) two sample tests.</s> <s>typical approaches to model criticism require a practitioner to select a statistic by which to measure discrepancies between data and a statistical model.</s> <s>mmd two sample tests are instead constructed as an analytic maximisation over a large space of possible statistics and therefore automatically select the statistic which most shows any discrepancy.</s> <s>we demonstrate on synthetic data that the selected statistic, called the witness function, can be used to identify where a statistical model most misrepresents the data it was trained on.</s> <s>we then apply the procedure to real data where the models being assessed are restricted boltzmann machines, deep belief networks and gaussian process regression and demonstrate the ways in which these models fail to capture the properties of the data they are trained on.</s></p></d>", "label": ["<d><p><s>statistical model criticism using kernel two sample tests</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in user-facing applications, displaying calibrated confidence measures---probabilities that correspond to true frequency---can be as important as obtaining high accuracy.</s> <s>we are interested in calibration for structured prediction problems such as speech recognition, optical character recognition, and medical diagnosis.</s> <s>structured prediction presents new challenges for calibration: the output space is large, and users may issue many types of probability queries (e.g., marginals) on the structured output.</s> <s>we extend the notion of calibration so as to handle various subtleties pertaining to the structured setting,  and then provide a simple recalibration method that trains a binary classifier to predict probabilities of interest.</s> <s>we explore a range of features appropriate for structured recalibration, and demonstrate their efficacy on three real-world datasets.</s></p></d>", "label": ["<d><p><s>calibrated structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the degree of confidence in one's choice or decision is a critical aspect of perceptual decision making.</s> <s>attempts to quantify a decision maker's confidence by measuring accuracy in a task have yielded limited success because confidence and accuracy are typically not equal.</s> <s>in this paper, we introduce a bayesian framework to model confidence in perceptual decision making.</s> <s>we show that this model, based on partially observable markov decision processes (pomdps), is able to predict confidence of a decision maker based only on the data available to the experimenter.</s> <s>we test our model on two experiments on confidence-based decision making involving the well-known random dots motion discrimination task.</s> <s>in both experiments, we show that our model's predictions closely match experimental data.</s> <s>additionally, our model is also consistent with other phenomena such as the hard-easy effect in perceptual decision making.</s></p></d>", "label": ["<d><p><s>a bayesian framework for modeling confidence in perceptual decision making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions.</s> <s>for example, nucleotides in a dna sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions.</s> <s>in all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the dna strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic.</s> <s>these dependencies are not naturally captured by the typical dirichlet-multinomial formulation.</s> <s>here, we leverage a logistic stick-breaking representation and recent innovations in p\\'{o}lya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly gaussian likelihoods, enabling us to take advantage of a host of bayesian inference techniques for gaussian models with minimal overhead.</s></p></d>", "label": ["<d><p><s>dependent multinomial models made easy: stick-breaking with the polya-gamma augmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian nonparametric hidden markov models are typically learned via fixed truncations of the infinite state space or local monte carlo proposals that make small changes to the state space.</s> <s>we develop an inference algorithm for the sticky hierarchical dirichlet process hidden markov model that scales to big datasets by processing a few sequences at a time yet allows rapid adaptation of the state space cardinality.</s> <s>unlike previous point-estimate methods, our novel variational bound penalizes redundant or irrelevant states and thus enables optimization of the state space.</s> <s>our birth proposals use observed data statistics to create useful new states that escape local optima.</s> <s>merge and delete proposals remove ineffective states to yield simpler models with more affordable future computations.</s> <s>experiments on speaker diarization, motion capture, and epigenetic chromatin datasets discover models that are more compact, more interpretable, and better aligned to ground truth segmentations than competitors.</s> <s>we have released an open-source python implementation which can parallelize local inference steps across sequences.</s></p></d>", "label": ["<d><p><s>scalable adaptation of state complexity for nonparametric hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a wide spectrum of discriminative methods is increasingly used  in diverse applications for classification or regression tasks.</s> <s>however, many existing discriminative methods assume that the input data is nearly noise-free, which limits their applications to solve real-world problems.</s> <s>particularly for disease diagnosis, the data acquired by the neuroimaging devices are always prone to different sources of noise.</s> <s>robust discriminative models are somewhat scarce and only a few attempts have been made to make them robust against noise or outliers.</s> <s>these methods focus on detecting either the sample-outliers or feature-noises.</s> <s>moreover, they usually use unsupervised de-noising procedures, or separately de-noise the training and the testing data.</s> <s>all these factors may induce biases in the learning process, and thus limit its performance.</s> <s>in this paper, we propose a classification method based on the least-squares formulation of linear discriminant analysis, which simultaneously detects the sample-outliers and feature-noises.</s> <s>the proposed method operates under a semi-supervised setting, in which both labeled training and unlabeled testing data are incorporated to form the intrinsic geometry of the sample space.</s> <s>therefore, the violating samples or feature values are identified  as sample-outliers or feature-noises, respectively.</s> <s>we test our algorithm on one synthetic and two brain neurodegenerative databases (particularly for parkinson's disease and alzheimer's disease).</s> <s>the results demonstrate that our method outperforms all baseline and state-of-the-art methods, in terms of both accuracy and the area under the roc curve.</s></p></d>", "label": ["<d><p><s>robust feature-sample linear discriminant analysis for brain disorders diagnosis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a bayesian mixed-effects model to learn typical scenarios of changes from longitudinal manifold-valued data, namely repeated measurements of the same objects or individuals at several points in time.</s> <s>the model allows to estimate a group-average trajectory in the space of measurements.</s> <s>random variations of this trajectory result from spatiotemporal transformations, which allow changes in the direction of the trajectory and in the pace at which trajectories are followed.</s> <s>the use of the tools of riemannian geometry allows to derive a generic algorithm for any kind of data with smooth constraints, which lie therefore on a riemannian manifold.</s> <s>stochastic approximations of the expectation-maximization algorithm is used to estimate the model parameters in this highly non-linear setting.the method is used to estimate a data-driven model of the progressive impairments of cognitive functions during the onset of alzheimer's disease.</s> <s>experimental results show that the model correctly put into correspondence the age at which each individual was diagnosed with the disease, thus validating the fact that it effectively estimated a normative scenario of disease progression.</s> <s>random effects provide unique insights into the variations in the ordering and timing of the succession of cognitive impairments across different individuals.</s></p></d>", "label": ["<d><p><s>learning spatiotemporal trajectories from manifold-valued longitudinal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multidimensional recurrent neural networks (mdrnns) have shown a remarkable performance in the area of speech and handwriting recognition.</s> <s>the performance of an mdrnn is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using hessian-free (hf) optimization.</s> <s>given that connectionist temporal classification (ctc) is utilized as an objective of learning an mdrnn for sequence labeling, the non-convexity of  ctc poses a problem when applying hf to the network.</s> <s>as a solution, a convex approximation of ctc is formulated and its relationship with the em algorithm and the fisher information matrix is discussed.</s> <s>an mdrnn up to a depth of 15 layers is successfully trained using hf, resulting in an improved performance for sequence labeling.</s></p></d>", "label": ["<d><p><s>hessian-free optimization for learning deep multidimensional recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a sparse method for scalable automated variational inference (avi) in a large class of models with gaussian process (gp) priors, multiple latent functions, multiple outputs and non-linear likelihoods.</s> <s>our approach maintains the statistical efficiency property of the original avi method, requiring only expectations over univariate gaussian distributions to approximate the posterior with a mixture of gaussians.</s> <s>experiments on small datasets for various problems including  regression, classification, log gaussian cox processes, and warped gps show that our method can perform as well as the full method under high levels of sparsity.</s> <s>on larger experiments using the mnist and the sarcos datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models.</s></p></d>", "label": ["<d><p><s>scalable inference for gaussian process models with black-box likelihoods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational bayes (sgvb) with global model parameters.</s> <s>regular sgvb estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w.r.t.</s> <s>the minibatch size.</s> <s>the efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch.</s> <s>such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence.we find an important connection with regularization by dropout: the original gaussian dropout objective corresponds to sgvb with local noise, a scale-invariant prior and proportionally fixed posterior variance.</s> <s>our method allows inference of more flexibly parameterized posteriors; specifically, we propose \\emph{variational dropout}, a generalization of gaussian dropout, but with a more flexibly parameterized posterior, often leading to better generalization.</s> <s>the method is demonstrated through several experiments.</s></p></d>", "label": ["<d><p><s>variational dropout and the local reparameterization trick</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose the infinite factorial dynamic model (ifdm), a general bayesian nonparametric model for source separation.</s> <s>our model builds on the markov indian buffet process to consider a potentially unbounded number of hidden markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous.</s> <s>for posterior inference, we develop an algorithm based on particle gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems.</s> <s>we evaluate the performance of our ifdm on four well-known applications: multitarget tracking, cocktail party,  power disaggregation, and multiuser detection.</s> <s>our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.</s></p></d>", "label": ["<d><p><s>infinite factorial dynamical model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents.</s> <s>most learning algorithms that involve optimisation of the mutual information rely on the blahut-arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications.</s> <s>this paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning.</s> <s>we develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment.</s> <s>using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.</s></p></d>", "label": ["<d><p><s>variational information maximisation for intrinsically motivated reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a general variational inference method that preserves dependency among the latent variables.</s> <s>our method uses copulas to augment the families of distributions used in mean-field and structured approximations.</s> <s>copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior.</s> <s>with stochastic optimization, inference on the augmented distribution is scalable.</s> <s>furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach.</s> <s>copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.</s></p></d>", "label": ["<d><p><s>copula variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a second-order (hessian or hessian-free) based optimization method for variational inference inspired by gaussian backpropagation, and argue that quasi-newton optimization can be developed as well.</s> <s>this is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity.</s> <s>as an illustrative example, we apply this approach to the problems of bayesian logistic regression and variational auto-encoder (vae).</s> <s>additionally, we compute bounds on the estimator variance of intractable expectations for the family  of lipschitz continuous function.</s> <s>our method is practical, scalable and model free.</s> <s>we demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.</s></p></d>", "label": ["<d><p><s>fast second order stochastic backpropagation for variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider moment matching techniques for estimation in latent dirichlet allocation (lda).</s> <s>by drawing explicit links between lda and discrete versions of independent component analysis (ica), we first derive a new set of cumulant-based tensors, with an improved sample complexity.</s> <s>moreover, we reuse standard ica techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method.</s> <s>in an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.</s></p></d>", "label": ["<d><p><s>rethinking lda: moment matching for discrete ica</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic search algorithms are general black-box optimizers.</s> <s>due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search.</s> <s>yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely.</s> <s>to alleviate these problems, we introduce a new surrogate-based stochastic search approach.</s> <s>we learn simple, quadratic surrogate models of the objective function.</s> <s>as the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models.</s> <s>the algorithm can be misled by an inaccurate optimum introduced by the surrogate.</s> <s>instead, we use information theoretic constraints to bound the `distance' between the new and old data distribution while maximizing the objective function.</s> <s>additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence.</s> <s>we compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task.the proposed method considerably outperforms the existing approaches.</s></p></d>", "label": ["<d><p><s>model-based relative entropy stochastic search</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently there has been substantial interest in spectral methods for learning dynamical systems.</s> <s>these methods are popular since they often offer a good tradeoffbetween computational and statistical efficiency.</s> <s>unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporateprior information such as sparsity or structure.</s> <s>to address this problem, we presenta new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, therebyallowing users to incorporate prior knowledge via standard techniques such asl 1 regularization.</s> <s>many existing spectral methods are special cases of this newframework, using linear regression as the supervised learner.</s> <s>we demonstrate theeffectiveness of our framework by showing examples where nonlinear regressionor lasso let us learn better state representations than plain linear regression does;the correctness of these instances follows directly from our general analysis.</s></p></d>", "label": ["<d><p><s>supervised learning for dynamical system learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an original particle-based implementation of the loopy belief propagation (lpb) algorithm for pairwise markov random fields (mrf) on a continuous state space.</s> <s>the algorithm constructs adaptively efficient proposal distributions approximating the local beliefs  at each note of the mrf.</s> <s>this is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an expectation propagation (ep) framework.</s> <s>the proposed particle scheme provides consistent estimation of the lbp marginals as the number of particles increases.</s> <s>we demonstrate that it provides more accurate results than the particle belief propagation (pbp) algorithm of ihler and mcallester (2009) at a fraction of the computational cost and is additionally more robust empirically.</s> <s>the computational complexity of our algorithm at each iteration is quadratic in the number of particles.</s> <s>we also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy bp marginal distributions and performs almost as well as the original procedure.</s></p></d>", "label": ["<d><p><s>expectation particle belief propagation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a key bottleneck in structured output prediction is the need for inference during training and testing, usually requiring some form of dynamic programming.</s> <s>rather than using approximate inference or tailoring a specialized inference method for a particular structure---standard responses to the scaling challenge---we propose to embed prediction constraints directly into the learned representation.</s> <s>by eliminating the need for explicit inference a more scalable approach to structured output prediction can be achieved, particularly at test time.</s> <s>we demonstrate the idea for multi-label prediction under subsumption and mutual exclusion constraints,  where a relationship to maximum margin structured output prediction can be established.</s> <s>experiments demonstrate that the benefits of structured output training can still be realized even after inference has been eliminated.</s></p></d>", "label": ["<d><p><s>embedding inference for structured multilabel prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tractable learning aims to learn probabilistic models where inference is guaranteed to be efficient.</s> <s>however, the particular class of queries that is tractable depends on the model and underlying representation.</s> <s>usually this class is mpe or conditional probabilities $\\pr(\\xs|\\ys)$ for joint assignments~$\\xs,\\ys$.</s> <s>we propose a tractable learner that guarantees efficient inference for a broader class of queries.</s> <s>it simultaneously learns a markov network and its tractable circuit representation, in order to guarantee and measure tractability.</s> <s>our approach differs from earlier work by using sentential decision diagrams (sdd) as the tractable language instead of arithmetic circuits (ac).</s> <s>sdds have desirable properties, which more general representations such as acs lack, that enable basic primitives for boolean circuit compilation.</s> <s>this allows us to support a broader class of complex probability queries, including counting, threshold, and parity, in polytime.</s></p></d>", "label": ["<d><p><s>tractable learning for complex probability queries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data.</s> <s>crowdsourcing is cheap and fast, but suffers from the problem of low-quality data.</s> <s>to address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest.</s> <s>we show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible.</s> <s>we also show that among all possible incentive-compatible  mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers.</s> <s>interestingly, this unique mechanism takes a multiplicative form.</s> <s>the simplicity of the mechanism is an added benefit.</s> <s>in preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.</s></p></d>", "label": ["<d><p><s>double or nothing: multiplicative incentive mechanisms for crowdsourcing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients by sampling from the variational distribution.</s> <s>this algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task explores intelligently the most relevant part of the variational distribution.</s> <s>this is achieved by performing an exact expectation over the single random variable that most correlates with the variational parameter of interest resulting in a rao-blackwellized estimate that has low variance.</s> <s>our method works efficiently for both continuous and discrete random variables.</s> <s>furthermore, the proposed algorithm has interesting similarities with gibbs sampling but at the same time, unlike gibbs sampling, can be trivially parallelized.</s></p></d>", "label": ["<d><p><s>local expectation gradients for black box variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions.</s> <s>in this paper we develop a loss function for multi-label learning, based on the wasserstein distance.</s> <s>the wasserstein distance provides a natural notion of dissimilarity for probability measures.</s> <s>although optimizing with respect to the exact wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed.</s> <s>we describe an efficient learning algorithm based on this regularization, as well as a novel extension of the wasserstein distance from probability measures to unnormalized measures.</s> <s>we also describe a statistical learning bound for the loss.</s> <s>the wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space.</s> <s>we demonstrate this property on a real-data tag prediction problem, using the yahoo flickr creative commons dataset, outperforming a baseline that doesn't use the metric.</s></p></d>", "label": ["<d><p><s>learning with a wasserstein loss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider in this work the space of probability measures $p(x)$ on a hilbert space $x$ endowed with the 2-wasserstein metric.</s> <s>given a finite family of probability measures in $p(x)$, we propose an iterative approach to compute geodesic principal components that summarize efficiently that dataset.</s> <s>the 2-wasserstein metric provides $p(x)$ with a riemannian structure and associated concepts (fr\\'echet mean, geodesics, tangent vectors) which prove crucial to follow the intuitive approach laid out by standard principal component analysis.</s> <s>to make our approach feasible, we propose to use an alternative parameterization of geodesics proposed by \\citet[\\s 9.2]{ambrosio2006gradient}.</s> <s>these \\textit{generalized} geodesics are parameterized with two velocity fields defined on the support of the wasserstein mean of the data, each pointing towards an ending point of the generalized geodesic.</s> <s>the resulting optimization problem of finding principal components is solved by adapting a projected gradient descend method.</s> <s>experiment results show the ability of the computed principal components to capture axes of variability on histograms and probability measures data.</s></p></d>", "label": ["<d><p><s>principal geodesic analysis for probability measures under the optimal transport metric</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show that the maximum-likelihood (ml) estimate of models derived from luce's choice axiom (e.g., the plackett-luce model) can be expressed as the stationary distribution of a markov chain.</s> <s>this conveys insight into several recently proposed spectral inference algorithms.</s> <s>we take advantage of this perspective and formulate a new spectral algorithm that is significantly more accurate than previous ones for the plackett--luce model.</s> <s>with a simple adaptation, this algorithm can be used iteratively, producing a sequence of estimates that converges to the ml estimate.</s> <s>the ml version runs faster than competing approaches on a benchmark of five datasets.</s> <s>our algorithms are easy to implement, making them relevant for practitioners at large.</s></p></d>", "label": ["<d><p><s>fast and accurate inference of plackett?luce models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a simple method to learn linear causal cyclic models in the presence of latent variables.</s> <s>the method relies on equilibrium data of the model recorded under a specific kind of interventions (``shift interventions'').</s> <s>the location and strength of these interventions do not have to be known and can be estimated from the data.</s> <s>our method, called backshift, only uses second moments of the data and performs simple joint matrix diagonalization, applied to differences between covariance matrices.</s> <s>we give a sufficient and necessary condition for identifiability of the system, which is fulfilled almost surely under some quite general assumptions if and only if there are at least three distinct experimental settings, one of which can be pure observational data.</s> <s>we demonstrate the performance on some simulated data and applications in flow cytometry and financial time series.</s></p></d>", "label": ["<d><p><s>backshift: learning causal cyclic graphs from unknown shift interventions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for weakly-supervised problems with deterministic constraints between the latent variables and observed output, learning necessitates performing inference over latent variables conditioned on the output, which can be intractable no matter how simple the model family is.</s> <s>even finding a single latent variable setting that satisfies the constraints could be difficult; for instance, the observed output may be the result of a latent database query or graphics program which must be inferred.</s> <s>here, the difficulty lies in not the model but the supervision, and poor approximations at this stage could lead to following the wrong learning signal entirely.</s> <s>in this paper, we develop a rigorous approach to relaxing the supervision, which yields asymptotically consistent parameter estimates despite altering the supervision.</s> <s>our approach parameterizes a family of increasingly accurate relaxations, and jointly optimizes both the model and relaxation parameters, while formulating constraints between these parameters to ensure efficient inference.</s> <s>these efficiency constraints allow us to learn in otherwise intractable settings, while asymptotic consistency ensures that we always follow a valid learning signal.</s></p></d>", "label": ["<d><p><s>learning with relaxed supervision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning.</s> <s>kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach.</s> <s>however, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic.</s> <s>such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the average run length in the online case.</s> <s>in this paper we propose two related computationally efficient m-statistics for kernel-based change-point detection when the amount of background data is large.</s> <s>a novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-of-measure.</s> <s>such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping.</s> <s>we show that our methods perform well in both synthetic and real world data.</s></p></d>", "label": ["<d><p><s>m-statistic for kernel change-point detection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a class of nonparametric two-sample tests with a cost linear in the sample size.</s> <s>two tests are given, both  based on an ensemble of distances between analytic functions representing each of the distributions.</s> <s>the first test uses smoothed empirical characteristic functions to represent the distributions, the second uses distribution embeddings in a reproducing kernel hilbert space.</s> <s>analyticity implies that differences in the distributions may be detected almost surely at a finite number of randomly chosen locations/frequencies.</s> <s>the new tests are consistent against a larger class of alternatives than the previous linear-time tests based on the (non-smoothed) empirical characteristic functions, while being much faster than the current state-of-the-art quadratic-time kernel-based or energy distance-based tests.</s> <s>experiments on artificial benchmarks and on challenging real-world testing problems demonstrate that our tests give a better power/time tradeoff than  competing approaches, and in some cases, better outright power than even the most expensive quadratic-time tests.</s> <s>this performance advantage is retained even in high dimensions, and in cases where the difference in distributions is not observable with low order statistics.</s></p></d>", "label": ["<d><p><s>fast two-sample testing with analytic representations of probability measures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multivariate loss functions are used to assess performance in many modern prediction tasks, including information retrieval and ranking applications.</s> <s>convex approximations are typically optimized in their place to avoid np-hard empirical risk minimization problems.</s> <s>we propose to approximate the training data instead of the loss function by posing multivariate prediction as an adversarial game between a loss-minimizing prediction player and a loss-maximizing evaluation player constrained to match specified properties of training data.</s> <s>this avoids the non-convexity of empirical risk minimization, but game sizes are exponential in the number of predicted variables.</s> <s>we overcome this intractability using the double oracle constraint generation method.</s> <s>we demonstrate the efficiency and predictive performance of our approach on tasks evaluated using the precision at k, the f-score and the discounted cumulative gain.</s></p></d>", "label": ["<d><p><s>adversarial prediction games for multivariate losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we are interested in supervised metric learning of mahalanobis like distances.</s> <s>existing approaches mainly focus on learning a new distance using similarity and dissimilarity constraints between examples.</s> <s>in this paper, instead of bringing closer examples of the same class and pushing far away examples of different classes we propose to move the examples with respect to virtual points.</s> <s>hence, each example is brought closer to a a priori defined virtual point reducing the number of constraints to satisfy.</s> <s>we show that our approach admits a closed form solution which can be kernelized.</s> <s>we provide a theoretical analysis showing the consistency of the approach and establishing some links with other classical metric learning methods.</s> <s>furthermore we propose an efficient solution to the difficult problem of selecting virtual points based in part on recent works in optimal transport.</s> <s>lastly, we evaluate our approach on several state of the art datasets.</s></p></d>", "label": ["<d><p><s>regressive virtual metric learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>random walk kernels measure graph similarity by counting matching walks in two graphs.</s> <s>in their most popular form of geometric random walk kernels, longer walks of length $k$ are downweighted by a factor of $\\lambda^k$ ($\\lambda < 1$) to ensure convergence of the corresponding geometric series.</s> <s>we know from the field of link prediction that this downweighting often leads to a phenomenon referred to as halting: longer walks are downweighted so much that the similarity score is completely dominated by the comparison of walks of length 1.</s> <s>this is a naive kernel between edges and vertices.</s> <s>we theoretically show that halting may occur in geometric random walk kernels.</s> <s>we also empirically quantify its impact in simulated datasets and popular graph classification benchmark datasets.</s> <s>our findings promise to be instrumental in future graph kernel development and applications of random walk kernels.</s></p></d>", "label": ["<d><p><s>halting in random walk kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>causal structure learning from time series data is a major scientific challenge.</s> <s>existing algorithms assume that measurements occur sufficiently quickly; more precisely, they assume that the system and measurement timescales are approximately equal.</s> <s>in many scientific domains, however, measurements occur at a significantly slower rate than the underlying system changes.</s> <s>moreover, the size of the mismatch between timescales is often unknown.</s> <s>this paper provides three distinct causal structure learning algorithms, all of which discover all dynamic graphs that could explain the observed measurement data as arising from undersampling at some rate.</s> <s>that is, these algorithms all learn causal structure without assuming any particular relation between the measurement and system timescales; they are thus rate-agnostic.</s> <s>we apply these algorithms to data from simulations.</s> <s>the results provide insight into the challenge of undersampling.</s></p></d>", "label": ["<d><p><s>rate-agnostic (causal) structure learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we design an online algorithm to classify the vertices of a graph.</s> <s>underpinning the algorithm is the probability distribution of an  ising model isomorphic to the graph.</s> <s>each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far.</s> <s>computing these classifications is unfortunately based on a $\\#p$-complete problem.</s> <s>this motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework.</s> <s>our algorithm is optimal when the graph is a tree matching the prior results in [1].for a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound.</s> <s>the algorithm is efficient as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.</s></p></d>", "label": ["<d><p><s>online prediction at the limit of zero temperature</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability.</s> <s>in this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes map inference in markov logic and similar statistical-relational languages.</s> <s>we introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory.</s> <s>we provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time.</s> <s>we show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain.</s> <s>we demonstrate the effectiveness of these techniques through experiments in two relational domains.</s> <s>we also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.</s></p></d>", "label": ["<d><p><s>lifted symmetry detection and breaking for map inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the multi-armed bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine.</s> <s>one of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts.</s> <s>the existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide.</s> <s>in this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting.</s> <s>the current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue.</s> <s>indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent.</s> <s>after this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.</s></p></d>", "label": ["<d><p><s>bandits with unobserved confounders: a causal approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with robustness analysis of decision making under uncertainty.</s> <s>we consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration.</s> <s>in particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs.</s> <s>a novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation.</s> <s>the bound serves as a high-confidence certificate for providing future performance or safety guarantees.</s> <s>the approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.</s></p></d>", "label": ["<d><p><s>sample complexity bounds for iterative stochastic policy optimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we provide a theoretical framework for analyzing basis function construction for linear value function approximation in markov decision processes (mdps).</s> <s>we show that important existing methods, such as krylov bases and bellman-error-based methods are a special case of the general framework we develop.</s> <s>we provide a general algorithmic framework for computing basis function refinements which ?respect?</s> <s>the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework.</s> <s>we also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding ?prototypes?</s> <s>that are diverse enough to represent the given mdp.</s></p></d>", "label": ["<d><p><s>basis refinement strategies for linear value function approximation in mdps</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an ``any-time'' fashion, expending more computation for tighter bounds.</s> <s>on the other hand, monte carlo estimators such as importance sampling have excellent any-time behavior, but depend critically on the proposal distribution.</s> <s>we propose a simple monte carlo based inference method that augments convex variational bounds by adding importance sampling (is).</s> <s>we argue that convex variational methods naturally provide good is proposals that ``cover the probability of the target distribution, and reinterpret the variational optimization as designing a proposal to minimizes an upper bound on the variance of our is estimator.</s> <s>this both provides an accurate estimator and enables the construction of any-time probabilistic bounds that improve quickly and directly on state of-the-art variational bounds, which provide certificates of accuracy given enough samples relative to the error in the initial bound.</s></p></d>", "label": ["<d><p><s>probabilistic variational bounds for graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent advances in bayesian learning with large-scale data have witnessed emergence of stochastic gradient mcmc algorithms (sg-mcmc), such as stochastic gradient langevin dynamics (sgld), stochastic gradient hamiltonian mcmc (sghmc), and the stochastic gradient thermostat.</s> <s>while finite-time convergence properties of the sgld with a 1st-order euler integrator have recently been studied, corresponding theory for general sg-mcmcs has not been explored.</s> <s>in this paper we consider general sg-mcmcs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures.</s> <s>our theoretical results show faster convergence rates and more accurate invariant measures for sg-mcmcs with higher-order integrators.</s> <s>for example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (mse) of the posterior average for the sghmc achieves an optimal convergence rate of $l^{-4/5}$ at $l$ iterations, compared to $l^{-2/3}$ for the sghmc and sgld with 1st-order euler integrators.</s> <s>furthermore, convergence results of decreasing-step-size sg-mcmcs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence.</s> <s>experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.</s></p></d>", "label": ["<d><p><s>on the convergence of stochastic gradient mcmc algorithms with high-order integrators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>let $f: \\{-1,1\\}^n \\rightarrow \\mathbb{r}$ be an $n$-variate polynomial consisting of $2^n$ monomials, in which only $s\\ll 2^n$ coefficients are non-zero.</s> <s>the goal is to learn the polynomial by querying the values of $f$.</s> <s>we introduce an active learning framework that is associated with a low query cost and computational runtime.</s> <s>the significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of {\\it sparse-graph codes}, such as low-density-parity-check (ldpc) codes, which represent the state-of-the-art of modern packet communications.</s> <s>more significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding.</s> <s>the key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size $n$ and sparsity $s$).</s> <s>our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to $s={o}(2^{\\delta n})$ for any $\\delta\\in(0,1)$, where $f$ is exactly learned using ${o}(ns)$ queries in time ${o}(n s \\log s)$, even if the queries are perturbed by gaussian noise.</s> <s>we further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts.</s> <s>by writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary $n$-node unknown graph using only few cut queries, which scales {\\it almost linearly} in the number of edges and {\\it sub-linearly} in the graph size $n$.</s> <s>experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.</s></p></d>", "label": ["<d><p><s>an active learning framework using sparse-graph codes for sparse polynomials and graph sketching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider the binary classification problem of predicting a target variable y from a discrete feature vector x = (x1,...,xd).</s> <s>when the probability distribution p(x,y) is known, the optimal classifier, leading to the minimum misclassification rate, is given by the maximum a-posteriori probability (map) decision rule.</s> <s>however, in practice, estimating the complete joint distribution p(x,y) is computationally and statistically impossible for large values of d. therefore, an alternative approach is to first estimate some low order marginals of the joint probability distribution p(x,y) and then design the classifier based on the estimated low order marginals.</s> <s>this approach is also helpful when the complete training data instances are not available due to privacy concerns.</s> <s>in this work, we consider the problem of designing the optimum classifier based on some estimated low order marginals of (x,y).</s> <s>we prove that for a given set of marginals, the minimum hirschfeld-gebelein-r?enyi (hgr) correlation principle introduced in [1] leads to a randomized classification rule which is shown to have a misclassification rate no larger than twice the misclassification rate of the optimal classifier.</s> <s>then, we show that under a separability condition, the proposed algorithm is equivalent to a randomized linear regression approach which naturally results in a robust feature selection method selecting a subset of features having the maximum worst case hgr correlation with the target variable.</s> <s>our theoretical upper-bound is similar to the recent discrete chebyshev classifier (dcc) approach [2], while the proposed algorithm has significant computational advantages since it only requires solving a least square optimization problem.</s> <s>finally, we numerically compare our proposed algorithm with the dcc classifier and show that the proposed algorithm results in better misclassification rate over various uci data repository datasets.</s></p></d>", "label": ["<d><p><s>discrete r?nyi classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>high dimensional regression benefits from sparsity promoting regularizations.</s> <s>screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers.</s> <s>when the procedure is proven not to discard features wrongly the rules are said to be safe.</s> <s>in this paper we derive new safe rules for generalized linear models regularized with l1 and l1/l2 norms.</s> <s>the rules are based on duality gap computations and spherical safe regions whose diameters converge to zero.</s> <s>this allows to discard safely more variables, in particular for low regularization parameters.</s> <s>the gap safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.</s></p></d>", "label": ["<d><p><s>gap safe screening rules for sparse multi-task and multi-class models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>marginal map inference involves making map predictions in systems defined with latent variables or missing information.</s> <s>it is significantly more difficult than pure marginalization and map tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist.</s> <s>in this work, we generalize dual decomposition to a generic powered-sum inference task, which includes marginal map, along with pure marginalization and map, as special cases.</s> <s>our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently.</s> <s>we demonstrate our approach on various inference queries over real-world problems from the uai approximate inference challenge, showing that our framework is faster and more reliable than previous methods.</s></p></d>", "label": ["<d><p><s>decomposition bounds for marginal map</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the paper studies transition phenomena in information cascades observed along a diffusion process over some graph.</s> <s>we introduce the laplace hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time.</s> <s>using this concept, we prove tight non-asymptotic bounds for the influence of a set of nodes, and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical.</s> <s>our contributions include formal definitions and tight lower bounds of critical explosion time.</s> <s>we illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models.</s> <s>finally, we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds.</s></p></d>", "label": ["<d><p><s>anytime influence bounds and the explosive behavior of continuous-time diffusion networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>mixture modeling is a general technique for making any simple model more expressive through weighted combination.</s> <s>this generality and simplicity in part explains the success of the expectation maximization (em) algorithm, in which updates are easy to derive for a wide class of mixture models.</s> <s>however, the likelihood of a mixture model is non-convex, so em has no known global convergence guarantees.</s> <s>recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist.</s> <s>in this work, we present polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in em.</s> <s>polymom is applicable when the moments of a single mixture component are polynomials of the parameters.</s> <s>our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a generalized moment problem.</s> <s>we solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra.</s> <s>this framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation.</s> <s>simulations show good empirical performance on several models.</s></p></d>", "label": ["<d><p><s>estimating mixture models via mixtures of polynomials</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gaussian graphical models (ggms) are popular tools for studying network structures.</s> <s>however, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the gaussian distribution.</s> <s>in this paper, we propose the trimmed graphical lasso for robust estimation of sparse ggms.</s> <s>our method guards against outliers by an implicit trimming mechanism akin to the popular least trimmed squares method used for linear regression.</s> <s>we provide a rigorous statistical analysis of our estimator in the high-dimensional setting.</s> <s>in contrast, existing approaches for robust sparse ggms estimation lack statistical guarantees.</s> <s>our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach.</s></p></d>", "label": ["<d><p><s>robust gaussian graphical modeling with the trimmed graphical lasso</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the completion of low rank matrices from few entries is a task with many practical applications.</s> <s>we consider here two aspects of this problem: detectability, i.e.</s> <s>the ability to estimate the rank $r$ reliably from the fewest possible random entries, and performance in achieving small reconstruction error.</s> <s>we propose a spectral algorithm for these two tasks called macbeth (for matrix completion with the bethe hessian).</s> <s>the rank is estimated as the number of negative eigenvalues of the bethe hessian matrix, and the corresponding eigenvectors are used as initial condition for the minimization of the discrepancy between the estimated matrix and the revealed entries.</s> <s>we analyze the performance in a random matrix setting using results from the statistical mechanics of the hopfield neural network, and show in particular that macbeth efficiently detects the rank $r$ of a large $n\\times m$ matrix from $c(r)r\\sqrt{nm}$ entries, where $c(r)$ is a constant close to $1$.</s> <s>we also evaluate the corresponding root-mean-square error empirically and show that macbeth compares favorably to other existing approaches.</s></p></d>", "label": ["<d><p><s>matrix completion from fewer entries: spectral detectability and rank estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the robust principal component analysis (rpca) problem seeks to separate low-rank trends from sparse outlierswithin a data matrix, that is, to approximate a $n\\times d$ matrix $d$ as the sum of a low-rank matrix $l$ and a sparse matrix $s$.we examine the robust principal component analysis (rpca) problem under data compression, wherethe data $y$ is approximately given by $(l + s)\\cdot c$, that is, a low-rank $+$ sparse data matrix that has been compressed to size $n\\times m$ (with $m$ substantially smaller than the original dimension $d$) via multiplication witha compression matrix $c$.</s> <s>we give a convex program for recovering the sparse component $s$ along with the compressed low-rank component $l\\cdot c$, along with upper bounds on the error of this reconstructionthat scales naturally with the compression dimension $m$ and coincides with existing results for the uncompressedsetting $m=d$.</s> <s>our results can also handle error introduced through additive noise or through missing data.the scaling of dimension, compression, and signal complexity in our theoretical results is verified empirically through simulations, and we also apply our method to a data set measuring chlorine concentration acrossa network of sensors, to test its performance in practice.</s></p></d>", "label": ["<d><p><s>robust pca with compressed data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate two novel mixed robust/average-case submodular data partitioning problems that we collectively call submodular partitioning.</s> <s>these problems generalize purely robust instances of the problem, namely max-min submodular fair allocation (sfa) and \\emph{min-max submodular load balancing} (slb), and also average-case instances, that is the submodular welfare problem (swp) and submodular multiway partition (smp).</s> <s>while the robust versions have been studied in the theory community, existing work has focused on tight approximation guarantees, and the resultant algorithms are not generally scalable to large real-world applications.</s> <s>this contrasts the average case instances, where most of the algorithms are scalable.</s> <s>in the present paper, we bridge this gap, by proposing several new algorithms (including greedy, majorization-minimization, minorization-maximization, and relaxation algorithms) that not only scale to large datasets but that also achieve theoretical approximation guarantees comparable to the state-of-the-art.</s> <s>we moreover provide new scalable algorithms that apply to additive combinations of the robust and average-case objectives.</s> <s>we show that these problems have many applications in machine learning (ml), including data partitioning and load balancing for distributed ml, data clustering, and image segmentation.</s> <s>we empirically demonstrate the efficacy of our algorithms on real-world problems involving data partitioning for distributed optimization (of convex and deep neural network objectives), and also purely unsupervised image segmentation.</s></p></d>", "label": ["<d><p><s>mixed robust/average submodular partitioning: fast algorithms, guarantees, and applications</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper considers the subspace clustering problem where the data contains irrelevant or corrupted features.</s> <s>we propose a method termed ``robust dantzig selector'' which can successfully identify the clustering structure even with the presence of irrelevant features.</s> <s>the idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features.</s> <s>we establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations.</s> <s>to the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.</s></p></d>", "label": ["<d><p><s>subspace clustering with irrelevant features via robust dantzig selector</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>finding communities in networks is a problem that remains difficult, in spite of the amount of attention it has recently received.</s> <s>the stochastic block-model (sbm) is a generative model for graphs with communities for which, because of its simplicity, the theoretical understanding has advanced fast in recent years.</s> <s>in particular, there have been various results showing that simple versions of spectralclustering using the normalized laplacian of the graph can recoverthe communities almost perfectly with high probability.</s> <s>here we show that essentially the same algorithm used for the sbm and for its extension called degree-corrected sbm, works on a wider class of block-models, which we call preference frame models, with essentially the same guarantees.</s> <s>moreover, the parametrization we introduce clearly exhibits the free parameters needed to specify this class of models, and results in bounds that expose with more clarity the parameters that control the recovery error in this model class.</s></p></d>", "label": ["<d><p><s>a class of network models recoverable by spectral clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a $k$-submodular function is a generalization of a submodular function, where the input consists of $k$ disjoint subsets, instead of a single subset, of the domain.many machine learning problems, including influence maximization with $k$ kinds of topics and sensor placement with $k$ kinds of sensors, can be naturally modeled as the problem of maximizing monotone $k$-submodular functions.in this paper, we give constant-factor approximation algorithms for maximizing monotone $k$-submodular functions subject to several size constraints.the running time of our algorithms are almost linear in the domain size.we experimentally demonstrate that our algorithms outperform baseline algorithms in terms of the solution quality.</s></p></d>", "label": ["<d><p><s>monotone k-submodular function maximization with size constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>maximum a-posteriori (map) inference is an important task for many applications.</s> <s>although the standard formulation gives rise to a hard combinatorial optimization problem, several effective approximations have been proposed and studied in recent years.</s> <s>we focus on linear programming (lp) relaxations, which have achieved state-of-the-art performance in many applications.</s> <s>however, optimization of the resulting program is in general challenging due to non-smoothness and complex non-separable constraints.therefore, in this work we study the benefits of augmenting the objective function of the relaxation with strong convexity.</s> <s>specifically, we introduce strong convexity by adding a quadratic term to the lp relaxation objective.</s> <s>we provide theoretical guarantees for the resulting programs, bounding the difference between their optimal value and the original optimum.</s> <s>further, we propose suitable optimization algorithms and analyze their convergence.</s></p></d>", "label": ["<d><p><s>smooth and strong: map inference with linear convergence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present and analyze several strategies for improving the performance ofstochastic variance-reduced gradient (svrg) methods.</s> <s>we first show that theconvergence rate of these methods can be preserved under a decreasing sequenceof errors in the control variate, and use this to derive variants of svrg that usegrowing-batch strategies to reduce the number of gradient calculations requiredin the early iterations.</s> <s>we further (i) show how to exploit support vectors to reducethe number of gradient computations in the later iterations, (ii) prove that thecommonly?used regularized svrg iteration is justified and improves the convergencerate, (iii) consider alternate mini-batch selection strategies, and (iv) considerthe generalization error of the method.</s></p></d>", "label": ["<d><p><s>stopwasting my gradients: practical svrg</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent literature~\\cite{ando} suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction.</s> <s>however, the choice of optimal embedding and an efficient algorithm to compute the same remains open.</s> <s>in this paper, we show that orthonormal representations, a class of unit-sphere graph embeddings are pac learnable.</s> <s>existing pac-based analysis do not  apply as the vc dimension of the function class is infinite.</s> <s>we propose an alternative pac-based bound, which do not depend on the vc dimension of the underlying function class, but is related to the famous lov\\'{a}sz~$\\vartheta$ function.</s> <s>the main contribution of the paper is spore, a spectral regularized orthonormal embedding for graph transduction, derived from the pac bound.</s> <s>spore is posed as a non-smooth convex function over an \\emph{elliptope}.</s> <s>these problems are usually solved as semi-definite programs (sdps) with time complexity $o(n^6)$.</s> <s>we present,  infeasible inexact proximal~(iip): an inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible.</s> <s>iip is more scalable than sdp, has an $o(\\frac{1}{\\sqrt{t}})$ convergence, and is generally applicable whenever a suitable approximate projection is available.</s> <s>we use iip to compute spore where the approximate projection step is computed by fista, an accelerated gradient descent procedure.</s> <s>we show that the method has a convergence rate of $o(\\frac{1}{\\sqrt{t}})$.</s> <s>the proposed algorithm easily scales to 1000's of vertices, while the standard sdp computation does not scale beyond few hundred vertices.</s> <s>furthermore, the analysis presented here easily extends to the multiple graph setting.</s></p></d>", "label": ["<d><p><s>spectral norm regularization of orthonormal representations for graph transduction</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we investigate the problem of learning an unknown probability distribution over a discrete population from random samples.</s> <s>our goal is to design efficient algorithms that simultaneously achieve low error in total variation norm while guaranteeing differential privacy to the individuals of the population.we describe a general approach that yields near sample-optimal and computationally efficient differentially private estimators for a wide range of well-studied and natural distribution families.</s> <s>our theoretical results show that for a wide variety of structured distributions there exist private estimation algorithms that are nearly as efficient - both in terms of sample size and running time - as their non-private counterparts.</s> <s>we complement our theoretical guarantees with an experimental evaluation.</s> <s>our experiments illustrate the speed and accuracy of our private estimators on both synthetic mixture models and a large public data set.</s></p></d>", "label": ["<d><p><s>differentially private learning of structured discrete distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a robust portfolio optimization approach based on quantile statistics.</s> <s>the proposed method is robust to extreme events in asset returns, and accommodates large portfolios under limited historical data.</s> <s>specifically, we show that the risk of the estimated portfolio converges to the oracle optimal risk with parametric rate under weakly dependent asset returns.</s> <s>the theory does not rely on higher order moment assumptions, thus allowing for heavy-tailed asset returns.</s> <s>moreover, the rate of convergence quantifies that the size of the portfolio under management is allowed to scale exponentially with the sample size of the historical data.</s> <s>the empirical effectiveness of the proposed method is demonstrated under both synthetic and real stock data.</s> <s>our work extends existing ones by achieving robustness in high dimensions, and by allowing serial dependence.</s></p></d>", "label": ["<d><p><s>robust portfolio optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling.</s> <s>most bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice.</s> <s>also, the existing bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical.</s> <s>our approach eliminates both requirements and achieves an exponential convergence rate.</s></p></d>", "label": ["<d><p><s>bayesian optimization with exponential convergence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest.</s> <s>here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance.by extending the notion of \\emph{statistical leverage scores} to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the \\emph{effective dimensionality} of the problem.</s> <s>this latter quantity is often much smaller than previous bounds that depend on the \\emph{maximal degrees of freedom}.</s> <s>we give an empirical evidence supporting this fact.</s> <s>our second contribution is to present a fast algorithm to quickly compute coarse approximations to thesescores in time linear in the number of samples.</s> <s>more precisely, the running time of the algorithm is $o(np^2)$ with $p$ only depending on the trace of the kernel matrix and the regularization parameter.</s> <s>this is obtained via a variant of squared length sampling that we adapt to the kernel setting.</s> <s>lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.</s></p></d>", "label": ["<d><p><s>fast randomized kernel ridge regression with statistical guarantees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient descent (sgd) is a ubiquitous algorithm for a variety of machine learning problems.</s> <s>researchers and industry have developed several techniques to optimize sgd's runtime performance, including asynchronous execution and reduced precision.</s> <s>our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques.</s> <s>specifically, we useour new analysis in three ways: (1) we derive convergence rates for the convex case (hogwild) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous sgd algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous sgd algorithm, called buckwild, that uses lower-precision arithmetic.</s> <s>we show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.</s></p></d>", "label": ["<d><p><s>taming the wild: a unified analysis of hogwild-style algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this poster has been moved from monday #86 to thursday #101.</s> <s>stochastic convex optimization is a basic and well studied primitive in machine learning.</s> <s>it is well known that convex and lipschitz functions can be minimized efficiently using stochastic gradient descent (sgd).the normalized gradient descent (ngd) algorithm, is an adaptation of gradient descent, which updates according to the direction of the gradients, rather than the gradients themselves.</s> <s>in this paper we analyze a stochastic version of ngd and prove its convergence to a global minimum for a wider class of functions: we require the functions to be quasi-convex and locally-lipschitz.</s> <s>quasi-convexity broadens the concept of unimodality to multidimensions and allows for certain types of saddle points, which are a known hurdle for first-order optimization methods such as gradient descent.</s> <s>locally-lipschitz functions are only required to be lipschitz in a small region around the optimum.</s> <s>this assumption circumvents gradient explosion, which is another known hurdle for gradient descent variants.</s> <s>interestingly, unlike the vanilla sgd algorithm, the stochastic normalized gradient descent algorithm provably requires a minimal minibatch size.</s></p></d>", "label": ["<d><p><s>beyond convexity: stochastic quasi-convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the following detection problem: given a realization of asymmetric matrix $x$ of dimension $n$, distinguish between the hypothesisthat all upper triangular variables are i.i.d.</s> <s>gaussians variableswith mean 0 and variance $1$ and the hypothesis that there is aplanted principal submatrix $b$ of dimension $l$ for which all upper triangularvariables are i.i.d.</s> <s>gaussians with mean $1$ and variance $1$, whereasall other upper triangular elements of $x$ not in $b$ are i.i.d.gaussians variables with mean 0 and variance $1$.</s> <s>we refer to this asthe `gaussian hidden clique problem'.</s> <s>when $l=( 1 + \\epsilon) \\sqrt{n}$ ($\\epsilon > 0$), it is possible to solve thisdetection problem with probability $1 - o_n(1)$ by computing thespectrum of $x$ and considering the largest eigenvalue of $x$.we prove that when$l < (1-\\epsilon)\\sqrt{n}$ no algorithm that examines only theeigenvalues of $x$can detect the existence of a hiddengaussian clique, with error probability vanishing as $n \\to \\infty$.the result above is an immediate consequence of a more general result on rank-oneperturbations of $k$-dimensional gaussian tensors.in this context we establish a lower bound on the criticalsignal-to-noise ratio below which a rank-one signal cannot be detected.</s></p></d>", "label": ["<d><p><s>on the limitation of spectral methods: from the gaussian hidden clique problem to rank-one perturbations of gaussian tensors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges.</s> <s>the popular em algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete.</s> <s>recently, work in [1] has demonstrated that for an important class of problems, em exhibits linear local convergence.</s> <s>in the high-dimensional setting, however, the m-step may not be well defined.</s> <s>we address precisely this setting through a unified treatment using regularization.</s> <s>while regularization for high-dimensional problems is by now well understood, the iterative em algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank).</s> <s>in particular, regularizing the m-step using the state-of-the-art high-dimensional prescriptions (e.g., \\`a la [19]) is not guaranteed to provide this balance.</s> <s>our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors.</s> <s>we specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.</s></p></d>", "label": ["<d><p><s>regularized em algorithms: a unified framework and statistical guarantees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of black-box optimization of a function $f$ of any dimension, given function evaluations perturbed by noise.</s> <s>the function is assumed to be locally smooth around one of its global optima, but this smoothness is unknown.</s> <s>our contribution is an adaptive optimization algorithm, poo or parallel optimistic optimization, that is able to deal with this setting.</s> <s>poo performs almost as well as the best known algorithms requiring the knowledge of the smoothness.</s> <s>furthermore, poo works for a larger class of functions than what was previously considered, especially for functions that are difficult to optimize, in a very precise sense.</s> <s>we provide a finite-time analysis of  poo's performance, which shows that its error after $n$ evaluations is at most a factor of $\\sqrt{\\ln n}$ away from the error of the best known optimization algorithms using the knowledge of the smoothness.</s></p></d>", "label": ["<d><p><s>black-box optimization of noisy functions with unknown smoothness</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one.</s> <s>the weights of the items are binary, stochastic, and drawn independently of each other.</s> <s>the agent observes the index of the first chosen item whose weight is zero.</s> <s>this observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path.</s> <s>we propose a ucb-like algorithm for solving our problems, combcascade; and prove gap-dependent and gap-free upper bounds on its n-step regret.</s> <s>our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability.</s> <s>we evaluate combcascade on two real-world problems and show that it performs well even when our modeling assumptions are violated.</s> <s>we also demonstrate that our setting requires a new learning algorithm.</s></p></d>", "label": ["<d><p><s>combinatorial cascading bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the alternating direction method of multipliers (admm) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently.</s> <s>the primal-dual hybrid gradient (pdhg) method is a powerful alternative that often has simpler substeps than admm, thus producing lower complexity solvers.</s> <s>despite the flexibility of this method, pdhg is often impractical because it requires the careful choice of multiple stepsize parameters.</s> <s>there is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence.</s> <s>we propose self-adaptive stepsize rules that automatically tune pdhg parameters for optimal convergence.</s> <s>we rigorously analyze our methods, and identify convergence rates.</s> <s>numerical experiments show that adaptive pdhg has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.</s></p></d>", "label": ["<d><p><s>adaptive primal-dual splitting methods for statistical learning and image processing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper establishes a statistical versus computational trade-offfor solving a basic high-dimensional machine learning problem via a basic convex relaxation method.</s> <s>specifically, we consider the {\\em sparse principal component analysis} (sparse pca) problem, and the family of {\\em sum-of-squares} (sos, aka lasserre/parillo) convex relaxations.</s> <s>it was well known that in large dimension $p$, a planted $k$-sparse unit vector can be {\\em in principle} detected using only $n \\approx k\\log p$ (gaussian or bernoulli) samples, but all {\\em efficient} (polynomial time) algorithms known require $n \\approx k^2 $ samples.</s> <s>it was also known that this quadratic gap cannot be improved by the the most basic {\\em semi-definite} (sdp, aka spectral) relaxation, equivalent to a degree-2 sos algorithms.</s> <s>here we prove that also degree-4 sos algorithms cannot improve this quadratic gap.</s> <s>this average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms.</s> <s>moreover, our design of moments (or ``pseudo-expectations'') for this lower bound is quite different than previous lower bounds.</s> <s>establishing lower bounds for higher degree sos algorithms for remains a challenging problem.</s></p></d>", "label": ["<d><p><s>sum-of-squares lower bounds for sparse pca</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we extend the theory of boosting for regression problems to the online learning setting.</s> <s>generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions.</s> <s>our main result is an online gradient boosting algorithm which converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class.</s> <s>we also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.</s></p></d>", "label": ["<d><p><s>online gradient boosting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing.</s> <s>estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity.</s> <s>in this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions.</s> <s>in this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of  regularization parameter, which entails knowledge of the noise level and/or tuning.</s> <s>by contrast, constrained least squaresestimation comes without any tuning parameter and may hence be preferred due to its simplicity.</s></p></d>", "label": ["<d><p><s>regularization-free estimation in trace regression with symmetric positive semidefinite matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders.</s> <s>the pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market.</s> <s>however, little is known about rates and guarantees for the convergence of these sequential mechanisms, and two recent papers cite this as an important open question.in this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent (rsd).</s> <s>we establish convergence rates for rsd and leverage them to prove rates for the two prediction market models above, answering the open questions.</s> <s>our results extend beyond standard centralized markets to arbitrary trade networks.</s></p></d>", "label": ["<d><p><s>convergence analysis of prediction markets via randomized subspace descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nonconvex and nonsmooth problems have recently received considerable attention in signal/image processing, statistics and machine learning.</s> <s>however, solving the nonconvex and nonsmooth optimization problems remains a big challenge.</s> <s>accelerated proximal gradient (apg) is an excellent method for convex programming.</s> <s>however, it is still unknown whether the usual apg can ensure the convergence to a critical point in nonconvex programming.</s> <s>to address this issue, we introduce a monitor-corrector step and extend apg for general nonconvex and nonsmooth programs.</s> <s>accordingly, we propose a monotone apg and a non-monotone apg.</s> <s>the latter waives the requirement on monotonic reduction of the objective function and needs less computation in each iteration.</s> <s>to the best of our knowledge, we are the first to provide apg-type algorithms for general nonconvex and nonsmooth problems ensuring that every accumulation point is a critical point, and the convergence rates remain $o(1/k^2)$ when the problems are convex, in which k is the number of iterations.</s> <s>numerical results testify to the advantage of our algorithms in speed.</s></p></d>", "label": ["<d><p><s>accelerated proximal gradient methods for nonconvex programming</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a nearly optimal differentially private version of the well known lasso estimator.</s> <s>our algorithm provides privacy protection with respect to each training data item.</s> <s>the excess risk of our algorithm, compared to the non-private version, is $\\widetilde{o}(1/n^{2/3})$, assuming all the input data has bounded $\\ell_\\infty$ norm.</s> <s>this is the first differentially private algorithm that achieves such a bound without the polynomial dependence on $p$ under no addition assumption on the design matrix.</s> <s>in addition, we show that this error bound is nearly optimal amongst all differentially private algorithms.</s></p></d>", "label": ["<d><p><s>nearly optimal private lasso</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an adversarial formulation of the problem ofpredicting a time series with square loss.</s> <s>the aim is to predictan arbitrary sequence of vectors almost as well as the bestsmooth comparator sequence in retrospect.</s> <s>our approach allowsnatural measures of smoothness such as the squared norm ofincrements.</s> <s>more generally, we consider a linear time seriesmodel and penalize the comparator sequence through the energy ofthe implied driving noise terms.</s> <s>we derive the minimax strategyfor all problems of this type and show that it can be implementedefficiently.</s> <s>the optimal predictions are linear in the previousobservations.</s> <s>we obtain an explicit expression for the regret interms of the parameters defining the problem.</s> <s>for typical,simple definitions of smoothness, the computation of the optimalpredictions involves only sparse matrices.</s> <s>in the case ofnorm-constrained data, where the smoothness is defined in termsof the squared norm of the comparator's increments, we show thatthe regret grows as $t/\\sqrt{\\lambda_t}$, where $t$ is the lengthof the game and $\\lambda_t$ is an increasing limit on comparatorsmoothness.</s></p></d>", "label": ["<d><p><s>minimax time series prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered.</s> <s>we identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible.</s> <s>among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.</s></p></d>", "label": ["<d><p><s>communication complexity of distributed convex learning and optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability.</s> <s>such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation.</s> <s>one of these modifications is forcing the learner to sample arms from the uniform distribution at least $\\omega(\\sqrt{t})$ times over $t$ rounds, which can adversely affect performance if many of the arms are suboptimal.</s> <s>while it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component.</s> <s>our result relies on a simple and intuitive loss-estimation strategy called implicit exploration (ix) that allows a remarkably clean analysis.</s> <s>to demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework.finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.</s></p></d>", "label": ["<d><p><s>explore no more: improved high-probability regret bounds for non-stochastic bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the estimation of low rank matrices via nonconvex optimization.</s> <s>compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation.</s> <s>however, the understanding of its theoretical guarantees are limited.</s> <s>in this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization.</s> <s>we illustrate the consequences of this general framework for matrix sensing and completion.</s> <s>in particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.</s></p></d>", "label": ["<d><p><s>a nonconvex optimization framework for low rank matrix estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this  paper  provides  the first  formalization  of  self-interested planning in multiagent settings using expectation-maximization (em).</s> <s>our   formalization  in   the   context   of  infinite-horizon   and finitely-nested interactive  pomdps (i-pomdp) is  distinct from em formulations  for pomdps  and cooperative  multiagent planning frameworks.</s> <s>we  exploit the graphical model structure  specific to i-pomdps, and present  a new  approach based  on block-coordinate  descent for  further speed up.</s> <s>forward  filtering-backward sampling -- a combination of exact filtering  with sampling -- is explored to exploit problem structure.</s></p></d>", "label": ["<d><p><s>individual planning in infinite-horizon multiagent settings: inference, structure and scalability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>since being analyzed by rokhlin, szlam, and tygert and popularized by halko, martinsson, and tropp, randomized simultaneous power iteration has become the method of choice for approximate singular value decomposition.</s> <s>it is more accurate than simpler sketching algorithms, yet still converges quickly for *any* matrix, independently of singular value gaps.</s> <s>after ~o(1/epsilon) iterations, it gives a low-rank approximation within (1+epsilon) of optimal for spectral norm error.we give the first provable runtime improvement on simultaneous iteration: a randomized block krylov method, closely related to the classic block lanczos algorithm, gives the same guarantees in just ~o(1/sqrt(epsilon)) iterations and performs substantially better experimentally.</s> <s>our analysis is the first of a krylov subspace method that does not depend on singular value gaps, which are unreliable in practice.furthermore, while it is a simple accuracy benchmark, even (1+epsilon) error for spectral norm low-rank approximation does not imply that an algorithm returns high quality principal components, a major issue for data applications.</s> <s>we address this problem for the first time by showing that both block krylov iteration and simultaneous iteration give nearly optimal pca for any matrix.</s> <s>this result further justifies their strength over non-iterative sketching methods.</s></p></d>", "label": ["<d><p><s>randomized block krylov methods for stronger and faster approximate singular value decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>max-product belief propagation (bp) is a popular message-passing algorithm for computing a maximum-a-posteriori (map) assignment over a distribution represented by a graphical model (gm).</s> <s>it has been shown that bp can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective linear programming (lp) relaxation is tight, i.e.,  no integrality gap is present.</s> <s>however, when lp shows an integrality gap, no model has been known which can be solved systematically via sequential applications of bp.</s> <s>in this paper, we develop the first such algorithm, coined blossom-bp, for solving the minimum weight matching problem over arbitrary graphs.</s> <s>each step of the sequential algorithm requires applying bp over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices.</s> <s>our scheme guarantees termination in o(n^2) of bp runs, where n is the number of vertices in the original graph.</s> <s>in essence, the blossom-bp offers a distributed version of the celebrated edmonds' blossom algorithm by jumping at once over many sub-steps with a single bp.</s> <s>moreover, our result provides an interpretation of the edmonds' algorithm as a sequence of lps.</s></p></d>", "label": ["<d><p><s>minimum weight perfect matching via blossom belief propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise.</s> <s>this signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) fourier measurements of an object.</s> <s>of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse fourier measurements (bounded by some \\emph{cutoff frequency}); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly.</s> <s>suppose we have $k$ point sources in $d$ dimensions, where the points are separated by at least $\\delta$ from each other (in euclidean distance).</s> <s>this work provides an algorithm with the following favorable guarantees:1.</s> <s>the algorithm uses fourier measurements, whose frequencies are bounded by $o(1/\\delta)$ (up to log factors).</s> <s>previous algorithms require a \\emph{cutoff frequency}  which may be as large as  $\\omega(\\sqrt{d}/\\delta)$.2.</s> <s>the number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points $k$ and the dimension $d$, with \\emph{no} dependence on the separation $\\delta$.</s> <s>in contrast, previous algorithms depended inverse polynomially  on the minimal separation and exponentially on the dimension for both of these quantities.our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid).</s> <s>furthermore, our analysis and algorithm are elementary (based on concentration bounds of sampling and singular value decomposition).</s></p></d>", "label": ["<d><p><s>super-resolution off the grid</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of sparse signal recovery from $m$ linear measurements quantized to $b$ bits.</s> <s>$b$-bit marginal regression is proposed as recovery algorithm.</s> <s>we study the question of choosing $b$ in the setting of a given budget of bits $b = m \\cdot b$ and derive a single easy-to-compute expression characterizing the trade-off between $m$ and $b$.</s> <s>the choice $b = 1$ turns out to be optimal for estimating the unit vector corresponding to the signal for any level of additive gaussian noise before quantization as well as for adversarial noise.</s> <s>for $b \\geq 2$, we show that lloyd-max quantization constitutes an optimal quantization scheme and that the norm of the signal canbe estimated consistently by maximum likelihood.</s></p></d>", "label": ["<d><p><s>b-bit marginal regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider estimating an unknown, but structured (e.g.</s> <s>sparse, low-rank, etc.</s> <s>), signal $x_0\\in r^n$ from a vector $y\\in r^m$ of measurements of the form $y_i=g_i(a_i^tx_0)$, where the $a_i$'s are the rows of a known measurement matrix $a$, and, $g$ is a (potentially unknown) nonlinear and random link-function.</s> <s>such measurement functions could arise in applications where the measurement device has nonlinearities and uncertainties.</s> <s>it could also arise by design, e.g., $g_i(x)=sign(x+z_i)$, corresponds to noisy 1-bit quantized measurements.</s> <s>motivated by the classical work of brillinger, and more recent work of plan and vershynin, we estimate $x_0$ via solving the generalized-lasso, i.e., $\\hat x=\\arg\\min_{x}\\|y-ax_0\\|_2+\\lambda f(x)$ for some regularization parameter $\\lambda >0$ and some (typically non-smooth) convex regularizer $f$ that promotes the structure of $x_0$, e.g.</s> <s>$\\ell_1$-norm, nuclear-norm.</s> <s>while this approach seems to naively ignore the nonlinear function $g$, both brillinger and plan and vershynin have shown that, when the entries of $a$ are iid standard normal, this is a good estimator of $x_0$ up to a constant of proportionality $\\mu$, which only depends on $g$.</s> <s>in this work, we considerably strengthen these results by obtaining explicit expressions for $\\|\\hat x-\\mu x_0\\|_2$, for the regularized generalized-lasso, that are asymptotically precise when $m$ and $n$ grow large.</s> <s>a main result is that the estimation performance of the generalized lasso with non-linear measurements is asymptotically the same as one whose measurements are linear $y_i=\\mu a_i^tx_0+\\sigma z_i$, with $\\mu=e[\\gamma g(\\gamma)]$ and $\\sigma^2=e[(g(\\gamma)-\\mu\\gamma)^2]$, and, $\\gamma$ standard normal.</s> <s>the derived expressions on the estimation performance are the first-known precise results in this context.</s> <s>one interesting consequence of our result is that the optimal quantizer of the measurements that minimizes the estimation error of the lasso is the celebrated lloyd-max quantizer.</s></p></d>", "label": ["<d><p><s>lasso with non-linear measurements is equivalent to one with linear measurements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel methods represent one of the most powerful tools in machine learning to tackle problems expressed in terms of function values and derivatives due to their capability to represent and model complex relations.</s> <s>while these methods show good versatility, they are computationally intensive and have poor scalability to large data as they require operations on gram matrices.</s> <s>in order to mitigate this serious computational limitation, recently randomized constructions have been proposed in the literature, which allow the application of fast linear algorithms.</s> <s>random fourier features (rff) are among the most popular and widely applied constructions: they provide an easily computable, low-dimensional feature representation for shift-invariant kernels.</s> <s>despite the popularity of rffs, very little is understood theoretically about their approximation quality.</s> <s>in this paper, we provide a detailed finite-sample theoretical analysis about the approximation quality of rffs by (i) establishing optimal (in terms of the rff dimension, and growing set size) performance guarantees in uniform norm, and (ii) presenting guarantees in l^r (1 ?</s> <s>r < ?)</s> <s>norms.</s> <s>we also propose an rff approximation to derivatives of a kernel with a theoretical study on its approximation quality.</s></p></d>", "label": ["<d><p><s>optimal rates for random fourier features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over.</s> <s>by exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics.</s> <s>additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).</s></p></d>", "label": ["<d><p><s>submodular hamming metrics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>class ambiguity is typical in image classification problems with a large number of classes.</s> <s>when classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss.</s> <s>we propose top-k multiclass svm as a direct method to optimize for top-k performance.</s> <s>our generalization of the well-known multiclass svm is based on a tight convex upper bound of the top-k error.</s> <s>we propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest.</s> <s>experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.</s></p></d>", "label": ["<d><p><s>top-k multiclass svm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper is concerned with finding a solution x to a quadratic system of equations y_i = |< a_i, x >|^2,  i = 1, 2, ..., m. we prove that it is possible to solve unstructured quadratic systems in n variables exactly from o(n) equations in linear time, that is, in time proportional to reading and evaluating the data.</s> <s>this is accomplished by a novel procedure, which starting from an initial guess given by a spectral initialization procedure, attempts to minimize a non-convex objective.</s> <s>the proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion, which discard terms bearing too much influence on the initial estimate or search directions.</s> <s>these careful selection rules---which effectively serve as a variance reduction scheme---provide a tighter initial guess, more robust descent directions, and thus enhanced practical performance.</s> <s>further, this procedure also achieves a near-optimal statistical accuracy in the presence of noise.</s> <s>finally, we demonstrate empirically that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.</s></p></d>", "label": ["<d><p><s>solving random quadratic systems of equations is nearly as easy as solving linear systems</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>submodular and supermodular functions have found wide applicability in machine learning, capturing  notions such as diversity and regularity, respectively.</s> <s>these notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention.</s> <s>however, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference.</s> <s>prominent, well-studied special cases include ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied.</s> <s>in this paper, we investigate the use of markov chain monte carlo sampling to perform approximate inference in general log-submodular and log-supermodular models.</s> <s>in particular, we consider a simple gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (o(nlogn)) mixing.</s> <s>we also evaluate the efficiency of the gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.</s></p></d>", "label": ["<d><p><s>sampling from probabilistic submodular models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes a distributionally robust approach to logistic regression.</s> <s>we use the wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples.</s> <s>if the radius of this wasserstein ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence.</s> <s>we then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the wasserstein ball.</s> <s>we prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases.</s> <s>we further propose a distributionally robust approach based on wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier.</s> <s>these bounds are given by the optimal values of two highly tractable linear programs.</s> <s>we validate our theoretical out-of-sample guarantees through simulated and empirical experiments.</s></p></d>", "label": ["<d><p><s>distributionally robust logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational inference is an efficient, popular heuristic used in the context of latent variable models.</s> <s>we provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models.</s> <s>our initializations are natural, one of them being used in lda-c, the mostpopular implementation of variational inference.in addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to usenon-standard proof arguments, which we believe might be of general theoretical interest.</s></p></d>", "label": ["<d><p><s>on some provably correct cases of variational inference for topic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems.</s> <s>whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of u-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area.</s> <s>yet, such data functionals are essential to describe global properties of a statistical population, with important examples including area under the curve, empirical variance, gini mean difference and within-cluster point scatter.</s> <s>this paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the u-statistic of interest.</s> <s>we establish convergence rate bounds of o(1 / t) and o(log t / t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms.</s> <s>beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.</s></p></d>", "label": ["<d><p><s>extending gossip algorithms to distributed estimation of u-statistics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (blbf), and proposes the use of an alternative estimator that avoids this problem.in the blbf setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy.this makes blbf algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs.the counterfactual risk minimization (crm) principle offers a general recipe for designing blbf algorithms.</s> <s>it requires a counterfactual risk estimator, and virtually all existing works on blbf have focused on a particular unbiased estimator.we show that this conventional estimator suffers from apropensity overfitting problem when used for learning over complex hypothesis spaces.we propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem.this naturally gives rise to a new learning algorithm -- normalized policy optimizer for exponential models (norm-poem) --for structured output prediction using linear rules.we evaluate the empirical effectiveness of norm-poem on severalmulti-label classification problems, finding that it consistently outperforms the conventional estimator.</s></p></d>", "label": ["<d><p><s>the self-normalized estimator for counterfactual learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>there is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation.</s> <s>current methods, such as bayesian quadrature, demonstrate impressive empirical performance but lack theoretical analysis.</s> <s>an important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees.</s> <s>in this paper, we present the first probabilistic integrator that admits such theoretical treatment, called frank-wolfe bayesian quadrature (fwbq).</s> <s>under fwbq, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential.</s> <s>in simulations, fwbq is competitive with state-of-the-art methods and out-performs alternatives based on frank-wolfe optimisation.</s> <s>our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.</s></p></d>", "label": ["<d><p><s>frank-wolfe bayesian quadrature: probabilistic integration with theoretical guarantees</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of efficiently computing the maximum likelihood estimator in generalized linear models (glms)when the number of observations is much larger than the number of coefficients (n > > p > > 1).</s> <s>in this regime, optimization algorithms can immensely benefit fromapproximate second order information.we propose an alternative way of constructing the curvature information by formulatingit as an estimation problem and applying a stein-type lemma, which allows further improvements through sub-sampling andeigenvalue thresholding.our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost.</s> <s>we provide its convergence analysis for the case where the rows of the design matrix are i.i.d.</s> <s>samples with bounded support.we show that the convergence has two phases, aquadratic phase followed by a linear phase.</s> <s>finally,we empirically demonstrate that our algorithm achieves the highest performancecompared to various algorithms on several datasets.</s></p></d>", "label": ["<d><p><s>newton-stein method: a second order method for glms via stein's lemma</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the asynchronous parallel implementations of stochastic gradient (sg) have been broadly used in solving deep neural network and received many successes in practice recently.</s> <s>however, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism.</s> <s>to fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of sg: one is on the computer network and the other is on the shared memory system.</s> <s>we establish an ergodic convergence rate $o(1/\\sqrt{k})$ for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by $\\sqrt{k}$ ($k$ is the total number of iterations).</s> <s>our results generalize and improve existing analysis for convex minimization.</s></p></d>", "label": ["<d><p><s>asynchronous parallel stochastic gradient for nonconvex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how can one find a subset, ideally as small as possible, that well represents a massive dataset?</s> <s>i.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset.</s> <s>in this paper, we formalize this challenge as a submodular cover problem.</s> <s>here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition preva- lent in many data summarization applications.</s> <s>the classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution.</s> <s>however, this sequential, centralized approach is imprac- tical for truly large-scale problems.</s> <s>in this work, we develop the first distributed algorithm ?</s> <s>discover ?</s> <s>for submodular set cover that is easily implementable using mapreduce-style computations.</s> <s>we theoretically analyze our approach, and present approximation guarantees for the solutions returned by discover.</s> <s>we also study a natural trade-off between the communication cost and the num- ber of rounds required to obtain such a solution.</s> <s>in our extensive experiments, we demonstrate the effectiveness of our approach on several applications, includ- ing active set selection, exemplar based clustering, and vertex cover on tens of millions of data points using spark.</s></p></d>", "label": ["<d><p><s>distributed submodular cover: succinctly summarizing massive data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in deterministic optimization, line searches are a standard tool ensuring stability and efficiency.</s> <s>where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space.</s> <s>we construct a probabilistic line search by combining the structure of existing deterministic methods with notions from bayesian optimization.</s> <s>our method retains a gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the wolfe conditions to monitor the descent.</s> <s>the algorithm has very low computational cost, and no user-controlled parameters.</s> <s>experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.</s></p></d>", "label": ["<d><p><s>probabilistic line searches for stochastic optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it.</s> <s>online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads.</s> <s>however, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics.we propose a temporal point process model, coevolve, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other.</s> <s>this model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks.</s> <s>furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces.</s> <s>we experimented with both synthetic data and data gathered from twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.</s></p></d>", "label": ["<d><p><s>coevolve: a joint point process model for information diffusion and network co-evolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>mean field variational bayes (mfvb) is a popular posterior approximation method due to its fast runtime on large-scale data sets.</s> <s>however, a well known failing of mfvb is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance.</s> <s>we generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables---both for individual variables and coherently across variables.</s> <s>we call our method linear response variational bayes (lrvb).</s> <s>when the mfvb posterior approximation is in the exponential family, lrvb has a simple, analytic form, even for non-conjugate models.</s> <s>indeed, we make no assumptions about the form of the true posterior.</s> <s>we demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.</s></p></d>", "label": ["<d><p><s>linear response methods for accurate covariance estimates from mean field variational bayes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in many statistical problems, a more coarse-grained model may be suitable for population-level behaviour,  whereas a more detailed model is appropriate for accurate modelling of individual behaviour.</s> <s>this raises the question of how to integrate both types of models.</s> <s>methods such as posterior regularization follow the idea of generalized moment matching, in that they allow matchingexpectations between two models, but sometimes both models are most conveniently expressed as latent variable models.</s> <s>we propose latent bayesian melding, which is motivated by averaging the distributions over populations statistics of both the individual-level and the population-level models under a logarithmic opinion pool framework.</s> <s>in a case study on electricity disaggregation, which is a type of single-channel blind source separation problem, we show that latent bayesian melding leads to significantly more accurate predictions than an approach based solely on generalized moment matching.</s></p></d>", "label": ["<d><p><s>latent bayesian melding for integrating individual and population models</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results.</s> <s>theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of gibbs may be exponential in the number of variables.</s> <s>to help understand the behavior of gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width.</s> <s>we show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time.</s> <s>our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width?regardless of the data used to instantiate them.</s> <s>we demonstrate a rich application from natural language processing in which gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.</s></p></d>", "label": ["<d><p><s>rapidly mixing gibbs sampling for a class of factor graphs using hierarchy width</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational inference is a scalable technique for approximate bayesian inference.</s> <s>deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use.</s> <s>we propose an automatic variational inference algorithm, automatic differentiation variational inference (advi); we implement it in stan (code available), a probabilistic programming system.</s> <s>in advi the user provides a bayesian model and a dataset, nothing else.</s> <s>we make no conjugacy assumptions and support a broad class of models.</s> <s>the algorithm automatically determines an appropriate variational family and optimizes the variational objective.</s> <s>we compare advi to mcmc sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model.</s> <s>we train the mixture model on a quarter million images.</s> <s>with advi we can use variational inference on any model we write in stan.</s></p></d>", "label": ["<d><p><s>automatic variational inference in stan</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we connect a broad class of generative models through their shared reliance on sequential decision making.</s> <s>motivated by this view, we develop extensions to an existing model, and then explore the idea further in the context of data imputation -- perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling.</s> <s>we formulate data imputation as an mdp and develop models capable of representing effective policies for it.</s> <s>we construct the models using neural networks and train them using a form of guided policy search.</s> <s>our models generate predictions through an iterative process of feedback and refinement.</s> <s>we show that this approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.</s></p></d>", "label": ["<d><p><s>data generation as sequential decision making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>expectation propagation (ep) is a deterministic approximation algorithm that is often used to perform approximate bayesian parameter learning.</s> <s>ep approximates the full intractable posterior distribution through a set of local-approximations that are iteratively refined for each datapoint.</s> <s>ep can offer analytic and computational advantages over other approximations, such as variational inference (vi), and is the method of choice for a number of models.</s> <s>the local nature of ep appears to make it an ideal candidate for performing bayesian learning on large models in large-scale datasets settings.</s> <s>however, ep has a crucial limitation in this context: the number approximating factors needs to increase with the number of data-points, n, which often entails a prohibitively large memory overhead.</s> <s>this paper presents an extension to ep, called stochastic expectation propagation (sep), that maintains a global posterior approximation (like vi) but updates it in a local way (like ep).</s> <s>experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that sep performs almost as well as full ep, but reduces the memory consumption by a factor of n. sep is therefore ideally suited to performing approximate bayesian learning in the large model, large dataset setting.</s></p></d>", "label": ["<d><p><s>stochastic expectation propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints.</s> <s>a new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master).</s> <s>the algorithm enables the local workers to perform more exploration, i.e.</s> <s>the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master.</s> <s>we empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance.</s> <s>we propose synchronous and asynchronous variants of the new algorithm.</s> <s>we provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method admm.</s> <s>we show that the stability of easgd is guaranteed when a simple stability condition is satisfied, which is not the case for admm.</s> <s>we additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings.</s> <s>asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the cifar and imagenet datasets.</s> <s>experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to downpour and other common baseline approaches and furthermore is very communication efficient.</s></p></d>", "label": ["<d><p><s>deep learning with elastic averaging sgd</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimating distributions over large alphabets is a fundamental machine-learning tenet.</s> <s>yet no method is known to estimate all distributions well.</s> <s>for example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, jelinek-mercer, and good-turing are not known to be near optimal for essentially any distribution.we describe the first universally near-optimal probability estimators.</s> <s>for every discrete distribution, they are provably nearly the best in the following two competitive ways.</s> <s>first they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation.</s> <s>second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times.specifically, for distributions over $k$ symbols and $n$ samples, we show that for both comparisons, a simple variant of good-turing estimator is always within kl divergence of $(3+o(1))/n^{1/3}$ from the best estimator, and that a more involved estimator is within $\\tilde{\\mathcal{o}}(\\min(k/n,1/\\sqrt n))$.</s> <s>conversely, we show that any estimator must have a kl divergence $\\ge\\tilde\\omega(\\min(k/n,1/ n^{2/3}))$ over the best estimator for the first comparison, and $\\ge\\tilde\\omega(\\min(k/n,1/\\sqrt{n}))$ for the second.</s></p></d>", "label": ["<d><p><s>competitive distribution estimation: why is good-turing good</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games.</s> <s>when each player in a game uses an algorithm from our class, their individual regret decays at $o(t^{-3/4})$, while the sum of utilities converges to an approximate optimum at $o(t^{-1})$--an improvement upon the worst case $o(t^{-1/2})$ rates.</s> <s>we show a black-box reduction for any algorithm in the class to achieve $\\tilde{o}(t^{-1/2})$ rates against an adversary, while maintaining the faster rates against algorithms in the class.</s> <s>our results extend those of rakhlin and shridharan~\\cite{rakhlin2013} and daskalakis et al.~\\cite{daskalakis2014}, who only analyzed two-player zero-sum games for specific algorithms.</s></p></d>", "label": ["<d><p><s>fast convergence of regularized learning in games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for training recurrent neural networks to act as near-optimal feedback controllers.</s> <s>it is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -- swimming, flying, biped and quadruped walking with different body morphologies.</s> <s>it does not require motion capture or task-specific features or state machines.</s> <s>the controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state.</s> <s>the action generated by the network is defined as velocity.</s> <s>thus the network is not learning a control policy, but rather the dynamics under an implicit policy.</s> <s>essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.</s></p></d>", "label": ["<d><p><s>interactive control of diverse complex characters with neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian nonparametric models, such as gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity.</s> <s>however, automating human expertise remains elusive; for example, gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners.</s> <s>in this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments.</s> <s>we use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels.</s> <s>finally, we investigate occam's razor in human and gaussian process based function learning.</s></p></d>", "label": ["<d><p><s>the human kernel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper develops a general approach, rooted in statistical learning theory, to learning an approximately revenue-maximizing auction from data.</s> <s>we introduce t-level auctions to interpolate between simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby balancing the competing demands of expressivity and simplicity.</s> <s>we prove that such auctions have small representation error, in the sense that for every product distribution f over bidders?</s> <s>valuations, there exists a t-level auction with small t and expected revenue close to optimal.</s> <s>we show that the set of t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning error.</s> <s>one consequence of our results is that, in arbitrary single-parameter settings, one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.</s></p></d>", "label": ["<d><p><s>on the pseudo-dimension of nearly optimal auctions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent factor models have been widely used to analyze simultaneous recordings of spike trains from large, heterogeneous neural populations.</s> <s>these models assume  the signal of interest in the population is a low-dimensional latent intensity that evolves over time, which is observed in high dimension via noisy point-process observations.</s> <s>these techniques have been well used to capture neural correlations across a population and to provide a smooth, denoised, and concise representation of high-dimensional spiking data.</s> <s>one limitation of many current models is that the observation model is assumed to be poisson, which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data, thereby introducing bias into estimates of covariance.</s> <s>here we develop the generalized count linear dynamical system, which relaxes the poisson assumption by using a more general exponential family for count data.</s> <s>in addition to containing poisson, bernoulli, negative binomial, and other common count distributions as special cases, we show that this model can be tractably learned by extending recent advances in variational inference techniques.</s> <s>we apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods, both in capturing the variance structure of the data and in held-out prediction.</s></p></d>", "label": ["<d><p><s>high-dimensional neural spike train analysis with generalized count linear dynamical systems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>to improve the efficiency of monte carlo estimation, practitioners are turning to biased markov chain monte carlo procedures that trade off asymptotic exactness for computational speed.</s> <s>the reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced.</s> <s>however, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias.</s> <s>to address these challenges, we introduce a new computable quality measure based on stein's method that bounds the discrepancy between sample and target expectations over a large class of test functions.</s> <s>we use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.</s></p></d>", "label": ["<d><p><s>measuring sample quality with stein's method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>perception is often described as a predictive process based on an optimal inference with respect to a generative model.</s> <s>we study here the principled construction of a generative model specifically crafted to probe motion perception.</s> <s>in that context, we first provide an axiomatic, biologically-driven derivation of the model.</s> <s>this model synthesizes random dynamic textures which are defined by stationary gaussian distributions obtained by the random aggregation of warped patterns.</s> <s>importantly, we show that this model can equivalently be described as a stochastic partial differential equation.</s> <s>using this characterization of motion in images, it allows us to recast motion-energy models into a principled bayesian inference framework.</s> <s>finally, we apply these textures in order to psychophysically probe speed perception in humans.</s> <s>in this framework, while the likelihood is derived from the generative model, the prior is estimated from the observed results and accounts for the perceptual bias in a principled fashion.</s></p></d>", "label": ["<d><p><s>biologically inspired dynamic textures for probing motion perception</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a scalable bayesian multi-label learning model based on learning low-dimensional label embeddings.</s> <s>our model assumes that each label vector is generated as a weighted combination of a set of topics (each topic being a distribution over labels), where the combination weights (i.e., the embeddings) for each label vector are conditioned on the observed feature vector.</s> <s>this construction, coupled with a bernoulli-poisson link function for each label of the binary label vector, leads to a model with a computational cost that scales in the number of positive labels in the label matrix.</s> <s>this makes the model particularly appealing for real-world multi-label learning problems where the label matrix is usually very massive but highly sparse.</s> <s>using a data-augmentation strategy leads to full local conjugacy in our model, facilitating simple and very efficient gibbs sampling, as well as an expectation maximization algorithm for inference.</s> <s>also, predicting the label vector at test time does not require doing an inference for the label embeddings and can be done in closed form.</s> <s>we report results on several benchmark data sets, comparing our model with various state-of-the art methods.</s></p></d>", "label": ["<d><p><s>large-scale bayesian multi-label learning via topic-based label embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a class of closed-form estimators for glms under high-dimensional sampling regimes.</s> <s>our class of estimators is based on deriving closed-form variants of the vanilla unregularized mle but which are (a) well-defined even under high-dimensional settings, and (b) available in closed-form.</s> <s>we then perform thresholding operations on this mle variant to obtain our class of estimators.</s> <s>we derive a unified statistical analysis of our class of estimators, and show that it enjoys strong statistical guarantees in both parameter error as well as variable selection, that surprisingly match those of the more complex regularized glm mles, even while our closed-form estimators are computationally much simpler.</s> <s>we derive instantiations of our class of closed-form estimators, as well as corollaries of our general theorem, for the special cases of logistic, exponential and poisson regression models.</s> <s>we corroborate the surprising statistical and computational performance of our class of estimators via extensive simulations.</s></p></d>", "label": ["<d><p><s>closed-form estimators for high-dimensional generalized linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the gaussian process convolution model (gpcm), a two-stage nonparametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from gaussian process.</s> <s>the gpcm is a continuous-time nonparametric-window moving average process and, conditionally, is itself a gaussian process with a nonparametric kernel defined in a probabilistic fashion.</s> <s>the generative model can be equivalently considered in the frequency domain, where the power spectral density of the signal is specified using a gaussian process.</s> <s>one of the main contributions of the paper is to develop a novel variational free-energy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process.</s> <s>in turn, this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios.</s> <s>additionally, the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data, leading to new bayesian nonparametric  approaches to spectrum estimation.</s> <s>the proposed gpcm is validated using synthetic and real-world signals.</s></p></d>", "label": ["<d><p><s>learning stationary time series using gaussian processes with nonparametric kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we introduce a generative model capable of producing high quality samples of natural images.</s> <s>our approach uses a cascade of convolutional networks (convnets) within a laplacian pyramid framework to generate images in a coarse-to-fine fashion.</s> <s>at each level of the pyramid a separate generative convnet model is trained using the generative adversarial nets (gan) approach.</s> <s>samples drawn from our model are of significantly higher quality than  existing models.</s> <s>in a quantitive assessment by human evaluators our cifar10 samples were mistaken for real images around 40%  of the time, compared to 10% for gan samples.</s> <s>we also show samples from more diverse datasets such as stl10 and lsun.</s></p></d>", "label": ["<d><p><s>deep generative image models using a ?laplacian pyramid of adversarial networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep learning has recently been introduced to the field of low-level computer vision and image processing.</s> <s>promising results have been obtained in a number of tasks including super-resolution, inpainting, deconvolution, filtering, etc.</s> <s>however, previously adopted neural network approaches such as convolutional neural networks and sparse auto-encoders are inherently with translation invariant operators.</s> <s>we found this property prevents the deep learning approaches from outperforming the state-of-the-art if the task itself requires translation variant interpolation (tvi).</s> <s>in this paper, we draw on shepard interpolation and design shepard convolutional neural networks (shcnn) which efficiently realizes end-to-end trainable tvi operators in the network.</s> <s>we show that by adding only a few feature maps in the new shepard layers, the network is able to achieve stronger results than a much deeper architecture.</s> <s>superior performance on both image inpainting and super-resolution is obtained where our system outperforms previous ones while keeping the running time competitive.</s></p></d>", "label": ["<d><p><s>shepard convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision.</s> <s>although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations.</s> <s>in this work, we develop a scalable deep conditional generative model for structured output variables using gaussian latent variables.</s> <s>the model is trained efficiently in the framework of stochastic gradient variational bayes, and allows a fast prediction using stochastic feed-forward inference.</s> <s>in addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods.</s> <s>in experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference.</s> <s>furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on caltech-ucsd birds 200 and the subset of labeled faces in the wild dataset.</s></p></d>", "label": ["<d><p><s>learning structured output representation using deep conditional generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an approach for generating a sequence of natural sentences for an image stream.</s> <s>since general users usually take a series of pictures on their special moments, much online visual information exists in the form of image streams, for which it would better take into consideration of the whole set to generate natural language descriptions.</s> <s>while almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences.</s> <s>to this end, we design a novel architecture called coherent recurrent convolutional network (crcn), which consists of convolutional networks, bidirectional recurrent networks, and entity-based local coherence model.</s> <s>our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data.</s> <s>we demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g.</s> <s>bleu and top-k recall) and user studies via amazon mechanical turk.</s></p></d>", "label": ["<d><p><s>expressing an image stream with a sequence of natural sentences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the problem of answering visual analogy questions.</s> <s>these questions take the form of image a is to image b as image c is to what.</s> <s>answering these questions entails discovering the mapping from image a to image b and then extending the mapping to image c and searching for the image d such that the relation from a to b holds for c to d. we pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple siamese architecture.</s> <s>we introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images.</s></p></d>", "label": ["<d><p><s>visalogy: answering visual analogy questions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>super resolving a low-resolution video is usually handled by either single-image super-resolution (sr) or multi-frame sr. single-image sr deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution.</s> <s>multi-frame sr generally extracts motion information, e.g.</s> <s>optical flow, to model the temporal dependency, which often shows high computational cost.</s> <s>considering that recurrent neural network (rnn) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame sr.different from vanilla rnn, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to current hidden layer are added for enhancing visual-temporal dependency modelling.</s> <s>with the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance.</s> <s>due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.</s></p></d>", "label": ["<d><p><s>bidirectional recurrent convolutional networks for multi-frame super-resolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper formulates the search for a set of bounding boxes (as needed in object proposal generation) as a monotone submodular maximization problem over the space of all possible bounding boxes in an image.</s> <s>since the number of possible bounding boxes in an image is very large $o(#pixels^2)$, even a single linear scan to perform the greedy augmentation for submodular maximization is intractable.</s> <s>thus, we formulate the greedy augmentation step as a branch-and-bound scheme.</s> <s>in order to speed up repeated application of b\\&b, we propose a novel generalization of minoux?s ?lazy greedy?</s> <s>algorithm to the b\\&b tree.</s> <s>theoretically, our proposed formulation provides a new understanding to the problem, and contains classic heuristic approaches such as sliding window+non-maximal suppression (nms) and and efficient subwindow search (ess) as special cases.</s> <s>empirically, we show that our approach leads to a state-of-art performance on object proposal generation via a novel diversity measure.</s></p></d>", "label": ["<d><p><s>submodboxes: near-optimal search for a set of diverse object proposals</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images.</s> <s>we propose a generative model for solving these problems of physical scene understanding from real-world videos and images.</s> <s>at the core of our generative model is a 3d physics engine, operating on an object-based representation of physical properties, including mass, position, 3d shape, and friction.</s> <s>we can infer these latent properties using relatively brief runs of mcmc, which drive simulations in the physics engine to fit key features of visual observations.</s> <s>we further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning.</s> <s>we name our model galileo, and evaluate it on a video dataset with simple yet physically rich scenarios.</s> <s>results show that galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects.</s> <s>our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.</s></p></d>", "label": ["<d><p><s>galileo: perceiving physical object properties by integrating a physics engine with deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>although the human visual system can recognize many concepts under challengingconditions, it still has some biases.</s> <s>in this paper, we investigate whether wecan extract these biases and transfer them into a machine recognition system.we introduce a novel method that, inspired by well-known tools in humanpsychophysics, estimates the biases that the human visual system might use forrecognition, but in computer vision feature spaces.</s> <s>our experiments aresurprising, and suggest that classifiers from the human visual system can betransferred into a machine with some success.</s> <s>since these classifiers seem tocapture favorable biases in the human visual system, we further present an svmformulation that constrains the orientation of the svm hyperplane to agree withthe bias from human visual system.</s> <s>our results suggest that transferring thishuman bias into machines may help object recognition systems generalize acrossdatasets and perform better when very little training data is available.</s></p></d>", "label": ["<d><p><s>learning visual biases from human imagination</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this article offers an empirical exploration on the use of character-level convolutional networks (convnets) for text classification.</s> <s>we constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results.</s> <s>comparisons are offered against traditional models such as bag of words, n-grams and their tfidf variants, and deep learning models such as word-based convnets and recurrent neural networks.</s></p></d>", "label": ["<d><p><s>character-level convolutional networks for text classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a winner-take-all method for learning hierarchical sparse representations in an unsupervised fashion.</s> <s>we first introduce fully-connected winner-take-all autoencoders which use mini-batch statistics to directly enforce a lifetime sparsity in the activations of the hidden units.</s> <s>we then propose the convolutional winner-take-all autoencoder which combines the benefits of convolutional architectures and autoencoders for learning shift-invariant sparse representations.</s> <s>we describe a way to train convolutional autoencoders layer by layer, where in addition to lifetime sparsity, a spatial sparsity within each feature map is achieved using winner-take-all activation functions.</s> <s>we will show that winner-take-all autoencoders can be used to to learn deep sparse representations from the mnist, cifar-10, imagenet, street view house numbers and toronto face datasets, and achieve competitive classification performance.</s></p></d>", "label": ["<d><p><s>winner-take-all autoencoders</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems.</s> <s>also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture.</s> <s>to address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections.</s> <s>our method prunes redundant connections using a three-step method.</s> <s>first, we train the network to learn which connections are important.</s> <s>next, we prune the unimportant connections.</s> <s>finally, we retrain the network to fine tune the weights of the remaining connections.</s> <s>on the imagenet dataset, our method reduced the number of parameters of alexnet by a factor of 9?, from 61 million to 6.7 million, without incurring accuracy loss.</s> <s>similar experiments with vgg-16 found that the total number of parameters can be reduced by 13?, from 138 million to 10.3 million, again with no loss of accuracy.</s></p></d>", "label": ["<d><p><s>learning both weights and connections for efficient neural network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce an unsupervised learning algorithmthat combines probabilistic modeling with solver-based techniques for program synthesis.we apply our techniques to both a visual learning domain and a language learning problem,showing that our algorithm can learn many visual concepts from only a few examplesand that it can recover some english inflectional morphology.taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures,and a technique for applying program synthesis tools to noisy data.</s></p></d>", "label": ["<d><p><s>unsupervised learning by program synthesis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new deep architecture for topic modeling, based on poisson factor analysis (pfa) modules.</s> <s>the model is composed of a poisson distribution to model observed vectors of counts, as well as a deep hierarchy of hidden binary units.</s> <s>rather than using logistic functions to characterize the probability that a latent binary unit is on, we employ a bernoulli-poisson link, which allows pfa modules to be used repeatedly in the deep architecture.</s> <s>we also describe an approach to build discriminative topic models, by adapting pfa modules.</s> <s>we derive efficient inference via mcmc and stochastic variational methods, that scale with the number of non-zeros in the data and binary units, yielding significant efficiency, relative to models based on logistic links.</s> <s>experiments on several corpora demonstrate the advantages of our model when compared to related deep models.</s></p></d>", "label": ["<d><p><s>deep poisson factor modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks currently demonstrate state-of-the-art performance in several domains.at the same time, models of this class are very demanding in terms of computational resources.</s> <s>in particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size.</s> <s>in this paper we convert the dense weight matrices of the fully-connected layers to the tensor train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved.in particular, for the very deep vgg networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.</s></p></d>", "label": ["<d><p><s>tensorizing neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>restricted boltzmann machines are undirected neural networks which have been shown tobe effective in many applications, including serving as initializations fortraining deep multi-layer neural networks.</s> <s>one of the main reasons for their success is theexistence of efficient and practical stochastic algorithms, such as contrastive divergence,for unsupervised training.</s> <s>we propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the thouless-anderson-palmer approach.</s> <s>we demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function.</s> <s>we believe that this strategycan be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training boltzmann machineswith hidden units.</s></p></d>", "label": ["<d><p><s>training restricted boltzmann machine via the ?thouless-anderson-palmer free energy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus.</s> <s>basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made.</s> <s>however, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability.</s> <s>we here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability.</s> <s>we then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings.</s> <s>our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.</s></p></d>", "label": ["<d><p><s>the brain uses reliability of stimulus information when making perceptual decisions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural population activity often exhibits rich variability.</s> <s>this variability is thought to arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as  from modulations of neural firing properties on long time-scales, often referred to as non-stationarity.</s> <s>to better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a  hierarchical dynamics model that is able to capture inter-trial modulations in firing rates, as well as neural population dynamics.</s> <s>we derive an algorithm for bayesian laplace propagation for fast posterior inference, and demonstrate that our model provides a better account of the structure of neural firing than existing stationary dynamics models, when applied to neural population recordings from primary visual cortex.</s></p></d>", "label": ["<d><p><s>unlocking neural population non-stationarities using hierarchical dynamics models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep structured output learning shows great promise in tasks like semantic image segmentation.</s> <s>we proffer a new, efficient deep structured model learning scheme, in which we show how deep convolutional neural networks (cnns) can be used to directly estimate the messages in message passing inference for structured prediction with conditional random fields crfs).</s> <s>with such cnn message estimators, we obviate the need to learn or evaluate potential functions for message calculation.</s> <s>this confers significant efficiency for learning, since otherwise when performing structured learning for a crf with cnn potentials it is necessary to undertake expensive inference for every stochastic gradient iteration.</s> <s>the network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials.</s> <s>hence it is more scalable for cases that a large number of classes are involved.</s> <s>we apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our cnn message learning method.</s></p></d>", "label": ["<d><p><s>deeply learning the messages in message passing inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the continuous-time hidden markov model (ct-hmm) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time.</s> <s>however, the lack of an efficient parameter learning algorithm for ct-hmm restricts its use to very small models or requires unrealistic constraints on the state transitions.</s> <s>in this paper, we present the first complete characterization of efficient em-based learning methods for ct-hmm models.</s> <s>we demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics.</s> <s>we solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden markov model.</s> <s>the second challenge is addressed by adapting three approaches from the continuous time markov chain literature to the ct-hmm domain.</s> <s>we demonstrate the use of ct-hmms with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an alzheimer's disease dataset.</s></p></d>", "label": ["<d><p><s>efficient learning of continuous-time hidden markov models for disease progression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many modern data analysis problems involve inferences from streaming data.</s> <s>however, streaming data is not easily amenable to the standard probabilistic modeling approaches, which assume that we condition on finite data.</s> <s>we develop population variational bayes, a new approach for using bayesian modeling to analyze streams of data.</s> <s>it approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with bayesian inference in a probabilistic model.</s> <s>we study our method with latent dirichlet allocation and dirichlet process mixtures on several large-scale data sets.</s></p></d>", "label": ["<d><p><s>the population posterior and bayesian modeling on streams</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning of low dimensional structure in multidimensional data is a canonical problem in machine learning.</s> <s>one common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold.</s> <s>there are a rich variety of manifold learning methods available, which allow mapping of data points to the manifold.</s> <s>however, there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data.</s> <s>the best attempt is the gaussian process latent variable model (gp-lvm), but identifiability issues lead to poor performance.</s> <s>we solve these issues by proposing a novel coulomb repulsive process (corp) for locations of points on the manifold, inspired by physical models of electrostatic interactions among particles.</s> <s>combining this process with a gp prior for the mapping function yields a novel electrostatic gp (electrogp) process.</s> <s>focusing on the simple case of a one-dimensional manifold, we develop efficient inference algorithms, and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video.</s></p></d>", "label": ["<d><p><s>probabilistic curve learning: coulomb repulsion and the electrostatic gaussian process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep learning presents notorious computational challenges.</s> <s>these challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients.</s> <s>while we do not address the non-convexity, we present an optimization solution that ex- ploits the so far unused ?geometry?</s> <s>in the objective function in order to best make use of the estimated gradients.</s> <s>previous work attempted similar goals with preconditioned methods in the euclidean space, such as l-bfgs, rmsprop, and ada-grad.</s> <s>in stark contrast, our approach combines a non-euclidean gradient method with preconditioning.</s> <s>we provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work.</s> <s>we theoretically formalize our arguments and derive novel preconditioned non-euclidean algorithms.</s> <s>the results are promising in both computational time and quality when applied to restricted boltzmann machines, feedforward neural nets, and convolutional neural nets.</s></p></d>", "label": ["<d><p><s>preconditioned spectral descent for deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a unified framework for learning continuous control policies usingbackpropagation.</s> <s>it supports stochastic control by treating stochasticity in thebellman equation as a deterministic function of exogenous noise.</s> <s>the productis a spectrum of general policy gradient algorithms that range from model-freemethods with value functions to model-based methods without value functions.we use learned models but only require observations from the environment insteadof observations from model-predicted trajectories, minimizing the impactof compounded model errors.</s> <s>we apply these algorithms first to a toy stochasticcontrol problem and then to several physics-based control problems in simulation.one of these variants, svg(1), shows the effectiveness of learning models, valuefunctions, and policies simultaneously in continuous domains.</s></p></d>", "label": ["<d><p><s>learning continuous control policies by stochastic value gradients</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we revisit the choice of sgd for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights.</s> <s>we argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest path-sgd, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization.</s> <s>path-sgd is easy and efficient to implement and leads to empirical gains over sgd and adagrad.</s></p></d>", "label": ["<d><p><s>path-sgd: path-normalized optimization in deep neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze in this paper a random feature map based on a  theory of invariance (\\emph{i-theory}) introduced in \\cite{anselmilrmtp13}.</s> <s>more specifically, a group invariant  signal signature is obtained through cumulative distributions of group-transformed random projections.</s> <s>our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected haar-integration kernel that is invariant to the specified group action.</s> <s>we show how this non-linear random feature map approximates this group invariant kernel uniformly on a  set of $n$ points.</s> <s>moreover, we show that it defines a function space that is dense in the equivalent invariant reproducing kernel hilbert space.</s> <s>finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting</s></p></d>", "label": ["<d><p><s>learning with group invariant features: a kernel perspective.</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper concerns the introduction of a new markov chain monte carlo scheme for posterior sampling in bayesian nonparametric mixture models with priors that belong to the general poisson-kingman class.</s> <s>we present a novel and compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous mcmc schemes.</s> <s>we describe  comparative  simulation  results  demonstrating  the  efficacy  of  the  proposed mcmc algorithm against existing marginal and conditional mcmc samplers.</s></p></d>", "label": ["<d><p><s>a hybrid sampler for poisson-kingman mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>normalized random measures (nrms) provide a broad class of discrete random measures that are often used as priors for bayesian nonparametric models.</s> <s>dirichlet process is a well-known example of nrms.</s> <s>most of posterior inference methods for nrm mixture models rely on mcmc methods since they are easy to implement and their convergence is well studied.</s> <s>however, mcmc often suffers from slow convergence when the acceptance rate is low.</s> <s>tree-based inference is an alternative deterministic posterior inference method, where bayesian hierarchical clustering (bhc) or incremental bayesian hierarchical clustering (ibhc) have been developed for dp or nrm mixture (nrmm) models, respectively.</s> <s>although ibhc is a promising method for posterior inference for nrmm models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made.</s> <s>in this paper, we present a hybrid inference algorithm for nrmm models, which combines the merits of both mcmc and ibhc.</s> <s>trees built by ibhc outlinespartitions of data, which guides metropolis-hastings procedure to employ appropriate proposals.</s> <s>inheriting the nature of mcmc, our tree-guided mcmc (tgmcmc) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees.</s> <s>experiments on both synthetic and real world datasets demonstrate the benefit of our method.</s></p></d>", "label": ["<d><p><s>tree-guided mcmc inference for normalized random measure mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hamiltonian monte carlo (hmc) is a successful approach for sampling from continuous densities.</s> <s>however, it has difficulty simulating hamiltonian dynamics with non-smooth functions, leading to poor performance.</s> <s>this paper is motivated by the behavior of hamiltonian dynamics in physical systems like optics.</s> <s>we introduce a modification of the leapfrog discretization of hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy.</s> <s>we prove that this method preserves the correct stationary distribution when boundaries are affine.</s> <s>experiments show that by reducing the number of rejected samples, this method improves on traditional hmc.</s></p></d>", "label": ["<d><p><s>reflection, refraction, and hamiltonian monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of hierarchical clustering on planar graphs.</s> <s>we formulate this in terms of finding the closest ultrametric to a specified set of distances and solve it using an lp relaxation that leverages minimum cost perfect matching as a subroutine to efficiently explore the space of planar partitions.</s> <s>we apply our algorithm to the problem of hierarchical image segmentation.</s></p></d>", "label": ["<d><p><s>planar ultrametrics for image segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for learning bayesian networks from data sets containingthousands of variables without the need for structure constraints.</s> <s>our approachis made of two parts.</s> <s>the first is a novel algorithm that effectively explores thespace of possible parent sets of a node.</s> <s>it guides the exploration towards themost promising parent sets on the basis of an approximated score function thatis computed in constant time.</s> <s>the second part is an improvement of an existingordering-based algorithm for structure optimization.</s> <s>the new algorithm provablyachieves a higher score compared to its original formulation.</s> <s>on very large datasets containing up to ten thousand nodes, our novel approach consistently outper-forms the state of the art.</s></p></d>", "label": ["<d><p><s>learning bayesian networks with thousands of variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop \\textit{parallel predictive entropy search} (ppes), a novel algorithm for bayesian optimization of expensive black-box objective functions.</s> <s>at each iteration, ppes aims to select a \\textit{batch} of points which will maximize the information gain about the global maximizer of the objective.</s> <s>well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel.</s> <s>the few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch.</s> <s>to the best of our knowledge, ppes is the first non-greedy batch bayesian optimization strategy.</s> <s>we demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.</s></p></d>", "label": ["<d><p><s>parallel predictive entropy search for batch global optimization of expensive objective functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient.</s> <s>the price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities.</s> <s>when these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.</s></p></d>", "label": ["<d><p><s>large-scale probabilistic predictors with and without guarantees of validity</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces.</s> <s>the problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature.</s> <s>in this paper, we analyze a recently proposed technique known as ``self-normalization'', which introduces a regularization term in training to penalize log normalizers for deviating from zero.</s> <s>this makes it possible to use unnormalized model scores as approximate probabilities.</s> <s>empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking.we prove upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributionsthat self-normalize easily, and construct explicit examples of high-variance input distributions.</s> <s>our theoretical results make predictions about the difficulty of fitting  self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions on both real and synthetic datasets.</s></p></d>", "label": ["<d><p><s>on the accuracy of self-normalized log-linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose the ?-return as an alternative to the ?-return currently used by the td(?)</s> <s>family of algorithms.</s> <s>the benefit of the ?-return is that it accounts for the correlation of different length returns.</s> <s>because it is difficult to compute exactly, we suggest one way of approximating the ?-return.</s> <s>we provide empirical studies that suggest that it is superior to the ?-return and ?-return for a variety of problems.</s></p></d>", "label": ["<d><p><s>policy evaluation using the ?-return</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new algorithm for community detection.</s> <s>the algorithm uses random walks to embed the graph in a space of measures, after which a modification of $k$-means in that space is applied.</s> <s>the algorithm is therefore fast and easily parallelizable.</s> <s>we evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks,  and find its performance to be better or at least as good as previously known  algorithms.</s> <s>we also prove a linear time (in number of edges) guarantee for the algorithm on a $p,q$-stochastic block model with  where $p \\geq c\\cdot n^{-\\half +  \\epsilon}$ and $p-q \\geq c' \\sqrt{p n^{-\\half +  \\epsilon} \\log n}$.</s></p></d>", "label": ["<d><p><s>community detection via measure space embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>link prediction and clustering are key problems for network-structureddata.</s> <s>while spectral clustering has strong theoretical guaranteesunder the popular stochastic blockmodel formulation of networks, itcan be expensive for large graphs.</s> <s>on the other hand, the heuristic ofpredicting links to nodes that share the most common neighbors withthe query node is much fast, and works very well in practice.</s> <s>we showtheoretically that the common neighbors heuristic can extract clustersw.h.p.</s> <s>when the graph is dense enough, and can do so even in sparsergraphs with the addition of a ``cleaning'' step.</s> <s>empirical results onsimulated and real-world data support our conclusions.</s></p></d>", "label": ["<d><p><s>the consistency of common neighbors for link prediction in stochastic blockmodels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determinantal point processes (dpps) are point process models thatnaturally encode diversity between the points of agiven realization, through a positive definite kernel $k$.</s> <s>dpps possess desirable properties, such as exactsampling or analyticity of the moments, but learning the parameters ofkernel $k$ through likelihood-based inference is notstraightforward.</s> <s>first, the kernel that appears in thelikelihood is not $k$, but another kernel $l$ related to $k$ throughan often intractable spectral decomposition.</s> <s>this issue is typically bypassed in machine learning bydirectly parametrizing the kernel $l$, at the price of someinterpretability of the model parameters.</s> <s>we follow this approachhere.</s> <s>second, the likelihood has an intractable normalizingconstant, which takes the form of large determinant in the case of adpp over a finite set of objects, and the form of a fredholm determinant in thecase of a dpp over a continuous domain.</s> <s>our main contribution is to derive bounds on the likelihood ofa dpp, both for finite and continuous domains.</s> <s>unlike previous work, our bounds arecheap to evaluate since they do not rely on approximating the spectrumof a large matrix or an operator.</s> <s>through usual arguments, these bounds thus yield cheap variationalinference and moderately expensive exact markov chain monte carlo inference methods for dpps.</s></p></d>", "label": ["<d><p><s>inference for determinantal point processes without spectral knowledge</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>metric learning seeks a transformation of the feature space that enhances prediction quality for a given task.</s> <s>in this work we provide pac-style sample complexity rates for supervised metric learning.</s> <s>we give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution.</s> <s>in addition, by leveraging the structure of the data distribution, we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset, allowing us to relax the dependence on representation dimension.</s> <s>we show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset?s intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported in previous works.</s></p></d>", "label": ["<d><p><s>sample complexity of learning mahalanobis distance metrics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we take a new look at parameter estimation for gaussian mixture model (gmms).</s> <s>specifically, we advance riemannian manifold optimization (on the manifold of positive definite matrices) as a potential replacement for expectation maximization (em), which has been the de facto standard for decades.</s> <s>an out-of-the-box invocation of riemannian optimization, however, fails spectacularly: it obtains the same solution as em, but vastly slower.</s> <s>building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences: it makes riemannian optimization not only match em (a nontrivial result on its own, given the poor record nonlinear programming has had against em), but also outperform it in many settings.</s> <s>to bring our ideas to fruition, we develop a well-tuned riemannian lbfgs method that proves superior to known competing methods (e.g., riemannian conjugate gradient).</s> <s>we hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics.</s></p></d>", "label": ["<d><p><s>matrix manifold optimization for gaussian mixtures</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>nonlinear component analysis such as kernel principle component analysis (kpca) and kernel canonical correlation analysis (kcca) are widely used in machine learning, statistics and data analysis, but they can not scale up to big datasets.</s> <s>recent attempts have employed random feature approximations to convert the problem to the primal form for linear computational complexity.</s> <s>however, to obtain high quality solutions, the number of random features should be the same order of magnitude as the number of data points, making such approach not directly applicable to the regime with millions of data points.we propose a simple, computationally efficient, and memory friendly algorithm based on the ``doubly stochastic gradients'' to scale up a range of kernel nonlinear component analysis, such as kernel pca, cca and svd.</s> <s>despite the \\emph{non-convex} nature of these problems, our method enjoys theoretical guarantees that it converges at the rate $\\otil(1/t)$ to the global optimum, even for the top $k$ eigen subspace.</s> <s>unlike many alternatives, our algorithm does not require explicit orthogonalization, which is infeasible on big datasets.</s> <s>we demonstrate the effectiveness and scalability of our algorithm on large scale synthetic and real world datasets.</s></p></d>", "label": ["<d><p><s>scale up nonlinear component analysis with doubly stochastic gradients</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a similarity graph between items, correlation clustering (cc) groups similar items together and dissimilar ones apart.</s> <s>one of the most popular cc algorithms is kwikcluster:  an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio.</s> <s>unfortunately, in practice kwikcluster requires a large number of clustering rounds, a potential bottleneck for large graphs.we present c4 and clusterwild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds, and provably achieve nearly linear speedups.</s> <s>c4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio.</s> <s>clusterwild!</s> <s>is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3 approximation ratio.we provide extensive experimental results for both algorithms,  where we outperform the state of the art, both in terms of clustering accuracy and running time.</s> <s>we show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15x speedup.</s></p></d>", "label": ["<d><p><s>parallel correlation clustering on big graphs</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop a new bidirectional algorithm for estimating markov chain multi-step transition probabilities: given a markov chain, we want to estimate the probability of hitting a given target state in $\\ell$ steps after starting from a given source distribution.</s> <s>given the target state $t$, we use a (reverse) local power iteration to construct an `expanded target distribution', which has the same mean as the quantity we want to estimate, but a smaller variance -- this can then be sampled efficiently by a monte carlo algorithm.</s> <s>our method extends to any markov chain on a discrete (finite or countable) state-space, and can be extended to compute functions of multi-step transition probabilities such as pagerank, graph diffusions, hitting/return times, etc.</s> <s>our main result is that in `sparse' markov chains -- wherein the number of transitions between states is  comparable to the number of states -- the running time of our algorithm for a uniform-random target node is order-wise smaller than monte carlo and power iteration based algorithms; in particular, our method can estimate a probability $p$ using only $o(1/\\sqrt{p})$ running time.</s></p></d>", "label": ["<d><p><s>fast bidirectional probability estimation in markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>biclustering (also known as submatrix localization) is a problem of high practical relevance in exploratory analysis of high-dimensional data.</s> <s>we develop a framework for performing statistical inference on biclusters found by score-based algorithms.</s> <s>since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm, this is a form of selective inference.</s> <s>our framework gives exact (non-asymptotic) confidence intervals and p-values for the significance of the selected biclusters.</s> <s>further, we generalize our approach to obtain exact inference for gaussian statistics.</s></p></d>", "label": ["<d><p><s>evaluating the statistical significance of biclusters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances.nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (cv) for obtaining a solution with sufficiently small cv error.in this paper we propose a novel framework for computing a lower bound of the cv errors as a function of the regularization parameter, which we call regularization path of cv error lower bounds.the proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the cv error of the current best solution could be away from best possible cv error in the entire range of the regularization parameters.we demonstrate through numerical experiments that a theoretically guaranteed a choice of regularization parameter in the above sense is possible with reasonable computational costs.</s></p></d>", "label": ["<d><p><s>regularization path of cross-validation error lower bounds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet.</s> <s>similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far.</s> <s>the multinomial logit model, a popular discrete choice model, captures  the structure of the hidden preferences  with a low-rank matrix.</s> <s>in order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data.</s> <s>a natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization.</s> <s>we present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling.</s> <s>in both cases, we show that the convex relaxation is minimax optimal.</s> <s>we prove an upper bound on the resulting error with finite samples, and  provide a matching information-theoretic lower bound.</s></p></d>", "label": ["<d><p><s>collaboratively learning preferences from ordinal data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many learning problems, ranging from clustering to ranking through metric learning, empirical estimates of the risk functional consist of an average over tuples (e.g., pairs or triplets) of observations, rather than over individual observations.</s> <s>in this paper, we focus on how to best implement a stochastic approximation approach to solve such risk minimization problems.</s> <s>we argue that in the large-scale setting, gradient estimates should be obtained by sampling tuples of data points with replacement (incomplete u-statistics) instead of sampling data points without replacement (complete u-statistics based on subsamples).</s> <s>we develop a theoretical framework accounting for the substantial impact of this strategy on the generalization ability of the prediction model returned by the stochastic gradient descent (sgd) algorithm.</s> <s>it reveals that the method we promote achieves a much better trade-off between statistical accuracy and computational cost.</s> <s>beyond the rate bound analysis, experiments on auc maximization and metric learning provide strong empirical evidence of the superiority of the proposed approach.</s></p></d>", "label": ["<d><p><s>sgd algorithms based on incomplete u-statistics: large-scale minimization of empirical risk</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (mle), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (ols) estimator.</s> <s>however, existing  literature compares ols and mle in terms of their asymptotic, not finite sample, guarantees.</s> <s>more crucially, computing the mle in general requires solving a non-convex optimization problem and is not known to be efficiently solvable.</s> <s>we provide finite sample upper and lower bounds on the estimation error of ols and mle, in two popular models: a) pooled model, b) seemingly unrelated regression (sur) model.</s> <s>we provide precise instances where the mle is significantly more accurate than ols.</s> <s>furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as mle, up to universal constants.</s> <s>finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding ols solutions but with error bound that depends only logarithmically on the data dimensionality.</s></p></d>", "label": ["<d><p><s>alternating minimization for regression problems with vector-valued outputs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study optimization algorithms based on variance reduction for stochastic gradientdescent (sgd).</s> <s>remarkable recent progress has been made in this directionthrough development of algorithms like sag, svrg, saga.</s> <s>these algorithmshave been shown to outperform sgd, both theoretically and empirically.</s> <s>however,asynchronous versions of these algorithms?a crucial requirement for modernlarge-scale applications?have not been studied.</s> <s>we bridge this gap by presentinga unifying framework that captures many variance reduction techniques.subsequently, we propose an asynchronous algorithm grounded in our framework,with fast convergence rates.</s> <s>an important consequence of our general approachis that it yields asynchronous versions of variance reduction algorithms such assvrg, saga as a byproduct.</s> <s>our method achieves near linear speedup in sparsesettings common to machine learning.</s> <s>we demonstrate the empirical performanceof our method through a concrete realization of asynchronous svrg.</s></p></d>", "label": ["<d><p><s>on variance reduction in stochastic gradient descent and its asynchronous variants</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>selecting the optimal subset from a large set of variables is a fundamental problem in various learning tasks such as feature selection, sparse regression, dictionary learning, etc.</s> <s>in this paper, we propose the poss approach which employs evolutionary pareto optimization to find a small-sized subset with good performance.</s> <s>we prove that for sparse regression, poss is able to achieve the best-so-far theoretically guaranteed approximation performance efficiently.</s> <s>particularly, for the \\emph{exponential decay} subclass, poss is proven to achieve an optimal solution.</s> <s>empirical study verifies the theoretical results, and exhibits the superior performance of poss to greedy and convex relaxation methods.</s></p></d>", "label": ["<d><p><s>subset selection by pareto optimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of recovering a low-rank tensor from its noisy observation.</s> <s>previous work has shown a recovery guarantee with signal to noise ratio $o(n^{\\ceil{k/2}/2})$ for recovering a $k$th order rank one tensor of size $n\\times \\cdots \\times n$ by recursive unfolding.</s> <s>in this paper, we first improve this bound to $o(n^{k/4})$ by a much simpler approach, but with a more careful analysis.</s> <s>then we propose a new norm called the \\textit{subspace} norm, which is based on the kronecker products of factors obtained by the proposed simple estimator.</s> <s>the imposed kronecker structure allows us to show a nearly ideal $o(\\sqrt{n}+\\sqrt{h^{k-1}})$ bound, in which the parameter $h$ controls the blend from the non-convex estimator to mode-wise nuclear norm minimization.</s> <s>furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with $h=o(1)$.</s></p></d>", "label": ["<d><p><s>interpolating convex and non-convex tensor decompositions via the subspace norm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a directed acyclic graph $g,$ and a set of values $y$ on the vertices, the isotonic regression of $y$ is a vector $x$ that respects the partial order described by $g,$ and minimizes $\\|x-y\\|,$ for a specified norm.</s> <s>this paper gives improved algorithms for computing the isotonic regression for all weighted $\\ell_{p}$-norms with rigorous performance guarantees.</s> <s>our algorithms are quite practical, and their variants can be implemented to run fast in practice.</s></p></d>", "label": ["<d><p><s>fast, provable algorithms for isotonic regression in all l_p-norms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new first-order optimization algorithm to solve high-dimensional  non-smooth composite minimization problems.</s> <s>typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty.</s> <s>the proposed algorithm, called semi-proximal mirror-prox, leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain.</s> <s>the algorithm stands in contrast with more classical proximal gradient algorithms with smoothing, which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems.</s> <s>we establish the theoretical convergence rate of semi-proximal mirror-prox, which exhibits the optimal complexity bounds for the number of calls to linear minimization oracle.</s> <s>we present promising experimental results showing the interest of the approach in comparison to competing methods.</s></p></d>", "label": ["<d><p><s>semi-proximal mirror-prox for nonsmooth composite minimization</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template.</s> <s>the algorithmic instances of our framework are universal since they can automatically adapt to the unknown holder continuity degree and constant within the dual formulation.</s> <s>they are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each holder smoothness degree.</s> <s>in contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function.</s> <s>we instead leverage computationally cheaper, fenchel-type operators, which are the main workhorses of the generalized conditional gradient (gcg)-type methods.</s> <s>in contrast to the gcg-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.</s></p></d>", "label": ["<d><p><s>a universal primal-dual convex optimization framework</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon markov decision processes (mdps) by deriving tight sample complexity bounds.</s> <s>however, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests.</s> <s>such scenarios can often be better treated as episodic fixed-horizon mdps, for which only looser bounds on the sample complexity exist.</s> <s>a natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (pac guarantee).</s> <s>in this paper, we derive an upper pac bound of order o(|s|?|a|h?</s> <s>log(1/?)/??)</s> <s>and a lower pac bound ?(|s||a|h?</s> <s>log(1/(?+c))/??)</s> <s>(ignoring log-terms) that match up to log-terms and an additional linear dependency on the number of states |s|.</s> <s>the lower bound is the first of its kind for this setting.</s> <s>our upper bound leverages bernstein's inequality to improve on previous bounds for episodic finite-horizon mdps which have a time-horizon dependency of at least h?.</s></p></d>", "label": ["<d><p><s>sample complexity of episodic fixed-horizon reinforcement learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we design algorithms for fitting a high-dimensional statistical model to a large, sparse network without revealing sensitive information of individual members.</s> <s>given a sparse input graph $g$, our algorithms output a node-differentially private nonparametric block model approximation.</s> <s>by node-differentially private, we mean that our output hides the insertion or removal of a vertex and all its adjacent edges.</s> <s>if $g$ is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon $w$, our model guarantees consistency: as the number of vertices tends to infinity, the output of our algorithm converges to $w$ in an appropriate version of the $l_2$ norm.</s> <s>in particular, this means we can estimate the sizes of all multi-way cuts in $g$.</s> <s>our results hold as long as $w$ is bounded, the average degree of $g$ grows at least like the log of the number of vertices, and the number of blocks goes to infinity at an appropriate rate.</s> <s>we give explicit error bounds in terms of the parameters of the model; in several settings, our bounds improve on or match known nonprivate results.</s></p></d>", "label": ["<d><p><s>private graphon estimation for sparse graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice.</s> <s>however, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging.</s> <s>in this paper, we propose an efficient \\underline{h}ybrid \\underline{o}ptimization algorithm for \\underline{no}n convex \\underline{r}egularized problems (honor).</s> <s>specifically, we develop a hybrid scheme which effectively integrates a quasi-newton (qn) step and a gradient descent (gd) step.</s> <s>our contributions are as follows: (1) honor incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse hessian matrix.</s> <s>(2)  we establish a rigorous convergence analysis for honor, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems.</s> <s>(3) we conduct empirical studies on large-scale data sets and results demonstrate that honor converges significantly faster than state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>honor: hybrid optimization for non-convex regularized problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs.</s> <s>with $o(r^3 \\kappa^2 n \\log n)$ random measurements of a positive semidefinite $n\\times n$ matrix of rank $r$ and condition number $\\kappa$, our method is guaranteed to converge linearly to the global optimum.</s></p></d>", "label": ["<d><p><s>a convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper investigates stochastic and adversarial combinatorial multi-armed bandit problems.</s> <s>in the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space.</s> <s>we propose escb, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret.</s> <s>escb has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice.</s> <s>in the adversarial setting under bandit feedback, we propose combexp, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.</s></p></d>", "label": ["<d><p><s>combinatorial bandits revisited</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>elicitation is the study of statistics or properties which are computable via empirical risk minimization.</s> <s>while several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question---all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable.</s> <s>specifically, what is the minimum number of regression parameters needed to compute the property?building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation.</s> <s>we establish several general results and techniques for proving upper and lower bounds on elicitation complexity.</s> <s>these results provide tight bounds for eliciting the bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.</s></p></d>", "label": ["<d><p><s>on elicitation complexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the performance of standard online learning algorithms when the feedback is delayed by an adversary.</s> <s>we show that \\texttt{online-gradient-descent} and \\texttt{follow-the-perturbed-leader} achieve regret $o(\\sqrt{d})$ in the delayed setting, where $d$ is the sum of delays of each round's feedback.</s> <s>this bound collapses to an optimal $o(\\sqrt{t})$ bound in the usual setting of no delays (where $d = t$).</s> <s>our main contribution is to show that standard algorithms for online learning already have simple regret bounds in the most general setting of delayed feedback, making adjustments to the analysis and not to the algorithms themselves.</s> <s>our results help affirm and clarify the success of recent algorithms in optimization and machine learning that operate in a delayed feedback model.</s></p></d>", "label": ["<d><p><s>online learning with adversarial delays</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular gaussian width of the unit norm ball, gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant.</s> <s>however, given an atomic norm, bounding these geometric measures can be difficult.</s> <s>in this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds.</s> <s>we show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.</s></p></d>", "label": ["<d><p><s>structured estimation with atomic norms: general bounds and applications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (csp), via a common generalization in terms of random bipartite graphs.</s> <s>our algorithm matches up to a constant factor the best-known bounds  for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used.</s> <s>the time complexity is significantly better than both spectral and sdp-based approaches.the main contribution of the algorithm is in the case of unequal sizes in the bipartition that arises in our reduction from the planted csp.</s> <s>here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix.other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.</s></p></d>", "label": ["<d><p><s>subsampled power iteration: a unified algorithm for block models and planted csp's</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes.</s> <s>our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions.</s> <s>we use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.</s></p></d>", "label": ["<d><p><s>learning theory and algorithms for forecasting non-stationary time series</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a novel parameter estimator for probabilistic models on discrete space.</s> <s>the proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant, which is frequently infeasible for models in the discrete space.</s> <s>we investigate statistical properties of the proposed estimator such as consistency and asymptotic normality, and reveal a relationship with the alpha-divergence.</s> <s>small experiments show that the proposed estimator attains comparable performance to the mle with drastically lower computational cost.</s></p></d>", "label": ["<d><p><s>empirical localization of homogeneous divergences on discrete sample spaces</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>tree structured group lasso (tgl) is a powerful technique in uncovering the tree structured sparsity over the features, where each node encodes a group of features.</s> <s>it has been applied successfully in many real-world applications.</s> <s>however, with extremely large feature dimensions, solving tgl remains a significant challenge due to its highly complicated regularizer.</s> <s>in this paper, we propose a novel multi-layer feature reduction method (mlfre) to quickly identify the inactive nodes (the groups of features with zero coefficients in the solution) hierarchically in a top-down fashion, which are guaranteed to be irrelevant to the response.</s> <s>thus, we can remove the detected nodes from the optimization without sacrificing accuracy.</s> <s>the major challenge in developing such testing rules is due to the overlaps between the parents and their children nodes.</s> <s>by a novel hierarchical projection algorithm, mlfre is able to test the nodes independently from any of their ancestor nodes.</s> <s>moreover, we can integrate mlfre---that has a low computational cost---with any existing solvers.</s> <s>experiments on both synthetic and real data sets demonstrate that the speedup gained by mlfre can be orders of magnitude.</s></p></d>", "label": ["<d><p><s>multi-layer feature reduction for tree structured group lasso via hierarchical projection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given samples from an unknown  distribution, p, is it possible to distinguish whether p belongs to some class of distributions c versus p being far from every distribution in c?</s> <s>this fundamental question has receivedtremendous attention in statistics, albeit focusing onasymptotic analysis, as well as in computer science, wherethe emphasis has been on small sample size and computationalcomplexity.</s> <s>nevertheless, even for basic classes ofdistributions such as monotone, log-concave, unimodal, and monotone hazard rate, the optimal sample complexity is unknown.we provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families.</s> <s>at the core of our approach is an algorithm which solves the following problem:given samplesfrom an unknown distribution p, and a known distribution q, are p and q close in chi^2-distance, or far in total variation distance?the optimality of all testers is established by providing matching lower bounds.</s> <s>finally, a necessary building block for our tester and important byproduct of our work are the first known computationally efficient proper learners for discretelog-concave and monotone hazard rate distributions.</s> <s>we exhibit the efficacy of our testers via experimental analysis.</s></p></d>", "label": ["<d><p><s>optimal testing for properties of distributions</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a market scoring rule (msr) ?</s> <s>a popular tool for designing algorithmic prediction markets ?</s> <s>is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents.</s> <s>in this paper, we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a msr incorporates private information from agents who deviate from the assumption of risk-neutrality.</s> <s>we first establish that, for a myopic trading agent with a risk-averse utility function, a msr satisfying mild regularity conditions elicits the agent?s risk-neutral probability conditional on the latest market state rather than her true subjective probability.</s> <s>hence, we show that a msr under these conditions effectively behaves like a more traditional method of belief aggregation, namely an opinion pool, for agents?</s> <s>true probabilities.</s> <s>in particular, the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents, and as a linear pool for an atypical budget-constrained agent utility with decreasing absolute risk aversion.</s> <s>we also point out the interpretation of a market maker under these conditions as a bayesian learner even when agent beliefs are static.</s></p></d>", "label": ["<d><p><s>market scoring rules act as opinion pools for risk-averse agents</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle.</s> <s>in particular, for a given function $x \\to f(x)$ we consider optimization when one is given access to absolute error oracles that return values in [f(x) - \\epsilon,f(x)+\\epsilon] or relative error oracles that return value in [(1+\\epsilon)f(x), (1 +\\epsilon)f (x)], for some \\epsilon larger than 0.</s> <s>we show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.</s></p></d>", "label": ["<d><p><s>information-theoretic lower bounds for convex optimization with erroneous oracles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bandit convex optimization is one of the fundamental problems in the field of online learning.</s> <s>the best algorithm for the general bandit convex optimization problem guarantees a regret of $\\widetilde{o}(t^{5/6})$, while the best known lower bound is $\\omega(t^{1/2})$.</s> <s>many attemptshave been made to bridge the huge gap between these bounds.</s> <s>a particularly interesting special case of this problem assumes that the loss functions are smooth.</s> <s>in this case, the best known algorithm guarantees a regret of $\\widetilde{o}(t^{2/3})$.</s> <s>we present an efficient algorithm for the banditsmooth convex optimization problem that guarantees a regret of $\\widetilde{o}(t^{5/8})$.</s> <s>our result rules out an $\\omega(t^{2/3})$ lower bound and takes a significant step towards the resolution of this open problem.</s></p></d>", "label": ["<d><p><s>bandit smooth convex optimization: improving the bias-variance tradeoff</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study accelerated mirror descent dynamics in continuous and discrete time.</s> <s>combining the original continuous-time motivation of mirror descent with a recent ode interpretation of nesterov's accelerated method, we propose a family of continuous-time descent dynamics for convex functions with lipschitz gradients, such that the solution trajectories are guaranteed to converge to the optimum at a $o(1/t^2)$ rate.</s> <s>we then show that a large family of first-order accelerated methods can be obtained as a discretization of the ode, and these methods converge at a $o(1/k^2)$ rate.</s> <s>this connection between accelerated mirror descent and the ode provides an intuitive approach to the design and analysis of accelerated first-order algorithms.</s></p></d>", "label": ["<d><p><s>accelerated mirror descent in continuous and discrete time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general framework for studying adaptive regret bounds in the online learning setting, subsuming model selection and data-dependent bounds.</s> <s>given a data- or model-dependent bound we ask, ?does there exist some algorithm achieving this bound??</s> <s>we show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved.</s> <s>in particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability.</s> <s>a cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes.our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second order data-dependent bounds, and small loss bounds.</s> <s>in addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online pac-bayes theorem.</s></p></d>", "label": ["<d><p><s>adaptive online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in addition to identifying the content within a single image, relating images and generating related images are critical tasks for image understanding.</s> <s>recently, deep convolutional networks have yielded breakthroughs in producing image labels, annotations and captions, but have only just begun to be used for producing high-quality image outputs.</s> <s>in this paper we develop a novel deep network trained end-to-end to perform visual analogy making, which is the task of transforming a query image according to an example pair of related images.</s> <s>solving this problem requires both accurately recognizing a visual relationship and generating a transformed query image accordingly.</s> <s>inspired by recent advances in language modeling, we propose to solve visual analogies by learning to map images to a neural embedding in which analogical reasoning is simple, such as by vector subtraction and addition.</s> <s>in experiments, our model effectively models visual analogies on several datasets: 2d shapes, animated video game sprites, and 3d car models.</s></p></d>", "label": ["<d><p><s>deep visual analogy-making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a neural network with a recurrent attention model over a possibly large external memory.</s> <s>the architecture is a form of memory network (weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings.</s> <s>it can also be seen as an extension of rnnsearch to the case where multiple computational steps (hops) are performed per output symbol.</s> <s>the flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling.</s> <s>for the former our approach is competitive with memory networks, but with less supervision.</s> <s>for the latter, on the penn treebank and text8 datasets our approach demonstrates comparable performance to rnns and lstms.</s> <s>in both cases we show that the key concept of multiple computational hops yields improved results.</s></p></d>", "label": ["<d><p><s>end-to-end memory networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation.</s> <s>we extend the attention-mechanism with features needed for speech recognition.</s> <s>we show that while an adaptation of the model used for machine translation reaches a competitive 18.6\\% phoneme error rate (per) on the timit phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on.</s> <s>we offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue.</s> <s>the new method yields a model that is robust to long inputs and achieves 18\\% per in single utterances and 20\\% in 10-times longer (repeated) utterances.</s> <s>finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces per to 17.6\\% level.</s></p></d>", "label": ["<d><p><s>attention-based models for speech recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans have the remarkable ability to follow the gaze of other people to identify what they are looking at.</s> <s>following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next.</s> <s>despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community.</s> <s>in this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset for thorough evaluation.</s> <s>given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at.</s> <s>after training, the network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food).</s> <s>the quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head.</s> <s>while our method outperforms several baseline approaches, we are still far from reaching human performance at this task.</s> <s>overall, we believe that this is a challenging and important task that deserves more attention from the community.</s></p></d>", "label": ["<d><p><s>where are they looking?</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this paper presents a new semi-supervised framework with convolutional neural networks (cnns) for text categorization.</s> <s>unlike the previous approaches that rely on word embeddings, our method learns embeddings of small text regions from unlabeled data for integration into a supervised cnn.</s> <s>the proposed scheme for embedding learning is based on the idea of two-view semi-supervised learning, which is intended to be useful for the task of interest even though the training is done on unlabeled data.</s> <s>our models achieve better results than previous approaches on sentiment classification and topic classification tasks.</s></p></d>", "label": ["<d><p><s>semi-supervised convolutional neural networks for text categorization via region embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success.</s> <s>however, training becomes more difficult as depth increases, and training of very deep networks remains an open problem.</s> <s>here we introduce a new architecture designed to overcome this.</s> <s>our so-called highway networks allow unimpeded information flow across many layers on information highways.</s> <s>they are inspired by long short-term memory recurrent networks and use adaptive gating units to regulate the information flow.</s> <s>even with hundreds of layers, highway networks can be trained directly through simple gradient descent.</s> <s>this enables the study of extremely deep and efficient architectures.</s></p></d>", "label": ["<d><p><s>training very deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents the deep convolution inverse graphics network (dc-ign), a model that aims to learn an interpretable representation of images, disentangled with respect to three-dimensional scene structure and viewing transformations such as depth rotations and lighting variations.</s> <s>the dc-ign model is composed of multiple layers of convolution and de-convolution operators and is trained using the stochastic gradient variational bayes (sgvb) algorithm.</s> <s>we propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g.</s> <s>pose or light).</s> <s>given a single input image, our model can generate new images of the same object with variations in pose and lighting.</s> <s>we present qualitative and quantitative tests of the model's efficacy at learning a 3d rendering engine for varied object classes including faces and chairs.</s></p></d>", "label": ["<d><p><s>deep convolutional inverse graphics network</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier.</s> <s>such approaches have been shown they can be fast, while achieving the state of the art in detection performance.</s> <s>in this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network.</s> <s>our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object.</s> <s>at test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score.</s> <s>we show that our model yields significant improvements over state-of-the-art object proposal algorithms.</s> <s>in particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals.</s> <s>we also show that our model is able to generalize to unseen categories it has not seen during training.</s> <s>unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.</s></p></d>", "label": ["<d><p><s>learning to segment object candidates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, approaches based on machine learning have achieved state-of-the-art performance on image restoration problems.</s> <s>successful approaches include both generative models of natural images as well as discriminative training of deep neural networks.</s> <s>discriminative training of feed forward architectures allows explicit control over the computational cost of performing restoration and therefore often leads to better performance at the same cost at run time.</s> <s>in contrast, generative models have the advantage that they can be trained once and then adapted to any image restoration task by a simple use of bayes' rule.</s> <s>in this paper we show how to combine the strengths of both approaches by training a discriminative, feed-forward architecture to predict the state of latent variables in a generative model of natural images.</s> <s>we apply this idea to the very successful gaussian mixture model (gmm) of natural images.</s> <s>we show that it is possible to achieve comparable performance as the original gmm but with two orders of magnitude improvement in run time while maintaining the advantage of generative models.</s></p></d>", "label": ["<d><p><s>the return of the gating network: combining generative models and discriminative training in natural image priors</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>convolutional neural networks define an exceptionallypowerful class of model, but are still limited by the lack of abilityto be spatially invariant to the input data in a computationally and parameterefficient manner.</s> <s>in this work we introduce a new learnable module, thespatial transformer, which explicitly allows the spatial manipulation ofdata within the network.</s> <s>this differentiable module can be insertedinto existing convolutional architectures, giving neural networks the ability toactively spatially transform feature maps, conditional on the feature map itself,without any extra training supervision or modification to the optimisation process.</s> <s>we show that the useof spatial transformers results in models which learn invariance to translation,scale, rotation and more generic warping, resulting in state-of-the-artperformance on several benchmarks, and for a numberof classes of transformations.</s></p></d>", "label": ["<d><p><s>spatial transformer networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-subject fmri data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity.</s> <s>we develop a shared response model for aggregating multi-subject fmri data that accounts for different functional topographies among anatomically aligned datasets.</s> <s>our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest.</s> <s>furthermore, by removing the identified shared response, it allows improved detection of group differences.</s> <s>the ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fmri studies.</s></p></d>", "label": ["<d><p><s>a reduced-dimension fmri shared response model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial.</s> <s>while the mechanism for rapid path planning is unknown, the ca3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal preplay periods may trace out future goal-directed trajectories.</s> <s>here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal.</s> <s>we show that this representation can be implemented in a neural attractor network model, resulting in bump--like activity profiles resembling those of the ca3 region of hippocampus.</s> <s>neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment.</s> <s>the network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location.</s> <s>we show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location.</s> <s>indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls.</s> <s>our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.</s></p></d>", "label": ["<d><p><s>attractor network dynamics enable preplay and rapid path planning in maze?like environments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite the recent achievements in machine learning,  we are still very far from achieving real artificial intelligence.</s> <s>in this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way.</s> <s>specifically,  we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences  which can only be learned by models which have the capacity to count and to memorize  sequences.</s> <s>we show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.</s></p></d>", "label": ["<d><p><s>inferring algorithmic patterns with stack-augmented recurrent nets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations.</s> <s>contrary to existing approaches posing semantic segmentation as region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task.</s> <s>in this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label by segmentation network.</s> <s>the decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively.</s> <s>it facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers.</s> <s>our algorithm shows outstanding performance compared to other semi-supervised approaches even with much less training images with strong annotations in pascal voc dataset.</s></p></d>", "label": ["<d><p><s>decoupled deep neural network for semi-supervised semantic segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>motivated by vision-based reinforcement learning (rl) problems, in particular atari games from the recent benchmark aracade learning environment (ale), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames.</s> <s>while not composed of natural scenes, frames in atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability.</s> <s>we propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks.</s> <s>experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games.</s> <s>to the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.</s></p></d>", "label": ["<d><p><s>action-conditional video prediction using deep networks in atari games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our goal is to deploy a high-accuracy system starting with zero training examples.</s> <s>we consider an ?on-the-job?</s> <s>setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident.</s> <s>as the model improves over time, the reliance on crowdsourcing queries decreases.</s> <s>we cast our setting as a stochastic game based on bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way.</s> <s>computing the optimal policy is intractable, so we develop an approximation based on monte carlo tree search.</s> <s>we tested our approach on three datasets-- named-entity recognition, sentiment classification, and image classification.</s> <s>on the ner task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels.</s> <s>we also achieve a 8% f1 improvement over having a single human label the whole set, and a 28% f1 improvement over online learning.</s></p></d>", "label": ["<d><p><s>on-the-job learning with bayesian decision theory</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>despite their success, convolutional neural networks are computationally expensive because they must examine all image locations.</s> <s>stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates.</s> <s>borrowing techniques from the literature on training deep generative models, we present the wake-sleep recurrent attention model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients.</s> <s>we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.</s></p></d>", "label": ["<d><p><s>learning wake-sleep recurrent attention models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient.</s> <s>for the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets.</s> <s>for the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency.</s> <s>to bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses.</s> <s>our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation.</s> <s>the trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging.</s> <s>to demonstrate, we trained a sparsely connected network that runs on the truenorth chip using the mnist dataset.</s> <s>with a high performance network (ensemble of $64$), we achieve $99.42\\%$ accuracy at $121 \\mu$j per image, and with a high efficiency network (ensemble of $1$) we achieve $92.7\\%$ accuracy at $0.408 \\mu$j per image.</s></p></d>", "label": ["<d><p><s>backpropagation for energy-efficient neuromorphic computing</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the process of dynamic state estimation (filtering) based on point process observations is in general intractable.</s> <s>numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to computational neuroscience.</s> <s>we develop an analytically tractable bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding.</s> <s>the analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers.</s> <s>interestingly, we find that the information gained from the absence of spikes may be crucial to performance.</s></p></d>", "label": ["<d><p><s>a tractable approximation to optimal point process filtering: application to neural encoding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces.</s> <s>in this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy.</s> <s>specifically, we describe an illuminant estimation method that is built around a classifier for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels).</s> <s>during inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants.</s> <s>a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels.</s> <s>we begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images.</s> <s>these histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels.</s> <s>despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods.</s> <s>next, we propose a method to learn the luminance-to-chromaticity classifier end-to-end.</s> <s>using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set.</s> <s>this leads to further improvements in accuracy, most significantly in the tail of the error distribution.</s></p></d>", "label": ["<d><p><s>color constancy by learning to predict chromaticity from luminance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important class of problems involves training deep neural networks with sparse prediction targets of very high dimension d. these occur naturally in e.g.</s> <s>neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size d (e.g.</s> <s>200,000).</s> <s>computing the equally large, but typically non-sparse d-dimensional output vector from a last hidden layer of reasonable dimension d (e.g.</s> <s>500) incurs a prohibitive $o(dd)$ computational cost for each example, as does updating the $d \\times d$ output weight matrix and computing the gradient needed for backpropagation to previous layers.</s> <s>while efficient handling of large sparse network inputs is trivial, this case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training.</s> <s>in this work we develop an original algorithmic approach that, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in $o(d^2)$  per example instead of $o(dd)$, remarkably without ever computing the d-dimensional output.</s> <s>the proposed algorithm yields a speedup of $\\frac{d}{4d}$, i.e.</s> <s>two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.</s></p></d>", "label": ["<d><p><s>efficient exact gradient update for training deep networks with very large sparse targets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence.such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and neural turing machines,because the number of target classes in eachstep of the output depends on the length of the input, which is variable.problems such as sorting variable sized sequences, and various combinatorialoptimization problems belong to this class.</s> <s>our model solvesthe problem of variable size output dictionaries using a recently proposedmechanism of neural attention.</s> <s>it differs from the previous attentionattempts in that, instead of using attention to blend hidden units of anencoder to a context vector at each decoder step, it uses attention asa pointer to select a member of the input sequence as the output.</s> <s>we call this architecture a pointer net (ptr-net).we show ptr-nets can be used to learn approximate solutions to threechallenging geometric problems -- finding planar convex hulls, computingdelaunay triangulations, and the planar travelling salesman problem-- using training examples alone.</s> <s>ptr-nets not only improve oversequence-to-sequence with input attention, butalso allow us to generalize to variable size output dictionaries.we show that the learnt models generalize beyond the maximum lengthsthey were trained on.</s> <s>we hope our results on these taskswill encourage a broader exploration of neural learning for discreteproblems.</s></p></d>", "label": ["<d><p><s>pointer networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>precision-recall analysis abounds in applications of binary classification where true negatives do not add value and hence should not affect assessment of the classifier's performance.</s> <s>perhaps inspired by the many advantages of receiver operating characteristic (roc) curves and the area under such curves for accuracy-based performance assessment, many researchers have taken to report precision-recall (pr) curves and associated areas as performance metric.</s> <s>we demonstrate in this paper that this practice is fraught with difficulties, mainly because of incoherent scale assumptions -- e.g., the area under a pr curve takes the arithmetic mean of precision values whereas the $f_{\\beta}$ score applies the harmonic mean.</s> <s>we show how to fix this by plotting pr curves in a different coordinate system, and demonstrate that the new precision-recall-gain curves inherit all key advantages of roc curves.</s> <s>in particular, the area under precision-recall-gain curves conveys an expected $f_1$ score on a harmonic scale, and the convex hull of a precision-recall-gain curve allows us to calibrate the classifier's scores so as to determine, for each operating point on the convex hull, the interval of $\\beta$ values for which the point optimises $f_{\\beta}$.</s> <s>we demonstrate experimentally that the area under traditional pr curves can easily favour models with lower expected $f_1$ score than others, and so the use of precision-recall-gain curves will result in better model selection.</s></p></d>", "label": ["<d><p><s>precision-recall-gain curves: pr analysis done right</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning.</s> <s>because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research.</s> <s>to facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation.</s> <s>the system, called next,  provides a unique platform for real-world, reproducible active learning research.</s> <s>this paper details the challenges of building the system and demonstrates its capabilities with several experiments.</s> <s>the results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.</s></p></d>", "label": ["<d><p><s>next: a system for real-world development, evaluation, and application of active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the task of building compact deep learning pipelines suitable for deploymenton storage and power constrained mobile devices.</s> <s>we propose a uni-fied framework to learn a broad family of structured parameter matrices that arecharacterized by the notion of low displacement rank.</s> <s>our structured transformsadmit fast function and gradient evaluation, and span a rich range of parametersharing configurations whose statistical modeling capacity can be explicitly tunedalong a continuum from structured to unstructured.</s> <s>experimental results showthat these transforms can significantly accelerate inference and forward/backwardpasses during training, and offer superior accuracy-compactness-speed tradeoffsin comparison to a number of existing techniques.</s> <s>in keyword spotting applicationsin mobile speech recognition, our methods are much more effective thanstandard linear low-rank bottleneck layers and nearly retain the performance ofstate of the art models, while providing more than 3.5-fold compression.</s></p></d>", "label": ["<d><p><s>structured transforms for small-footprint deep learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks.</s> <s>following recent work that strongly suggests that most of thecritical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the hessian could help us design better suited adaptive learning rate schemes.</s> <s>we show that the popular jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems.</s> <s>we introduce a novel adaptive learning rate scheme, called esgd, based on the equilibration preconditioner.</s> <s>our experiments demonstrate that both schemes yield very similar step directions but that esgd sometimes surpasses rmsprop in terms of convergence speed, always clearly improving over plain stochastic gradient descent.</s></p></d>", "label": ["<d><p><s>equilibrated adaptive learning rates for non-convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application.</s> <s>although our method can work with arbitrary models, we focus on actively learning the appropriate structure for gaussian process (gp) models with arbitrary observation likelihoods.</s> <s>we then apply this framework to rapid screening for noise-induced hearing loss (nihl), a widespread and preventible disability, if diagnosed early.</s> <s>we construct a gp model for pure-tone audiometric responses of patients with nihl.</s> <s>using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of nihl with drastically fewer samples than existing approaches.</s> <s>further, the method is extremely fast and enables the diagnosis to be performed in real time.</s></p></d>", "label": ["<d><p><s>bayesian active model selection with an application to automated audiometry</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts.</s> <s>to be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters.</s> <s>recent work has started to tackle this automated machine learning (automl) problem with the help of efficient bayesian optimization methods.</s> <s>in this work we introduce a robust new automl system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters).</s> <s>this system, which we dub auto-sklearn, improves on existing automl methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization.</s> <s>our system won the first phase of the ongoing chalearn automl challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in automl.</s> <s>we also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.</s></p></d>", "label": ["<d><p><s>efficient and robust automated machine learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for many complex diseases, there is a wide variety of ways in which an individual can manifest the disease.</s> <s>the challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual's disease, which can in turn enable clinicians to optimize treatments.</s> <s>we represent an individual's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time.</s> <s>we propose a hierarchical latent variable model that individualizes predictions of disease trajectories.</s> <s>this model shares statistical strength across observations at different resolutions--the population, subpopulation and the individual level.</s> <s>we describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters.</s> <s>finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma.</s> <s>we compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy.</s></p></d>", "label": ["<d><p><s>a framework for individualizing predictions of disease trajectories by exploiting multi-resolution structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications.</s> <s>we introduce a new approximation for large-scale gaussian processes, the gaussian process random field (gprf), in which local gps are coupled via pairwise potentials.</s> <s>the gprf likelihood is a simple, tractable, and parallelizeable approximation to the full gp marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets.</s> <s>we demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.</s></p></d>", "label": ["<d><p><s>gaussian process random fields</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gaussian process (gp) models form a core part of probabilistic machine learning.</s> <s>considerable research effort has been made into attacking three issues with gp models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not gaussian and how to estimate covariance function parameter posteriors.</s> <s>this paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in sup- port of the function but otherwise free-form.</s> <s>the result is a hybrid monte-carlo sampling scheme which allows for a non-gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse gps.</s></p></d>", "label": ["<d><p><s>mcmc for variationally sparse gaussian processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a methodology for creating streaming, distributed inference algorithms for bayesian nonparametric (bnp) models.</s> <s>in the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model.</s> <s>in contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free.</s> <s>the key challenge in developing the framework, arising from fact that bnp models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central bnp posterior components before performing each update.</s> <s>to address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique.</s> <s>the paper concludes with an application of the methodology to the dp mixture model, with experimental results demonstrating its practical scalability and performance.</s></p></d>", "label": ["<d><p><s>streaming, distributed variational inference for bayesian nonparametrics</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a novel distribution that generalizes the multinomial distribution to enable dependencies between dimensions.</s> <s>our novel distribution is based on the parametric form of the poisson mrf model [yang et al., 2012] but is fundamentally different because of the domain restriction to a fixed-length vector like in a multinomial where the number of trials is fixed or known.</s> <s>thus, we propose the fixed-length poisson mrf (lpmrf) distribution.</s> <s>we develop methods to estimate the likelihood and log partition function (i.e.</s> <s>the log normalizing constant), which was not developed for the poisson mrf model.</s> <s>in addition, we propose novel mixture and topic models that use lpmrf as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed admixture of poisson mrfs [inouye et al., 2014].</s> <s>we show the effectiveness of our lpmrf distribution over multinomial models by evaluating the test set perplexity on a dataset of abstracts and wikipedia.</s> <s>qualitatively, we show that the positive dependencies discovered by lpmrf are interesting and intuitive.</s> <s>finally, we show that our algorithms are fast and have good scaling (code available online).</s></p></d>", "label": ["<d><p><s>fixed-length poisson mrf: adding dependencies to the multinomial</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>imagine a random walk that outputs a state only when visiting it for the first time.</s> <s>the observed output is therefore a repeat-censored version of the underlying walk, and consists of a permutation of the states or a prefix of it.</s> <s>we call this model initial-visit emitting random walk (invite).</s> <s>prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks, which is of great interest in both the study of human cognition and various clinical applications.</s> <s>however, parameter estimation in invite is challenging, because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable.</s> <s>in this paper, we propose the first efficient maximum likelihood estimate (mle) for invite by decomposing the censored output into a series of absorbing random walks.</s> <s>we also prove theoretical properties of the mle including identifiability and consistency.</s> <s>we show that invite outperforms several existing methods on real-world human response data from memory search tasks.</s></p></d>", "label": ["<d><p><s>human memory search as initial-visit emitting random walk</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a latent variable model and an efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types.</s> <s>a natural model for chromatin data in one cell type is a hidden markov model (hmm); we model the relationship between multiple cell types by connecting their hidden states by a fixed tree of known structure.</s> <s>the main challenge with learning parameters of such models is that iterative methods such as em are very slow, while naive spectral methods result in time and space complexity exponential in the number of cell types.</s> <s>we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets.</s> <s>we provide sample complexity bounds for our algorithm and evaluate it experimentally on biological data from nine human cell types.</s> <s>finally, we show that beyond our specific model, some of our algorithmic ideas can be applied to other graphical models.</s></p></d>", "label": ["<d><p><s>spectral learning of large structured hmms for comparative epigenomics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a general smoothing framework for graph kernels by  taking \\textit{structural similarity} into account, and apply it to  derive smoothed variants of popular graph kernels.</s> <s>our framework is inspired by state-of-the-art smoothing  techniques used in natural language processing (nlp).</s> <s>however, unlike  nlp applications which primarily deal with strings, we show how one  can apply smoothing to a richer class of inter-dependent  sub-structures that naturally arise in graphs.</s> <s>moreover, we discuss  extensions of the pitman-yor process that can be adapted to smooth  structured objects thereby leading to novel graph kernels.</s> <s>our  kernels are able to tackle the diagonal dominance problem, while  respecting the structural similarity between sub-structures,   especially under the presence of edge or label noise.</s> <s>experimental evaluation shows that not only our kernels outperform  the unsmoothed variants, but also achieve statistically significant  improvements in classification accuracy over several other graph  kernels that have been recently proposed in literature.</s> <s>our kernels  are competitive in terms of runtime, and offer a viable option for  practitioners.</s></p></d>", "label": ["<d><p><s>a structural smoothing framework for robust graph comparison</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we describe an embarrassingly parallel, anytime monte carlo method for likelihood-free models.</s> <s>the algorithm starts with the view that the stochasticity of the pseudo-samples generated by the simulator can be controlled externally by a vector of random numbers u, in such a way that the outcome, knowing u, is deterministic.</s> <s>for each instantiation of u we run an optimization procedure to minimize the distance between summary statistics of the simulator and the data.</s> <s>after reweighing these samples using the prior and the jacobian (accounting for the change of volume in transforming from the space of summary statistics to the space of parameters) we show that this weighted ensemble represents a monte carlo estimate of the posterior distribution.</s> <s>the procedure can be run embarrassingly parallel (each node handling one sample) and anytime (by allocating resources to the worst performing sample).</s> <s>the procedure is validated on six experiments.</s></p></d>", "label": ["<d><p><s>optimization monte carlo: efficient and embarrassingly parallel likelihood-free inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>existing inverse reinforcement learning (irl) algorithms have assumed each expert?s demonstrated trajectory to be produced by only a single reward function.</s> <s>this paper presents a novel generalization of the irl problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts?</s> <s>behaviors.</s> <s>solving our generalized irl problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states).</s> <s>by representing our irl problem with a probabilistic graphical model, an expectation-maximization (em) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert?s demonstrated trajectories.</s> <s>as a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by em can be derived.</s> <s>empirical evaluation on synthetic and real-world datasets shows that our irl algorithm outperforms the state-of-the-art em clustering with maximum likelihood irl, which is, interestingly, a reduced variant of our approach.</s></p></d>", "label": ["<d><p><s>inverse reinforcement learning with locally consistent reward functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects.</s> <s>to this end, we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations.</s> <s>in particular, we show that for multilabel metrics constructed as instance-, micro- and macro-averages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold.</s> <s>thus, our analysis extends the state of the art from a few known multilabel classification metrics such as hamming loss, to a general framework applicable to many of the classification metrics in common use.</s> <s>based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest.</s> <s>empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.</s></p></d>", "label": ["<d><p><s>consistent multilabel classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives.</s> <s>it seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved.</s> <s>we challenge this assertion by proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative, given k-approval votes.</s> <s>while the answer is generally positive, our theoretical and empirical results call attention to situations where approval voting is suboptimal.</s></p></d>", "label": ["<d><p><s>is approval voting optimal given approval votes?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>to make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs.</s> <s>because such analysis begins with dimensionality reduction, modelling early sensory processing requires biologically plausible online dimensionality reduction algorithms.</s> <s>recently, we derived such an algorithm, termed similarity matching, from a multidimensional scaling (mds) objective function.</s> <s>however, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed.</s> <s>because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction.</s> <s>here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix.</s> <s>we formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix.</s> <s>in turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix.</s> <s>in the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules.</s> <s>remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.</s></p></d>", "label": ["<d><p><s>a normative theory of adaptive dimensionality reduction in neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>decision trees and randomized forests are widely used in computer vision and machine learning.</s> <s>standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria.</s> <s>this greedy procedure often leads to suboptimal trees.</s> <s>in this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective.</s> <s>we show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss.</s> <s>computing the gradient of the proposed surrogate objective with respect to each training exemplar is o(d^2), where d is the tree depth, and thus training deep trees is feasible.</s> <s>the use of stochastic gradient descent for optimization enables effective training with large datasets.</s> <s>experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.</s></p></d>", "label": ["<d><p><s>efficient non-greedy optimization of decision trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of statistical computations with persistence diagrams, a summary representation of topological features in data.</s> <s>these diagrams encode persistent homology, a widely used invariant in topological data analysis.</s> <s>while several avenues towards a statistical treatment of the diagrams have been explored recently, we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel hilbert spaces.</s> <s>in fact, a positive definite kernel on persistence diagrams has recently been proposed, connecting persistent homology to popular kernel-based learning techniques such as support vector machines.</s> <s>however, important properties of that kernel which would enable a principled use in the context of probability measure embeddings remain to be explored.</s> <s>our contribution is to close this gap by proving universality of a variant of the original kernel, and to demonstrate its effective use in two-sample hypothesis testing on synthetic as well as real-world data.</s></p></d>", "label": ["<d><p><s>statistical topological data analysis - a kernel perspective</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>practitioners of bayesian statistics have long depended on markov chain monte carlo (mcmc) to obtain samples from intractable posterior distributions.</s> <s>unfortunately, mcmc algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning.</s> <s>the recently proposed consensus monte carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (scott et al, 2013).</s> <s>a fixed aggregation function then combines these samples, yielding approximate posterior samples.</s> <s>we introduce variational consensus monte carlo (vcmc), a variational bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target.</s> <s>the resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions.</s> <s>we illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step.</s> <s>our algorithm achieves a relative error reduction (measured against serial mcmc) of up to 39% compared to consensus monte carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a gaussian mixture model with 8 components in 8 dimensions.</s> <s>furthermore, these gains come at moderate cost compared to the runtime of serial mcmc, achieving near-ideal speedup in some instances.</s></p></d>", "label": ["<d><p><s>variational consensus monte carlo</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself.</s> <s>this higher-level abstraction improves generalization in different prediction settings, but computing predictions often becomes intractable in large decision spaces.</s> <s>we propose the softstar algorithm, a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior.</s> <s>this approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods.</s> <s>we present the algorithm, analyze approximation guarantees, and compare performance with simulation-based inference on two distinct complex decision tasks.</s></p></d>", "label": ["<d><p><s>softstar: heuristic-guided probabilistic inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose kernel hamiltonian monte carlo (kmc), a gradient-free adaptive mcmc algorithm based on hamiltonian monte carlo (hmc).</s> <s>on target densities where classical hmc is not an option due to intractable gradients, kmc adaptively learns the target's gradient structure by fitting an exponential family model in a reproducing kernel hilbert space.</s> <s>computational costs are reduced by two novel efficient approximations to this gradient.</s> <s>while being asymptotically exact, kmc mimics hmc in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers.</s> <s>we support our claims with experimental studies on both toy and real-world applications, including approximate bayesian computation and exact-approximate mcmc.</s></p></d>", "label": ["<d><p><s>gradient-free hamiltonian monte carlo with efficient kernel exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many recent markov chain monte carlo (mcmc) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution.</s> <s>in tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations.</s> <s>however, such stochastic gradient mcmc samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial.</s> <s>even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise.</s> <s>in this paper, we provide a general recipe for constructing mcmc samplers--including stochastic gradient versions--based on continuous markov processes specified via two matrices.</s> <s>we constructively prove that the framework is complete.</s> <s>that is, any continuous markov process that provides samples from the target distribution can be written in our framework.</s> <s>we show how previous continuous-dynamic samplers can be trivially reinvented in our framework, avoiding the complicated sampler-specific proofs.</s> <s>we likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient riemann hamiltonian monte carlo (sgrhmc).</s> <s>our experiments on simulated data and a streaming wikipedia analysis demonstrate that the proposed sgrhmc sampler inherits the benefits of riemann hmc, with the scalability of stochastic gradient methods.</s></p></d>", "label": ["<d><p><s>a complete recipe for stochastic gradient mcmc</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a globally-convergent algorithm for optimizing the tree-reweighted (trw) variational objective over the marginal polytope.</s> <s>the algorithm is based on the conditional gradient method (frank-wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (map) calls.</s> <s>this modular structure enables us to leverage black-box map solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation.</s> <s>theoretically, we bound the sub-optimality for the proposed algorithm despite the trw objective having unbounded gradients at the boundary of the marginal polytope.</s> <s>empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.</s></p></d>", "label": ["<d><p><s>barrier frank-wolfe for marginal inference</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we show the existence of a locality-sensitive hashing (lsh) family for the angular distance that yields an approximate near neighbor search algorithm with the asymptotically optimal  running time exponent.</s> <s>unlike earlier algorithms with this property (e.g., spherical lsh (andoni-indyk-nguyen-razenshteyn 2014) (andoni-razenshteyn 2015)), our algorithm is also practical, improving upon the well-studied hyperplane lsh (charikar 2002) in practice.</s> <s>we also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.we complement the above positive results with a fine-grained lower bound for the quality of any lsh family for angular distance.</s> <s>our lower bound implies that the above lsh family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of lsh functions.</s></p></d>", "label": ["<d><p><s>practical and optimal lsh for angular distance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce principal differences analysis for analyzing differences between high-dimensional distributions.</s> <s>the method operates by finding the projection that maximizes the wasserstein divergence between the resulting univariate populations.</s> <s>relying on the cramer-wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences.</s> <s>a sparse variant of the method is introduced to identify features responsible for the differences.</s> <s>we provide algorithms for both the original minimax formulation as well as its semidefinite relaxation.</s> <s>in addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell rna-seq.</s> <s>our broader framework extends beyond the specific choice of wasserstein divergence.</s></p></d>", "label": ["<d><p><s>principal differences analysis: interpretable characterization of differences between distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new variational inference method based on the kullback-leibler (kl) proximal term.</s> <s>we make two contributions towards improving efficiency of variational inference.</s> <s>firstly, we derive a kl proximal-point algorithm and show its equivalence to gradient descent with natural gradient in stochastic variational inference.</s> <s>secondly, we use the proximal framework to derive efficient variational algorithms for non-conjugate models.</s> <s>we propose a splitting procedure to separate non-conjugate terms from conjugate ones.</s> <s>we then linearize the non-conjugate terms and show that the resulting subproblem admits a closed-form solution.</s> <s>overall, our approach converts a non-conjugate model to subproblems that involve inference in well-known conjugate models.</s> <s>we apply our method to many models and derive generalizations for non-conjugate exponential family.</s> <s>applications to real-world datasets show that our proposed algorithms are easy to implement, fast to converge, perform well, and reduce computations.</s></p></d>", "label": ["<d><p><s>kullback-leibler proximal variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we address the question of identifiability and learning algorithms for large-scale poisson directed acyclic graphical (dag) models.</s> <s>we define general poisson dag models as models where each node is a poisson random variable with rate parameter depending on the values of the parents in the underlying dag.</s> <s>first, we prove that poisson dag models are identifiable from observational data, and present a polynomial-time algorithm that learns the poisson dag model under suitable regularity conditions.</s> <s>the main idea behind our algorithm is based on overdispersion, in that variables that are conditionally poisson are overdispersed relative to variables that are marginally poisson.</s> <s>our algorithms exploits overdispersion along with methods for learning sparse poisson undirected graphical models for faster computation.</s> <s>we provide both theoretical guarantees and simulation results for both small and large-scale dags.</s></p></d>", "label": ["<d><p><s>learning large-scale poisson dag models based on overdispersion scoring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications, the data is of rich structure that can be represented by a hypergraph, where the data items are represented by vertices and the associations among items are represented by hyperedges.</s> <s>equivalently, we are given an input bipartite graph with two types of vertices: items, and associations (which we refer to as topics).</s> <s>we consider the problem of partitioning the set of items into a given number of parts such that the maximum number of topics covered by a part of the partition is minimized.</s> <s>this is a natural clustering problem, with various applications, e.g.</s> <s>partitioning of a set of  information objects such as documents, images, and videos, and load balancing in the context of computation platforms.in this paper, we focus on the streaming computation model for this problem, in which items arrive online one at a time and each item must be assigned irrevocably to a part of the partition at its arrival time.</s> <s>motivated by scalability requirements, we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of the parts of the partition.</s> <s>we show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions.</s> <s>we also report results of an extensive empirical evaluation, which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches.</s></p></d>", "label": ["<d><p><s>streaming min-max hypergraph partitioning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other.</s> <s>while previously the relationship between tasks had to be user-defined in the form of an output kernel, recent  approaches jointly learn the tasks and the output kernel.</s> <s>as the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the  number of tasks as an eigendecomposition is required in each step.</s> <s>using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem.</s> <s>this leads to an unconstrained dual problem which can be solved efficiently.</s> <s>experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.</s></p></d>", "label": ["<d><p><s>efficient output kernel learning for multiple tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world.</s> <s>estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems.</s> <s>we introduce the formalism of stochastic computation graphs--directed acyclic graphs that include both deterministic functions and conditional probability distributions and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient.</s> <s>the resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm.</s> <s>the generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein.</s> <s>it could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.</s></p></d>", "label": ["<d><p><s>gradient estimation using stochastic computation graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>lifted inference rules exploit symmetries for fast reasoning in statistical rela-tional models.</s> <s>computational complexity of these rules is highly dependent onthe choice of the constraint language they operate on and therefore coming upwith the right kind of representation is critical to the success of lifted inference.in this paper, we propose a new constraint language, called setineq, which allowssubset, equality and inequality constraints, to represent substitutions over the vari-ables in the theory.</s> <s>our constraint formulation is strictly more expressive thanexisting representations, yet easy to operate on.</s> <s>we reformulate the three mainlifting rules: decomposer, generalized binomial and the recently proposed singleoccurrence for map inference, to work with our constraint representation.</s> <s>exper-iments on benchmark mlns for exact and sampling based inference demonstratethe effectiveness of our approach over several other existing techniques.</s></p></d>", "label": ["<d><p><s>lifted inference rules with constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the following multi-component sparse pca problem:given a set of data points, we seek to extract a small number of sparse components with \\emph{disjoint} supports that jointly capture the maximum possible variance.such components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but this greedy procedure is suboptimal.we present a novel algorithm for sparse pca that jointly optimizes multiple disjoint components.</s> <s>the extracted features capture variance that lies within a multiplicative factor arbitrarily close to $1$ from the optimal.our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem.its complexity grows as a low order polynomial in the ambient dimension of the input data, but exponentially in its rank.however, it can be effectively applied on a low-dimensional sketch of the input data.we evaluate our algorithm on real datasets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches.</s></p></d>", "label": ["<d><p><s>sparse pca via bipartite matchings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a unifying generalization of the lov?sz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges.</s> <s>we show how it can be computed exactly by semidefinite programming, and how to approximate it using svm computations.</s> <s>we show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for max-cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.</s></p></d>", "label": ["<d><p><s>weighted theta functions and embeddings with applications to max-cut, clustering and summarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of online rank elicitation, assuming that rankings of a set of alternatives obey the plackett-luce distribution.</s> <s>following the setting of the dueling bandits problem, the learner is allowed to query pairwise comparisons between alternatives, i.e., to sample pairwise marginals of the distribution in an online fashion.</s> <s>using this information, the learner seeks to reliably predict the most probable ranking (or top-alternative).</s> <s>our approach is based on constructing a surrogate probability distribution over rankings based on a sorting procedure, for which the pairwise marginals provably coincide with the marginals of the plackett-luce distribution.</s> <s>in addition to a formal performance and complexity analysis, we present first experimental studies.</s></p></d>", "label": ["<d><p><s>online rank elicitation for plackett-luce: a dueling bandits approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables.</s> <s>markov random fields (mrfs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks.</s> <s>a chain graph model under the lauritzen-wermuth-frydenberg interpretation (hereafter a chain graph model) generalizes both bayesian networks and mrfs, and can represent asymmetric and symmetric relationships together.as in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model.</s> <s>one recent approach to the study of marginal graphical models is to consider a well-behaved supermodel.</s> <s>such a supermodel of marginals of bayesian networks, defined only by conditional independences, and termed the ordinary markov model, was studied at length in (evans and richardson, 2014).in this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a markov property, with supermodels of a marginal of chain graphs defined only by conditional independences.</s> <s>special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and ordinary markov model carry over.</s> <s>our results suggest that segregated graphs define an analogue of the ordinary markov model for marginals of chain graph models.</s></p></d>", "label": ["<d><p><s>segregated graphs and marginals of chain graph models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study how well one can recover sparse principal componentsof a data matrix using a sketch formed from a few of its elements.</s> <s>we show that for a wide class of optimization problems,if the sketch is close (in the spectral norm) to the original datamatrix, then one can recover a near optimal solution to the optimizationproblem by using the sketch.</s> <s>in particular, we use this approach toobtain sparse principal components and show that for \\math{m} data pointsin \\math{n} dimensions,\\math{o(\\epsilon^{-2}\\tilde k\\max\\{m,n\\})} elements gives an\\math{\\epsilon}-additive approximation to the sparse pca problem(\\math{\\tilde k} is the stable rank of the data matrix).we demonstrate our algorithms extensivelyon image, text, biological and financial data.the results show that not only are we able to recover the sparse pcas from the incomplete data, but by using our sparse sketch, the running timedrops by a factor of five or more.</s></p></d>", "label": ["<d><p><s>approximating sparse pca from incomplete data</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the stochastic block model (sbm) has recently gathered significant attention due to new threshold phenomena.</s> <s>however, most developments rely on the knowledge of the model parameters, or at least on the number of communities.</s> <s>this paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in abbe-sandon focs15.</s> <s>in the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees.</s> <s>this lower-bound requirement is removed for the regime of arbitrarily slowly diverging degrees, and the model parameters are learned efficiently.</s> <s>for the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that achieves the ch-limit for exact recovery in quasi-linear time.</s> <s>these provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the sbm.</s></p></d>", "label": ["<d><p><s>recovering communities in the general stochastic block model without knowing the parameters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge.</s> <s>one way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters.</s> <s>this paper explores an alternative notion of a tractable set, namely a set of ?fast-mixing parameters?</s> <s>where markov chain monte carlo (mcmc) inference can be guaranteed to quickly converge to the stationary distribution.</s> <s>while it is common in practice to approximate the likelihood gradient using samples obtained from mcmc, such procedures lack theoretical guarantees.</s> <s>this paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability.</s> <s>when unregularized, to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon, disregarding logarithmic factors.</s> <s>when ridge-regularized, strong convexity allows a solution epsilon-accurate in parameter distance with an effort quadratic in 1/epsilon.</s> <s>both of these provide of a fully-polynomial time randomized approximation scheme.</s></p></d>", "label": ["<d><p><s>maximum likelihood learning with arbitrary treewidth via fast-mixing parameter sets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of testing whether two unequal-sized samples were drawn from identical distributions, versus distributions that differ significantly.</s> <s>specifically, given a target error parameter $\\eps > 0$,  $m_1$ independent draws from an unknown distribution $p$ with discrete support, and $m_2$ draws from an unknown distribution $q$ of discrete support, we describe a test for distinguishing the case that $p=q$ from the case that $||p-q||_1 \\geq \\eps$.</s> <s>if $p$ and $q$ are supported on at most $n$ elements, then our test is successful with high probability provided $m_1\\geq n^{2/3}/\\varepsilon^{4/3}$ and $m_2 = \\omega\\left(\\max\\{\\frac{n}{\\sqrt m_1\\varepsilon^2}, \\frac{\\sqrt n}{\\varepsilon^2}\\}\\right).$ we show that this tradeoff is information theoretically optimal throughout this range, in the dependencies on all parameters, $n,m_1,$ and $\\eps$, to constant factors.</s> <s>as a consequence, we obtain an algorithm for estimating the mixing time of a markov chain on $n$ states up to a $\\log n$ factor that uses $\\tilde{o}(n^{3/2} \\tau_{mix})$ queries to a ``next node'' oracle.</s> <s>the core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic data and on natural language data.</s> <s>we believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems.</s></p></d>", "label": ["<d><p><s>testing closeness with unequal sized samples</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of learning causal networks with interventions, when each intervention is limited in size under pearl's structural equation model with independent errors (sem-ie).</s> <s>the objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph.</s> <s>previous work has focused on the use of separating systems for complete graphs for this task.</s> <s>we prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case.</s> <s>in addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics.</s> <s>we also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms.</s> <s>for general chordal graphs, we derive worst case lower bounds on the number of interventions.</s> <s>building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely.</s> <s>in the worst case, our achievable scheme is an $\\alpha$-approximation algorithm where $\\alpha$ is the independence number of the graph.</s> <s>we also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound.</s> <s>in the other extreme, there are graph classes for which the required number of experiments is multiplicatively $\\alpha$ away from our lower bound.</s> <s>in simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs.</s></p></d>", "label": ["<d><p><s>learning causal graphs with small interventions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>counterfactual regret minimization (cfr) is a leading algorithm for finding a nash equilibrium in large zero-sum imperfect-information games.</s> <s>cfr is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set.we introduce an improvement to cfr that prunes any path of play in the tree, and its descendants, that has negative regret.</s> <s>it revisits that sequence at the earliest subsequent cfr iteration where the regret could have become positive, had that path been explored on every iteration.</s> <s>the new algorithm maintains cfr's convergence guarantees while making iterations significantly faster---even if previously known pruning techniques are used in the comparison.</s> <s>this improvement carries over to cfr+, a recent variant of cfr.</s> <s>experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.</s></p></d>", "label": ["<d><p><s>regret-based pruning in extensive-form games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose and analyse estimators for statistical functionals of one or moredistributions under nonparametric assumptions.our estimators are derived from the von mises expansion andare based on the theory of influence functions, which appearin the semiparametric statistics literature.we show that estimators based either on data-splitting or a leave-one-out techniqueenjoy fast rates of convergence and other favorable theoretical properties.we apply this framework to derive estimators for several popular informationtheoretic quantities, and via empirical evaluation, show the advantage of thisapproach over existing estimators.</s></p></d>", "label": ["<d><p><s>nonparametric von mises estimators for entropies, divergences and mutual informations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>expectation propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees.</s> <s>in this article, we prove that the approximation errors made by ep can be bounded.</s> <s>our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study ep's convergence with respect to the true posterior.</s> <s>in particular, we show that ep converges at a rate of $o(n^{-2})$ for the mean, up to an order of magnitude faster than the traditional gaussian approximation at the mode.</s> <s>we also give similar asymptotic expansions for moments of order 2 to 4, as well as excess kullback-leibler cost (defined as the additional kl cost incurred by using ep rather than the ideal gaussian approximation).</s> <s>all these expansions highlight the superior convergence properties of ep.</s> <s>our approach for deriving those results is likely applicable to many similar approximate inference methods.</s> <s>in addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.</s></p></d>", "label": ["<d><p><s>bounding errors of expectation-propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>abstract we propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including stochastic variance reduced gradient (svrg) and stochastic dual coordinate ascent (sdca).</s> <s>for a large family of penalized empirical risk minimization problems, our methods exploit data dependent local smoothness of the loss functions near the optimum, while maintaining convergence guarantees.</s> <s>our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better.</s> <s>empirically, we provide thorough numerical results to back up our theory.</s> <s>additionally we present algorithms exploiting local smoothness in more aggressive ways, which perform even better in practice.</s></p></d>", "label": ["<d><p><s>local smoothness in variance reduced optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a general theory of the expectation-maximization (em) algorithm for inferring high dimensional latent variable models.</s> <s>in particular, we make two contributions: (i) for parameter estimation, we propose a novel high dimensional em algorithm which naturally incorporates sparsity structure into parameter estimation.</s> <s>with an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence.</s> <s>(ii) based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters.</s> <s>for a broad family of statistical models,  our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.</s></p></d>", "label": ["<d><p><s>high dimensional em algorithm: statistical optimization and asymptotic normality</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an associative memory is a structure learned from a dataset $\\mathcal{m}$ of vectors (signals) in a way such that, given a noisy version of one of the vectors as input, the nearest valid  vector from $\\mathcal{m}$ (nearest neighbor) is provided as output, preferably via a fast iterative algorithm.</s> <s>traditionally, binary (or $q$-ary) hopfield neural networks are used to  model the above structure.</s> <s>in this paper, for the first time, we propose a model of associative memory based on sparse recovery of signals.</s> <s>our basic premise is simple.</s> <s>for a dataset, we learn a set of linear constraints that every vector in the  dataset must satisfy.</s> <s>provided these linear constraints possess  some special properties, it is possible to cast the task of finding nearest neighbor as a sparse recovery problem.</s> <s>assuming  generic random models for the dataset, we show that it is possible to store super-polynomial or exponential number of $n$-length vectors in a neural network of size $o(n)$.</s> <s>furthermore, given a noisy version of one of the  stored vectors corrupted in near-linear number of coordinates, the vector can be correctly recalled using a neurally feasible algorithm.</s></p></d>", "label": ["<d><p><s>associative memory via a sparse recovery model</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>most recent results in matrix completion assume that the matrix under consideration is low-rank or that the columns are in a union of low-rank subspaces.</s> <s>in real-world settings, however, the linear structure underlying these models is distorted by a (typically unknown) nonlinear transformation.</s> <s>this paper addresses the challenge of matrix completion in the face of such nonlinearities.</s> <s>given a few observations of a matrix that are obtained by applying a lipschitz, monotonic function to a low rank matrix, our task is to estimate the remaining unobserved entries.</s> <s>we propose a novel matrix completion method that alternates between low-rank matrix estimation and monotonic function estimation to estimate the missing matrix elements.</s> <s>mean squared error bounds provide insight into how well the matrix can be estimated based on the size,  rank of the matrix and properties of the nonlinear transformation.</s> <s>empirical results on synthetic and real-world datasets demonstrate the competitiveness of the proposed approach.</s></p></d>", "label": ["<d><p><s>matrix completion under monotonic single index models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>over the past decades, linear programming (lp) has been widely used in different areas and considered as one of the mature technologies in numerical optimization.</s> <s>however, the complexity offered by state-of-the-art algorithms (i.e.</s> <s>interior-point method and primal, dual simplex methods) is still unsatisfactory for problems in machine learning with huge number of variables and constraints.</s> <s>in this paper, we investigate a general lp algorithm based on the combination of augmented lagrangian and coordinate descent (al-cd), giving an iteration complexity of $o((\\log(1/\\epsilon))^2)$ with $o(nnz(a))$ cost per iteration, where $nnz(a)$ is the number of non-zeros in the $m\\times n$ constraint matrix $a$, and in practice, one can further reduce cost per iteration to the order of non-zeros in columns (rows) corresponding to the active primal (dual) variables through an active-set strategy.</s> <s>the algorithm thus yields a tractable alternative to standard lp methods for large-scale problems of sparse solutions and $nnz(a)\\ll mn$.</s> <s>we conduct experiments on large-scale lp instances from $\\ell_1$-regularized multi-class svm, sparse inverse covariance estimation, and nonnegative matrix factorization, where the proposed approach finds solutions of $10^{-3}$ precision orders of magnitude faster than state-of-the-art implementations of interior-point and simplex methods.</s></p></d>", "label": ["<d><p><s>sparse linear programming via primal and dual augmented coordinate descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of minimizing a sum of $n$ functions via projected iterations onto a convex parameter set $\\c \\subset \\reals^p$, where $n\\gg p\\gg 1$.</s> <s>in this regime, algorithms which utilize sub-sampling techniques are known to be effective.in this paper, we use sub-sampling techniques together with low-rank approximation to design a new randomized batch algorithm which possesses comparable convergence rate to newton's method, yet has much smaller per-iteration cost.</s> <s>the proposed algorithm is robust in terms of starting point and step size, and enjoys a composite convergence rate, namely, quadratic convergence at start and linear convergence when the iterate is close to the minimizer.</s> <s>we develop its theoretical analysis which also allows us to select near-optimal algorithm parameters.</s> <s>our theoretical results can be used to obtain convergence rates of previously proposed sub-sampling based algorithms as well.</s> <s>we demonstrate how our results apply to well-known machine learning problems.lastly, we evaluate the performance of our algorithm on several datasets under various scenarios.</s></p></d>", "label": ["<d><p><s>convergence rates of sub-sampled newton methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient descent (sgd) is a workhorse in machine learning, yet it is also known to be slow relative to steepest descent.</s> <s>recently, variance reduction techniques such as svrg and saga have been proposed to overcome this weakness.</s> <s>with asymptotically vanishing variance, a constant step size can be maintained, resulting in geometric convergence rates.</s> <s>however, these methods are either based on occasional computations of full gradients at pivot points (svrg), or on keeping per data point corrections in memory (saga).</s> <s>this has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to sgd may need a certain number of epochs in order to materialize.</s> <s>this paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points.</s> <s>while not meant to be offering advantages in an asymptotic setting, there are significant benefits in the transient optimization phase, in particular in a streaming or single-epoch setting.</s> <s>we investigate this family of algorithms in a thorough analysis and show supporting experimental results.</s> <s>as a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.</s></p></d>", "label": ["<d><p><s>variance reduced stochastic gradient descent with neighbors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data.</s> <s>to facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a kronecker product structure.</s> <s>the penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function.</s> <s>in spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery.</s> <s>notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work.</s> <s>our theoretical results are backed by thorough numerical studies.</s></p></d>", "label": ["<d><p><s>non-convex statistical optimization for sparse tensor graphical model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well.</s> <s>previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the pac or the agnostic pac model.</s> <s>in this paper, we shift our attention to a more general setting -- maximum likelihood estimation.</s> <s>provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem.</s> <s>the conditions we require are fairly general, and cover the widely popular class of generalized linear models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields.</s> <s>we provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms.</s> <s>our analysis shows that unlike binary classification in the realizable case, just a single extraround of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation.</s> <s>on the empirical side, the recent work in (gu et al.</s> <s>2012) and (gu et al.</s> <s>2014) (on active linear and logistic regression) shows the promise of this approach.</s></p></d>", "label": ["<d><p><s>convergence rates of active learning for maximum likelihood estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the restless bandit associated with an extremely simple scalar kalman filter model in discrete time.</s> <s>under certain assumptions, we prove that the problem is {\\it indexable} in the sense that the {\\it whittle index} is a non-decreasing function of the relevant belief state.</s> <s>in spite of the long history of this problem, this appears to be the first such proof.</s> <s>we use results about {\\it schur-convexity} and {\\it mechanical words}, which are particularbinary strings intimately related to {\\it palindromes}.</s></p></d>", "label": ["<d><p><s>when are kalman-filter restless bandits indexable?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost.</s> <s>these studies have focused on specific risk-measures, such as the variance or conditional value at risk (cvar).</s> <s>in this work, we extend the policy gradient method to the whole class of coherent risk measures, which is widely accepted in finance and operations research, among other fields.</s> <s>we consider both static and time-consistent dynamic risk measures.</s> <s>for static risk measures, our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming.</s> <s>for dynamic risk measures, our approach is actor-critic style and involves explicit approximation of value function.</s> <s>most importantly, our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.</s></p></d>", "label": ["<d><p><s>policy gradient for coherent risk measures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in past few years, several techniques have been proposed for training of linear support vector machine (svm) in limited-memory setting, where a dual block-coordinate descent (dual-bcd) method was used to balance cost spent on i/o and computation.</s> <s>in this paper, we consider the more general setting of regularized \\emph{empirical risk minimization (erm)} when data cannot fit into memory.</s> <s>in particular, we generalize the existing block minimization framework based on strong duality and \\emph{augmented lagrangian} technique to achieve global convergence for erm with arbitrary convex loss function and regularizer.</s> <s>the block minimization framework is flexible in the sense that, given a solver working under sufficient memory, one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition.</s> <s>we conduct experiments on l1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to  algorithms adopted from online and distributed settings, which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.</s></p></d>", "label": ["<d><p><s>a dual augmented block minimization framework for learning with limited memory</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the frank-wolfe (fw) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications.</s> <s>however, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary.</s> <s>a simple less-known fix is to add the possibility to take `away steps' during optimization, an operation that importantly does not require a feasibility oracle.</s> <s>in this paper, we highlight and clarify several variants of the frank-wolfe optimization algorithm that has been successfully applied in practice: fw with away steps, pairwise fw, fully-corrective fw and wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence under a weaker condition than strong convexity.</s> <s>the constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of the `condition number' of the constraint set.</s> <s>we provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.</s></p></d>", "label": ["<d><p><s>on the global linear convergence of frank-wolfe optimization variants</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of minimizing the average of a large number of smooth convex functions penalized with a strongly convex regularizer.</s> <s>we propose and analyze a novel primal-dual method (quartz) which at every iteration samples and updates a random subset of the dual variables, chosen according to an arbitrary distribution.</s> <s>in contrast to typical analysis, we directly bound the decrease of the primal-dual error (in expectation), without the need to first analyze the dual error.</s> <s>depending on the choice of the sampling, we obtain efficient serial and mini-batch variants of the method.</s> <s>in the serial case, our bounds match the best known bounds for sdca (both with uniform and importance sampling).</s> <s>with standard mini-batching, our bounds predict initial data-independent speedup as well as additional data-driven speedup which depends on spectral and sparsity properties of the data.</s></p></d>", "label": ["<d><p><s>quartz: randomized dual coordinate ascent with arbitrary sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider a generalization of the submodular cover problem based on the concept of diminishing return property on the integer lattice.</s> <s>we are motivated by real scenarios in machine learning that cannot be captured by (traditional) submodular set functions.</s> <s>we show that the generalized submodular cover problem can be applied to various problems and devise a bicriteria approximation algorithm.</s> <s>our algorithm is guaranteed to output a log-factor approximate solution that satisfies the constraints with the desired accuracy.</s> <s>the running time of our algorithm is roughly $o(n\\log (nr) \\log{r})$, where $n$ is the size of the ground set and $r$ is the maximum value of a coordinate.</s> <s>the dependency on $r$ is exponentially better than the naive reduction algorithms.</s> <s>several experiments on real and artificial datasets demonstrate that the solution quality of our algorithm is comparable to naive algorithms, while the running time is several orders of magnitude faster.</s></p></d>", "label": ["<d><p><s>a generalization of submodular cover via the diminishing return property on the integer lattice</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce a generic scheme for accelerating first-order optimization methods in the sense of nesterov, which builds upon a new analysis of the accelerated proximal point algorithm.</s> <s>our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence.</s> <s>this strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, sag, saga, sdca, svrg, finito/miso, and their proximal variants.</s> <s>for all of these methods, we provide acceleration and explicit support for non-strongly convex objectives.</s> <s>in addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.</s></p></d>", "label": ["<d><p><s>a universal catalyst for first-order optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we revisit the problem of constructing a near-optimal rank $k$ approximation of a matrix $m\\in [0,1]^{m\\times n}$ under the streaming data model where the columns of $m$ are revealed sequentially.</s> <s>we present sla (streaming low-rank approximation), an algorithm that is asymptotically accurate, when $k s_{k+1} (m) = o(\\sqrt{mn})$ where $s_{k+1}(m)$ is the $(k+1)$-th largest singular value of $m$.</s> <s>this means that its average mean-square error converges to 0 as $m$ and $n$ grow large (i.e., $\\|\\hat{m}^{(k)}-m^{(k)} \\|_f^2 = o(mn)$ with high probability, where $\\hat{m}^{(k)}$ and $m^{(k)}$ denote the output of sla and the optimal rank $k$ approximation of $m$, respectively).</s> <s>our algorithm makes one pass on the data if the columns of $m$ are revealed in a random order, and two passes if the columns of $m$ arrive in an arbitrary order.</s> <s>to reduce its memory footprint and complexity, sla uses random sparsification, and samples each entry of $m$ with a small probability $\\delta$.</s> <s>in turn, sla is memory optimal as its required memory space scales as $k(m+n)$, the dimension of its output.</s> <s>furthermore, sla is computationally efficient as it runs in $o(\\delta  kmn)$ time (a constant number of operations is made for each observed entry of $m$), which can be as small as $o(k\\log(m)^4 n)$ for an appropriate choice of $\\delta$ and if $n\\ge m$.</s></p></d>", "label": ["<d><p><s>fast and memory optimal low-rank matrix approximation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the greedy algorithm is extensively studied in the field of combinatorial optimization for decades.</s> <s>in this paper, we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time.</s> <s>we first propose the greedy regret and $\\epsilon$-quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm.</s> <s>we then propose two online greedy learning algorithms with semi-bandit feedbacks, which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning, one for each of the regret metrics respectively.</s> <s>both algorithms achieve $o(\\log t)$ problem-dependent regret bound ($t$ being the time horizon) for a general class of combinatorial structures and reward functions that allow greedy solutions.</s> <s>we further show that the bound is tight in $t$ and other problem instance parameters.</s></p></d>", "label": ["<d><p><s>stochastic online greedy learning with semi-bandit feedbacks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study an idealised sequential resource allocation problem.</s> <s>in each time step the learner chooses an allocation of several resource types between a number of tasks.</s> <s>assigning more resources to a task increases the probability that it is completed.</s> <s>the problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy.</s> <s>our main contribution is the new setting and an algorithm with nearly-optimal regret analysis.</s> <s>along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise.</s> <s>we also present some new results for stochastic linear bandits on the hypercube that significantly out-performs existing work, especially in the sparse case.</s></p></d>", "label": ["<d><p><s>linear multi-resource allocation with semi-bandit feedback</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>computing the map assignment in graphical models is generally intractable.</s> <s>as a result, for discrete graphical models, the map problem is often approximated using linear programming relaxations.</s> <s>much research has focused on characterizing when these lp relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog.</s> <s>in this work, we use graph covers to provide necessary and sufficient conditions for continuous map relaxations to be tight.</s> <s>we use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models.</s> <s>we conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the map relaxation can and cannot be tight.</s></p></d>", "label": ["<d><p><s>exactness of approximate map inference in continuous mrfs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variable screening is a fast dimension reduction technique for assisting high dimensional feature selection.</s> <s>as a preselection method, it selects a moderate size subset of candidate variables for further refining via feature selection to produce the final model.</s> <s>the performance of variable screening depends on both computational efficiency and the ability to dramatically reduce the number of variables without discarding the important ones.</s> <s>when the data dimension $p$ is substantially larger than the sample size $n$, variable screening becomes crucial as 1) faster feature selection algorithms are needed; 2) conditions guaranteeing selection consistency might fail to hold.this article studies a class of linear screening methods and establishes consistency theory for this special class.</s> <s>in particular, we prove the restricted diagonally dominant (rdd) condition is a necessary and sufficient condition for strong screening consistency.</s> <s>as concrete examples, we show two screening methods $sis$ and $holp$ are both strong screening consistent (subject to additional constraints) with large probability if $n > o((\\rho s + \\sigma/\\tau)^2\\log p)$ under random designs.</s> <s>in addition, we relate the rdd condition to the irrepresentable condition, and highlight limitations of $sis$.</s></p></d>", "label": ["<d><p><s>on the consistency theory of high dimensional variable screening</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the projected langevin monte carlo (lmc) algorithm, a close cousin of projected stochastic gradient descent (sgd).</s> <s>we show that lmc allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood.</s> <s>this gives the first markov chain to sample from a log-concave distribution with a first-order oracle, as the existing chains with provable guarantees (lattice walk, ball walk and hit-and-run) require a zeroth-order oracle.</s> <s>our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand sgd and its variants.</s></p></d>", "label": ["<d><p><s>finite-time analysis of projected langevin monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of high-dimensional structured estimation with norm-regularized estimators, such as lasso, when the design matrix and noise are drawn from sub-exponential distributions.existing results only consider sub-gaussian designs and noise, and both the sample complexity and non-asymptotic estimation error have been shown to depend on the gaussian width of suitable sets.</s> <s>in contrast, for the sub-exponential setting, we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets, and the analysis holds for any norm.</s> <s>further, using generic chaining, we show that the exponential width for any set will be at most $\\sqrt{\\log p}$ times the gaussian width of the set, yielding gaussian width based results even for the sub-exponential case.</s> <s>further, for certain popular estimators, viz lasso and group lasso, using a vc-dimension based analysis, we show that the sample complexity will in fact be the same order as gaussian designs.</s> <s>our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.</s></p></d>", "label": ["<d><p><s>beyond sub-gaussian measurements: high-dimensional structured estimation with sub-exponential designs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study nystr?m type subsampling approaches  to large   scale  kernel methods, and  prove   learning bounds in the  statistical learning setting,  where random sampling and high probability estimates are considered.</s> <s>in particular, we prove that these approaches  can achieve optimal learning bounds, provided the subsampling level is suitably chosen.</s> <s>these results suggest a simple  incremental variant of nystr?m kernel ridge regression, where the subsampling level  controls at the same time  regularization and computations.</s> <s>extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.</s></p></d>", "label": ["<d><p><s>less is more: nystr?m computational regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes.</s> <s>we develop top-down tree construction approaches for constructing logarithmic depth trees.</s> <s>on the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced.</s> <s>we demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy.</s> <s>however, the objective function at the nodes is challenging to optimize computationally.</s> <s>we address the empirical problem with a new online decision tree construction procedure.</s> <s>experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.</s></p></d>", "label": ["<d><p><s>logarithmic time online multiclass prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space.</s> <s>often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions.</s> <s>we tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph.</s> <s>we formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods.</s> <s>on the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees.</s> <s>we validate our results on both real and synthetic datasets.</s></p></d>", "label": ["<d><p><s>collaborative filtering with graph information: consistency and scalable methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a new active learning algorithm for the streaming settingsatisfying three important properties: 1) it provably works for anyclassifier representation and classification problem including thosewith severe noise.</s> <s>2) it is efficiently implementable with an ermoracle.</s> <s>3) it is more aggressive than all previous approachessatisfying 1 and 2.</s> <s>to do this, we create an algorithm based on a newlydefined optimization problem and analyze it.</s> <s>we also conduct the firstexperimental analysis of all efficient agnostic active learningalgorithms, evaluating their strengths and weaknesses in differentsettings.</s></p></d>", "label": ["<d><p><s>efficient and parsimonious agnostic active learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study matrix completion problem with side information.</s> <s>side information has been considered in several matrix completion applications, and is generally shown to be useful empirically.</s> <s>recently, xu et al.</s> <s>studied the effect of side information for matrix completion under a theoretical viewpoint, showing that sample complexity can be significantly reduced given completely clean features.</s> <s>however, since in reality most given features are noisy or even weakly informative, how to develop a general model to handle general feature set, and how much the noisy features can help matrix recovery in theory, is still an important issue to investigate.</s> <s>in this paper, we propose a novel model that balances between features and observations simultaneously, enabling us to leverage feature information yet to be robust to feature noise.</s> <s>moreover, we study the effectof general features in theory, and show that by using our model, the sample complexity can still be lower than matrix completion as long as features are sufficiently informative.</s> <s>this result provides a theoretical insight of usefulness for general side information.</s> <s>finally, we consider synthetic data and two real applications - relationship prediction and semi-supervised clustering, showing that our model outperforms other methods for matrix completion with features both in theory and practice.</s></p></d>", "label": ["<d><p><s>matrix completion with noisy side information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>convex potential minimisation is the de facto approach to binary classification.</s> <s>however, long and servedio [2008] proved that under symmetric label noise (sln), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing.</s> <s>this ostensibly shows that convex losses are not sln-robust.</s> <s>in this paper, we propose a convex, classification-calibrated loss and prove that it is sln-robust.</s> <s>the loss avoids the long and servedio [2008] result by virtue of being negatively unbounded.</s> <s>the loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss.</s> <s>we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners sln-robust.</s> <s>experiments confirm the unhinged loss?</s> <s>sln-robustness.</s></p></d>", "label": ["<d><p><s>learning with symmetric label noise: the importance of being unhinged</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers.</s> <s>the algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements.</s> <s>it does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning.</s> <s>we empirically demonstrate these performance gains with random forests.</s></p></d>", "label": ["<d><p><s>scalable semi-supervised aggregation of classifiers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem.</s> <s>among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials.</s> <s>meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l2 norm.</s> <s>the question we address in this work is: if we know a priori that data is so normalized, can we devise a more compact map?</s> <s>we show that a putative affirmative answer to this question based on random fourier features is impossible in this setting, and introduce a new approximation paradigm, spherical random fourier (srf) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere.</s> <s>compared to prior work, srf features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials.</s> <s>the resulting predictions have lower variance and typically yield better classification accuracy.</s></p></d>", "label": ["<d><p><s>spherical random features for polynomial kernels</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>tensor candecomp/parafac (cp) decomposition has wide applications in statistical learning of latent variable models and in data mining.</s> <s>in this paper, we propose fast and randomized tensor cp decomposition algorithms based on sketching.</s> <s>we build on the idea of count sketches, but introduce many novel ideas which are unique to tensors.</s> <s>we develop novel methods for randomized com- putation of tensor contractions via ffts, without explicitly forming the tensors.</s> <s>such tensor contractions are encountered in decomposition methods such as ten- sor power iterations and alternating least squares.</s> <s>we also design novel colliding hashes for symmetric tensors to further save time in computing the sketches.</s> <s>we then combine these sketching ideas with existing whitening and tensor power iter- ative techniques to obtain the fastest algorithm on both sparse and dense tensors.</s> <s>the quality of approximation under our method does not depend on properties such as sparsity, uniformity of elements, etc.</s> <s>we apply the method for topic mod- eling and obtain competitive results.</s></p></d>", "label": ["<d><p><s>fast and guaranteed tensor decomposition via sketching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>teaching machines to read natural language documents remains an elusive challenge.</s> <s>machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation.</s> <s>in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data.</s> <s>this allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.</s></p></d>", "label": ["<d><p><s>teaching machines to read and comprehend</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we present a definition for visual saliency grounded in information theory.</s> <s>this proposal is shown to relate to a variety of classic research contributions in scale-space theory, interest point detection, bilateral filtering, and to existing models of visual saliency.</s> <s>based on the proposed definition of visual saliency, we demonstrate results competitive with the state-of-the art for both prediction of human fixations, and segmentation of salient objects.</s> <s>we also characterize different properties of this model including robustness to image transformations, and extension to a wide range of other data types with 3d mesh models serving as an example.</s> <s>finally, we relate this proposal more generally to the role of saliency computation in visual information processing and draw connections to putative mechanisms for saliency computation in human vision.</s></p></d>", "label": ["<d><p><s>saliency, scale and information: towards a unifying theory</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we combine supervised learning with unsupervised learning in deep neural networks.</s> <s>the proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pre-training.</s> <s>our work builds on top of the ladder network proposed by valpola (2015) which we extend by combining the model with supervision.</s> <s>we show that the resulting model reaches state-of-the-art performance in semi-supervised mnist and cifar-10 classification in addition to permutation-invariant mnist classification with all labels.</s></p></d>", "label": ["<d><p><s>semi-supervised learning with ladder networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>to predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback.</s> <s>however, it remains unclear how such supervised learning is implemented in biological neural networks.</s> <s>learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics.</s> <s>the most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible.</s> <s>furthermore, reproducing the poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition.</s> <s>such balance is easily destroyed during learning.</s> <s>using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input.</s> <s>the network uses two types of recurrent connections: fast and slow.</s> <s>the fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule.</s> <s>the slow connections are trained to minimize the error feedback using a current-based hebbian learning rule.</s> <s>importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections.</s> <s>this demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using e/i balance as the key rather than an additional constraint.</s> <s>the resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.</s></p></d>", "label": ["<d><p><s>enforcing balance allows local supervised learning in spiking recurrent networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present two approaches to use unlabeled data to improve sequence learningwith recurrent networks.</s> <s>the first approach is to predict what comes next in asequence, which is a language model in nlp.</s> <s>the second approach is to use asequence autoencoder, which reads the input sequence into a vector and predictsthe input sequence again.</s> <s>these two algorithms can be used as a ?pretraining?algorithm for a later supervised sequence learning algorithm.</s> <s>in other words, theparameters obtained from the pretraining step can then be used as a starting pointfor other supervised training models.</s> <s>in our experiments, we find that long shortterm memory recurrent networks after pretrained with the two approaches becomemore stable to train and generalize better.</s> <s>with pretraining, we were able toachieve strong performance in many classification tasks, such as text classificationwith imdb, dbpedia or image recognition in cifar-10.</s></p></d>", "label": ["<d><p><s>semi-supervised sequence learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we describe an approach for unsupervised learning of a generic, distributed sentence encoder.</s> <s>using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage.</s> <s>sentences that share semantic and syntactic properties are thus mapped to similar vector representations.</s> <s>we next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words.</s> <s>after training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets.</s> <s>the end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.</s> <s>we will make our encoder publicly available.</s></p></d>", "label": ["<d><p><s>skip-thought vectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>training deep feature hierarchies to solve supervised learning tasks has achieving state of the art performance on many problems in computer vision.</s> <s>however, a principled way in which to train such hierarchies in the unsupervised setting has remained elusive.</s> <s>in this work we suggest a new architecture and loss for training deep feature hierarchies that linearize the transformations observed in unlabelednatural video sequences.</s> <s>this is done by training a generative model to predict video frames.</s> <s>we also address the problem of inherent uncertainty in prediction by introducing a latent variables that are non-deterministic functions of the input into the network architecture.</s></p></d>", "label": ["<d><p><s>learning to linearize under uncertainty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks.</s> <s>we propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters.</s> <s>this view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters.</s> <s>it explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience.</s> <s>in simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances.</s> <s>furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks.</s></p></d>", "label": ["<d><p><s>synaptic sampling: a bayesian approach to neural network plasticity and rewiring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce natural neural networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the fisher matrix.</s> <s>in particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network.</s> <s>such networks can be trained efficiently via the proposed projected natural gradient descent algorithm (prong), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the mirror descent online learning algorithm.</s> <s>we highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale imagenet challenge dataset.</s></p></d>", "label": ["<d><p><s>natural neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a convolutional neural network that operates directly on graphs.these networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.the architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.we show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.</s></p></d>", "label": ["<d><p><s>convolutional networks on graphs for learning molecular fingerprints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time.</s> <s>very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective.</s> <s>in this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences.</s> <s>by extending the fully connected lstm (fc-lstm) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional lstm (convlstm) and use it to build an end-to-end trainable model for the precipitation nowcasting problem.</s> <s>experiments show that our convlstm network captures spatiotemporal correlations better and consistently outperforms fc-lstm and the state-of-the-art operational rover algorithm for precipitation nowcasting.</s></p></d>", "label": ["<d><p><s>convolutional lstm network: a machine learning approach for precipitation nowcasting</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent neural networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning.</s> <s>the current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token.</s> <s>at inference, the unknown previous token is then replaced by a token generated by the model itself.</s> <s>this discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence.</s> <s>we propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead.</s> <s>experiments on several sequence prediction tasks show that this approach yields significant improvements.</s> <s>moreover, it was used successfully in our winning bid to the mscoco image captioning challenge, 2015.</s></p></d>", "label": ["<d><p><s>scheduled sampling for sequence prediction with recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the mind the gap model (mgm), an approach for interpretable feature extraction and selection.</s> <s>by placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation.</s> <s>mgm extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence.</s> <s>it also maintains or improves performance when compared to related approaches.</s> <s>we perform a user study with domain experts to show the mgm's ability to help with dataset exploration.</s></p></d>", "label": ["<d><p><s>mind the gap: a generative approach to interpretable feature selection and extraction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep generative models (dgms) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability.</s> <s>however, little work has been done on examining or empowering the discriminative ability of dgms on making accurate predictions.</s> <s>this paper presents max-margin deep generative models (mmdgms), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of dgms, while retaining the generative capability.</s> <s>we develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective.</s> <s>empirical results on mnist and svhn datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of dgms and meanwhile retain the generative ability; and (2) mmdgms are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (cnns) as both recognition and generative models.</s></p></d>", "label": ["<d><p><s>max-margin deep generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations.</s> <s>each instance is assumed to be represented as a multiset of features, e.g., a bag-of-words representation for documents.</s> <s>the major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured.</s> <s>to overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space.</s> <s>to represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions.</s> <s>the embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart.</s> <s>in our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual wikipedia articles, between documents and tags, and between images and tags.</s></p></d>", "label": ["<d><p><s>cross-domain matching for bag-of-words data via kernel embeddings of latent distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a method for combining two sources of astronomical data, spectroscopy and photometry, that carry information about sources of light (e.g., stars, galaxies, and quasars) at extremely different spectral resolutions.</s> <s>our model treats the spectral energy distribution (sed) of the radiation from a source as a latent variable that jointly explains both photometric and spectroscopic observations.</s> <s>we place a flexible, nonparametric prior over the sed of a light source that admits a physically interpretable decomposition, and allows us to tractably perform inference.</s> <s>we use our model to predict the distribution of the redshift of a quasar from five-band (low spectral resolution) photometric data, the so called ``photo-z'' problem.</s> <s>our method shows that tools from machine learning and bayesian statistics allow us to leverage multiple resolutions of information to make accurate predictions with well-characterized uncertainties.</s></p></d>", "label": ["<d><p><s>a gaussian process model of quasar spectral energy distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sequential monte carlo (smc), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions.</s> <s>like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution.</s> <s>this paper presents a new method for automatically adapting the proposal using an approximation of the kullback-leibler divergence between the true posterior and the proposal distribution.</s> <s>the method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants.</s> <s>we use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to neural adaptive sequential monte carlo (nasmc).</s> <s>experiments indicate that nasmc significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the extended kalman and unscented particle filters.</s> <s>experiments also indicate that improved inference translates into improved parameter learning when nasmc is used as a subroutine of particle marginal metropolis hastings.</s> <s>finally we show that nasmc is able to train a latent variable recurrent neural network (lv-rnn) achieving results that compete with the state-of-the-art for polymorphic music modelling.</s> <s>nasmc can be seen as bridging the gap between adaptive smc methods and the recent work in scalable, black-box variational inference.</s></p></d>", "label": ["<d><p><s>neural adaptive sequential monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>subunit models provide a powerful yet parsimonious description of  neural spike responses to complex stimuli.</s> <s>they can be expressed by  a cascade of two linear-nonlinear (ln) stages, with the first linear  stage defined by convolution with one or more filters.</s> <s>recent  interest in such models has surged due to their biological  plausibility and accuracy for characterizing early sensory  responses.</s> <s>however, fitting subunit models poses a difficult  computational challenge due to the expense of evaluating the  log-likelihood and the ubiquity of local optima.</s> <s>here we address  this problem by forging a theoretical connection between  spike-triggered covariance analysis and nonlinear subunit models.</s> <s>specifically, we show that a ''convolutional'' decomposition of the  spike-triggered average (sta) and covariance (stc) provides an  asymptotically efficient estimator for the subunit model under  certain technical conditions.</s> <s>we also prove the identifiability of  such convolutional decomposition under mild assumptions.</s> <s>our  moment-based methods outperform highly regularized versions of the  gqm on neural data from macaque primary visual cortex, and achieves  nearly the same prediction performance as the full  maximum-likelihood estimator, yet with substantially lower cost.</s></p></d>", "label": ["<d><p><s>convolutional spike-triggered covariance analysis for neural subunit models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose rectified factor networks (rfns) to efficiently construct very sparse, non-linear, high-dimensional representations of the input.</s> <s>rfn models identify rare and small events, have a low interference between code units, have a small reconstruction error, and explain the data covariance structure.</s> <s>rfn learning is a generalized alternating minimization algorithm derived from the posterior regularization method which enforces non-negative and normalized posterior means.</s> <s>we proof convergence and correctness of the rfn learning algorithm.on benchmarks, rfns are compared to other unsupervised methods like autoencoders, rbms, factor analysis, ica, and pca.</s> <s>in contrast to previous sparse coding methods, rfns yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error.</s> <s>we test rfns as pretraining technique of deep networks on different vision datasets, where rfns were superior to rbms and autoencoders.</s> <s>on gene expression data from two pharmaceutical drug discovery studies, rfns detected small and rare gene modules that revealed highly relevant new biological insights which were so far missed by other unsupervised methods.rfn package for gpu/cpu is available at http://www.bioinf.jku.at/software/rfn.</s></p></d>", "label": ["<d><p><s>rectified factor networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce embed to control (e2c), a method for model learning and control of non-linear dynamical systems from raw pixel images.</s> <s>e2c consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear.</s> <s>our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.</s></p></d>", "label": ["<d><p><s>embed to control: a locally linear latent dynamics model for control from raw images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x, d), e.g., for applications involving bandits or active learning.</s> <s>one simple approach to this is to use online monte carlo methods, such as sgld (stochastic gradient langevin dynamics).</s> <s>unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time).we describe a method for ?distilling?</s> <s>a monte carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network.</s> <s>we compare to two very recent approaches to bayesian neural networks, namely an approach based on expectation propagation [hla15] and an approach based on variational bayes [bckw15].</s> <s>our method performs better than both of these, is much simpler to implement, and uses less computation at test time.</s></p></d>", "label": ["<d><p><s>bayesian dark knowledge</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>multi-output gaussian processes provide a convenient framework for multi-task problems.</s> <s>an illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are interested in both power and phase coherence between channels.</s> <s>recently, wilson and adams (2013) proposed the spectral mixture (sm) kernel to model the spectral density of a single task in a gaussian process framework.</s> <s>in this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (csm) kernel.</s> <s>this new, flexible kernel represents both the power and phase relationship between multiple observation channels.</s> <s>we demonstrate the expressive capabilities of the csm kernel through implementation of a bayesian hidden markov model, where the emission distribution is a multi-output gaussian process with a csm covariance kernel.</s> <s>results are presented for measured multi-region electrophysiological data.</s></p></d>", "label": ["<d><p><s>gp kernels for cross-spectrum analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a fully discriminative learning approach for supervised latent dirichlet allocation (lda) model using back propagation (i.e., bp-slda), which maximizes the posterior probability of the prediction variable given the input document.</s> <s>different from traditional variational learning or gibbs sampling approaches, the proposed learning method applies (i) the mirror descent algorithm for maximum a posterior inference and (ii) back propagation over a deep architecture together with stochastic gradient/mirror descent for model parameter estimation, leading to scalable and end-to-end discriminative learning of the model.</s> <s>as a byproduct, we also apply this technique to develop a new learning method for the traditional unsupervised lda model (i.e., bp-lda).</s> <s>experimental results on three real-world regression and classification tasks show that the proposed methods significantly outperform the previous supervised topic models, neural networks, and is on par with deep neural networks.</s></p></d>", "label": ["<d><p><s>end-to-end learning of lda by mirror-descent back propagation over a deep architecture</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>infinite hidden markov models (ihmm's) are an attractive, nonparametric generalization of the classical hidden markov model which can automatically infer the number of hidden states in the system.</s> <s>however, due to the infinite-dimensional nature of the transition dynamics, performing inference in the ihmm is difficult.</s> <s>in this paper, we present an infinite-state particle gibbs (pg) algorithm to resample state trajectories for the ihmm.</s> <s>the proposed algorithm uses an efficient proposal optimized for ihmms, and leverages ancestor sampling to improve the mixing of the standard pg algorithm.</s> <s>our algorithm demonstrates significant convergence improvements on synthetic and real world data sets.</s></p></d>", "label": ["<d><p><s>particle gibbs for infinite hidden markov models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set.</s> <s>embedding based approaches make training and prediction tractable by assuming that the training label matrix is low-rank and hence the effective number of labels can be reduced by projecting the high dimensional label vectors onto a low dimensional linear subspace.</s> <s>still, leading embedding approaches have been unable to deliver high prediction accuracies or scale to large problems as the low rank assumption is violated in most real world applications.this paper develops the sleec classifier to address both limitations.</s> <s>the main technical contribution in sleec is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels.</s> <s>this allows sleec to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors.</s> <s>we conducted extensive experiments on several real-world as well as benchmark data sets and compare our method against state-of-the-art methods for extreme multi-label classification.</s> <s>experiments reveal that sleec can make significantly more accurate predictions then the state-of-the-art methods including both embeddings (by as much as 35%) as well as trees (by as much as 6%).</s> <s>sleec can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.</s></p></d>", "label": ["<d><p><s>sparse local embeddings for extreme multi-label classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral inference provides fast algorithms and provable optimality for latent topic analysis.</s> <s>but for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results.</s> <s>we explain this poor performance by casting the problem of topic inference in the framework of joint stochastic matrix factorization (jsmf) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist.</s> <s>we then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data.</s> <s>this method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.</s></p></d>", "label": ["<d><p><s>robust spectral inference for joint stochastic matrix factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>space-time is a profound concept in physics.</s> <s>this concept was shown to be useful for dimensionality reduction.</s> <s>we present basic definitions with interesting counter-intuitions.</s> <s>we give theoretical propositions to show that space-time is a more powerful representation than euclidean space.</s> <s>we apply this concept to manifold learning for preserving local information.</s> <s>empirical results on non-metric datasets show that more information can be preserved in space-time.</s></p></d>", "label": ["<d><p><s>space-time local embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns.</s> <s>the result is a low-dimensional projection of each input pattern.</s> <s>a common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net.</s> <s>this can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters.</s> <s>using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping.</s> <s>this has two advantages: 1) the algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping.</s> <s>a user can then try possible mappings and embeddings with less effort.</s> <s>2) the algorithm is fast, and it can reuse n-body methods developed for nonlinear embeddings, yielding linear-time iterations.</s></p></d>", "label": ["<d><p><s>a fast, universal algorithm to learn parametric nonlinear embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the locally linear latent variable model (ll-lvm), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships.</s> <s>the model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data.</s> <s>thus, the ll-lvm encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (lle).</s> <s>its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.</s></p></d>", "label": ["<d><p><s>bayesian manifold learning: the locally linear latent variable model (ll-lvm)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we focus on the discovery and identification of direct causes and effects of a target variable in a causal network.</s> <s>state-of-the-art algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs in order to identify the direct causes and effects of a target variable.</s> <s>while these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in one target variable (such as class labels).</s> <s>we propose a new local causal discovery algorithm, called causal markov blanket (cmb), to identify the direct causes and effects of a target variable based on markov blanket discovery.</s> <s>cmb is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables.</s> <s>under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.</s></p></d>", "label": ["<d><p><s>local causal discovery of direct causes and effects</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper proposes a framework for learning features that are robust to data variation, which is particularly important when only a limited number of trainingsamples are available.</s> <s>the framework makes it possible to tradeoff the discriminative value of learned features against the generalization error of the learning algorithm.</s> <s>robustness is achieved by encouraging the transform that maps data to features to be a local isometry.</s> <s>this geometric property is shown to improve (k, \\epsilon)-robustness, thereby providing theoretical justification for reductions in generalization error observed in experiments.</s> <s>the proposed optimization frameworkis used to train standard learning algorithms such as deep neural networks.</s> <s>experimental results obtained on benchmark datasets, such as labeled faces in the wild,demonstrate the value of being able to balance discrimination and robustness.</s></p></d>", "label": ["<d><p><s>discriminative robust transformation learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning-from-crowds aims to design proper aggregation strategies to infer the unknown true labels from the noisy labels provided by ordinary web workers.</s> <s>this paper presents max-margin majority voting (m^3v) to improve the discriminative ability of majority voting and further presents a bayesian generalization to incorporate the flexibility of generative methods on modeling noisy observations with worker confusion matrices.</s> <s>we formulate the joint learning as a regularized bayesian inference problem, where the posterior regularization is derived by maximizing the margin between the aggregated score of a potential true label and that of any alternative label.</s> <s>our bayesian model naturally covers the dawid-skene estimator and m^3v.</s> <s>empirical results demonstrate that our methods are competitive, often achieving better results than state-of-the-art estimators.</s></p></d>", "label": ["<d><p><s>max-margin majority voting for learning from crowds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of finding m best diverse solutions of energy minimization problems for graphical models.</s> <s>contrary to the sequential method of batra et al., which greedily finds one solution after another, we infer all $m$ solutions jointly.</s> <s>it was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones.</s> <s>the only obstacle for using this new technique is the complexity of the corresponding inference problem, since it is considerably slower algorithm than the method of batra et al.</s> <s>in this work we show that the joint inference of $m$ best diverse solutions can be formulated as a submodular energy minimization if the original map-inference problem is submodular, hence fast inference techniques can be used.</s> <s>in addition to the theoretical results we provide practical algorithms that outperform the current state-of-the art and can be used in both submodular and non-submodular case.</s></p></d>", "label": ["<d><p><s>m-best-diverse labelings for submodular energies and beyond</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>monte carlo sampling for bayesian posterior inference is a common approach used in machine learning.</s> <s>the markov chain monte carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (sdes).</s> <s>these sdes are guaranteed to leave invariant the required posterior distribution.</s> <s>an area of current research addresses the computational benefits of stochastic gradient methods in this setting.</s> <s>existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance.</s> <s>in this article, we propose a covariance-controlled adaptive langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution.</s> <s>the proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.</s></p></d>", "label": ["<d><p><s>covariance-controlled adaptive langevin thermostat for large-scale bayesian sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>by making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services.</s> <s>however, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users.</s> <s>two central but less explored questions are how to recommend the most desirable item \\emph{at the right moment}, and how to predict \\emph{the next returning time} of a user to a service.</s> <s>to address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs.</s> <s>we show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains $o(1 / \\epsilon)$ convergence rate, scales up to problems with millions of user-item pairs and thousands of millions of temporal events.</s> <s>compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation questions.</s> <s>finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.</s></p></d>", "label": ["<d><p><s>time-sensitive recommendation from recurrent user activities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the paper presents and evaluates the power of parallel search for exact map inference in graphical models.</s> <s>we introduce a new parallel shared-memory recursive best-first and/or search algorithm, called sprbfaoo, that explores the search space in a best-first manner while operating with restricted memory.</s> <s>our experiments show that sprbfaoo is often superior to the current state-of-the-art sequential and/or search approaches, leading to considerable speed-ups (up to 7-fold with 12 threads), especially on hard problem instances.</s></p></d>", "label": ["<d><p><s>parallel recursive best-first and/or search for exact map inference in graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, there has been growing interest in systematic search-based and importance sampling-based lifted inference algorithms for statistical relational models (srms).</s> <s>these lifted algorithms achieve significant complexity reductions over their propositional counterparts by using lifting rules that leverage symmetries in the relational representation.</s> <s>one drawback of these algorithms is that they use an inference-blind representation of the search space, which makes it difficult to efficiently pre-compute tight upper bounds on the exact cost of inference without running the algorithm to completion.</s> <s>in this paper, we present a principled approach to address this problem.</s> <s>we introduce a lifted analogue of the propositional and/or search space framework, which we call a lifted and/or schematic.</s> <s>given a schematic-based representation of an srm, we show how to efficiently compute a tight upper bound on the time and space cost of exact inference from a current assignment and the remaining schematic.</s> <s>we show how our bounding method can be used within a lifted importance sampling algorithm, in order to perform effective rao-blackwellisation, and demonstrate experimentally that the rao-blackwellised version of the algorithm yields more accurate estimates on several real-world datasets.</s></p></d>", "label": ["<d><p><s>bounding the cost of search-based lifted inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of reducing test-time acquisition costs in classification systems.</s> <s>our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction.</s> <s>we model our system as a directed acyclic graph (dag) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements.</s> <s>this problem can be naturally posed as an empirical risk minimization over training data.</s> <s>rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming.</s> <s>we learn node policies in the dag by reducing the global objective to a series of cost sensitive learning problems.</s> <s>our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture.</s> <s>in addition, we present an extension to map other budgeted learning problems with large number of sensors to our dag architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.</s></p></d>", "label": ["<d><p><s>efficient learning by directed acyclic graph for resource constrained prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the jaccard index is a standard statistics for comparing the pairwise similarity between data samples.</s> <s>this paper investigates the problem of estimating a jaccard index matrix when there are missing observations in data samples.</s> <s>starting from a jaccard index matrix approximated from the incomplete data, our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints, through a simple alternating projection algorithm.</s> <s>compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the frobenius norm than the un-calibrated matrix (except in special cases they are identical).</s> <s>we carried out a series of empirical experiments and the results confirmed our theoretical justification.</s> <s>the evaluation also reported significantly improved results in real learning tasks on benchmarked datasets.</s></p></d>", "label": ["<d><p><s>estimating jaccard index with missing observations: a matrix calibration approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a data-driven stochastic optimal control framework that is derived using the path integral (pi) control approach.</s> <s>we find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model.</s> <s>the proposed algorithm operates in a forward-backward sweep manner which differentiate it from other pi-related methods that perform forward sampling to find open-loop optimal controls.</s> <s>our method uses significantly less sampled data to find analytic control laws compared to other approaches within the pi control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion.</s> <s>in addition, the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework.we provide experimental results on three different systems and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework.</s></p></d>", "label": ["<d><p><s>sample efficient path integral control under uncertainty</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix factorization (mf) collaborative filtering is an effective and widely used method in recommendation systems.</s> <s>however, the problem of finding an optimal trade-off between exploration and exploitation (otherwise known as the bandit problem), a crucial problem in collaborative filtering from cold-start, has not been previously addressed.in this paper, we present a novel algorithm for online mf recommendation that automatically combines finding the most relevantitems with exploring new or less-recommended items.our approach, called particle thompson sampling for matrix-factorization, is based on the general thompson sampling framework, but augmented with a novel efficient online bayesian probabilistic matrix factorization method based on the rao-blackwellized particle filter.extensive experiments in collaborative filtering using several real-world datasets demonstrate that our proposed algorithm significantly outperforms the current state-of-the-arts.</s></p></d>", "label": ["<d><p><s>efficient thompson sampling for online ?matrix-factorization recommendation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the modern scale of data has brought new challenges to bayesian inference.</s> <s>in particular, conventional mcmc algorithms are computationally very expensive for large data sets.</s> <s>a promising approach to solve this problem is embarrassingly parallel mcmc (ep-mcmc), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset.</s> <s>the subset posterior draws are then aggregated via some combining rules to obtain the final approximation.</s> <s>existing ep-mcmc algorithms are limited by approximation accuracy and difficulty in resampling.</s> <s>in this article, we propose a new ep-mcmc algorithm part that solves these problems.</s> <s>the new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to resample from and can adapt to multiple scales.</s> <s>we provide theoretical justification and extensive experiments illustrating empirical performance.</s></p></d>", "label": ["<d><p><s>parallelizing mcmc with random partition trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, there has been growing interest in lifting map inference algorithms for markov logic networks (mlns).</s> <s>a key advantage of these lifted algorithms is that they have much smaller computational complexity than propositional algorithms when symmetries are present in the mln and these symmetries can be detected using lifted inference rules.</s> <s>unfortunately, lifted inference rules are sound but not complete and can often miss many symmetries.</s> <s>this is problematic because when symmetries cannot be exploited, lifted inference algorithms ground the mln, and search for solutions in the much larger propositional space.</s> <s>in this paper, we present a novel approach, which cleverly introduces new symmetries at the time of grounding.</s> <s>our main idea is to partition the ground atoms and force the inference algorithm to treat all atoms in each part as indistinguishable.</s> <s>we show that by systematically and carefully refining (and growing) the partitions, we can build advanced any-time and any-space map inference algorithms.</s> <s>our experiments on several real-world datasets clearly show that our new algorithm is superior to previous approaches and often finds useful symmetries in the search space that existing lifted inference rules are unable to detect.</s></p></d>", "label": ["<d><p><s>fast lifted map inference via partitioning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible.this work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels.</s> <s>an example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler).</s> <s>our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler.</s> <s>we provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.</s></p></d>", "label": ["<d><p><s>active learning from weak and strong labelers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we establish pac learnability of influence functions for three common influence models, namely, the linear threshold (lt), independent cascade (ic) and voter models, and present concrete sample complexity results in each case.</s> <s>our results for the lt model are based on interesting connections with neural networks; those for the ic model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments; and those for the voter model are based on a reduction to linear regression.</s> <s>we show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced.</s> <s>we also provide efficient polynomial time learning algorithms for a setting with full observation, i.e.</s> <s>where the cascades also contain the time steps in which nodes are influenced.</s></p></d>", "label": ["<d><p><s>learnability of influence in networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>independent component analysis (ica) is a popular model for blind signal separation.</s> <s>the ica model assumes that a number of independent source signals are linearly mixed to form the observed signals.</s> <s>we propose a new algorithm, pegi (for pseudo-euclidean gradient iteration), for provable model recovery for ica with gaussian noise.</s> <s>the main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-euclidean (indefinite ?inner product?)</s> <s>space.</s> <s>the use of this indefinite ?inner product?</s> <s>resolves technical issues common to several existing algorithms for noisy ica.</s> <s>this leads to an algorithm which is conceptually simple, efficient and accurate in testing.our second contribution is combining pegi with the analysis of objectives for optimal recovery in the noisy ica model.</s> <s>it has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural signal to interference plus noise ratio (sinr) criterion.</s> <s>there have been several partial solutions proposed in the ica literature.</s> <s>it turns out that any solution to the mixing matrix reconstruction problem can be used to construct an sinr-optimal ica demixing, despite the fact that sinr itself cannot be computed from data.</s> <s>that allows us to obtain a practical and provably sinr-optimal recovery method for ica with arbitrary gaussian noise.</s></p></d>", "label": ["<d><p><s>a pseudo-euclidean iteration for optimal recovery in noisy ica</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple ``clusters'' so that data points in a single cluster lie approximately on a low-dimensional linear subspace.</s> <s>it is originally motivated by 3d motion segmentation in computer vision, but has recently been generically applied to a wide range of statistical machine learning problems, which often involves sensitive datasets about human subjects.</s> <s>this raises a dire concern for data privacy.</s> <s>in this work, we build on the framework of ``differential privacy'' and present two provably private subspace clustering algorithms.</s> <s>we demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice.</s> <s>along the course of the proof, we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.</s></p></d>", "label": ["<d><p><s>differentially private subspace clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral embedding based on the singular value decomposition (svd) is a widely used preprocessing step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value).</s> <s>however, the number of such vectors required to capture problem structure grows with problem size, and even partial svd computation becomes a bottleneck.</s> <s>in this paper, we propose a low-complexity it compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to svd-based embedding.</s> <s>for an m times n matrix with t non-zeros, its time complexity is o((t+m+n)log(m+n)), and the embedding dimension is o(log(m+n)), both of which are independent of the number of singular vectors whose effect we wish to capture.</s> <s>to the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general svd-based embeddings.</s> <s>the key to sidestepping the svd is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the euclidean norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial svd tries to do.</s> <s>our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks.</s></p></d>", "label": ["<d><p><s>compressive spectral embedding: sidestepping the svd</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>overfitting is the bane of data analysts, even when data are plentiful.</s> <s>formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures.</s> <s>yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused.</s> <s>an investigation of this gap has recently been initiated by the authors in (dwork et al., 2014), where we focused on the problem of estimating expectations of adaptively chosen functions.in this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set.</s> <s>reusing a  holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself.</s> <s>we give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting.</s> <s>we illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment.we also formalize and address the general problem of data reuse in adaptive data analysis.</s> <s>we show how the differential-privacy based approach  in (dwork et al., 2014) is applicable much more broadly to adaptive data analysis.</s> <s>we then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings.</s> <s>finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce.</s> <s>this, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.</s></p></d>", "label": ["<d><p><s>generalization in adaptive data analysis and holdout reuse</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the f-measure is an important and commonly used performance metric for binary prediction tasks.</s> <s>by combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions.</s> <s>the problem of optimizing the f-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors.</s> <s>in this paper, we study the problem of f-measure maximization in the setting of online learning.</s> <s>we propose an efficient online algorithm and provide a formal analysis of its convergence properties.</s> <s>moreover, first experimental results are presented, showing that our method performs well in practice.</s></p></d>", "label": ["<d><p><s>online f-measure optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a mechanism for purchasing information from a sequence of participants.the participants may simply hold data points they wish to sell, or may have more sophisticated information; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism's future prediction on a test set.the mechanism, which draws on the principles of prediction markets, has a bounded budget and minimizes generalization error for bregman divergence loss functions.we then show how to modify this mechanism to preserve the privacy of participants' information: at any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately aggregated.</s></p></d>", "label": ["<d><p><s>a market framework for eliciting private data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the concept of coverage risk as an error measure for density ridge estimation.the coverage risk generalizes the mean integrated square error to set estimation.we propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk.we study the rate of convergence for coverage risk and prove consistency of the risk estimators.we apply our method to three simulated datasets and to cosmology data.in all the examples, the proposed method successfully recover the underlying density structure.</s></p></d>", "label": ["<d><p><s>optimal ridge detection using coverage risk</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>clustering large data is a fundamental problem with a vast number of applications.</s> <s>due to the increasing size of data, practitioners interested in clustering have turned to distributed computation methods.</s> <s>in this work, we consider the widely used k-center clustering problem and its variant used to handle noisy data, k-center with outliers.</s> <s>in the noise-free setting we demonstrate how a previously-proposed distributed method is actually an o(1)-approximation algorithm, which accurately explains its strong empirical performance.</s> <s>additionally, in the noisy setting, we develop a novel distributed algorithm that is also an o(1)-approximation.</s> <s>these algorithms are highly parallel and lend themselves to virtually any distributed computing framework.</s> <s>we compare both empirically against the best known noisy sequential clustering methods and show that both distributed algorithms are consistently close to their sequential versions.</s> <s>the algorithms are all one can hope for in distributed settings: they are fast, memory efficient and they match their sequential counterparts.</s></p></d>", "label": ["<d><p><s>fast distributed k-center clustering with outliers on massive data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>orthogonal nonnegative matrix factorization {(onmf)} aims to approximate a nonnegative matrix as the product of two $k$-dimensional nonnegative factors, one of which has orthonormal columns.</s> <s>it yields potentially useful data representations as superposition of disjoint parts, while it has been shown to work well for clustering tasks where traditional methods underperform.</s> <s>existing algorithms rely mostly on heuristics, which despite their good empirical performance, lack provable performance guarantees.we present a new onmf algorithm with provable approximation guarantees.for any constant dimension~$k$, we obtain an additive eptas without any assumptions on the input.</s> <s>our algorithm relies on a novel approximation to the related nonnegative principal component analysis (nnpca) problem; given an arbitrary data matrix, nnpca seeks $k$ nonnegative components that jointly capture most of the variance.</s> <s>our nnpca algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component.</s> <s>we evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art.</s></p></d>", "label": ["<d><p><s>orthogonal nmf through subspace exploration</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of binary classification when the covariates conditioned on the each of the response values follow multivariate gaussian distributions.</s> <s>we focus on the setting where the covariance matrices for the two conditional distributions are the same.</s> <s>the corresponding generative model classifier, derived via the bayes rule, also called linear discriminant analysis, has been shown to behave poorly in high-dimensional settings.</s> <s>we present a novel analysis of the classification error of any linear discriminant approach given conditional gaussian models.</s> <s>this allows us to compare the generative model classifier, other recently proposed discriminative approaches that directly learn the discriminant function, and then finally logistic regression which is another classical discriminative model classifier.</s> <s>as we show, under a natural sparsity assumption, and letting $s$ denote the sparsity of the bayes classifier, $p$ the number of covariates, and $n$ the number of samples, the simple ($\\ell_1$-regularized) logistic regression classifier achieves the fast misclassification error rates of $o\\left(\\frac{s \\log p}{n}\\right)$, which is much better than the other approaches, which are either inconsistent under high-dimensional settings, or achieve a slower rate of $o\\left(\\sqrt{\\frac{s \\log p}{n}}\\right)$.</s></p></d>", "label": ["<d><p><s>fast classification rates for high-dimensional gaussian generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning.</s> <s>these problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions.</s> <s>we offer a general framework to derive mistake driven online algorithms and associated loss bounds.</s> <s>the key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm.</s> <s>our general algorithm, predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification.</s> <s>for multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds.</s> <s>a simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.</s></p></d>", "label": ["<d><p><s>predtron: a family of online algorithms for general prediction problems</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>to capture the interdependencies between labels in multi-label classification problems, classifier chain (cc)  tries to take the multiple labels of each instance into account under a deterministic high-order markov chain model.</s> <s>since its  performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for cc.</s> <s>in this work, we first generalize the cc model over a random label order.</s> <s>then, we present a theoretical analysis of the generalization error for the proposed generalized model.</s> <s>based on our results, we propose a dynamic programming based classifier chain (cc-dp) algorithm to search the globally optimal label order for cc and a greedy classifier chain (cc-greedy) algorithm to find a locally optimal cc.</s> <s>comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed cc-dp algorithm outperforms state-of-the-art approaches and the cc-greedy algorithm achieves comparable prediction performance with cc-dp.</s></p></d>", "label": ["<d><p><s>on the optimality of classifier chain for multi-label classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few (cost-weighted) actions as possible.</s> <s>it models settings where there is uncertainty regarding which submodular function to optimize.</s> <s>in this paper, we propose a new extension, which we call smooth interactive submodular set cover, that allows the target threshold to vary depending on the plausibility of each hypothesis.</s> <s>we present the first algorithm for this more general setting with theoretical guarantees on optimality.</s> <s>we further show how to extend our approach to deal with real-valued functions, which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings.</s></p></d>", "label": ["<d><p><s>smooth interactive submodular set cover</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>both learning and inference tasks on bayesian networks are np-hard in general.</s> <s>bounded tree-width bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains np-hard even for tree-width~2.</s> <s>in this paper, we propose bounded vertex cover number bayesian networks as an alternative to bounded tree-width networks.</s> <s>in particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound $k$, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is w[1]-hard in parameter $k$.</s> <s>furthermore, we give an alternative way to learn bounded vertex cover number bayesian networks using integer linear programming (ilp),  and show this is feasible in practice.</s></p></d>", "label": ["<d><p><s>tractable bayesian network structure learning with bounded vertex cover number</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of multi-party interactive function computation under differential privacy.</s> <s>in this setting, each party is interested in computing a function on its private bit and all the other parties' bits.</s> <s>the function to be computed can vary from one party to the other.</s> <s>moreover, there could be a central observer who is interested in computing a separate function on all the parties' bits.</s> <s>differential privacy ensures that there remains an uncertainty in any party's bit even when given the transcript of interactions and all other parties' bits.</s> <s>performance at each party is measured via the accuracy of the function to be computed.</s> <s>we allow for an arbitrary cost metric to measure the distortion between the true and the computed function values.</s> <s>our main result is the optimality of a simple non-interactive protocol: each party randomizes its bit (sufficiently) and shares the privatized version with the other parties.</s> <s>this optimality result is very general: it holds for all types of functions, heterogeneous privacy conditions on the parties, all types of cost metrics, and both average and worst-case (over the inputs) measures of accuracy.</s></p></d>", "label": ["<d><p><s>secure multi-party differential privacy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>adaptive stochastic optimization optimizes an objective function adaptively under uncertainty.</s> <s>adaptive stochastic optimization plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general.</s> <s>this paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which enable efficient approximate solution of adaptive stochastic optimization.</s> <s>several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning.</s> <s>we describe recursive adaptive coverage (rac),  a new adaptive stochastic optimization algorithm that exploits these conditions, and apply it to two planning tasks under uncertainty.</s> <s>in constrast to the earlier submodular optimization approach, our algorithm applies to adaptive stochastic optimization algorithm over both sets and paths.</s></p></d>", "label": ["<d><p><s>adaptive stochastic optimization: from sets to paths</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications.</s> <s>current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods.</s> <s>in this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model.</s> <s>we consider probabilities that are members of an infinite dimensional exponential family, which is parametrized by a reproducing kernel hilbert space (rkhs) h and its kernel $k$.</s> <s>one difficulty in learning nonparametric densities is evaluation of the normalizing constant.</s> <s>in order to avoid this issue, our procedure minimizes the penalized score matching objective.</s> <s>we show how to efficiently minimize the proposed objective using existing group lasso solvers.</s> <s>furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions.</s> <s>simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.</s></p></d>", "label": ["<d><p><s>learning structured densities via infinite dimensional exponential families</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we aim at extending theoretical foundations of lifelong learning.</s> <s>previous work analyzing this scenario is based on the assumption that the tasks are sampled i.i.d.</s> <s>from a task environment or limited to strongly constrained data distributions.</s> <s>instead we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d.</s> <s>sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time.</s> <s>in the first case we prove a pac-bayesian theorem, which can be seen as a direct generalization of the analogous previous result for the i.i.d.</s> <s>case.</s> <s>for the second scenario we propose to learn an inductive bias in form of a transfer procedure.</s> <s>we present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.</s></p></d>", "label": ["<d><p><s>lifelong learning with non-i.i.d. tasks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study contextual bandits with budget and time constraints under discrete contexts, referred to as constrained contextual bandits.</s> <s>the time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time.</s> <s>to gain insight, we first study unit-cost systems with known context distribution.</s> <s>when the expected rewards are known, we develop an approximation of the oracle, referred to adaptive-linear-programming(alp), which achieves near-optimality and only requires the ordering of expected rewards.</s> <s>with these highly desirable features,  we  then combine alp with the upper-confidence-bound (ucb) method in the general case where the expected rewards are unknown a priori.</s> <s>we show that the proposed ucb-alp algorithm achieves logarithmic regret except in certain boundary cases.further, we design algorithms and obtain similar regret analysis results for  more general systems with unknown context distribution or heterogeneous costs.</s> <s>to the best of our knowledge, this is the  first work that shows how to achieve logarithmic regret in constrained contextual bandits.</s> <s>moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.</s></p></d>", "label": ["<d><p><s>algorithms with logarithmic or sublinear regret for  constrained contextual bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large unweighted directed graphs are commonly used to capture relations between entities.</s> <s>a fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices.</s> <s>despite the significance of this problem, statistical characterization of the proposed metrics has been limited.we introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus.</s> <s>using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the laplace transformed hitting time (ltht).</s> <s>the metric serves as a natural, provably well-behaved alternative to the expected hitting time.</s> <s>we establish a general correspondence between hitting times of the brownian motion and analogous hitting times on the graph.</s> <s>we show that the ltht is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges.</s> <s>tests on simulated and real-world data show that the ltht matches theoretical predictions and outperforms alternatives.</s></p></d>", "label": ["<d><p><s>from random walks to distances on unweighted graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of robust least squares regression (rlsr) where several response variables can be adversarially corrupted.</s> <s>more specifically, for a data matrix x \\in \\r^{p x n} and an underlying model w*, the response vector is generated as y = x'w* + b where b \\in n is the corruption vector supported over at most c.n coordinates.</s> <s>existing exact recovery results for rlsr focus solely on l1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of x.in this work, we study a simple hard-thresholding algorithm called torrent which, under mild conditions on x, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e.</s> <s>both the support and entries of b are selected adversarially after observing x and w*.</s> <s>our results hold under deterministic assumptions which are satisfied if x is sampled from any sub-gaussian distribution.</s> <s>finally unlike existing results that apply only to a fixed w*, generated independently of x, our results are universal and hold for any w* \\in \\r^p.next, we propose gradient descent-based extensions of torrent that can scale efficiently to large scale problems, such as high dimensional sparse recovery.</s> <s>and prove similar recovery guarantees for these extensions.</s> <s>empirically we find torrent, and more so its extensions, offering significantly faster recovery than the state-of-the-art l1 solvers.</s> <s>for instance, even on moderate-sized datasets (with p = 50k) with around 40% corrupted responses, a variant of our proposed method called torrent-hyb is more than 20x faster than the best l1 solver.</s></p></d>", "label": ["<d><p><s>robust regression via hard thresholding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning.</s> <s>we propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm.</s> <s>our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms.</s> <s>our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.</s></p></d>", "label": ["<d><p><s>column selection via adaptive sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the generalization performance of multi-class classification algorithms, for which we obtain, for the first time, a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis.</s> <s>the theoretical analysis motivates us to introduce a new multi-class classification machine based on lp-norm regularization, where the parameter p controls the complexity of the corresponding bounds.</s> <s>we derive an efficient optimization algorithm based on fenchel duality theory.</s> <s>benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.</s></p></d>", "label": ["<d><p><s>multi-class svms: from tighter data-dependent generalization bounds to novel algorithms</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>linear regression studies the problem of estimating a model parameter $\\beta^* \\in \\r^p$, from $n$ observations $\\{(y_i,x_i)\\}_{i=1}^n$ from linear model $y_i = \\langle \\x_i,\\beta^* \\rangle + \\epsilon_i$.</s> <s>we consider a significant generalization in which the relationship between $\\langle x_i,\\beta^* \\rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown.</s> <s>this model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing.</s> <s>we propose a novel spectral-based estimation procedure and show that we can recover $\\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail.</s> <s>in general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\\langle x_i,\\beta^* \\rangle$.</s> <s>we also consider the high dimensional setting where $\\beta^*$ is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \\gg n$.</s> <s>for a broad class of link functions between $\\langle x_i,\\beta^* \\rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.</s></p></d>", "label": ["<d><p><s>optimal linear estimation under unknown nonlinear transform</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we address the problem of decision making within a markov decision process (mdp) framework where risk and modeling errors are taken into account.</s> <s>our approach is to  minimize a risk-sensitive conditional-value-at-risk (cvar) objective, as opposed to a standard risk-neutral expectation.</s> <s>we refer to such problem as cvar mdp.</s> <s>our first contribution is to show that a cvar objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget.</s> <s>this result, which is of independent interest,  motivates cvar mdps as a unifying framework for risk-sensitive and robust decision making.</s> <s>our second contribution is to present a value-iteration algorithm for cvar mdps, and analyze its convergence rate.</s> <s>to our knowledge, this is the first solution algorithm for cvar mdps that enjoys error guarantees.</s> <s>finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.</s></p></d>", "label": ["<d><p><s>risk-sensitive and robust decision-making: a cvar optimization approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>within a statistical learning setting,  we propose and study an iterative regularization algorithm for least squares defined by  an incremental gradient method.</s> <s>in particular, we show that, if all other parameters are fixed a priori, the number of passes over the data (epochs) acts as a regularization parameter, and  prove strong universal consistency, i.e.</s> <s>almost sure convergence of the risk, as well as  sharp finite sample bounds for the iterates.</s> <s>our  results are a step towards understanding the effect of multiple epochs in  stochastic gradient techniques in machine learning and rely  on  integrating  statistical and optimizationresults.</s></p></d>", "label": ["<d><p><s>learning with incremental iterative regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare.</s> <s>this work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., bayesian games.</s> <s>first, near-optimal welfare in bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public.</s> <s>second, no-regret learning dynamics converge to bayesian coarse correlated equilibrium in these incomplete information games.</s> <s>these results are enabled by interpretation of a bayesian game as a stochastic game of complete information.</s></p></d>", "label": ["<d><p><s>no-regret learning in bayesian games</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition.</s> <s>we present an efficient computational algorithm that modifies leurgans' algoirthm for tensor factorization.</s> <s>our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction.</s> <s>we use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor.</s> <s>we delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm.</s> <s>we validate our algorithm with numerical experiments.</s></p></d>", "label": ["<d><p><s>sparse and low-rank tensor decomposition</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we investigate the robust pca problem of decomposing an observed matrix into the sum of a low-rank and a sparse error matrices via convex programming principal component pursuit (pcp).</s> <s>in contrast to previous studies that assume the support of the error matrix is generated by uniform bernoulli sampling, we allow non-uniform sampling, i.e., entries of the low-rank matrix are corrupted by errors with unequal probabilities.</s> <s>we characterize conditions on error corruption of each individual entry based on the local incoherence of the low-rank matrix, under which correct matrix decomposition by pcp is guaranteed.</s> <s>such a refined analysis of robust pca captures how robust each entry of the low rank matrix combats error corruption.</s> <s>in order to deal with non-uniform error corruption, our technical proof introduces a new weighted norm and develops/exploits the concentration properties that such a norm satisfies.</s></p></d>", "label": ["<d><p><s>analysis of robust pca via local incoherence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience.</s> <s>this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations.</s> <s>in this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions.</s> <s>we provide various interpretations of this result.</s> <s>for instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning.</s> <s>in addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods.</s> <s>finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical pac result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.</s></p></d>", "label": ["<d><p><s>algorithmic stability and uniform generalization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this article provides the first procedure for computing a fully data-dependent interval that traps the mixing time $t_{mix}$ of a finite reversible ergodic markov chain at a prescribed confidence level.</s> <s>the interval is computed from a single finite-length sample path from the markov chain, and does not require the knowledge of any parameters of the chain.</s> <s>this stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge.</s> <s>the interval is constructed around the relaxation time $t_{relax}$, which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path.</s> <s>upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy.</s> <s>the lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before seeing each state at least $\\omega(t_{relax})$ times on the average.</s> <s>finally, future directions of research are identified.</s></p></d>", "label": ["<d><p><s>mixing time estimation in reversible markov chains from a single sample path</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements.</s> <s>the proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially.in recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees.</s> <s>the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target.</s> <s>given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix.</s> <s>however, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive.deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace, then the low-rank and sparse structures of the target signal can be effectively decoupled.</s> <s>we show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is $\\mathsf{o}(k\\,\\log\\frac{d}{k})$, where $k$ and $d$ denote the sparsity level and the dimension of the input signal.</s> <s>we also evaluate the algorithm through numerical simulation.</s></p></d>", "label": ["<d><p><s>efficient compressive phase retrieval with constrained sensing vectors</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms.</s> <s>in this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\\em any} norm regularization.we consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error.</s> <s>our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain \\textit{\\modified}~complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization.</s> <s>further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral $k$-support norm.</s></p></d>", "label": ["<d><p><s>unified view of matrix completion under general structural constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a version of the dueling bandit problem is addressed in which a condorcet winner may not exist.</s> <s>two algorithms are proposed that instead seek to minimize regret with respect to the copeland winner, which, unlike the condorcet winner, is guaranteed to exist.</s> <s>the first, copeland confidence bound (ccb), is designed for small numbers of arms, while the second, scalable copeland bandits (scb), works better for large-scale problems.</s> <s>we provide theoretical results bounding the regret accumulated by ccb and scb, both substantially improving existing results.</s> <s>such existing results either offer bounds of the form o(k log t) but require restrictive assumptions, or offer bounds of the form o(k^2 log t) without requiring such assumptions.</s> <s>our results offer the best of both worlds: o(k log t) bounds without restrictive assumptions.</s></p></d>", "label": ["<d><p><s>copeland dueling bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players.</s> <s>in this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal.</s> <s>the goal of the learner is to minimize the total loss.</s> <s>in this paper, we study partial monitoring with finite actions and stochastic outcomes.</s> <s>we derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem.</s> <s>inspired by the dmed algorithm (honda and takemura, 2010) for the multi-armed bandit problem, we propose pm-dmed, an algorithm that minimizes the distribution-dependent regret.</s> <s>pm-dmed significantly outperforms state-of-the-art algorithms in numerical experiments.</s> <s>to show the optimality of pm-dmed with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (pm-dmed-hinge).</s> <s>then, we derive an asymptotical optimal regret upper bound of pm-dmed-hinge that matches the lower bound.</s></p></d>", "label": ["<d><p><s>regret lower bound and optimal algorithm in finite stochastic partial monitoring</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting.</s> <s>in this work we extend the notion of learning with memory to the general online convex optimization (oco) framework, and present two algorithms that attain low regret.</s> <s>the first algorithm applies to lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses.</s> <s>the second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring lipschitz continuity, yet is more complicated to implement.</s> <s>we complement the theoretic results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.</s></p></d>", "label": ["<d><p><s>online learning for adversaries with memory: price of past mistakes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a revenue optimization algorithm for posted-price auctions when facing a buyer with random valuations who seeks to optimize his $\\gamma$-discounted surplus.</s> <s>to analyze this problem, we introduce the notion of  epsilon-strategic buyer, a more natural notion of strategic behavior than what has been used in the past.</s> <s>we improve upon the previous state-of-the-art and achieve an optimal regret bound in  $o\\big( \\log t + \\frac{1}{\\log(1/\\gamma)} \\big)$ when the seller can offer prices from a finite set $\\cp$ and provide a regret bound in  $\\widetilde o \\big(\\sqrt{t} + \\frac{t^{1/4}}{\\log(1/\\gamma)} \\big)$ when the buyer is offered prices from the interval $[0, 1]$.</s></p></d>", "label": ["<d><p><s>revenue optimization against strategic buyers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper discusses how to efficiently choose from $n$ unknowndistributions the $k$ ones whose means are the greatest by a certainmetric, up to a small relative error.</s> <s>we study the topic under twostandard settings---multi-armed bandits and hidden bipartitegraphs---which differ in the nature of the input distributions.</s> <s>in theformer setting, each distribution can be sampled (in the i.i.d.manner) an arbitrary number of times, whereas in the latter, eachdistribution is defined on a population of a finite size $m$ (andhence, is fully revealed after $m$ samples).</s> <s>for both settings, weprove lower bounds on the total number of samples needed, and proposeoptimal algorithms whose sample complexities match those lower bounds.</s></p></d>", "label": ["<d><p><s>on top-k selection in multi-armed bandits and hidden bipartite graphs</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the iteration complexity of the block-coordinate descent (bcd) type algorithm has been under extensive investigation.</s> <s>it was recently shown that for convex problems the classical cyclic bcgd (block coordinate gradient descent) achieves an o(1/r) complexity (r is the number of passes of all blocks).</s> <s>however, such bounds are at least linearly depend on $k$ (the number of variable blocks), and are at least $k$ times worse than those of the gradient descent (gd) and proximal gradient (pg) methods.in this paper, we close such theoretical performance gap between cyclic bcd and gd/pg.</s> <s>first we show that for a family of quadratic nonsmooth problems, the complexity bounds for cyclic block coordinate proximal gradient (bcpg), a popular variant of bcd, can match those of the gd/pg in terms of dependency on $k$ (up to a \\log^2(k) factor).</s> <s>second, we establish an improved complexity bound for coordinate gradient descent (cgd) for general convex problems which can match that of gd in certain scenarios.</s> <s>our bounds are sharper than the known bounds as they are always at least $k$ times worse than gd.</s> <s>{our analyses do not depend on the update order of block variables inside each cycle, thus our results also apply to bcd methods with random permutation (random sampling without replacement, another popular variant).</s></p></d>", "label": ["<d><p><s>improved iteration complexity bounds of cyclic block coordinate descent for convex problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the restless bandit problem where arms are associated with stationary $\\varphi$-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of carefully recovering some independence by `ignoring' the values of some rewards.</s> <s>as we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off, which we do by considering the idea of a {\\em waiting arm} in the new remix-ucb algorithm, a generalization of improved-ucb for the problem at hand, that we introduce.</s> <s>we provide a regret analysis for this bandit strategy; two noticeable features of remix-ucb are that i) it reduces to the regular improved-ucb when the $\\varphi$-mixing coefficients are all $0$, i.e.</s> <s>when the i.i.d scenario is recovered, and ii) when $\\varphi(n)=o(n^{-\\alpha})$, it is able to ensure a controlled regret of order $\\ot\\left(  \\delta_*^{(\\alpha- 2)/\\alpha} \\log^{1/\\alpha} t\\right),$ where $\\delta_*$ encodes the distance between the best arm and the best suboptimal arm, even in the case when $\\alpha<1$, i.e.</s> <s>the case when the $\\varphi$-mixing coefficients {\\em are not} summable.</s></p></d>", "label": ["<d><p><s>cornering stationary and restless mixing bandits with remix-ucb</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we focus on the adversarial multi-armed bandit problem.</s> <s>the exp3 algorithm of auer et al.</s> <s>(2003) was shown to have a regret bound of $o(\\sqrt{t n \\log n})$, where $t$ is the time horizon and $n$ is the number of available actions (arms).</s> <s>more recently, audibert and bubeck (2009) improved the bound by a logarithmic factor via an entirely different method.</s> <s>in the present work, we provide a new set of analysis tools, using the notion of convex smoothing, to provide several novel algorithms with optimal guarantees.</s> <s>first we show that regularization via the tsallis entropy matches the minimax rate of audibert and bubeck (2009) with an even tighter constant; it also fully generalizes exp3.</s> <s>second we show that a wide class of perturbation methods lead to near-optimal bandit algorithms as long as a simple condition on the perturbation distribution $\\mathcal{d}$ is met: one needs that the hazard function of $\\mathcal{d}$ remain bounded.</s> <s>the gumbel, weibull, frechet, pareto, and gamma distributions all satisfy this key property; interestingly, the gaussian and uniform distributions do not.</s></p></d>", "label": ["<d><p><s>fighting bandits with a new kind of smoothness</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures.</s> <s>roughly, the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony.</s> <s>we also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods.</s> <s>in short, we show that for many stochastic approximation problems, as freddie mercury sings in queen's \\emph{bohemian rhapsody}, ``nothing really matters.''</s></p></d>", "label": ["<d><p><s>asynchronous stochastic convex optimization: the noise is in the noise and sgd don't care</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions.</s> <s>i show that the price for such unbalanced worst-case regret guarantees is rather high.</s> <s>specifically, if an algorithm enjoys a worst-case regret of b with respect to some action, then there must exist another action for which the worst-case regret is at least ?</s> <s>(nk/b), where n is the horizon and k the number of actions.</s> <s>i also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved.</s> <s>for the stochastic case the pareto regret frontier is characterised exactly up to constant factors.</s></p></d>", "label": ["<d><p><s>the pareto regret frontier for bandits</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider a sequential learning problem with gaussian payoffs and side information: after selecting an action $i$, the learner receives information about the payoff of every action $j$ in the form of gaussian observations whose mean is the same as the mean payoff, but the variance depends on the pair $(i,j)$ (and may be infinite).</s> <s>the setup allows a more refined information transfer from one action to another than previous partial monitoring setups, including the recently introduced graph-structured feedback case.</s> <s>for the first time in the literature, we provide non-asymptotic problem-dependent lower bounds on the regret of any algorithm, which recover existing asymptotic problem-dependent lower bounds and finite-time minimax lower bounds available in the literature.</s> <s>we also provide algorithms that achieve the problem-dependent lower bound (up to some universal constant factor) or the minimax lower bounds (up to  logarithmic factors).</s></p></d>", "label": ["<d><p><s>online learning with gaussian payoffs and side observations</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider empirical risk minimization (erm) in the context of stochastic optimization with exp-concave and smooth losses---a general optimization framework that captures several important learning problems including linear and logistic regression, learning svms with the squared hinge-loss, portfolio selection and more.</s> <s>in this setting, we establish the first evidence that erm is able to attain fast generalization rates, and show that the expected loss of the erm solution in $d$ dimensions converges to the optimal expected loss in a rate of $d/n$.</s> <s>this rate matches existing lower bounds up to constants and improves by a $\\log{n}$ factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.</s></p></d>", "label": ["<d><p><s>fast rates for exp-concave empirical risk minimization</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we develop a sequential low-complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori.</s> <s>we present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors.</s> <s>motivated by large-sample asymptotics, we propose a noveladaptive low-complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate.</s> <s>we further prove that in the large-sample limit, the conditional likelihood and datapredictive distribution become asymptotically gaussian.</s> <s>we demonstrate through experiments on synthetic and real data sets that our approach is superior to otheronline state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>adaptive low-complexity sequential inference for dirichlet process mixture models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>starting with the thomspon sampling algorithm, recent years have seen a resurgence of interest in bayesian algorithms for the multi-armed bandit (mab) problem.</s> <s>these algorithms seek to exploit prior information on arm biases and while several have been shown to be regret optimal, their design has not emerged from a principled approach.</s> <s>in contrast, if one cared about bayesian regret discounted over an infinite horizon at a fixed, pre-specified rate, the celebrated gittins index theorem offers an optimal algorithm.</s> <s>unfortunately, the gittins analysis does not appear to carry over to minimizing bayesian regret over all sufficiently large horizons and computing a gittins index is onerous relative to essentially any incumbent index scheme for the bayesian mab problem.</s> <s>the present paper proposes a sequence of 'optimistic' approximations to the gittins index.</s> <s>we show that the use of these approximations in concert with the use of an increasing discount factor appears to offer a compelling alternative to a variety of index schemes proposed for the bayesian mab problem in recent years.</s> <s>in addition, we show that the simplest of these approximations yields regret that matches the lai-robbins lower bound, including achieving matching constants.</s></p></d>", "label": ["<d><p><s>optimistic gittins indices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of finding the minimizer of a convex function $f: \\mathbb r^d \\rightarrow \\mathbb r$ of the form $f(w) \\defeq \\sum_{i=1}^n f_i(w) + r(w)$ where a low-rank factorization of $\\nabla^2 f_i(w)$ is readily available.we consider the regime where $n \\gg d$.</s> <s>we propose randomized newton-type algorithms that exploit \\textit{non-uniform} sub-sampling of $\\{\\nabla^2 f_i(w)\\}_{i=1}^{n}$, as well as inexact updates, as means to reduce the computational complexity, and are applicable to a wide range of problems in machine learning.</s> <s>two non-uniform sampling distributions based on {\\it block norm squares} and {\\it block partial leverage scores} are considered.</s> <s>under certain assumptions, we show that our algorithms inherit a linear-quadratic convergence rate in $w$ and achieve a lower computational complexity compared to similar existing methods.</s> <s>in addition, we show that our algorithms exhibit more robustness and better dependence on problem specific quantities, such as the condition number.</s> <s>we numerically demonstrate the advantages of our algorithms on several real datasets.</s></p></d>", "label": ["<d><p><s>sub-sampled newton methods with non-uniform sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled.</s> <s>for pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity.</s> <s>in contrast, there have been few methods for stream-based active learning based on adaptive submodularity.</s> <s>in this paper, we propose a new class of utility functions, policy-adaptive submodular functions, and prove this class includes many existing adaptive submodular functions appearing in real world problems.</s> <s>we provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance.</s> <s>in addition we empirically demonstrate their effectiveness comparing with existing heuristics on common benchmark datasets.</s></p></d>", "label": ["<d><p><s>budgeted stream-based active learning via adaptive submodular maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks?</s> <s>this paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model.</s> <s>the clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model?s posterior distribution.</s> <s>by retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the blizzard and timit speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.</s></p></d>", "label": ["<d><p><s>sequential neural models with stochastic layers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance.</s> <s>our methods apply to distributionally robust optimization problems proposed by ben-tal et al., which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution.</s> <s>our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures.</s> <s>for a sample of size n, the per-iteration cost of our method scales as o(log n), which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.</s></p></d>", "label": ["<d><p><s>stochastic gradient methods for distributionally robust optimization with f-divergences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many information systems use tags and keywords to describe and annotate content.</s> <s>these allow for efficient organization and categorization of items, as well as facilitate relevant search queries.</s> <s>as such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item.</s> <s>in tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable.</s> <s>in this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized.</s> <s>we formulate the problem by modeling traffic using a markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest.</s> <s>the resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint.</s> <s>we show that the optimization problem is np-hard, but has a (1-1/e)-approximation via a simple greedy algorithm due to monotonicity and submodularity.</s> <s>furthermore, the structure of the problem allows for an efficient computation of the greedy step.</s> <s>to demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.</s></p></d>", "label": ["<d><p><s>optimal tagging with markov chain optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility.</s> <s>in these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks.</s> <s>by embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability.</s> <s>we propose two approaches for learning in these domains: reinforced inter-agent learning (rial) and differentiable inter-agent learning (dial).</s> <s>the former uses deep q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels.</s> <s>hence, this approach uses centralised learning but decentralised execution.</s> <s>our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.</s></p></d>", "label": ["<d><p><s>learning to communicate with deep multi-agent reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times.</s> <s>by considering a novel problem formulation?the minimization of a sum of piecewise functions?we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization.</s> <s>this result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization.</s> <s>in empirical comparisons, we study the scalability of our methods.</s> <s>we find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.</s></p></d>", "label": ["<d><p><s>unified methods for exploiting piecewise linear structure in convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a sampling-based optimization method for quadratic functions is   proposed.</s> <s>our method approximately solves the following   $n$-dimensional quadratic minimization problem in constant time,   which is independent of $n$:   $z^*=\\min_{\\bv \\in \\bbr^n}\\bracket{\\bv}{a \\bv} +   n\\bracket{\\bv}{\\diag(\\bd)\\bv} + n\\bracket{\\bb}{\\bv}$,   where $a \\in \\bbr^{n \\times n}$ is a matrix and $\\bd,\\bb \\in \\bbr^n$   are vectors.</s> <s>our theoretical analysis specifies the number of   samples $k(\\delta, \\epsilon)$ such that the approximated solution   $z$ satisfies $|z - z^*| = o(\\epsilon n^2)$ with probability   $1-\\delta$.</s> <s>the empirical performance (accuracy and runtime) is   positively confirmed by numerical experiments.</s></p></d>", "label": ["<d><p><s>minimizing quadratic functions in constant time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however, their adoption in the computer graphics and geometry processing communities is limited due to the non-euclidean structure of their data.</s> <s>in this paper, we propose anisotropic convolutional neural network (acnn), a generalization of classical cnns to non-euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels.</s> <s>we use acnns to effectively learn intrinsic dense correspondences between deformable shapes, a fundamental problem in geometry processing, arising in a wide variety of applications.</s> <s>we tested acnns performance in very challenging settings, achieving state-of-the-art results on some of the most difficult recent correspondence benchmarks.</s></p></d>", "label": ["<d><p><s>learning shape correspondence with anisotropic convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the value iteration network (vin): a fully differentiable neural network with a `planning module' embedded within.</s> <s>vins can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.</s> <s>key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.</s> <s>we evaluate vin based policies on discrete and continuous path-planning domains, and on a natural-language based search task.</s> <s>we show that by learning an explicit planning computation, vin policies generalize better to new, unseen domains.</s></p></d>", "label": ["<d><p><s>value iteration networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>expectation maximization (em) is among the most popular algorithms for estimating parameters of statistical models.</s> <s>however, em, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer.</s> <s>this article addresses this disconnect between the statistical principles behind em and its algorithmic properties.</s> <s>specifically, it provides a global analysis of em for specific models in which the observations comprise an i.i.d.</s> <s>sample from a mixture of two gaussians.</s> <s>this is achieved by (i) studying the sequence of parameters from idealized execution of em in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by em.</s></p></d>", "label": ["<d><p><s>global analysis of expectation maximization for mixtures of two gaussians</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems.</s> <s>simple non-convex optimization algorithms are popular and effective in practice.</s> <s>despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice.</s> <s>we prove that the commonly used non-convex objective function for matrix completion has no spurious local minima \\--- all local minima must also be global.</s> <s>therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve matrix completion with \\textit{arbitrary} initialization in polynomial time.</s></p></d>", "label": ["<d><p><s>matrix completion has no spurious local minimum</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation.</s> <s>a simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions.</s> <s>one more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set.</s> <s>unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance.</s> <s>in this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted m-estimator that is both statistically efficient and practically powerful.</s> <s>both theoretical and empirical analysis is provided to demonstrate our methods.</s></p></d>", "label": ["<d><p><s>bootstrap model aggregation for distributed statistical learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the exponential mechanism is a general method to construct a randomized estimator that satisfies $(\\varepsilon, 0)$-differential privacy.</s> <s>recently, wang et al.</s> <s>showed that the gibbs posterior, which is a data-dependent probability distribution that contains the bayesian posterior, is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function.</s> <s>while the exponential mechanism provides a way to build an $(\\varepsilon, 0)$-differential private algorithm, it requires boundedness of the loss function, which is quite stringent for some learning problems.</s> <s>in this paper, we focus on $(\\varepsilon, \\delta)$-differential privacy of gibbs posteriors with convex and lipschitz loss functions.</s> <s>our result extends the classical exponential mechanism, allowing the loss functions to have an unbounded sensitivity.</s></p></d>", "label": ["<d><p><s>differential privacy without sensitivity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a deep generative model for learning to distill the hidden factors of variation within a set of labeled observations into two complementary codes.</s> <s>one code describes the factors of variation relevant to solving a specified task.</s> <s>the other code describes the remaining factors of variation that are irrelevant to solving this task.</s> <s>the only available source of supervision during the training process comes from our ability to distinguish among different observations belonging to the same category.</s> <s>concrete examples include multiple images of the same object from different viewpoints, or multiple speech samples from the same speaker.</s> <s>in both of these instances, the factors of variation irrelevant to classification are implicitly expressed by intra-class variabilities, such as the relative position of an object in an image, or the linguistic content of an utterance.</s> <s>most existing approaches for solving this problem rely heavily on having access to pairs of observations only sharing a single factor of variation, e.g.</s> <s>different objects observed in the exact same conditions.</s> <s>this assumption is often not encountered in realistic settings where data acquisition is not controlled and labels for the uninformative components are not available.</s> <s>in this work, we propose to overcome this limitation by augmenting deep convolutional autoencoders with a form of adversarial training.</s> <s>both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies.</s> <s>experimental results on synthetic and real datasets show that the proposed method is capable of disentangling the influences of style and content factors using a flexible representation, as well as generalizing to unseen styles or content classes.</s></p></d>", "label": ["<d><p><s>disentangling factors of variation in deep representation using adversarial training</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications such as advertisement placement or automated dialog systems, an intelligent system optimizes performance over a sequence of interactions with each user.</s> <s>such tasks often involve many states and potentially time-dependent transition dynamics, and can be modeled well as episodic markov decision processes (mdps).</s> <s>in this paper, we present a pac algorithm for reinforcement learning in episodic finite mdps with time-dependent transitions that acts epsilon-optimal in all but o(s a h^3  / epsilon^2 log(1 / delta)) episodes.</s> <s>our algorithm has a polynomial computational complexity, and our sample complexity bound accounts for the fact that we may only be able to approximately solve the internal planning problems.</s> <s>in addition, our pac sample complexity bound has only linear dependency on the number of states s and actions a and strictly improves previous bounds with s^2 dependency in this setting.</s> <s>compared against other methods for infinite horizon reinforcement learning with linear state space sample complexity our method has much lower dependency on the (effective) horizon.</s> <s>indeed, our bound is optimal up to a factor of h.</s></p></d>", "label": ["<d><p><s>only h is left: near-tight episodic pac rl</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>practical applications of machine learning often involve successive training iterations with changes to features and training examples.</s> <s>ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn.</s> <s>these changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive.</s> <s>in this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier.</s> <s>we use a markov chain monte carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy.</s> <s>we investigate the properties of the proposal with theoretical analysis.</s> <s>experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.</s></p></d>", "label": ["<d><p><s>launch and iterate: reducing prediction churn</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>adaptive stochastic gradient methods such as adagrad have gained popularity in particular for training deep neural networks.</s> <s>the most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively.</s> <s>in certain situations the full-matrix variant of adagrad is expected to attain better performance, however in high dimensions it is computationally impractical.</s> <s>we present ada-lr and radagrad two computationally efficient approximations to full-matrix adagrad based on randomized dimensionality reduction.</s> <s>they are able to capture dependencies between features and achieve similar performance to full-matrix adagrad but at a much smaller computational cost.</s> <s>we show that the regret of ada-lr is close to the regret of full-matrix adagrad which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant.</s> <s>empirically, we show that ada-lr and radagrad perform similarly to full-matrix adagrad.</s> <s>on the task of training convolutional neural networks as well as recurrent neural networks, radagrad achieves faster convergence than diagonal adagrad.</s></p></d>", "label": ["<d><p><s>scalable adaptive stochastic optimization using random projections</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we introduce the forget-me-not process, an efficient, non-parametric meta-algorithm for online probabilistic sequence prediction for piecewise stationary, repeating sources.</s> <s>our method works by taking a bayesian approach to partition a stream of data into postulated task-specific segments, while simultaneously building a model for each task.</s> <s>we provide regret guarantees with respect to piecewise stationary data sources under the logarithmic loss, and validate the method empirically across a range of sequence prediction and task identification problems.</s></p></d>", "label": ["<d><p><s>the forget-me-not process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we formalize notions of robustness for composite estimators via the notion of a breakdown point.</s> <s>a composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator.</s> <s>and so on, if the composition is of more than two estimators.</s> <s>informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point.</s> <s>our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators.</s> <s>we also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.</s></p></d>", "label": ["<d><p><s>the robustness of estimator composition</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs.</s> <s>there is no good reason for this restriction.</s> <s>synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights.</s> <s>these ``fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models.</s> <s>by using fast weights we can avoid the need to store copies of neural activity patterns.</s></p></d>", "label": ["<d><p><s>using fast weights to attend to the recent past</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions.</s> <s>we show a significant gap between the complexity of deterministic vs randomized optimization.</s> <s>for smooth functions, we show that accelerated gradient descent (agd) and an accelerated variant of svrg are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate.</s> <s>for non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.</s></p></d>", "label": ["<d><p><s>tight complexity bounds for optimizing composite objectives</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy.</s> <s>% one critical shortcoming of classical experimental methods, however, is that they typically do not take into account the dynamic nature of response to policy changes.</s> <s>for instance, in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue, agents may adapt their bidding in response to the experimental pricing changes.</s> <s>thus, causal effects of the new pricing policy after such adaptation period, the {\\em long-term causal effects}, are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy.</s> <s>%  here, we formalize a framework to define and estimate long-term causal effects of   policy changes in multiagent economies.</s> <s>central to our approach is behavioral game theory, which we leverage   to formulate the ignorability assumptions that are necessary for causal inference.</s> <s>under such assumptions we estimate long-term causal effects through a latent space approach, where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time.</s></p></d>", "label": ["<d><p><s>long-term causal effects via behavioral game theory</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare.</s> <s>this paper introduces composable generative population models (cgpms), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques.</s> <s>examples include discriminative machine learning, hierarchical bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs.</s> <s>we demonstrate the integration of cgpms into bayesdb, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language.</s> <s>the practical value is illustrated in two ways.</s> <s>first, the paper describes an analysis on a database of earth satellites, which identifies records that probably violate kepler?s third law by composing causal probabilistic programs with non-parametric bayes in 50 lines of probabilistic code.</s> <s>second, it reports the lines of code and accuracy of cgpms compared with baseline solutions from standard machine learning libraries.</s></p></d>", "label": ["<d><p><s>a probabilistic programming approach to probabilistic data analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper puts forth a novel algorithm, termed \\emph{truncated generalized gradient flow} (tggf), to solve for $\\bm{x}\\in\\mathbb{r}^n/\\mathbb{c}^n$ a system of $m$ quadratic equations $y_i=|\\langle\\bm{a}_i,\\bm{x}\\rangle|^2$, $i=1,2,\\ldots,m$, which even for $\\left\\{\\bm{a}_i\\in\\mathbb{r}^n/\\mathbb{c}^n\\right\\}_{i=1}^m$ random is known to be \\emph{np-hard} in general.</s> <s>we prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$, tggf recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data $\\left\\{\\left(\\bm{a}_i;\\,y_i\\right)\\right\\}_{i=1}^m$.</s> <s>specifically, tggf proceeds in two stages: s1) a novel \\emph{orthogonality-promoting} initialization that is obtained with simple power iterations; and, s2) a refinement of the initial estimate by successive updates of scalable \\emph{truncated generalized gradient iterations}.</s> <s>the former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth \\emph{amplitude-based} cost function.</s> <s>numerical tests demonstrate that: i) the novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and ii) even with the same initialization, our refinement/truncation outperforms wirtinger-based alternatives, all corroborating the superior performance of tggf over state-of-the-art algorithms.</s></p></d>", "label": ["<d><p><s>solving random systems of quadratic equations via truncated generalized gradient flow</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a bayesian model for decision-making under time pressure with endogenous information acquisition.</s> <s>in our model, the decision-maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence/non-occurrence of an adverse event which will terminate the decision-making process.</s> <s>in her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping).</s> <s>we show that the optimal policy has a \"rendezvous\" structure, i.e.</s> <s>a structure in which whenever a new information sample is gathered from the time series, the optimal \"date\" for acquiring the next sample becomes computable.</s> <s>the optimal interval between two information samples balances a trade-off between the decision maker?s \"surprise\", i.e.</s> <s>the drift in her posterior belief after observing new information, and \"suspense\", i.e.</s> <s>the probability that the adverse event occurs in the time interval between two information samples.</s> <s>moreover, we characterize the continuation and stopping regions in the decision-maker?s state-space, and show that they depend not only on the decision-maker?s beliefs, but also on the \"context\", i.e.</s> <s>the current realization of the time series.</s></p></d>", "label": ["<d><p><s>balancing suspense and surprise: timely decision making with endogenous information acquisition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of recovering a signal observed in gaussian noise.</s> <s>if the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk.</s> <s>however, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem.</s> <s>we propose a new family of estimators to recover signals observed in gaussian noise.</s> <s>instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator.</s> <s>proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization.</s> <s>we present several numerical illustrations that show the potential of the approach.</s></p></d>", "label": ["<d><p><s>structure-blind signal recovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments.</s> <s>in this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears.</s> <s>the agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed.</s> <s>we release a software tool, called webnav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make wikinav, a dataset constructed from the english wikipedia.</s> <s>we extensively evaluate different variants of neural net based artificial agents on wikinav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress.</s> <s>furthermore, we extend the wikinav with question-answer pairs from jeopardy!</s> <s>and test the proposed agent based on recurrent neural networks against strong inverted index based search engines.</s> <s>the artificial agents trained on wikinav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.</s></p></d>", "label": ["<d><p><s>end-to-end goal-driven web navigation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>area under roc (auc) is a metric which is widely used for measuring the classification performance for imbalanced data.</s> <s>it is of theoretical and practical interest to develop online learning algorithms that maximizes auc for large-scale data.</s> <s>a specific challenge in developing online auc maximization algorithm is that the learning objective function is usually defined over a pair of training examples of opposite classes, and existing methods achieves on-line processing with higher space and time complexity.</s> <s>in this work, we propose a new stochastic online algorithm for auc maximization.</s> <s>in particular, we show that auc optimization can  be equivalently formulated as a convex-concave saddle point problem.</s> <s>from this saddle representation, a stochastic online algorithm (solam) is proposed which has time and space complexity of one datum.</s> <s>we establish theoretical convergence of solam with high probability and demonstrate its effectiveness and efficiency on standard benchmark datasets.</s></p></d>", "label": ["<d><p><s>stochastic online auc maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>generative neural networks are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights.</s> <s>these models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization.</s> <s>the generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network.</s> <s>we show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach.</s> <s>we show that any $f$-divergence can be used for training generative neural networks.</s> <s>we discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.</s></p></d>", "label": ["<d><p><s>f-gan: training generative neural samplers using variational divergence minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features.</s> <s>rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task.</s> <s>we enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism.</s> <s>we achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations.</s> <s>in contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities.</s> <s>we evaluate our method on multi-digit classification of very cluttered images that require texture segmentation.</s> <s>remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism.</s> <s>furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline ladder network on our dataset.</s> <s>these results are evidence that grouping is a powerful tool that can help to improve sample efficiency.</s></p></d>", "label": ["<d><p><s>tagger: deep unsupervised perceptual grouping</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>one-shot learning is usually tackled by using generative models or discriminative embeddings.</s> <s>discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data.</s> <s>in this paper, we propose a method to learn the parameters of a deep model in one shot.</s> <s>we construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar.</s> <s>in this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation.</s> <s>in order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network.</s> <s>we demonstrate encouraging results by learning characters from single exemplars in omniglot, and by tracking visual objects from a single initial exemplar in the visual object tracking benchmark.</s></p></d>", "label": ["<d><p><s>learning feed-forward one-shot learners</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>concurrent measurements of neural activity at multiple scales, sometimes performed with multimodal techniques, become increasingly important for studying brain function.</s> <s>however, statistical methods for their concurrent analysis are currently lacking.</s> <s>here we introduce such techniques in a framework based on vine copulas with mixed margins to construct multivariate stochastic models.</s> <s>these models can describe detailed mixed interactions between discrete variables such as neural spike counts, and continuous variables such as local field potentials.</s> <s>we propose efficient methods for likelihood calculation, inference, sampling and mutual information estimation within this framework.</s> <s>we test our methods on simulated data and demonstrate applicability on mixed data generated by a biologically realistic neural network.</s> <s>our methods hold the promise to considerably improve statistical analysis of neural data recorded simultaneously at different scales.</s></p></d>", "label": ["<d><p><s>mixed vine copulas as joint models of spike counts and local field potentials</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>n this paper, we propose and study an asynchronous parallel greedy coordinate descent (asy-gcd) algorithm for minimizing a smooth function with bounded constraints.</s> <s>at each iteration, workers asynchronously conduct greedy coordinate descent updates on a block of variables.</s> <s>in the first part of the paper, we analyze the theoretical behavior of asy-gcd and prove a linear convergence rate.</s> <s>in the second part, we develop an efficient kernel svm solver based on asy-gcd in the shared memory multi-core setting.</s> <s>since our algorithm is fully asynchronous---each core does not need to idle and wait for the other cores---the  resulting algorithm enjoys good speedup and outperforms existing multi-core kernel svm solvers including asynchronous stochastic coordinate descent and multi-core libsvm.</s></p></d>", "label": ["<d><p><s>asynchronous parallel greedy coordinate descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods.</s> <s>in addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters.</s> <s>we show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.</s></p></d>", "label": ["<d><p><s>generative shape models: joint text recognition and segmentation with very little training data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the trade-off between two different kinds of pure exploration: breadth versus depth.</s> <s>we focus on the most biased coin problem, asking how many total coin flips are required to identify a ``heavy'' coin from an infinite bag containing both ``heavy'' coins with mean $\\theta_1 \\in (0,1)$, and ``light\" coins with mean $\\theta_0 \\in (0,\\theta_1)$, where heavy coins are drawn from the bag with proportion $\\alpha \\in (0,1/2)$.</s> <s>when $\\alpha,\\theta_0,\\theta_1$ are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare.</s> <s>while existing solutions to this problem require some prior knowledge of the parameters $\\theta_0,\\theta_1,\\alpha$, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees.</s> <s>in contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples.</s> <s>in characterizing this gap between adaptive and nonadaptive strategies,  we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.</s></p></d>", "label": ["<d><p><s>the power of adaptivity in identifying statistical alternatives</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>online optimization covers problems such as online resource allocation, online bipartite matching, adwords (a central problem in e-commerce and advertising), and adwords with separable concave returns.</s> <s>we analyze the worst case competitive ratio of two primal-dual algorithms for a class of online convex (conic) optimization problems that contains the previous examples as special cases defined on the positive orthant.</s> <s>we derive a sufficient condition on the objective function that guarantees a constant worst case competitive ratio (greater than or equal to $\\frac{1}{2}$) for monotone objective functions.</s> <s>we provide new examples of online problems on the positive orthant % and the positive semidefinite cone  that satisfy the sufficient condition.</s> <s>we show how smoothing can improve the competitive ratio of these algorithms, and in particular for separable functions, we show that the optimal smoothing can be derived by solving a convex optimization problem.</s> <s>this result allows us to directly optimize the competitive ratio bound over a class of smoothing functions, and hence  design effective smoothing customized for a given cost function.</s></p></d>", "label": ["<d><p><s>designing smoothing functions for improved worst-case competitive ratio in online optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many problems in real-world applications involve predicting continuous-valued random variables that are statistically related.</s> <s>in this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the   dependencies between continuous output variables.</s> <s>we show that  inference in our  model using proximal methods can be efficiently solved as a feed-foward pass of a special  type of  deep recurrent neural network.</s> <s>we demonstrate the  effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.</s></p></d>", "label": ["<d><p><s>proximal deep structured models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we present a new algorithm for computing a low rank approximation of the product $a^tb$ by taking only a single pass of the two matrices $a$ and $b$.</s> <s>the straightforward way to do this is to (a) first sketch $a$ and $b$ individually, and then (b) find the top components using pca on the sketch.</s> <s>our algorithm in contrast retains additional summary information about $a,b$ (e.g.</s> <s>row and column norms etc.)</s> <s>and uses this additional information to obtain an improved approximation from the sketches.</s> <s>our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an apache spark implementation that shows better computational and statistical performance on real-world and synthetic evaluation datasets.</s></p></d>", "label": ["<d><p><s>single pass pca of matrix products</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most learning algorithms are not invariant to the scale of the signal that is being approximated.</s> <s>we propose to adaptively normalize the targets used in the learning updates.</s> <s>this is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior.</s> <s>our main motivation is prior work on learning to play atari games, where the rewards were clipped to a predetermined range.</s> <s>this clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior.</s> <s>using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.</s></p></d>", "label": ["<d><p><s>learning values across many orders of magnitude</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>latent dirichlet allocation (lda) is a very popular model for topic modeling as well as many other problems with latent groups.</s> <s>it is both simple and effective.</s> <s>when the number of topics (or latent groups) is unknown, the hierarchical dirichlet process (hdp) provides an elegant non-parametric extension; however, it is a complex model and it is difficult to incorporate prior knowledge since the distribution over topics is implicit.</s> <s>we propose two new models that extend lda in a simple and intuitive fashion by directly expressing a distribution over the number of topics.</s> <s>we also propose a new online bayesian moment matching technique to learn the parameters and the number of topics of those models based on streaming data.</s> <s>the approach achieves higher log-likelihood than batch and online hdp with fixed hyperparameters on several corpora.</s></p></d>", "label": ["<d><p><s>online bayesian moment matching for topic modeling with unknown number of topics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of reconstructing a mixture of markov chains from the trajectories generated by random walks through the state space.</s> <s>under mild non-degeneracy conditions, we show that we can uniquely reconstruct the underlying chains by only considering trajectories of length three, which represent triples of states.</s> <s>our algorithm is spectral in nature, and is easy to implement.</s></p></d>", "label": ["<d><p><s>on mixtures of markov chains</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>high dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices.</s> <s>in this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm.</s> <s>we present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give non-asymptotic bounds on the componentwise estimation error.</s> <s>we use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of gaussian widths of suitable spherical caps.</s></p></d>", "label": ["<d><p><s>high dimensional structured superposition models</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a new algorithm, truncated variance reduction (truvar), that treats bayesian optimization (bo) and level-set estimation (lse) with gaussian processes in a unified fashion.</s> <s>the algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (bo) or unclassified points (lse), which is updated based on confidence bounds.</s> <s>truvar is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise.</s> <s>we provide a general theoretical guarantee for truvar covering these aspects, and use it to recover and strengthen existing results on bo and lse.</s> <s>moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs.</s> <s>we demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.</s></p></d>", "label": ["<d><p><s>truncated variance reduction: a unified approach to bayesian optimization and level-set estimation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this work, we are interested in generalizing convolutional neural networks (cnns) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words?</s> <s>embedding, represented by graphs.</s> <s>we present a formulation of cnns in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs.</s> <s>importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical cnns, while being universal to any graph structure.</s> <s>experiments on mnist and 20news demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.</s></p></d>", "label": ["<d><p><s>convolutional neural networks on graphs with fast localized spectral filtering</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>towards learning programs from data, we introduce the problem of   sampling programs from posterior distributions conditioned on that   data.</s> <s>within this setting, we propose an algorithm that uses a   symbolic solver to efficiently sample programs.</s> <s>the proposal   combines constraint-based program synthesis with sampling via random   parity constraints.</s> <s>we give theoretical guarantees on how well the   samples approximate the true posterior, and have empirical results   showing the algorithm is efficient in practice, evaluating our   approach on 22 program learning problems in the domains of text   editing and computer-aided programming.</s></p></d>", "label": ["<d><p><s>sampling for bayesian program learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents a dynamical system based on the poisson-gamma construction for sequentially observed multivariate count data.</s> <s>inherent to the model is a novel bayesian nonparametric prior that ties and shrinks parameters in a powerful way.</s> <s>we develop theory about the model's infinite limit and its steady-state.</s> <s>the model's inductive bias is demonstrated on a variety of real-world datasets where it is shown to learn interpretable structure and have superior predictive performance.</s></p></d>", "label": ["<d><p><s>poisson-gamma dynamical systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many statistical models can be simulated forwards but have intractable likelihoods.</s> <s>approximate bayesian computation (abc) methods are used to infer properties of these models from data.</s> <s>traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ?-ball around the observed data, which is only correct in the limit ??0.</s> <s>monte carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters.</s> <s>these algorithms critically slow down as ?</s> <s>?0, and in practice draw samples from a broader distribution than the posterior.</s> <s>we propose a new approach to likelihood-free inference based on bayesian conditional density estimation.</s> <s>preliminary inferences based on limited simulation data are used to guide later simulations.</s> <s>in some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than monte carlo abc methods need to produce a single sample from an approximate posterior.</s></p></d>", "label": ["<d><p><s>fast ?-free inference of simulation models with bayesian conditional density estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>online allocation problems have been widely studied due to their numerous practical applications (particularly to internet advertising), as well as considerable theoretical interest.</s> <s>the main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability.</s> <s>in many important applications, the algorithm designer is faced with multiple objectives to optimize.</s> <s>in particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as ?share of voice?, and ?buyer surplus?.</s> <s>while there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input.</s> <s>in this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions.</s> <s>we also study practically relevant special cases of this problem related to internet advertising, and obtain improved results.</s> <s>all our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.</s></p></d>", "label": ["<d><p><s>bi-objective online matching and submodular  allocations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, there has been a surge of interest in using spectral methods for estimating latent variable models.</s> <s>however, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family.</s> <s>in this paper, we study the estimation of an $m$-state hidden markov model (hmm) with only smoothness assumptions, such as h\\\"olderian conditions, on the emission densities.</s> <s>by leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric hmms.</s> <s>our technique is based on computing an svd on nonparametric estimates of density functions by viewing them as \\emph{continuous matrices}.</s> <s>we derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices.</s> <s>we implement our method using chebyshev polynomial approximations.</s> <s>our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.</s></p></d>", "label": ["<d><p><s>learning hmms with nonparametric emissions via spectral decompositions of continuous matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure.</s> <s>however, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds.</s> <s>indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than $\\co(d/n)$ (where $d$ is the dimension and $n$ is the number of samples).</s> <s>in this work, we extend the framework of arjevani et al.</s> <s>\\cite{arjevani2015lower,arjevani2016iteration} to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., sag, saga, svrg, sdca without duality, as well as stochastic coordinate-descent methods, such as sdca and accelerated proximal sdca.</s></p></d>", "label": ["<d><p><s>dimension-free iteration complexity of finite sum optimization problems</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses.</s> <s>in contrast with empirical risk minimization (erm) methods, which use convex surrogate losses to approximate the desired non-convex target loss function, adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game.</s> <s>despite this difference in formulation, we recast adversarial classification under zero-one loss as an erm method with a novel prescribed loss function.</s> <s>we demonstrate a number of theoretical and practical advantages over the very closely related hinge loss erm methods.</s> <s>this establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification, simultaneously guaranteeing fisher consistency and universal consistency, while also providing dual parameter sparsity and high accuracy predictions in practice.</s></p></d>", "label": ["<d><p><s>adversarial multiclass classification: a risk minimization perspective</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we develop a theory of hierarchical clustering for graphs.</s> <s>our modelling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks.</s> <s>graphons are a  far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering.</s> <s>we define what it means for an algorithm to produce the ``correct\" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties.</s></p></d>", "label": ["<d><p><s>graphons, mergeons, and so on!</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles.</s> <s>however, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings.</s> <s>discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation.</s> <s>we present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators.</s> <s>we show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks.</s> <s>our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation.</s> <s>we evaluate our approach on synthetic tracking task with raw image inputs and on the visual odometry task in the kitti dataset.</s> <s>the results show significant improvement over both standard generative approaches and regular recurrent neural networks.</s></p></d>", "label": ["<d><p><s>backprop kf: learning discriminative deterministic state estimators</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>variational inference is an umbrella term for algorithms which cast bayesian inference as optimization.</s> <s>classically, variational inference uses the kullback-leibler divergence to define the optimization.</s> <s>though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties.</s> <s>to address this, we reexamine variational inference from its roots as an optimization problem.</s> <s>we use operators, or functions of functions, to design variational objectives.</s> <s>as one example, we design a variational objective with a langevin-stein operator.</s> <s>we develop a black box algorithm, operator variational inference (opvi), for optimizing any operator objective.</s> <s>importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference.</s> <s>we can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density.</s> <s>we illustrate the benefits of opvi on a mixture model and a generative model of images.</s></p></d>", "label": ["<d><p><s>operator variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the multiple quantile graphical model (mqgm), which extends the neighborhood selection approach of meinshausen and buhlmann for learning sparse graphical models.</s> <s>the latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others.</s> <s>our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates.</s> <s>we establish that, under suitable regularity conditions, the mqgm identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic gaussian data model.</s> <s>we develop an efficient algorithm for fitting the mqgm using the alternating direction method of multipliers.</s> <s>we also describe a strategy for sampling from the joint distribution that underlies the mqgm estimate.</s> <s>lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the mqgm in modeling hetereoskedastic non-gaussian data.</s></p></d>", "label": ["<d><p><s>the multiple quantile graphical model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose and analyze a regularization approach for structured prediction problems.</s> <s>we characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space.</s> <s>we exploit this fact to  design learning  algorithms using a surrogate loss approach and regularization techniques.</s> <s>we prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed method.</s> <s>experimental results are provided to demonstrate the practical usefulness of the proposed approach.</s></p></d>", "label": ["<d><p><s>a consistent regularization approach for structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of noisy high-dimensional phase retrieval is to estimate an $s$-sparse parameter $\\boldsymbol{\\beta}^*\\in \\mathbb{r}^d$ from $n$ realizations of the model $y = (\\boldsymbol{x}^{\\top} \\boldsymbol{\\beta}^*)^2 + \\varepsilon$.</s> <s>based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (mpr), in which $y = f(\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}^*, \\varepsilon)$ with unknown $f$ and $\\operatorname{cov}(y, (\\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}^*)^2) > 0$.</s> <s>for example, mpr encompasses $y = h(|\\boldsymbol{x}^{\\top} \\boldsymbol{\\beta}^*|) + \\varepsilon$ with increasing $h$ as a special case.</s> <s>despite the generality of the mpr model, it eludes the reach of most existing semi-parametric estimators.</s> <s>in this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of $\\boldsymbol{\\beta}^*$.</s> <s>our theory is backed up by thorough numerical results.</s></p></d>", "label": ["<d><p><s>agnostic estimation for misspecified phase retrieval models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time.</s> <s>in this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor.</s> <s>in contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process.</s> <s>instead, we formulate a complexity measure that captures the diversity of the observed tasks.</s> <s>we provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average).</s> <s>we show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure.</s> <s>further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.</s></p></d>", "label": ["<d><p><s>lifelong learning with weighted majority votes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of 3d object generation.</s> <s>we propose a novel framework, namely 3d generative adversarial network (3d-gan), which generates 3d objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets.</s> <s>the benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3d objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3d objects, so that we can sample objects without a reference image or cad models, and explore the 3d object manifold; third, the adversarial discriminator provides a powerful 3d shape descriptor which, learned without supervision, has wide applications in 3d object recognition.</s> <s>experiments demonstrate that our method generates high-quality 3d objects, and our unsupervisedly learned features achieve impressive performance on 3d object recognition, comparable with those of supervised learning methods.</s></p></d>", "label": ["<d><p><s>learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel framework, called grab (graphical models with overlapping blocks), to capture densely connected components in a network estimate.</s> <s>grab takes as input a data matrix of p variables and n samples, and jointly learns both a network among p variables and densely connected groups of variables (called `blocks').</s> <s>grab has four major novelties as compared to existing network estimation methods: 1) it does not require the blocks to be given a priori.</s> <s>2) blocks can overlap.</s> <s>3) it can jointly learn a network structure and overlapping blocks.</s> <s>4) it solves a joint optimization problem with the block coordinate descent method that is convex in each step.</s> <s>we show that grab reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data.</s> <s>when applied to cancer gene expression data, grab outperforms its competitors in revealing known functional gene sets and potentially novel genes that drive cancer.</s></p></d>", "label": ["<d><p><s>learning sparse gaussian graphical models with overlapping blocks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present discriminative gaifman models, a novel family of relational machine learning models.</s> <s>gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (kbs).</s> <s>considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of overfitting, and facilitates weight sharing.</s> <s>gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world kbs.</s> <s>we present the core ideas of gaifman models and apply them to large-scale relational learning problems.</s> <s>we also discuss the ways in which gaifman models relate to some existing relational machine learning approaches.</s></p></d>", "label": ["<d><p><s>discriminative gaifman models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the teacher forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network?s own one-step-ahead predictions to do multi-step sampling.</s> <s>we introduce the professor forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps.</s> <s>we apply professor forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation.</s> <s>empirically we find that professor forcing acts as a regularizer, improving test likelihood on character level penn treebank and sequential mnist.</s> <s>we also find that the model qualitatively improves samples, especially when sampling for a large number of time steps.</s> <s>this is supported by human evaluation of sample quality.</s> <s>trade-offs between professor forcing and scheduled sampling are discussed.</s> <s>we produce t-snes showing that professor forcing successfully makes the dynamics of the network during training and sampling more similar.</s></p></d>", "label": ["<d><p><s>professor forcing: a new algorithm for training recurrent networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a pool-based non-parametric active learning algorithm for general metric spaces, called margin regularized metric active nearest neighbor (marmann), which outputs a nearest-neighbor classifier.</s> <s>we give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners.</s> <s>we prove that the label complexity of marmann is significantly lower than that of any passive learner with similar error guarantees.</s> <s>our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure.</s></p></d>", "label": ["<d><p><s>active nearest-neighbor learning in metric spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications, it is desirable to extract only the relevant aspects of data.</s> <s>a principled way to do this is the information bottleneck (ib) method, where one seeks a code that maximises information about  a relevance variable, y, while constraining the information encoded about the original data, x.</s> <s>unfortunately however, the ib method is computationally demanding when data are high-dimensional and/or non-gaussian.</s> <s>here we propose an approximate variational scheme for maximising a lower bound on the ib objective, analogous to variational em.</s> <s>using this method, we derive an ib algorithm to recover features that are both relevant and sparse.</s> <s>finally, we demonstrate how kernelised versions of the algorithm can be used to address a broad range of problems with non-linear relation between x and y.</s></p></d>", "label": ["<d><p><s>relevant sparse codes with variational information bottleneck</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider control problems for multi-stage campaigning over social networks.</s> <s>the dynamic programming framework is employed to balance the high present reward and large penalty on low future outcome in the presence of extensive uncertainties.</s> <s>in particular, we establish theoretical foundations of optimal campaigning over social networks where the user activities are modeled as a multivariate hawkes process, and we derive a time dependent linear relation between the intensity of exogenous events and several commonly used objective functions of campaigning.</s> <s>we further develop a convex dynamic programming framework for determining the optimal intervention policy that prescribes the required level of external drive at each stage for the desired campaigning result.</s> <s>experiments on both synthetic data and the real-world memetracker dataset show that our algorithm can steer the user activities for optimal campaigning much more accurately than baselines.</s></p></d>", "label": ["<d><p><s>multistage campaigning in social networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a coordinate-wise version of the power method from an optimization viewpoint.</s> <s>the vanilla power method simultaneously updates all the coordinates of the iterate, which is essential for its convergence analysis.</s> <s>however, different coordinates converge to the optimal value at different speeds.</s> <s>our proposed algorithm, which we call coordinate-wise power method, is able to select and update the most important k coordinates in o(kn) time at each iteration, where n is the dimension of the matrix and k <= n is the size of the active set.</s> <s>inspired by the ''greedy'' nature of our method, we further propose a greedy coordinate descent algorithm applied on a non-convex objective function specialized for symmetric matrices.</s> <s>we provide convergence analyses for both methods.</s> <s>experimental results on both synthetic and real data show that our methods achieve up to 20 times speedup over the basic power method.</s> <s>meanwhile, due to their coordinate-wise nature, our methods are very suitable for the important case when data cannot fit into memory.</s> <s>finally, we introduce how the coordinate-wise mechanism could be applied to other iterative methods that are used in machine learning.</s></p></d>", "label": ["<d><p><s>coordinate-wise power method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails.</s> <s>to enable such analyses, we introduce two new conditions: (i)  the envelope function $\\sup_{f \\in \\mathcal{f}}|\\ell \\circ f|$, where $\\ell$ is the loss function and $\\mathcal{f}$ is the hypothesis class, exists and is $l^r$-integrable, and (ii) $\\ell$ satisfies the multi-scale bernstein's condition on $\\mathcal{f}$.</s> <s>under these assumptions, we prove that learning rate faster than $o(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale bernstein's powers, can be arbitrarily close to $o(n^{-1})$.</s> <s>we then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions.</s> <s>the analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.</s></p></d>", "label": ["<d><p><s>fast learning rates with heavy-tailed losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space.</s> <s>instead, these methods use supervised learning to train the policy to mimic a ?teacher?</s> <s>algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method.</s> <s>guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations.</s> <s>we show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact.</s> <s>we derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded.</s> <s>we provide empirical results on several simulated robotic manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.</s></p></d>", "label": ["<d><p><s>guided policy search via approximate mirror descent</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we investigate a subclass of exponential family graphical models of which the sufficient statistics are defined by arbitrary additive forms.</s> <s>we propose two $\\ell_{2,1}$-norm regularized maximum likelihood estimators to learn the model parameters from i.i.d.</s> <s>samples.</s> <s>the first one is a joint mle estimator which estimates all the parameters simultaneously.</s> <s>the second one is a node-wise conditional mle estimator which estimates the parameters for each node individually.</s> <s>for both estimators, statistical analysis shows that under mild conditions the extra flexibility gained by the additive exponential family models comes at almost no cost of statistical efficiency.</s> <s>a monte-carlo approximation method is developed to efficiently optimize the proposed estimators.</s> <s>the advantages of our estimators over gaussian graphical models and nonparanormal estimators are demonstrated on synthetic and real data sets.</s></p></d>", "label": ["<d><p><s>learning additive exponential family graphical models via </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>controlled interventions provide the most direct source of information for learning causal effects.</s> <s>in particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes.</s> <s>however, interventions can be expensive and time-consuming.</s> <s>observational data, where the treatment is not controlled by a known mechanism, is sometimes available.</s> <s>under some strong assumptions, observational data allows for the estimation of dose-response curves.</s> <s>estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized.</s> <s>in this paper, we introduce a hierarchical gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions.</s> <s>this function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.</s></p></d>", "label": ["<d><p><s>observational-interventional priors for dose-response learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we introduce the framework of {\\em blind regression} motivated by {\\em matrix completion} for recommendation systems: given $m$ users, $n$ movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix.</s> <s>following the framework of non-parametric statistics, we posit that user $u$ and movie $i$ have features $x_1(u)$ and $x_2(i)$ respectively, and their corresponding rating $y(u,i)$ is a noisy measurement of $f(x_1(u), x_2(i))$ for some unknown function $f$.</s> <s>in contrast with classical regression, the features $x = (x_1(u), x_2(i))$ are not observed, making it challenging to apply standard regression methods to  predict the unobserved ratings.</s> <s>inspired by the classical taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all lipschitz functions.</s> <s>in fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice.</s> <s>assuming each entry is sampled independently with probability at least $\\max(m^{-1+\\delta},n^{-1/2+\\delta})$ with $\\delta > 0$, we prove that the expected fraction of our estimates with error greater than $\\epsilon$ is less than $\\gamma^2 / \\epsilon^2$ plus a polynomially decaying term, where $\\gamma^2$ is the variance of the additive entry-wise noise term.</s> <s>experiments with the movielens and netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.</s></p></d>", "label": ["<d><p><s>blind regression: nonparametric regression for latent variable models via collaborative filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present seboost, a technique for boosting the performance of existing stochastic optimization methods.</s> <s>seboost applies a secondary optimization process in the subspace spanned by the last steps and descent directions.</s> <s>the method was inspired by the sesop optimization method for large-scale problems, and has been adapted for the stochastic learning framework.</s> <s>it can be applied on top of any existing optimization method with no need to tweak the internal algorithm.</s> <s>we show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters.</s> <s>as the boosting steps of seboost are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden.</s> <s>we introduce two hyper-parameters that control the balance between the baseline method and the secondary optimization process.</s> <s>the method was evaluated on several deep learning tasks, demonstrating promising results.</s></p></d>", "label": ["<d><p><s>seboost - boosting stochastic learning using subspace optimization techniques</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the recent success of deep neural networks relies on massive amounts of labeled data.</s> <s>for a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain.</s> <s>in this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain.</s> <s>we relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function.</s> <s>we enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier.</s> <s>we fuse features of multiple layers with tensor product and embed them into reproducing kernel hilbert spaces to match distributions for feature adaptation.</s> <s>the adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation.</s> <s>empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.</s></p></d>", "label": ["<d><p><s>unsupervised domain adaptation with residual transfer networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>generative adversarial networks (gans) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers.</s> <s>while existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location.</s> <s>we propose a new model, the generative adversarial what-where network (gawwn), that synthesizes images given instructions describing what content to draw in which location.</s> <s>we show high-quality 128 ?</s> <s>128 image synthesis on the caltech-ucsd birds dataset, conditioned on both informal text descriptions and also object location.</s> <s>our system exposes control over both the bounding box around the bird and its constituent parts.</s> <s>by modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g.</s> <s>only the beak and tail), yielding an efficient interface for picking part locations.</s></p></d>", "label": ["<d><p><s>learning what and where to draw</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the conference on learning theory (colt) 2015.</s> <s>for an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers).</s> <s>moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions.</s> <s>as a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory?</s> <s>it is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points).</s> <s>we note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.</s></p></d>", "label": ["<d><p><s>deep learning without poor local minima</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate an experiential learning paradigm for acquiring an internal model of intuitive physics.</s> <s>our model is evaluated on a real-world robotic manipulation task that requires displacing objects to target locations by poking.</s> <s>the robot gathered over 400 hours of experience by executing more than 50k pokes on different objects.</s> <s>we propose a novel approach based on deep neural networks for modeling the dynamics of robot's interactions directly from images, by jointly estimating forward and inverse models of dynamics.</s> <s>the inverse model objective provides supervision to construct informative visual features, which the forward model can then predict and in turn regularize the feature space for the inverse model.</s> <s>the interplay between these two objectives creates useful, accurate models that can then be used for multi-step decision making.</s> <s>this formulation has the additional benefit that it is possible to learn forward models in an abstract feature space and thus alleviate the need of predicting pixels.</s> <s>our experiments show that this joint modeling approach outperforms alternative methods.</s> <s>we also demonstrate that active data collection using the learned model further improves performance.</s></p></d>", "label": ["<d><p><s>learning to poke by poking: experiential learning of intuitive physics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction.</s> <s>by reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent.</s> <s>our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch.</s> <s>this means that our method can also be applied successfully to recurrent models such as lstms and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited.</s> <s>although our method is much simpler, it still provides much of the speed-up of full batch normalization.</s> <s>in addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time.</s> <s>we demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.</s></p></d>", "label": ["<d><p><s>weight normalization: a simple reparameterization to accelerate training of deep neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recently, several works have shown that natural modifications of the classical conditional gradient method (aka frank-wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when the feasible set is a polytope, and the objective is smooth and strongly-convex.</s> <s>however, all of these results suffer from two significant shortcomings: i) large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration ii) the worst case convergence rate depends unfavorably on the dimension in this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings.</s> <s>in particular, both memory and computation overheads are only linear in the dimension, and in addition, in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous works, with a linear dependence on the number of non-zeros in the optimal solution at the heart of our method, and corresponding analysis, is a novel way to compute decomposition-invariant away-steps.</s> <s>while our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more.</s> <s>our theoretical findings are complemented by empirical evidence that shows that our method delivers state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>linear-memory and decomposition-invariant linearly convergent conditional gradient algorithm for structured polytopes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex.</s> <s>surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited.</s> <s>for example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point.</s> <s>to tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches.</s> <s>furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent.</s> <s>our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions.</s> <s>we also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works.</s></p></d>", "label": ["<d><p><s>proximal stochastic methods for nonsmooth nonconvex finite-sum optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian optimization is a prominent method for optimizing expensive to evaluate black-box functions that is prominently applied to tuning the hyperparameters of machine learning algorithms.</s> <s>despite its successes, the prototypical bayesian optimization approach - using gaussian process models - does not scale well to either many hyperparameters or many function evaluations.</s> <s>attacking this lack of scalability and flexibility is thus one of the key challenges of the field.</s> <s>we present a general approach for using flexible parametric models (neural networks) for bayesian optimization, staying as close to a truly bayesian treatment as possible.</s> <s>we obtain scalability through stochastic gradient hamiltonian monte carlo, whose robustness we improve via a scale adaptation.</s> <s>experiments including multi-task bayesian optimization with 21 tasks, parallel optimization of deep neural networks and deep reinforcement learning show the power and flexibility of this approach.</s></p></d>", "label": ["<d><p><s>bayesian optimization with robust bayesian neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function $\\func$.</s> <s>traditional methods for this problem assume just the availability of this single function.</s> <s>however, in many cases, cheap approximations to $\\func$ may be obtainable.</s> <s>for example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation.</s> <s>we can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of $\\func$ in a small but promising region and speedily identify the optimum.</s> <s>we formalise this task as a \\emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a gaussian process.</s> <s>we develop \\mfgpucb, a novel method based on upper confidence bound techniques.</s> <s>in our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information.</s> <s>\\mfgpucbs outperforms such naive strategies and other multi-fidelity methods  on several synthetic and real experiments.</s></p></d>", "label": ["<d><p><s>gaussian process bandit optimisation with multi-fidelity evaluations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>influence maximization in social networks has typically been studied in the context of contagion models and irreversible processes.</s> <s>in this paper, we consider an alternate model that treats individual opinions as spins in an ising system at dynamic equilibrium.</s> <s>we formalize the \\textit{ising influence maximization} problem, which has a natural physical interpretation as maximizing the magnetization given a budget of external magnetic field.</s> <s>under the mean-field (mf) approximation, we present a gradient ascent algorithm that uses the susceptibility to efficiently calculate local maxima of the magnetization, and we develop a number of sufficient conditions for when the mf magnetization is concave and our algorithm converges to a global optimum.</s> <s>we apply our algorithm on random and real-world networks, demonstrating, remarkably, that the mf optimal external fields (i.e., the external fields which maximize the mf magnetization) exhibit a phase transition from focusing on high-degree individuals at high temperatures to focusing on low-degree individuals at low temperatures.</s> <s>we also establish a number of novel results about the structure of steady-states in the ferromagnetic mf ising model on general graphs, which are of independent interest.</s></p></d>", "label": ["<d><p><s>maximizing influence in an ising network: a mean-field optimal solution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work we analyze the class prediction of parallel randomized ensembles by majority voting as an urn model.</s> <s>for a given test instance, the ensemble can be viewed as an urn of marbles of different colors.</s> <s>a marble represents an individual classifier.</s> <s>its color represents the class label prediction of the corresponding classifier.</s> <s>the sequential querying of classifiers in the ensemble can be seen as draws without replacement from the urn.</s> <s>an analysis of this classical urn model based on the hypergeometric distribution makes it possible to estimate the confidence on the outcome of majority voting when only a fraction of the individual predictions is known.</s> <s>these estimates can be used to speed up the prediction by the ensemble.</s> <s>specifically, the aggregation of votes can be halted when the confidence in the final prediction is sufficiently high.</s> <s>if one assumes a uniform prior for the distribution of possible votes the analysis is shown to be equivalent to a previous one based on dirichlet distributions.</s> <s>the advantage of the current approach is that prior knowledge on the possible vote outcomes can be readily incorporated in a bayesian framework.</s> <s>we show how incorporating this type of problem-specific knowledge into the statistical analysis of majority voting leads to faster classification by the ensemble and allows us to estimate the expected average speed-up beforehand.</s></p></d>", "label": ["<d><p><s>an urn model for majority voting in classification ensembles</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network.</s> <s>we propose a simple duality between this dense associative memory and neural networks commonly used in deep learning.</s> <s>on the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed.</s> <s>one limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime.</s> <s>on the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer.</s> <s>this family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees.</s> <s>the proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning.</s> <s>the utility of the dense memories is illustrated for two test cases: the logical gate xor and the recognition of handwritten digits from the mnist data set.</s></p></d>", "label": ["<d><p><s>dense associative memory for pattern recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a rich family of distributions that capture variable interactions significantly more expressive than those representable with low-treewidth or pairwise graphical models, or log-supermodular models.</s> <s>we call these cooperative graphical models.</s> <s>yet, this family retains structure, which we carefully exploit for efficient inference techniques.</s> <s>our algorithms combine the polyhedral structure of submodular functions in new ways with variational inference methods to obtain both lower and upper bounds on the partition function.</s> <s>while our fully convex upper bound is minimized as an sdp or via tree-reweighted belief propagation, our lower bound is tightened via belief propagation or mean-field algorithms.</s> <s>the resulting algorithms are easy to implement and, as our experiments show, effectively obtain good bounds and marginals for synthetic and real-world examples.</s></p></d>", "label": ["<d><p><s>cooperative graphical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density, including entropies and divergences.</s> <s>rather than plugging a consistent density estimate (which requires k ?</s> <s>?</s> <s>as the sample size n ?</s> <s>?)</s> <s>into the functional of interest, the estimators we consider fix k and perform a bias correction.</s> <s>this can be more efficient computationally, and, as we show, statistically, leading to faster convergence rates.</s> <s>our framework unifies several previous estimators, for most of which ours are the first finite sample guarantees.</s></p></d>", "label": ["<d><p><s>finite-sample analysis of fixed-k nearest neighbor density functional estimators</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget.</s> <s>however, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal.</s> <s>to bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing data sets.</s> <s>under this generalized dawid-skene model, we characterize the fundamental trade-off between budget and accuracy, and introduce a novel adaptive scheme that matches this fundamental limit.</s> <s>we further quantify the gain of adaptivity, by comparing the trade-off with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.</s></p></d>", "label": ["<d><p><s>achieving budget-optimality with adaptive schemes in crowdsourcing</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (gans) framework.</s> <s>using our new techniques, we achieve state-of-the-art results in semi-supervised classification on mnist, cifar-10 and svhn.</s> <s>the generated images are of high quality as confirmed by a visual turing test: our model generates mnist samples that humans cannot distinguish from real data, and cifar-10 samples that yield a human error rate of 21.3%.</s> <s>we also present imagenet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of imagenet classes.</s></p></d>", "label": ["<d><p><s>improved techniques for training gans</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers.</s> <s>in general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions.</s> <s>in this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression.</s> <s>we show that two outliers in a dataset are enough to breakdown this clustering procedure.</s> <s>however, if we focus on ?well-structured?</s> <s>datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers.</s> <s>finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.</s></p></d>", "label": ["<d><p><s>robust k-means: a theoretical revisit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has lipschitz continuous gradient as well as restricted strong convexity.</s> <s>our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems.</s> <s>we prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term.</s> <s>our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method.</s> <s>finally, numerical evidence supports the effectiveness of our method in real-world problems.</s></p></d>", "label": ["<d><p><s>stochastic three-composite convex minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the algorithmic advancement of synchronizing maps is important in order to solve a wide range of practice problems  with possible large-scale dataset.</s> <s>in this paper, we provide theoretical justifications for spectral techniques for the map synchronization problem, i.e., it takes as input a collection of objects and noisy maps estimated between pairs of objects, and outputs clean maps between all pairs of objects.</s> <s>we show that a simple normalized spectral method that projects the blocks of the top eigenvectors of a data matrix to the map space leads to surprisingly good results.</s> <s>as the noise is modelled naturally as random permutation matrix, this algorithm normspecsync leads to competing theoretical guarantees as state-of-the-art convex optimization techniques, yet it is much more efficient.</s> <s>we demonstrate the usefulness of our algorithm in a couple of applications, where it is optimal in both complexity and exactness among existing methods.</s></p></d>", "label": ["<d><p><s>normalized spectral map synchronization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities.</s> <s>knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics.</s> <s>unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data.</s> <s>moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete.</s> <s>we introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network.</s> <s>the method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.</s></p></d>", "label": ["<d><p><s>reconstructing parameters of spreading models from partial observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how do people learn about complex functional structure?</s> <s>taking inspiration from other areas of cognitive science, we propose that this is accomplished by harnessing compositionality: complex structure is decomposed into simpler building blocks.</s> <s>we formalize this idea within the framework of bayesian regression using a grammar over gaussian process kernels.</s> <s>we show that participants prefer compositional over non-compositional function extrapolations, that samples from the human prior over functions are best described by a compositional model, and that people perceive compositional functions as more predictable than their non-compositional but otherwise similar counterparts.</s> <s>we argue that the compositional nature of intuitive functions is consistent with broad principles of human cognition.</s></p></d>", "label": ["<d><p><s>probing the compositionality of intuitive functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in neuroscience, the similarity matrix of neural activity patterns in response to different sensory stimuli or under different cognitive states reflects the structure of neural representational space.</s> <s>existing methods derive point estimations of neural activity patterns from noisy neural imaging data, and the similarity is calculated from these point estimations.</s> <s>we show that this approach translates structured noise from estimated patterns into spurious bias structure in the resulting similarity matrix, which is especially severe when signal-to-noise ratio is low and experimental conditions cannot be fully randomized in a cognitive task.</s> <s>we propose an alternative bayesian framework for computing representational similarity in which we treat the covariance structure of neural activity patterns as a hyper-parameter in a generative model of the neural data, and directly estimate this covariance structure from imaging data while marginalizing over the unknown activity patterns.</s> <s>converting the estimated covariance structure into a correlation matrix offers a much less biased estimate of neural representational similarity.</s> <s>our method can also simultaneously estimate a signal-to-noise map that informs where the learned representational structure is supported more strongly, and the learned covariance matrix can be used as a structured prior to constrain bayesian estimation of neural activity patterns.</s> <s>our code is freely available in brainiak (https://github.com/intelpni/brainiak), a python toolkit for brain imaging analysis.</s></p></d>", "label": ["<d><p><s>a bayesian method for reducing bias in neural representational similarity analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the restricted isometry property (rip) for design matrices gives guarantees for optimal recovery in sparse linear models.</s> <s>it is of high interest in compressed sensing and statistical learning.</s> <s>this property is particularly important for computationally efficient recovery methods.</s> <s>as a consequence, even though it is in general np-hard to check that rip holds, there have been substantial efforts to find tractable proxies for it.</s> <s>these would allow the construction of rip matrices and the polynomial-time verification of rip given an arbitrary matrix.</s> <s>we consider the framework of average-case certifiers, that never wrongly declare that a matrix is rip, while being often correct for random instances.</s> <s>while there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime.</s> <s>our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.</s></p></d>", "label": ["<d><p><s>average-case hardness of rip certification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games.</s> <s>our property, which simply requires that each learner has small regret compared to a (1+eps)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla hedge forecaster.</s> <s>our results improve upon recent work of syrgkanis et al.</s> <s>in a number of ways.</s> <s>we require only that players observe payoffs under other players' realized actions, as opposed to expected payoffs.</s> <s>we further show that convergence occurs with high probability, and show convergence under bandit feedback.</s> <s>finally, we improve upon the speed of convergence by a factor of n, the number of players.</s> <s>both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work.</s> <s>our framework applies to dynamic population games via a low approximate regret property for shifting experts.</s> <s>here we strengthen the results of lykouris et al.</s> <s>in two ways: we allow players to select learning algorithms from a larger class, which includes a minor variant of the basic hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved.</s> <s>in the bandit setting we present a new algorithm which provides a \"small loss\"-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient.</s> <s>this result may be of independent interest.</s></p></d>", "label": ["<d><p><s>learning in games: robustness of fast convergence</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.</s> <s>we present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods.</s> <s>we present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm.</s> <s>best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.</s></p></d>", "label": ["<d><p><s>stochastic structured prediction under bandit feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character.</s> <s>in contrast, by building a hierarchy of nested subgraphs, the multiscale laplacian graph kernels (mlg kernels) that we define in this paper can account for structure at a range of different scales.</s> <s>at the heart of the mlg construction is another new graph kernel, called the feature space laplacian graph kernel (flg kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs.</s> <s>the mlg kernel applies such flg kernels to subgraphs recursively.</s> <s>to make the mlg kernel computationally feasible, we also introduce a randomized projection procedure, similar to the nystro ?m method, but for rkhs operators.</s></p></d>", "label": ["<d><p><s>the multiscale laplacian graph kernel</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task.</s> <s>then, we introduce the notion of the local stability of parametric feature mapping and  parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms.</s> <s>as an application of parameter transfer learning, we discuss the performance of sparse coding in self-taught learning.</s> <s>although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied.</s> <s>in this paper, we also provide the first theoretical learning bound for self-taught learning.</s></p></d>", "label": ["<d><p><s>learning bound for parameter transfer learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed.</s> <s>one difference with the single arm variant is that the dependency structure of the arms is crucial.</s> <s>previous works on this setting either used a worst-case approach or imposed independence of the arms.</s> <s>we introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it.</s> <s>the algorithm is based on linear regression and the analysis uses techniques from the linear bandit literature.</s> <s>by comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of arms pulled.</s></p></d>", "label": ["<d><p><s>combinatorial semi-bandit with known covariance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>calcium imaging is an important technique for monitoring the activity of thousands of neurons simultaneously.</s> <s>as calcium imaging datasets grow in size, automated detection of individual neurons is becoming important.</s> <s>here we apply a supervised learning approach to this problem and show that convolutional networks can achieve near-human accuracy and superhuman speed.</s> <s>accuracy is superior to the popular pca/ica method based on precision and recall relative to ground truth annotation by a human expert.</s> <s>these results suggest that convolutional networks are an efficient and flexible tool for the analysis of large-scale calcium imaging data.</s></p></d>", "label": ["<d><p><s>automatic neuron detection in calcium imaging data using convolutional networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>accurately measuring the similarity between text documents lies at the core of many real world applications of machine learning.</s> <s>these include web-search ranking, document recommendation, multi-lingual document matching, and article categorization.</s> <s>recently, a new document metric, the word mover's distance (wmd), has been proposed with unprecedented results on knn-based document classification.</s> <s>the wmd elevates high quality word embeddings to document metrics by formulating the distance between two documents as an optimal transport problem between the embedded words.</s> <s>however, the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available.</s> <s>in this paper we propose an efficient technique to learn a supervised metric, which we call the supervised wmd (s-wmd) metric.</s> <s>our algorithm learns document distances that measure the underlying semantic differences between documents by leveraging semantic differences between individual words discovered during supervised training.</s> <s>this is achieved with an linear transformation of the underlying word embedding space and tailored word-specific weights, learned to minimize the stochastic leave-one-out nearest neighbor classification error on a per-document level.</s> <s>we evaluate our metric on eight real-world text classification tasks on which s-wmd consistently  outperforms almost all of our 26 competitive baselines.</s></p></d>", "label": ["<d><p><s>supervised word mover's distance</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering graphs under the stochastic block model (sbm) and extensions are well studied.</s> <s>guarantees of correctness exist under the assumption that the data is sampled from a model.</s> <s>in this paper, we propose a framework, in which we obtain \"correctness\" guarantees without assuming the data comes from a model.</s> <s>the guarantees we obtain depend instead on the statistics of the data that can be checked.</s> <s>we also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.</s></p></d>", "label": ["<d><p><s>graph clustering: block-models and model free results</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present an architecture which lets us train deep, directed generative models with many layers of latent variables.</s> <s>we include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training.</s> <s>to improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution.</s> <s>these techniques permit end-to-end training of models with 10+ layers of latent variables.</s> <s>experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.</s></p></d>", "label": ["<d><p><s>an architecture for deep, hierarchical generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recommendation and collaborative filtering systems are important in modern information and e-commerce applications.</s> <s>as these systems are becoming increasingly popular in industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems.</s> <s>we introduce a data poisoning attack on collaborative filtering systems.</s> <s>we demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behaviors to avoid being detected.</s> <s>while the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks.</s> <s>we present efficient solutions for two popular factorization-based collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method.</s> <s>finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.</s></p></d>", "label": ["<d><p><s>data poisoning attacks on factorization-based collaborative filtering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new type of probabilistic model which we call dissimilarity coefficient networks (disco nets).</s> <s>disco nets allow us to efficiently sample from a posterior distribution parametrised by a neural network.</s> <s>during training, disco nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution.</s> <s>this allows us to tailor the training to the loss related to the task at hand.</s> <s>we empirically show that (i) by modeling uncertainty on the output value, disco nets outperform equivalent non-probabilistic predictive networks and (ii) disco nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.</s></p></d>", "label": ["<d><p><s>disco nets : dissimilarity coefficients networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>factorization machines (fms) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional.</s> <s>unfortunately, despite increasing interest in fms, there exists to date no efficient training algorithm for higher-order fms (hofms).</s> <s>in this paper, we present the first generic yet efficient algorithms for training arbitrary-order hofms.</s> <s>we also present new variants of hofms with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy.</s> <s>we demonstrate the proposed approaches on four different link prediction tasks.</s></p></d>", "label": ["<d><p><s>higher-order factorization machines</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the question of how to parallelize the stochastic gradient descent (sgd) method has received much attention in the literature.</s> <s>in this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information.</s> <s>in order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration.</s> <s>this can cause difficulties because l-bfgs employs gradient differences to update the hessian approximations, and when these gradients are computed using different data points the process can be unstable.</s> <s>this paper shows how to perform stable quasi-newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.</s></p></d>", "label": ["<d><p><s>a multi-batch l-bfgs method for machine learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild.</s> <s>we leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos.</s> <s>unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound.</s> <s>we propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge.</s> <s>our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification.</s> <s>visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.</s></p></d>", "label": ["<d><p><s>soundnet: learning sound representations from unlabeled video</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we unify slice sampling and hamiltonian monte carlo (hmc) sampling, demonstrating their connection via the hamiltonian-jacobi equation from hamiltonian mechanics.</s> <s>this insight enables extension of hmc and slice sampling to a broader family of samplers, called monomial gamma samplers (mgs).</s> <s>we provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the mgs draws decorrelated samples from the desired target distribution.</s> <s>we further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues.</s> <s>our theoretical results are validated with synthetic data and real-world applications.</s></p></d>", "label": ["<d><p><s>towards unifying hamiltonian monte carlo and slice sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features).</s> <s>the features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features.</s> <s>the result is a parsimonious and interpretable indication of how and where two distributions differ locally.</s> <s>an empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features.</s> <s>in real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.</s></p></d>", "label": ["<d><p><s>interpretable distribution features with maximum testing power</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the \\emph{threshold bandit} setting, a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \\emph{threshold value}.</s> <s>the learner selects one of $k$ actions (arms), this action generates a random sample from a fixed distribution, and the action then receives a unit payoff in the event that this sample exceeds the threshold value.</s> <s>we consider two versions of this problem, the \\emph{uncensored} and \\emph{censored} case, that determine whether the sample is always observed or only when the threshold is not met.</s> <s>using new tools to understand the popular ucb algorithm, we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting.</s> <s>finally we show that the censored case exhibits more challenges, but we give guarantees in the event that the sequence of threshold values is generated optimistically.</s></p></d>", "label": ["<d><p><s>threshold bandits, with and without censored feedback</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we define and study the problem of predicting the solution to a linear program (lp) given only partial information about its objective and constraints.</s> <s>this generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, that has been studied under the name ?learning from revealed preferences\".</s> <s>we give mistake bound learning algorithms in two settings: in the first, the objective of the lp is known to the learner but there is an arbitrary, fixed set of constraints which are unknown.</s> <s>each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the lp given the union of the known and unknown constraints.</s> <s>this models the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown.</s> <s>in the second setting, the objective of the lp is unknown, and changing in a controlled way.</s> <s>the constraints of the lp may also change every day, but are known.</s> <s>an example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known lp.</s></p></d>", "label": ["<d><p><s>learning from rational behavior: predicting solutions to unknown linear programs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>how does our motor system solve the problem of anticipatory control in spite of a wide spectrum of response dynamics from different musculo-skeletal systems, transport delays as well as response latencies throughout the central nervous system?</s> <s>to a great extent, our highly-skilled motor responses are a result of a reactive feedback system, originating in the brain-stem and spinal cord, combined with a feed-forward anticipatory system, that is adaptively fine-tuned by sensory experience and originates in the cerebellum.</s> <s>based on that interaction we design the counterfactual predictive control (cfpc) architecture, an anticipatory adaptive motor control scheme in which a feed-forward module, based on the cerebellum, steers an error feedback controller with counterfactual error signals.</s> <s>those are signals that trigger reactions as actual errors would, but that do not code for any current of forthcoming errors.</s> <s>in order to determine the optimal learning strategy, we derive a novel learning rule for the feed-forward module that involves an eligibility trace and operates at the synaptic level.</s> <s>in particular, our eligibility trace provides a mechanism beyond co-incidence detection in that it convolves a history of prior synaptic inputs with error signals.</s> <s>in the context of cerebellar physiology, this solution implies that purkinje cell synapses should generate eligibility traces using a forward model of the system being controlled.</s> <s>from an engineering perspective, cfpc provides a general-purpose anticipatory control architecture equipped with a learning rule that exploits the full dynamics of the closed-loop system.</s></p></d>", "label": ["<d><p><s>a forward model at purkinje cell synapses facilitates cerebellar anticipatory control</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many real phenomena, including behaviors, involve strategic interactions that can be learned from data.</s> <s>we focus on learning tree structured potential games where equilibria are represented by local maxima of an underlying potential function.</s> <s>we cast the learning problem within a max margin setting and show that the problem is np-hard even when the strategic interactions form a tree.</s> <s>we develop a variant of dual decomposition to estimate the underlying game and demonstrate with synthetic and real decision/voting data that the game theoretic perspective (carving out local maxima) enables meaningful recovery.</s></p></d>", "label": ["<d><p><s>learning tree structured potential games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>jointly characterizing neural responses in terms of several external variables promises novel insights into circuit function, but remains computationally prohibitive in practice.</s> <s>here we use gaussian process (gp) priors and exploit recent advances in fast gp inference and learning based on kronecker methods, to efficiently estimate multidimensional nonlinear tuning functions.</s> <s>our estimator require considerably less data than traditional methods and further provides principled uncertainty estimates.</s> <s>we apply these tools to hippocampal recordings during open field exploration and use them to characterize the joint dependence of ca1 responses on the position of the animal and several other variables, including the animal's speed, direction of motion, and network oscillations.our results provide an unprecedentedly detailed quantification of the tuning of hippocampal neurons.</s> <s>the model's generality suggests that our approach can be used to estimate neural response properties in other brain regions.</s></p></d>", "label": ["<d><p><s>estimating nonlinear neural response functions using gp priors and kronecker methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>abstract we describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced saga method.</s> <s>our method achieves an accelerated convergence rate on strongly convex smooth problems.</s> <s>our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums.</s> <s>additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.</s></p></d>", "label": ["<d><p><s>a simple practical accelerated method for finite sums</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a theoretical analysis of active learning with more realistic interactions with human oracles.</s> <s>previous empirical studies have shown oracles abstaining on difficult queries until accumulating enough information to make label decisions.</s> <s>we formalize this phenomenon with an ?oracle epiphany model?</s> <s>and analyze active learning query complexity under such oracles for both the realizable and the agnos- tic cases.</s> <s>our analysis shows that active learning is possible with oracle epiphany, but incurs an additional cost depending on when the epiphany happens.</s> <s>our results suggest new, principled active learning approaches with realistic oracles.</s></p></d>", "label": ["<d><p><s>active learning with oracle epiphany</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>during the past few years, the machine learning community has paid attention to developping new methods for learning from weakly labeled data.</s> <s>this field covers different settings like semi-supervised learning, learning with label proportions, multi-instance learning, noise-tolerant learning, etc.</s> <s>this paper presents a generic framework to deal with these weakly labeled scenarios.</s> <s>we introduce the beta-risk as a generalized formulation of the standard empirical risk based on surrogate margin-based loss functions.</s> <s>this risk allows us to express the reliability on the labels and to derive different kinds of learning algorithms.</s> <s>we specifically focus on svms and propose a soft margin beta-svm algorithm  which behaves better that the state of the art.</s></p></d>", "label": ["<d><p><s>beta-risk: a new surrogate risk for learning from weakly labeled data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a double thompson sampling (d-ts) algorithm for dueling bandit problems.</s> <s>as its name suggests, d-ts selects both the first and the second candidates according to thompson sampling.</s> <s>specifically, d-ts maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison according to two sets of samples independently drawn from the posterior distribution.</s> <s>this simple algorithm applies to general copeland dueling bandits, including condorcet dueling bandits as its special case.</s> <s>for general copeland dueling bandits, we show that d-ts achieves $o(k^2 \\log t)$ regret.</s> <s>moreover, using a back substitution argument, we refine the regret to $o(k \\log t + k^2 \\log \\log t)$ in condorcet dueling bandits and many practical copeland dueling bandits.</s> <s>in addition, we propose an enhancement of d-ts, referred to as d-ts+, that reduces the regret by carefully breaking ties.</s> <s>experiments based on both synthetic and real-world data demonstrate that d-ts and d-ts$^+$ significantly improve the overall performance, in terms of regret and robustness.</s></p></d>", "label": ["<d><p><s>double thompson sampling for dueling bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a class of loss functions, which we call deep perceptual similarity metrics (deepsim), allowing to generate sharp high resolution images from compressed abstract representations.</s> <s>instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks.</s> <s>this metric reflects perceptual similarity of images much better and, thus, leads to better results.</s> <s>we demonstrate two examples of use cases of the proposed loss: (1) networks that invert the alexnet convolutional network; (2) a modified version of a variational autoencoder that generates realistic high-resolution random images.</s></p></d>", "label": ["<d><p><s>generating images with perceptual similarity metrics based on deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in the recent years, a number of parameter-free algorithms have been developed for online linear optimization over hilbert spaces and for learning with expert advice.</s> <s>these algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices.</s> <s>we present a new intuitive framework to design parameter-free algorithms for both online linear optimization over hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins.</s> <s>we instantiate it using a betting algorithm based on the krichevsky-trofimov estimator.</s> <s>the resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.</s></p></d>", "label": ["<d><p><s>coin betting and parameter-free online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting.</s> <s>for example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000.</s> <s>in addition, the data is generally noisy and full of missing values.</s> <s>thus, modern applications require methods that are  highly scalable, and can deal with noisy data in terms of corruptions or missing values.</s> <s>however, classical time series methods usually fall short of handling these issues.</s> <s>in this paper, we present a temporal regularized matrix factorization  (trmf) framework which supports data-driven temporal learning and  forecasting.</s> <s>we develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values.</s> <s>our proposed trmf is highly general, and subsumes many existing approaches for time series analysis.</s> <s>we make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework.</s> <s>experimental results show the superiority of trmf in terms of scalability and prediction quality.</s> <s>in particular,  trmf is two orders of magnitude  faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as wal-mart e-commerce datasets.</s></p></d>", "label": ["<d><p><s>temporal regularized matrix factorization for high-dimensional time series prediction</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>a core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment.</s> <s>many existing methods for learning the dynamics of physical interactions require labeled object information.</s> <s>however, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical.</s> <s>to learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames.</s> <s>because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects.</s> <s>to explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects.</s> <s>in this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a \"visual imagination\" of different futures based on different courses of action.</s> <s>our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.</s></p></d>", "label": ["<d><p><s>unsupervised learning for physical interaction through video prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study active learning where the labeler can not only return incorrect labels but also abstain from labeling.</s> <s>we consider different noise and abstention conditions of the labeler.</s> <s>we propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler.</s> <s>this algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler.</s> <s>we couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.</s></p></d>", "label": ["<d><p><s>active learning from imperfect labelers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>hybrid methods that utilize both content and rating information are commonly used in many recommender systems.</s> <s>however, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough.</s> <s>to address this problem, we develop a collaborative recurrent autoencoder (crae) which is a denoising recurrent autoencoder (drae) that models the generation of content sequences in the collaborative filtering (cf) setting.</s> <s>the model generalizes recent advances in recurrent deep learning from i.i.d.</s> <s>input to non-i.i.d.</s> <s>(cf-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder.</s> <s>to do this, we first develop a hierarchical bayesian model for the drae and then generalize it to the cf setting.</s> <s>the synergy between denoising and cf enables crae to make accurate recommendations while learning to fill in the blanks in sequences.</s> <s>experiments on real-world datasets from different domains (citeulike and netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing cf for the ratings, crae is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.</s></p></d>", "label": ["<d><p><s>collaborative recurrent autoencoder: recommend while learning to fill in the blanks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>signed networks allow to model positive and negative relationships.</s> <s>we analyze existing extensions of spectral clustering to signed networks.</s> <s>it turns out that existing approaches do not recover the ground truth clustering in several situations where either the positive or the negative network structures contain no noise.</s> <s>our analysis shows that these problems arise as existing approaches take some form of arithmetic mean of the laplacians of the positive and negative part.</s> <s>as a solution we propose to use the geometric mean of the laplacians of positive and negative part and show that it outperforms the existing approaches.</s> <s>while the geometric mean of matrices is computationally expensive, we show that eigenvectors of the geometric mean can be computed efficiently, leading to a numerical scheme for sparse matrices which is of independent interest.</s></p></d>", "label": ["<d><p><s>clustering signed networks with the geometric mean of laplacians</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep learning has become a ubiquitous technology to improve machine intelligence.</s> <s>however, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power.</s> <s>in this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning.</s> <s>unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance.</s> <s>the effectiveness of our method is proved with experiments.</s> <s>without any accuracy loss, our method can efficiently compress the number of parameters in lenet-5 and alexnet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins.</s> <s>code and some models are available at https://github.com/yiwenguo/dynamic-network-surgery.</s></p></d>", "label": ["<d><p><s>dynamic network surgery for efficient dnns</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>the success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data.</s> <s>a leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts.</s> <s>assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity.</s> <s>in general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods.</s> <s>we characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels.</s> <s>these base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection.</s> <s>we apply these results by developing the weisfeiler-lehman optimal assignment kernel for graphs.</s> <s>it provides high classification accuracy on widely-used benchmark data sets improving over the original weisfeiler-lehman kernel.</s></p></d>", "label": ["<d><p><s>on valid optimal assignment kernels and applications to graph classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel extension of the encoder-decoder framework, called a review network.</s> <s>the review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider rnn decoders with both cnn and rnn encoders.</s> <s>the review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder.</s> <s>we show that conventional encoder-decoders are a special case of our framework.</s> <s>empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.</s></p></d>", "label": ["<d><p><s>review networks for caption generation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data.</s> <s>in recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise.</s> <s>we build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers.</s> <s>we prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data.</s> <s>the theoretical development and parametric and nonparametric algorithms proposed here constitute an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.</s></p></d>", "label": ["<d><p><s>estimating the class prior and posterior from noisy positives and unlabeled data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the well known maximum-entropy principle due to jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family has been very popular in machine learning due to its ?occam?s razor?</s> <s>interpretation.</s> <s>unfortunately, calculating the potentials in the maximum entropy distribution is intractable [bgs14].</s> <s>we provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments.</s> <s>we additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for ising models without any assumptions on the potentials of the model.</s> <s>more precisely, we show that we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube.</s> <s>([an06])</s></p></d>", "label": ["<d><p><s>approximate maximum entropy principles via goemans-williamson with applications to provable variational methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we initiate the study of adaptive composition in differential privacy when the length of the composition, and the privacy parameters themselves can be chosen adaptively, as a function of the outcome of previously run analyses.</s> <s>this case is much more delicate than the setting covered by existing composition theorems, in which the algorithms themselves can be chosen adaptively, but the privacy parameters must be fixed up front.</s> <s>indeed, it isn't even clear how to define differential privacy in the adaptive parameter setting.</s> <s>we proceed by defining two objects which cover the two main use cases of composition theorems.</s> <s>a privacy filter is a stopping time rule that allows an analyst to halt a computation before his pre-specified privacy budget is exceeded.</s> <s>a privacy odometer allows the analyst to track realized privacy loss as he goes, without needing to pre-specify a privacy budget.</s> <s>we show that unlike the case in which privacy parameters are fixed, in the adaptive parameter setting, these two use cases are distinct.</s> <s>we show that there exist privacy filters with bounds comparable (up to constants) with existing privacy composition theorems.</s> <s>we also give a privacy odometer that nearly matches non-adaptive private composition theorems, but is sometimes worse by a small asymptotic factor.</s> <s>moreover, we show that this is inherent, and that any valid privacy odometer in the adaptive parameter setting must lose this factor, which shows a formal separation between the filter and odometer use-cases.</s></p></d>", "label": ["<d><p><s>privacy odometers and filters: pay-as-you-go composition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study regression and classification in a setting where the learning algorithm is allowed to access only a limited number of attributes per example, known as the limited attribute observation model.</s> <s>in this well-studied model, we provide the first lower bounds giving a limit on the precision attainable by any algorithm for several variants of regression, notably linear regression with the absolute loss and the squared loss, as well as for classification with the hinge loss.</s> <s>we complement these lower bounds with a general purpose algorithm that gives an upper bound on the achievable precision limit in the setting of learning with missing data.</s></p></d>", "label": ["<d><p><s>the limits of learning with missing data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a very deep fully convolutional encoding-decoding framework for image restoration such as denoising and super-resolution.</s> <s>the network is composed of multiple layers of convolution and deconvolution operators, learning end-to-end mappings from corrupted images to the original ones.</s> <s>the convolutional layers act as the feature extractor, which capture the abstraction of image contents while eliminating noises/corruptions.</s> <s>deconvolutional layers are then used to recover the image details.</s> <s>we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum.</s> <s>first, the skip connections allow the signal to be back-propagated to bottom layers directly, and thus tackles the problem of gradient vanishing, making training deep networks easier and achieving restoration performance gains consequently.</s> <s>second, these skip connections pass image details from convolutional layers to deconvolutional layers, which is beneficial in recovering the original image.</s> <s>significantly, with the large capacity, we can handle different levels of noises using a single model.</s> <s>experimental results show that our network achieves better performance than recent state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering is a fundamental step in many information-retrieval and data-mining applications.</s> <s>detecting clusters in graphs is also a key tool for finding the community structure in social and behavioral networks.</s> <s>in many of these applications, the input graph evolves over time in a continual and decentralized manner, and, to maintain a good clustering, the clustering algorithm needs to repeatedly probe the graph.</s> <s>furthermore, there are often limitations on the frequency of such probes, either imposed explicitly by the online platform (e.g., in the case of crawling proprietary social networks like twitter) or implicitly because of resource limitations (e.g., in the case of crawling the web).</s> <s>in this paper, we study a model of clustering on evolving graphs that captures this aspect of the problem.</s> <s>our model is based on the classical stochastic block model, which has been used to assess rigorously the quality of various static clustering methods.</s> <s>in our model, the algorithm is supposed to reconstruct the planted clustering, given the ability to query for small pieces of local information about the graph, at a limited rate.</s> <s>we design and analyze clustering algorithms that work in this model, and show asymptotically tight upper and lower bounds on their accuracy.</s> <s>finally, we perform simulations, which demonstrate that our main asymptotic results hold true also in practice.</s></p></d>", "label": ["<d><p><s>community detection on evolving graphs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>kernel-based quadrature rules are becoming important in machine learning and statistics, as they achieve super-$?sqrt{n}$ convergence rates in numerical integration, and thus provide alternatives to monte carlo integration in challenging settings where integrands are expensive to evaluate or where integrands are high dimensional.</s> <s>these rules are based on the assumption that the integrand has a certain degree of smoothness, which is expressed as that the integrand belongs to a certain reproducing kernel hilbert space (rkhs).</s> <s>however, this assumption can be violated in practice (e.g., when the integrand is a black box function), and no general theory has been established for the convergence of kernel quadratures in such misspecified settings.</s> <s>our contribution is in proving that kernel quadratures can be consistent even when the integrand does not belong to the assumed rkhs, i.e.,  when the integrand is less smooth than assumed.</s> <s>specifically, we derive convergence rates that depend on the (unknown) lesser smoothness of the integrand, where the degree of smoothness is expressed via powers of rkhss or via sobolev spaces.</s></p></d>", "label": ["<d><p><s>convergence guarantees for kernel-based quadrature rules in misspecified settings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we initiate the study of the classical submodular cover (sc) problem in the data streaming model which we refer to as the streaming submodular cover (ssc).</s> <s>we show that any single pass streaming algorithm using sublinear memory in the size of the stream will fail to provide any non-trivial approximation guarantees for ssc.</s> <s>hence, we consider a relaxed version of ssc, where we only seek to find a partial cover.</s> <s>we design the first efficient bicriteria submodular cover streaming (esc-streaming) algorithm for this problem, and provide theoretical guarantees for its performance supported by numerical evidence.</s> <s>our algorithm finds solutions that are competitive with the near-optimal offline greedy algorithm despite requiring only a single pass over the data stream.</s> <s>in our numerical experiments, we evaluate the performance of esc-streaming on active set selection and large-scale graph cover problems.</s></p></d>", "label": ["<d><p><s>an efficient streaming algorithm for the submodular cover problem</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bregman divergences play a central role in the design and analysis of a range of machine learning algorithms through a handful of popular theorems.</s> <s>we present a new theorem which shows that ``bregman distortions'' (employing a potentially non-convex generator) may be exactly re-written as a scaled bregman divergence computed over transformed data.</s> <s>this property can be viewed from the standpoints of geometry (a scaled isometry with adaptive metrics) or convex optimization (relating generalized perspective transforms).</s> <s>admissible distortions include {geodesic distances} on curved manifolds and projections or gauge-normalisation.</s> <s>our theorem allows one to leverage to the wealth and convenience of bregman divergences when analysing algorithms relying on the aforementioned bregman distortions.</s> <s>we illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing  dual norm mirror descent algorithm,  and a reduction from clustering on flat manifolds to clustering on curved manifolds.</s> <s>experiments on each of these domains validate the analyses and suggest that the scaled bregman theorem might be a worthy addition to the popular handful of bregman divergence properties that have been pervasive in machine learning.</s></p></d>", "label": ["<d><p><s>a scaled bregman theorem with applications</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication.</s> <s>longitudinal data extracted from individual electronic health records (ehr) offer an exciting new way to study subtle differences in the way these diseases progress over time.</s> <s>in this paper, we focus on answering two questions that can be asked using these databases of longitudinal ehr data.</s> <s>first, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population.</s> <s>second, we want to understand how important clinical outcomes are associated with disease trajectories.</s> <s>to answer these questions, we propose the disease trajectory map (dtm), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data.</s> <s>we propose a stochastic variational inference algorithm for learning the dtm that allows the model to scale to large modern medical datasets.</s> <s>to demonstrate the dtm, we analyze data collected on patients with the complex autoimmune disease, scleroderma.</s> <s>we find that dtm learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.</s></p></d>", "label": ["<d><p><s>disease trajectory maps</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of recommending relevant labels (items) for a given data point (user).</s> <s>in particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the $f_1$ measure, \\emph{and} training data has missing labels.</s> <s>to this end, we propose a generic framework that given a performance metric $\\psi$, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive.</s> <s>we show that the regret or generalization error in the given metric $\\psi$ is bounded ultimately by estimation error of certain underlying parameters.</s> <s>in particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilabel classification, and c) pu (positive-unlabeled) learning.</s> <s>for each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing.</s> <s>our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like $f_1$ score) when compared to methods that do not model missing label information carefully.</s></p></d>", "label": ["<d><p><s>regret bounds for non-decomposable metrics with missing labels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of minimising regret in two-armed bandit problems with gaussian rewards.</s> <s>our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal.</s> <s>the results hold regardless of whether or not the difference in means between the two arms is known.</s> <s>besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense.</s> <s>furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case.</s></p></d>", "label": ["<d><p><s>on explore-then-commit strategies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks.</s> <s>however, such methods require a user-defined kernel as input.</s> <s>we extend the randomized-feature approach to the task of learning a kernel (via its associated random features).</s> <s>specifically, we present an efficient optimization problem that learns a kernel in a supervised manner.</s> <s>we prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets.</s> <s>our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.</s></p></d>", "label": ["<d><p><s>learning kernels with random features</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study the problem of learning influence functions under incomplete observations of node activations.</s> <s>incomplete observations are a major concern as most (online and real-world) social networks are not fully observable.</s> <s>we establish both proper and improper pac learnability of influence functions under randomly missing observations.</s> <s>proper pac learnability under the discrete-time linear threshold (dlt) and discrete-time independent cascade (dic) models is established by reducing incomplete observations to complete observations in a modified graph.</s> <s>our improper pac learnability result applies for the dlt and dic models as well as the continuous-time independent cascade (cic) model.</s> <s>it is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic.</s> <s>experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations.</s></p></d>", "label": ["<d><p><s>learning influence functions from incomplete observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study probability measures induced by set functions with constraints.</s> <s>such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints.</s> <s>we consider the task of rapidly sampling from such constrained measures, and develop fast markov chain samplers for them.</s> <s>our first main result is for mcmc sampling from strongly rayleigh (sr) measures, for which we present sharp polynomial bounds on the mixing time.</s> <s>as a corollary, this result yields a fast mixing sampler for determinantal point processes (dpps), yielding (to our knowledge) the first provably fast mcmc sampler for dpps since their inception over four decades ago.</s> <s>beyond sr measures, we develop mcmc samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly.</s> <s>we illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.</s></p></d>", "label": ["<d><p><s>fast mixing markov chains for strongly rayleigh measures, dpps, and constrained sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate active learning with access to two distinct oracles: label (which is standard) and search (which is not).</s> <s>the search oracle models the situation where a human searches a database to seed or counterexample an existing solution.</s> <s>search is stronger than label while being natural to implement in many situations.</s> <s>we show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over label alone.</s></p></d>", "label": ["<d><p><s>search improves label for active learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we introduce a new image representation based on a multilayer kernel machine.</s> <s>unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision.</s> <s>we proceed by first proposing improvements of the recently-introduced convolutional kernel networks (ckns) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data.</s> <s>the resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel hilbert space (rkhs).</s> <s>we show that our method achieves reasonably competitive performance for image classification on some standard ``deep learning'' datasets such as cifar-10 and svhn, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.</s></p></d>", "label": ["<d><p><s>end-to-end kernel learning with supervised convolutional kernel networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties.</s> <s>however, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits.</s> <s>in particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains.</s> <s>here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (glm).</s> <s>our approach combines the glm with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns.</s> <s>fully bayesian inference via p?lya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains.</s> <s>we demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.</s></p></d>", "label": ["<d><p><s>bayesian latent structure discovery from multi-neuron recordings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>humans learn to speak before they can read or write, so why can?t computers do the same?</s> <s>in this paper, we present a deep neural network model capable of rudimentary spoken language acquisition using untranscribed audio training data, whose only supervision comes in the form of contextually relevant visual images.</s> <s>we describe the collection of our data comprised of over 120,000 spoken audio captions for the places image dataset and evaluate our model on an image search and annotation task.</s> <s>we also provide some visualizations which suggest that our model is learning to recognize meaningful words within the caption spectrograms.</s></p></d>", "label": ["<d><p><s>unsupervised learning of spoken language with visual context</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most existing approaches to distributed sparse regression assume the data is partitioned by samples.</s> <s>however, for high-dimensional data (d >> n), it is more natural to partition the data by features.</s> <s>we propose an algorithm to distributed sparse regression when the data is partitioned by features rather than samples.</s> <s>our approach allows the user to tailor our general method to various distributed computing platforms by trading-off the total amount of data (in bits) sent over the communication network and the number of rounds of communication.</s> <s>we show that an implementation of our approach is capable of solving l1-regularized l2 regression problems with millions of features in minutes.</s></p></d>", "label": ["<d><p><s>feature-distributed sparse regression: a screen-and-clean approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of optimizing an expensive objective function when a finite budget of total evaluations is prescribed.</s> <s>in that context, the optimal solution strategy for bayesian optimization can be formulated as a dynamic programming instance.</s> <s>this results in a complex problem with uncountable, dimension-increasing state space and an uncountable control space.</s> <s>we show how to approximate the solution of this dynamic programming problem  using rollout, and propose rollout heuristics specifically designed for the bayesian optimization setting.</s> <s>we present numerical experiments showing that the resulting algorithm for optimization with a finite budget outperforms several popular bayesian optimization algorithms.</s></p></d>", "label": ["<d><p><s>bayesian optimization with a finite budget: an approximate dynamic programming approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of estimating the latent state of a spatiotemporally evolving continuous function using very few sensor measurements.</s> <s>we show that layering a dynamical systems prior over temporal evolution of weights of a kernel model is a valid approach to spatiotemporal modeling that does not necessarily require the design of complex nonstationary kernels.</s> <s>furthermore, we show that such a predictive model can be utilized to determine sensing locations that guarantee that the hidden state of the phenomena can be recovered with very few measurements.</s> <s>we provide sufficient conditions on the number and spatial location of samples required to guarantee state recovery, and provide a lower bound on the minimum number of samples required to robustly infer the hidden states.</s> <s>our approach outperforms existing methods in numerical experiments.</s></p></d>", "label": ["<d><p><s>kernel observers: systems-theoretic modeling and inference of spatiotemporally evolving processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a learner's problem of acquiring data dynamically for training a regression model, where the training data are collected from strategic data sources.</s> <s>a fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data, despite that the quality is not directly verifiable by the learner.</s> <s>in this work, we study a dynamic data acquisition process where data holders can contribute multiple times.</s> <s>using a bandit framework, we leverage on the long-term incentive of future job opportunities to incentivize high-quality contributions.</s> <s>we propose a strategic regression-upper confidence bound (sr-ucb) framework, an ucb-style index combined with a simple payment rule, where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work.</s> <s>for linear regression and certain family of non-linear regression problems, we show that sr-ucb enables a $o(\\sqrt{\\log t/t})$-bayesian nash equilibrium (bne) where each worker exerting a target effort level that the learner has chosen, with $t$ being the number of data acquisition stages.</s> <s>the sr-ucb framework also has some other desirable properties: (1) the indexes can be updated in an online fashion (hence computationally light).</s> <s>(2) a slight variant, namely private sr-ucb (psr-ucb), is able to preserve $(o(\\log^{-1} t), o(\\log^{-1} t))$-differential privacy for workers' data, with only a small compromise on incentives (achieving $o(\\log^{6} t/\\sqrt{t})$-bne).</s></p></d>", "label": ["<d><p><s>a bandit framework for strategic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>observable operator models (ooms) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems.</s> <s>they exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data.</s> <s>in this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint.</s> <s>in addition, we propose a binless extension of spectral learning for continuous data.</s> <s>in comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.</s></p></d>", "label": ["<d><p><s>spectral learning of dynamic systems from nonequilibrium data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>linkages are essentially determined by similarity measures that may be derived from multiple perspectives.</s> <s>for example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations.</s> <s>many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered.</s> <s>similarities based on these models are usually overdetermined on linkages.</s> <s>we propose a unified multi-metric learning (um2l) framework to exploit multiple types of metrics.</s> <s>in um2l, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce flexibilities for representing and utilizing both spatial and semantic linkages.</s> <s>besides, we propose a uniform solver for um2l which is guaranteed to converge.</s> <s>extensive experiments on diverse applications exhibit the superior classification performance and comprehensibility of um2l.</s> <s>visualization results also validate its ability on physical meanings discovery.</s></p></d>", "label": ["<d><p><s>what makes objects similar: a unified multi-metric learning approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>social media and social networking sites have become a global pinboard for exposition and discussion of news, topics, and ideas, where social media users often update their opinions about a particular topic by learning from the opinions shared by their friends.</s> <s>in this context, can we learn a data-driven model of opinion dynamics that is able to accurately forecast users' opinions?</s> <s>in this paper, we introduce slant, a probabilistic modeling framework of opinion dynamics, which represents users' opinions over time by means of marked jump  diffusion stochastic differential equations, and allows for efficient model simulation and parameter estimation from historical fine grained event data.</s> <s>we then leverage our framework to derive a set of efficient predictive formulas for opinion forecasting and identify conditions under which opinions converge to a steady state.</s> <s>experiments on data gathered from twitter show that our model provides a good fit to the data and our formulas achieve more accurate forecasting than alternatives.</s></p></d>", "label": ["<d><p><s>learning and forecasting opinion dynamics in social networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g.</s> <s>action classification) and video generation tasks (e.g.</s> <s>future prediction).</s> <s>we propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background.</s> <s>experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images.</s> <s>moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning.</s> <s>we believe generative video models can impact many applications in video understanding and simulation.</s></p></d>", "label": ["<d><p><s>generating videos with scene dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment.</s> <s>our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches.</s> <s>we propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.</s></p></d>", "label": ["<d><p><s>causal bandits: learning good interventions via causal inference</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of community detection or clustering in the labeled stochastic block model (lsbm) with a finite number $k$ of clusters of sizes linearly growing with the global population of items $n$.</s> <s>every pair of items is labeled independently at random, and label $\\ell$ appears with probability $p(i,j,\\ell)$ between two items in clusters indexed by $i$ and $j$, respectively.</s> <s>the objective is to reconstruct the clusters from the observation of these random labels.</s> <s>clustering under the sbm and their extensions has attracted much attention recently.</s> <s>most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters.</s> <s>we find  the set of parameters such that there exists a clustering algorithm with at most $s$ misclassified items in average under the general lsbm and for any $s=o(n)$, which solves one open problem raised in \\cite{abbe2015community}.</s> <s>we further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within $o(n \\mbox{polylog}(n))$ computations and without the a-priori knowledge of the model parameters.</s></p></d>", "label": ["<d><p><s>optimal cluster recovery in the labeled stochastic block model</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in multi-step learning, where a final learning task is accomplished via a sequence of intermediate learning tasks, the intuition is that successive steps or levels transform the initial data into representations more and more ``suited\" to the final learning task.</s> <s>a related principle arises in transfer-learning where baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner.</s> <s>the most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised, then supervised.</s> <s>several authors (castelli-cover, 1996; balcan-blum, 2005; niyogi, 2008; ben-david et al, 2008; urner et al, 2011) have analyzed ssl, with balcan-blum (2005) proposing a version of the pac learning framework augmented by a ``compatibility function\" to link concept class and unlabeled data distribution.</s> <s>we propose to analyze ssl and other multi-step learning approaches, much in the spirit of baxter's framework, by defining a learning problem generatively as a joint statistical model on $x \\times y$.</s> <s>this determines in a natural way the class of conditional distributions that are possible with each marginal, and amounts to an abstract form of compatibility function.</s> <s>it also allows to analyze both discrete and non-discrete settings.</s> <s>as tool for our analysis, we define a notion of $\\gamma$-uniform shattering for statistical models.</s> <s>we use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches.</s> <s>in particular, we recover a more general version of a result of poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups.</s></p></d>", "label": ["<d><p><s>multi-step learning and underlying structure in statistical models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner.</s> <s>recently, a general model of combinatorial partial monitoring (cpm) games was proposed \\cite{lincombinatorial2014}, where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution.</s> <s>the paper gave a confidence bound based algorithm (gcb) that achieves $o(t^{2/3}\\log t)$ distribution independent and $o(\\log t)$ distribution dependent regret bounds.</s> <s>the implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner.</s> <s>adopting their cpm model, our first contribution is a phased exploration with greedy exploitation (pege) algorithmic framework for the problem.</s> <s>different algorithms within the framework achieve $o(t^{2/3}\\sqrt{\\log t})$ distribution independent and $o(\\log^2 t)$ distribution dependent regret respectively.</s> <s>crucially, our framework needs only the simpler ``argmax'' oracle from gcb and the distribution dependent regret does not require existence of a unique optimal action.</s> <s>our second contribution is another algorithm, pege2, which combines gap estimation with a pege algorithm, to achieve an $o(\\log t)$ regret bound, matching the gcb guarantee but removing the dependence on size of  the learner's action space.</s> <s>however, like gcb, pege2 requires access to both offline oracles and the existence of a unique optimal action.</s> <s>finally, we discuss how our algorithm can be efficiently applied to a cpm problem of practical interest: namely, online ranking with feedback at the top.</s></p></d>", "label": ["<d><p><s>phased exploration with greedy exploitation in stochastic combinatorial partial monitoring games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>utilizing the structure of a probabilistic model can significantly increase its learning speed.</s> <s>motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected kl-risk.</s> <s>this choice makes smoothing, that is the careful handling of low-probability elements, paramount.</s> <s>we derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk.</s> <s>we then derive sample-complexity bounds for the global minimizer of the penalized risk and show that it is within a small factor of the optimal sample complexity.</s> <s>this framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.</s></p></d>", "label": ["<d><p><s>near-optimal smoothing of structured conditional probability matrices</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep metric learning has gained much popularity in recent years, following the success of deep learning.</s> <s>however, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update.</s> <s>in this paper, we propose to address this problem with a new metric learning objective called multi-class n-pair loss.</s> <s>the proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples ?</s> <s>more specifically, n-1 negative examples ?</s> <s>and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only n pairs of examples, instead of (n+1)?n.</s> <s>we demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.</s></p></d>", "label": ["<d><p><s>improved deep metric learning with multi-class n-pair loss objective</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we show how to estimate a model?s test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test.</s> <s>we do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family.</s> <s>we can also efficiently compute gradients of the estimated error and hence perform unsupervised discriminative learning.</s> <s>our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model.</s> <s>our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as conditional random fields.</s></p></d>", "label": ["<d><p><s>unsupervised risk estimation using only conditional independence structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a number of recent works have proposed attention models for visual question answering (vqa) that generate spatial maps highlighting image regions relevant to answering the question.</s> <s>in this paper, we argue that in addition to modeling \"where to look\" or visual attention, it is equally important to model \"what words to listen to\" or question attention.</s> <s>we present a novel co-attention model for vqa that jointly reasons about image and question attention.</s> <s>in addition, our model reasons about the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a novel 1-dimensional convolution neural networks (cnn).</s> <s>our model improves the state-of-the-art on the vqa dataset from 60.3% to 60.5%, and from 61.6% to 63.3% on the coco-qa dataset.</s> <s>by using resnet, the performance is further improved to 62.1% for vqa and 65.4% for coco-qa.</s></p></d>", "label": ["<d><p><s>hierarchical question-image co-attention for visual question answering</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we study characteristics of receptive fields of units in deep convolutional networks.</s> <s>the receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects.</s> <s>we introduce the notion of an effective receptive field size, and show that it both has a gaussian distribution and   only occupies a fraction of the full theoretical receptive field size.</s> <s>we analyze the effective receptive field in several architecture designs, and the effect of sub-sampling, skip connections, dropout and nonlinear activations on it.</s> <s>this leads to suggestions for ways to address its tendency to be too small.</s></p></d>", "label": ["<d><p><s>understanding the effective receptive field in deep convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications.</s> <s>most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously.</s> <s>however, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing.</s> <s>while existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting.</s> <s>in this paper, we propose the first provable, efficient online algorithm for matrix completion.</s> <s>our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (sgd).</s> <s>after every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime.</s> <s>our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms.</s> <s>our proofs introduce a general framework to show that sgd updates tend to stay away from saddle surfaces and could be of broader interests to other non-convex problems.</s></p></d>", "label": ["<d><p><s>provable efficient online matrix completion via non-convex stochastic gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe swapout, a new stochastic training method, that outperforms resnets of identical network structure yielding impressive results on cifar-10 and cifar-100.</s> <s>swapout samples from a rich set of architectures including dropout, stochastic depth and residual architectures as special cases.</s> <s>when viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers.</s> <s>we conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers.</s> <s>when viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth.</s> <s>we propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored.</s> <s>we show that our formulation suggests an efficient training method and validate our conclusions on cifar-10 and cifar-100 matching state of the art accuracy.</s> <s>remarkably, our 32 layer wider model performs similar to a 1001 layer resnet model.</s></p></d>", "label": ["<d><p><s>swapout: learning an ensemble of deep architectures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>understanding the 3d world is a fundamental problem in computer vision.</s> <s>however, learning a good representation of 3d objects is still an open problem due to the high dimensionality of the data and many factors of variation involved.</s> <s>in this work, we investigate the task of single-view 3d object reconstruction from a learning agent's perspective.</s> <s>we formulate the learning process as an interaction between 3d and 2d representations and propose an encoder-decoder network with a novel projection loss defined by the projective transformation.</s> <s>more importantly, the projection loss enables the unsupervised learning using 2d observation without explicit 3d supervision.</s> <s>we demonstrate the ability of the model in generating 3d volume from a single 2d image with three sets of experiments: (1) learning from single-class objects; (2) learning from multi-class objects and (3) testing on novel object classes.</s> <s>results show superior performance and better generalization ability for 3d object reconstruction when the projection loss is involved.</s></p></d>", "label": ["<d><p><s>perspective transformer nets: learning single-view 3d object reconstruction without 3d supervision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose sketched online newton (son), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data.</s> <s>son is an enhanced version of the online newton step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size.</s> <s>we further develop sparse forms of the sketching methods (such as oja's rule), making the computation linear in the sparsity of features.</s> <s>together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.</s></p></d>", "label": ["<d><p><s>efficient second order online learning by sketching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces the variational r?nyi bound (vr) that extends traditional variational inference to r?nyi's alpha-divergences.</s> <s>this new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence.</s> <s>the reparameterization trick, monte carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation.</s> <s>we further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework.</s> <s>experiments on bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the vr bound.</s></p></d>", "label": ["<d><p><s>r?nyi divergence variational inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider samples from two different data sources $\\{\\mathbf{x_s^i}\\} \\sim p_{\\rm source}$ and $\\{\\mathbf{x_t^i}\\} \\sim p_{\\rm target}$.</s> <s>we only observe their transformed versions $h(\\mathbf{x_s^i})$ and $g(\\mathbf{x_t^i})$, for some known function class $h(\\cdot)$ and $g(\\cdot)$.</s> <s>our goal is to perform a statistical test checking if $p_{\\rm source}$ = $p_{\\rm target}$ while removing the distortions induced by the transformations.</s> <s>this problem is closely related to concepts underlying numerous domain adaptation algorithms, and in our case, is motivated by the need to combine clinical and imaging based biomarkers from multiple sites and/or batches, where this problem is fairly common and an impediment in the conduct of analyses with much larger sample sizes.</s> <s>we develop a framework that addresses this problem using ideas from hypothesis testing on the transformed measurements, where in the distortions need to be estimated {\\it in tandem} with the testing.</s> <s>we derive a simple algorithm and study its convergence and consistency properties in detail, and we also provide lower-bound strategies based on recent work in continuous optimization.</s> <s>on a dataset of individuals at risk for neurological disease, our results are competitive with alternative procedures that are twice as expensive and in some cases operationally infeasible to implement.</s></p></d>", "label": ["<d><p><s>hypothesis testing in unsupervised domain adaptation with applications in alzheimer's disease</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>approximations of laplace-beltrami operators on manifolds through graph laplacians have become popular tools in data analysis and machine learning.</s> <s>these discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem.</s> <s>in this paper, we address this problem for the unormalized graph laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection.</s> <s>our approach relies on recent results by lacour and massart (2015) on the so-called lepski's method.</s></p></d>", "label": ["<d><p><s>data driven estimation of laplace-beltrami operator</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications.</s> <s>we demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models.</s> <s>for the mnist data set we obtain less than 1% test set classification error.</s> <s>we discuss an interpretation of the additional structure imparted by the tensor network to the learned model.</s></p></d>", "label": ["<d><p><s>supervised learning with tensor networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present diffusion-convolutional neural networks (dcnns), a new model for graph-structured data.</s> <s>through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification.</s> <s>dcnns have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on a gpu.</s> <s>through several experiments with real structured datasets, we demonstrate that dcnns are able to  outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.</s></p></d>", "label": ["<d><p><s>diffusion-convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we analyze the learning  properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed.</s> <s>in particular, we consider the square loss and show that    for  a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds  can be achieved by early-stopping.</s> <s>moreover, we show that larger step-sizes are allowed when considering mini-batches.</s> <s>our analysis is based on  a unifying approach, encompassing both batch and stochastic gradient methods as special cases.</s></p></d>", "label": ["<d><p><s>optimal learning for multi-pass stochastic gradient methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the parameter-space geometry of recurrent neural networks (rnns), and develop an adaptation of path-sgd optimization method, attuned to this geometry, that can learn plain rnns with relu activations.</s> <s>on several datasets that require capturing long-term dependency structure, we show that path-sgd can significantly improve trainability of relu rnns compared to rnns trained with sgd, even with various recently suggested initialization schemes.</s></p></d>", "label": ["<d><p><s>path-normalized optimization of recurrent neural networks with relu activations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a general simple structural design called ?multiplicative integration?</s> <s>(mi) to improve recurrent neural networks (rnns).</s> <s>mi changes the way of how the information flow gets integrated in the computational building block of an rnn, while introducing almost no extra parameters.</s> <s>the new structure can be easily embedded into many popular rnn models, including lstms and grus.</s> <s>we empirically analyze its learning behaviour and conduct evaluations on several tasks using different rnn models.</s> <s>our experimental results demonstrate that multiplicative integration can provide a substantial performance boost over many of the existing rnn models.</s></p></d>", "label": ["<d><p><s>on multiplicative integration with recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a general adversarial online learning problem, in which we are given a decision set x' in a reflexive banach space x and a sequence of reward vectors in the dual space of x.</s> <s>at each iteration, we choose an action from x', based on the observed sequence of previous rewards.</s> <s>our goal is to minimize regret, defined as the gap between the realized reward and the reward of the best fixed action in hindsight.</s> <s>using results from infinite dimensional convex analysis, we generalize the method of dual averaging (or follow the regularized leader) to our setting and obtain upper bounds on the worst-case regret that generalize many previous results.</s> <s>under the assumption of uniformly continuous rewards, we obtain explicit regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space s. importantly, we make no convexity assumptions on either the set s or the reward functions.</s> <s>we also prove a general lower bound on the worst-case regret for any online algorithm.</s> <s>we then apply these results to the problem of learning in repeated two-player zero-sum games on compact metric spaces.</s> <s>in doing so, we first prove that if both players play a hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of nash equilibria of the game.</s> <s>we then show that, under mild assumptions, dual averaging on the (infinite-dimensional) space of probability distributions indeed achieves hannan-consistency.</s></p></d>", "label": ["<d><p><s>minimizing regret on reflexive banach spaces and nash equilibria in continuous zero-sum games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given $iid$ observations from an unknown continuous distribution defined on some domain $\\omega$, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function.</s> <s>our density estimate is a piecewise constant function defined on a binary partition of $\\omega$.</s> <s>the key ingredient of the algorithm is to use discrepancy, a concept originates from quasi monte carlo analysis, to control the partition process.</s> <s>the resulting algorithm is simple, efficient, and has  provable convergence rate.</s> <s>we demonstrate empirically its efficiency as a density estimation method.</s> <s>we also show how it can be utilized to find good initializations for k-means.</s></p></d>", "label": ["<d><p><s>density estimation via discrepancy based adaptive sequential partition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>rapid categorization paradigms have a long history in experimental psychology: characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories.</s> <s>previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions.</s> <s>at the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures.</s> <s>but it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes.</s> <s>we have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task.</s> <s>we considered visual representations of varying complexity by analyzing the output of different stages of processing in three state-of-the-art deep networks.</s> <s>we found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages.</s> <s>overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.</s></p></d>", "label": ["<d><p><s>how deep is the feature analysis underlying rapid visual categorization?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>inference in markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications.</s> <s>in order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances.</s> <s>unfortunately this comes at the price of a serious computational bottleneck.</s> <s>in this paper we suggest to tackle consistency by incorporating constraints on beliefs.</s> <s>this permits derivation of a closed-form message-passing algorithm which we refer to as the constraints based convex belief propagation (cbcbp).</s> <s>experiments show that cbcbp outperforms the standard approach while being at least an order of magnitude faster.</s></p></d>", "label": ["<d><p><s>constraints based convex belief propagation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for testing two vector random variables for independence, we  propose testing whether the distance of one vector from an arbitrary  center point is independent from the distance of the other vector from another arbitrary center point by a univariate test.</s> <s>we prove that under minimal assumptions, it is enough to have a consistent univariate independence test on the distances, to guarantee that the power to detect dependence between the random vectors increases to one with sample size.</s> <s>if the univariate test is  distribution-free, the multivariate test will also be distribution-free.</s> <s>if we consider multiple center points and aggregate the  center-specific univariate tests, the power may be further improved, and the resulting multivariate test may be distribution-free for specific aggregation methods (if the univariate test is distribution-free).</s> <s>we show that certain multivariate tests recently proposed in the literature can be viewed as instances of this general approach.</s> <s>moreover, we show in experiments that novel tests constructed using our approach can have better power and computational time than competing approaches.</s></p></d>", "label": ["<d><p><s>multivariate tests of association based on univariate tests</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel approach to reduce memory consumption of the backpropagation through time (bptt) algorithm when training recurrent neural networks (rnns).</s> <s>our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation.</s> <s>the algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost.</s> <s>computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case.</s> <s>we provide asymptotic computational upper bounds for various regimes.</s> <s>the algorithm is particularly effective for long sequences.</s> <s>for sequences of length 1000, our algorithm saves 95\\% of memory usage while using only one third more time per iteration than the standard bptt.</s></p></d>", "label": ["<d><p><s>memory-efficient backpropagation through time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we developed task-optimized deep neural networks (dnns) that achieved state-of-the-art performance in different evaluation scenarios for automatic music tagging.</s> <s>these dnns were subsequently used to probe the neural representations of music.</s> <s>representational similarity analysis revealed the existence of a representational gradient across the superior temporal gyrus (stg).</s> <s>anterior stg was shown to be more sensitive to low-level stimulus features encoded in shallow dnn layers whereas posterior stg was shown to be more sensitive to high-level stimulus features encoded in deep dnn layers.</s></p></d>", "label": ["<d><p><s>brains on beats</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply.</s> <s>we first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model.</s> <s>we then show that any semi-markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power.</s> <s>finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.</s></p></d>", "label": ["<d><p><s>identification and overidentification of linear structural equation models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the assortment optimization problem when customer preferences follow a mixture of mallows distributions.</s> <s>the assortment optimization problem focuses on determining the revenue/profit maximizing subset of products from a large universe of products; it is an important decision that is commonly faced by retailers in determining what to offer their customers.</s> <s>there are two key challenges: (a) the mallows distribution lacks a closed-form expression (and requires summing an exponential number of terms) to compute the choice probability and, hence, the expected revenue/profit per customer; and (b) finding the best subset may require an exhaustive search.</s> <s>our key contributions are an efficiently computable closed-form expression for the choice probability under the mallows model and a compact mixed integer linear program (mip) formulation for the assortment problem.</s></p></d>", "label": ["<d><p><s>assortment optimization under the mallows model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of variational inference in probabilistic models with both log-submodular and log-supermodular higher-order potentials.</s> <s>these models can represent arbitrary distributions over binary variables, and thus generalize the commonly used pairwise markov random fields and models with log-supermodular potentials only, for which efficient approximate inference algorithms are known.</s> <s>while inference in the considered models is #p-hard in general, we present efficient approximate algorithms exploiting recent advances in the field of discrete optimization.</s> <s>we demonstrate the effectiveness of our approach in a large set of experiments, where our model allows reasoning about preferences over sets of items with complements and substitutes.</s></p></d>", "label": ["<d><p><s>variational inference in mixed probabilistic submodular models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a theoretical and algorithmic framework for multi-way graph partitioning that relies on a multiplicative cut-based objective.</s> <s>we refer to this objective as the product cut.</s> <s>we provide a detailed investigation of the mathematical properties of this objective and an effective algorithm for its optimization.</s> <s>the proposed model has strong mathematical underpinnings, and the corresponding algorithm achieves state-of-the-art performance on benchmark data sets.</s></p></d>", "label": ["<d><p><s>the product cut</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fast algorithms for nearest neighbor (nn) search have in large part focused on l2 distance.</s> <s>here we develop an approach for l1 distance that begins with an explicit and exact embedding of the points into l2.</s> <s>we show how this embedding can efficiently be combined with random projection methods for l2 nn search, such as locality-sensitive hashing or random projection trees.</s> <s>we rigorously establish the correctness of the methodology and show by experimentation that it is competitive in practice with available alternatives.</s></p></d>", "label": ["<d><p><s>an algorithm for l1 nearest neighbor search via monotonic embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the blind application of machine learning runs the risk of amplifying biases present in data.</s> <s>such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks.</s> <s>we show that even word embeddings trained on google news articles exhibit female/male gender stereotypes to a disturbing extent.</s> <s>this raises concerns because their widespread use, as we describe, often tends to amplify these biases.</s> <s>geometrically, gender bias is first shown to be captured by a direction in the word embedding.</s> <s>second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding.</s> <s>using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.</s> <s>using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks.</s> <s>the resulting embeddings can be used in applications without amplifying gender bias.</s></p></d>", "label": ["<d><p><s>man is to computer programmer as woman is to homemaker? debiasing word embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples.</s> <s>one of the most important global properties is the number of vertices/nodes in the network.</s> <s>estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis.</s> <s>in this paper we consider a population random graph g = (v;e) from the stochastic block model (sbm) with k communities/blocks.</s> <s>a sample is obtained by randomly choosing a subset w and letting g(w) be the induced subgraph in g of the vertices in w. in addition to g(w), we observe the total degree of each sampled vertex and its block membership.</s> <s>given this partial information, we propose an efficient population size estimation algorithm, called pulse, that accurately estimates the size of the whole population as well as the size of each community.</s> <s>to support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, k, and sbm model parameters on the accuracy of the estimates.</s> <s>the experimental results also demonstrate that pulse significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios.</s></p></d>", "label": ["<d><p><s>estimating the size of a large network and its communities from a random sample</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a framework for efficient inference in structured image models that explicitly reason about objects.</s> <s>we achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time.</s> <s>crucially, the model itself learns to choose the appropriate number of inference steps.</s> <s>we use this scheme to learn to perform inference in partially specified 2d models (variable-sized variational auto-encoders) and fully specified 3d models (probabilistic renderers).</s> <s>we show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3d images with various numbers of objects in a single forward pass of a neural network at unprecedented speed.</s> <s>we further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.</s></p></d>", "label": ["<d><p><s>attend, infer, repeat: fast scene understanding with generative models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a probabilistic framework for deep learning based on the deep rendering mixture model (drmm), a new generative probabilistic model that explicitly capture variations in data due to latent task nuisance variables.</s> <s>we demonstrate that max-sum inference in the drmm yields an algorithm that exactly reproduces the operations in deep convolutional neural networks (dcns), providing a first principles derivation.</s> <s>our framework provides new insights into the successes and shortcomings of dcns as well as a principled route to their improvement.</s> <s>drmm training via the expectation-maximization (em) algorithm is a powerful alternative to dcn back-propagation, and initial training results are promising.</s> <s>classification based on the drmm and other variants outperforms dcns in supervised digit classification, training 2-3x faster while achieving similar accuracy.</s> <s>moreover, the drmm is applicable to semi-supervised and unsupervised learning tasks, achieving results that are state-of-the-art in several categories on the mnist benchmark and comparable to state of the art on the cifar10 benchmark.</s></p></d>", "label": ["<d><p><s>a probabilistic framework for deep learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a method for learning treewidth-bounded bayesian networks from data sets containing thousands of variables.</s> <s>bounding the treewidth of a bayesian network greatly reduces the complexity of inferences.</s> <s>yet, being a global property of the graph, it considerably increases the difficulty of the learning process.</s> <s>our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths.</s> <s>our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.</s></p></d>", "label": ["<d><p><s>learning treewidth-bounded bayesian networks with thousands of variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms.</s> <s>one of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies.</s> <s>intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals.</s> <s>such intrinsic behaviors could eventually help the agent solve tasks posed by the environment.</s> <s>we present hierarchical-dqn (h-dqn), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning.</s> <s>a top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals.</s> <s>h-dqn allows for flexible goal specifications, such as functions over entities and relations.</s> <s>this provides an efficient space for exploration in complicated environments.</s> <s>we demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic atari game -- `montezuma's revenge'.</s></p></d>", "label": ["<d><p><s>hierarchical deep reinforcement learning: integrating temporal abstraction and intrinsic motivation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose confusions over time (cot), a novel generative framework which facilitates a multi-granular analysis of the decision making process.</s> <s>the cot not only models the confusions or error properties of individual decision makers and their evolution over time, but also allows us to obtain diagnostic insights into the collective decision making process in an interpretable manner.</s> <s>to this end, the cot models the confusions of the decision makers and their evolution over time via time-dependent confusion matrices.</s> <s>interpretable insights are obtained by grouping similar decision makers (and items being judged) into clusters and representing each such cluster with an appropriate prototype and identifying the most important features characterizing the cluster via a subspace feature indicator vector.</s> <s>experimentation with real world data on bail decisions, asthma treatments, and insurance policy approval decisions demonstrates that cot can accurately model and explain the confusions of decision makers and their evolution over time.</s></p></d>", "label": ["<d><p><s>confusions over time: an interpretable bayesian model to characterize trends in decision making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel hilbert space (rkhs) embedding of the bayesian posterior distribution.</s> <s>this equivalence provides a new understanding of kernel bayesian inference.</s> <s>moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel bayes' rule.</s> <s>this regularization coincides with a former thresholding approach used in kernel pomdps whose consistency remains to be established.</s> <s>our theoretical work solves this open problem and provides consistency analysis in regression settings.</s> <s>based on our optimizational formulation, we propose a flexible bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level.</s> <s>we apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines.</s></p></d>", "label": ["<d><p><s>kernel bayesian inference with posterior regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of maximizing a function that is approximately submodular under a cardinality constraint.</s> <s>approximate submodularity implicitly appears in a wide range of applications as in many cases errors in evaluation of a submodular function break submodularity.</s> <s>say that $f$ is $\\eps$-approximately submodular if there exists a submodular function $f$ such that $(1-\\eps)f(s) \\leq f(s)\\leq (1+\\eps)f(s)$ for all subsets $s$.</s> <s>we are interested in characterizing the query-complexity of maximizing $f$ subject to a cardinality constraint $k$ as a function of the error level $\\eps > 0$.</s> <s>we provide both lower and upper bounds: for $\\eps > n^{-1/2}$ we show an exponential query-complexity lower bound.</s> <s>in contrast, when $\\eps < {1}/{k}$ or under a stronger bounded curvature assumption, we give constant approximation algorithms.</s></p></d>", "label": ["<d><p><s>maximization of approximately submodular functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>models for collecting and aggregating categorical data on crowdsourcing platforms typically fall into two broad categories: those assuming agents honest and consistent but with heterogeneous error rates, and those assuming agents strategic and seek to maximize their expected reward.</s> <s>the former often leads to tractable aggregation of elicited data, while the latter usually focuses on optimal elicitation and does not consider aggregation.</s> <s>in this paper, we develop a bayesian model, wherein agents have differing quality of information, but also respond to incentives.</s> <s>our model generalizes both categories and enables the joint exploration of optimal elicitation and aggregation.</s> <s>this model enables our exploration, both analytically and experimentally, of optimal aggregation of categorical data and optimal multiple-choice interface design.</s></p></d>", "label": ["<d><p><s>eliciting categorical data for optimal aggregation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the optimization problem behind neural networks is highly non-convex.</s> <s>training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum.</s> <s>in contrast we show under quite weak assumptions on the data that a particular class of feedforward  neural networks can be trained globally optimal with a linear convergence rate.</s> <s>up to our knowledge this is the first practically feasible method which achieves such a guarantee.</s> <s>while the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one- and two hidden layer networks.</s> <s>our experiments confirms that these models are already rich enough to achieve good performance on a series of real-world datasets.</s></p></d>", "label": ["<d><p><s>globally optimal training of generalized polynomial neural networks with nonlinear spectral methods</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced.</s> <s>the proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing.</s> <s>moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm.</s> <s>numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.</s></p></d>", "label": ["<d><p><s>joint quantile regression in vector-valued rkhss</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the mixed linear regression (mlr) problem, where the goal is to recover multiple underlying linear models from their unlabeled linear measurements.</s> <s>we propose a non-convex objective function which we show is {\\em locally strongly convex} in the neighborhood of the ground truth.</s> <s>we use a tensor method for initialization so that the initial models are in the local strong convexity region.</s> <s>we then employ general convex optimization algorithms to minimize the objective function.</s> <s>to the best of our knowledge, our approach provides first exact recovery guarantees for the mlr problem with $k \\geq 2$ components.</s> <s>moreover,  our method has near-optimal computational complexity $\\tilde o (nd)$ as well as near-optimal sample complexity $\\tilde o (d)$ for {\\em constant} $k$.</s> <s>furthermore, we show that our non-convex formulation can be extended to solving the {\\em subspace clustering} problem as well.</s> <s>in particular, when initialized within a small constant distance to the true subspaces, our method converges to the global optima (and recovers true subspaces) in time {\\em linear} in the number of points.</s> <s>furthermore, our empirical results indicate that even with random initialization, our approach converges to the global optima in linear time, providing speed-up of up to two orders of magnitude.</s></p></d>", "label": ["<d><p><s>mixed linear regression with multiple components</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent neural networks (rnns) stand at the forefront of many recent developments in deep learning.</s> <s>yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers.</s> <s>recent results at the intersection of bayesian modelling and deep learning offer a bayesian interpretation of common deep learning techniques such as dropout.</s> <s>this grounding of dropout in approximate bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with rnn models.</s> <s>we apply this new variational inference based dropout technique in lstm and gru models, assessing it on language modelling and sentiment analysis tasks.</s> <s>the new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the penn treebank (73.4 test perplexity).</s> <s>this extends our arsenal of variational tools in deep learning.</s></p></d>", "label": ["<d><p><s>a theoretically grounded application of dropout in recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider cooperative multi-agent consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate.</s> <s>the objective is to minimize the sum of agent-specific composite convex functions over agent-specific private conic constraint sets; hence, the optimal consensus decision should lie in the intersection of these private sets.</s> <s>we provide convergence rates in sub-optimality, infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.</s></p></d>", "label": ["<d><p><s>a primal-dual method for conic constrained distributed optimization problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the correlation between events is ubiquitous and important for temporal events modelling.</s> <s>in many cases, the correlation exists between not only events' emitted observations, but also their arrival times.</s> <s>state space models (e.g., hidden markov model) and stochastic interaction point process models (e.g., hawkes process) have been studied extensively yet separately for the two types of correlations in the past.</s> <s>in this paper, we propose a bayesian nonparametric approach that considers both types of correlations via unifying and generalizing hidden semi-markov model and interaction point process model.</s> <s>the proposed approach can simultaneously model both the observations and arrival times of temporal events, and determine the number of latent states from data.</s> <s>a metropolis-within-particle-gibbs sampler with ancestor resampling is developed for efficient posterior inference.</s> <s>the approach is tested on both synthetic and real-world data with promising outcomes.</s></p></d>", "label": ["<d><p><s>infinite hidden semi-markov modulated interaction point process</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>whole-brain neural connectivity data are now available from viral tracing experiments, which reveal the connections between a source injection site and elsewhere in the brain.</s> <s>these hold the promise of revealing spatial patterns of connectivity throughout the mammalian brain.</s> <s>to achieve this goal, we seek to fit a weighted, nonnegative adjacency matrix among 100 ?m brain ?voxels?</s> <s>using viral tracer data.</s> <s>despite a multi-year experimental effort, injections provide incomplete coverage, and the number of voxels in our data is orders of magnitude larger than the number of injections, making the problem severely underdetermined.</s> <s>furthermore, projection data are missing within the injection site because local connections there are not separable from the injection signal.</s> <s>we use a novel machine-learning algorithm to meet these challenges and develop a spatially explicit, voxel-scale connectivity map of the mouse visual system.</s> <s>our method combines three features: a matrix completion loss for missing data, a smoothing spline penalty to regularize the problem, and (optionally) a low rank factorization.</s> <s>we demonstrate the consistency of our estimator using synthetic data and then apply it to newly available allen mouse brain connectivity atlas data for the visual system.</s> <s>our algorithm is significantly more predictive than current state of the art approaches which assume regions to be homogeneous.</s> <s>we demonstrate the efficacy of a low rank version on visual cortex data and discuss the possibility of extending this to a whole-brain connectivity matrix at the voxel scale.</s></p></d>", "label": ["<d><p><s>high resolution neural connectivity from incomplete tracing data using nonnegative spline regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled *with* replacement.</s> <s>in contrast, sampling *without* replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better.</s> <s>in this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data.</s> <s>moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes.</s> <s>our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.</s></p></d>", "label": ["<d><p><s>without-replacement sampling for stochastic gradient methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an intriguing discovery related to random fourier features: replacing multiplication by a random gaussian matrix with multiplication by a properly scaled random orthogonal matrix significantly decreases kernel approximation error.</s> <s>we call this technique orthogonal random features (orf), and provide theoretical and empirical justification for its effectiveness.</s> <s>motivated by the discovery, we further propose structured orthogonal random features (sorf), which uses a class of structured discrete orthogonal matrices to speed up the computation.</s> <s>the method reduces the time cost from $\\mathcal{o}(d^2)$ to $\\mathcal{o}(d \\log d)$, where $d$ is the data dimensionality, with almost no compromise in kernel approximation quality compared to orf.</s> <s>experiments on several datasets verify the effectiveness of orf and sorf over the existing methods.</s> <s>we also provide discussions on using the same type of discrete orthogonal structure for a broader range of kernels and applications.</s></p></d>", "label": ["<d><p><s>orthogonal random features</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a task of predicting y from x, a loss function l, and a set of probability distributions gamma on (x,y), what is the optimal decision rule minimizing the worst-case expected loss over gamma?</s> <s>in this paper, we address this question by introducing a generalization of the maximum entropy principle.</s> <s>applying this principle to sets of distributions with marginal on x constrained to be the empirical marginal, we provide a minimax interpretation of the maximum likelihood problem over generalized linear models, which connects the minimax problem for each loss function to a generalized linear model.</s> <s>while in some cases such as quadratic and logarithmic loss functions we revisit well-known linear and logistic regression models, our approach reveals novel models for other loss functions.</s> <s>in particular, for the 0-1 loss we derive a classification approach which we call the minimax svm.</s> <s>the minimax svm minimizes the worst-case expected 0-1 loss over the proposed gamma by solving a tractable optimization problem.</s> <s>we perform several numerical experiments in all of which the minimax svm outperforms the svm.</s></p></d>", "label": ["<d><p><s>a minimax approach to supervised learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions.</s> <s>parameters of the model are usually learned by minimizing the kullback-leibler (kl) divergence from training samples to the learned model.</s> <s>we propose in this work a novel approach for boltzmann machine training which assumes that a meaningful metric between observations is given.</s> <s>this metric can be represented by the wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters.</s> <s>minimization of this new objective leads to generative models with different statistical properties.</s> <s>we demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role.</s></p></d>", "label": ["<d><p><s>wasserstein training of restricted boltzmann machines</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>abstract subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints---i.e.</s> <s>decision-makers are bounded in their rationality.</s> <s>here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory.</s> <s>we systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations.</s> <s>we found that our bounded-rational model accounts well for the data.</s> <s>the decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits.</s> <s>our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations.</s></p></d>", "label": ["<d><p><s>human decision-making under limited time</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose to prune a random forest (rf) for resource-constrained prediction.</s> <s>we first construct a rf and then prune it to optimize expected feature cost & accuracy.</s> <s>we pose pruning rfs as a novel 0-1 integer program with linear constraints that encourages feature re-use.</s> <s>we establish total unimodularity of the constraint set to prove that the corresponding lp relaxation solves the original integer program.</s> <s>we then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets.</s> <s>in contrast to our bottom-up approach, which benefits from good rf initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics.</s> <s>empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.</s></p></d>", "label": ["<d><p><s>pruning random forests for prediction on a budget</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image.</s> <s>we propose back-propagating one step further---to learn camera sensor designs jointly with networks that carry out inference on the images they capture.</s> <s>in this paper, we specifically consider the design and inference problems in a typical color camera---where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image.</s> <s>we learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location.</s> <s>these weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image.</s> <s>our network achieves significant improvements in accuracy over the traditional bayer pattern used in most color cameras.</s> <s>it automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements.</s></p></d>", "label": ["<d><p><s>learning sensor multiplexing design through back-propagation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>principal components analysis~(pca) is the optimal linear  encoder of data.</s> <s>sparse linear encoders (e.g., sparse pca) produce more interpretable features that  can promote better generalization.</s> <s>(\\rn{1}) given a level of sparsity, what is the best approximation to pca?</s> <s>(\\rn{2}) are there efficient algorithms which can achieve this optimal  combinatorial tradeoff?</s> <s>we answer both questions by  providing the first polynomial-time algorithms to construct \\emph{optimal} sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.</s></p></d>", "label": ["<d><p><s>optimal sparse linear encoders and sparse pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the sampling-based planning problem in markov decision processes (mdps) that we can access only through a generative model, usually referred to as monte-carlo planning.</s> <s>our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e.</s> <s>the sample complexity.</s> <s>we propose a new algorithm, trailblazer, able to handle mdps with a finite or an infinite number of transitions from state-action to next states.</s> <s>trailblazer is an adaptive algorithm that exploits possible structures of the mdp by exploring only a subset of states reachable by following near-optimal policies.</s> <s>we provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states.</s> <s>the algorithm behavior can be considered as an extension of monte-carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states).</s> <s>finally, another appealing feature of trailblazer is that it is simple to implement and computationally efficient.</s></p></d>", "label": ["<d><p><s>blazing the trails before beating the path: sample-efficient monte-carlo planning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive.</s> <s>one approach circumventing this cost is training models on synthetic data where annotations are provided automatically.</s> <s>despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied.</s> <s>existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted.</s> <s>however, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain.</s> <s>we hypothesize that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features.</s> <s>inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains.</s> <s>our model is trained to not only perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains.</s> <s>our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces  visualizations of the private and shared representations enabling interpretation of the domain adaptation process.</s></p></d>", "label": ["<d><p><s>domain separation networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>maximum mean discrepancy (mmd) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding.</s> <s>in this paper, we present conditional generative moment-matching networks (cgmmn), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (cmmd) criterion.</s> <s>the learning is performed by stochastic gradient descent with the gradient calculated by back-propagation.</s> <s>we evaluate cgmmn on a wide range of tasks, including predictive modeling, contextual generation, and bayesian dark knowledge, which distills knowledge from a bayesian model by learning a relatively small cgmmn student network.</s> <s>our results demonstrate competitive performance in all the tasks.</s></p></d>", "label": ["<d><p><s>conditional generative moment-matching networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many machine learning applications involve jointly predicting multiple mutually dependent output variables.</s> <s>learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space.</s> <s>although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward.</s> <s>in this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler.</s> <s>altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time.</s> <s>we demonstrate the feasibility of our approach on multiple joint prediction tasks.</s> <s>in all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.</s></p></d>", "label": ["<d><p><s>a credit assignment compiler for joint prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>offline handwriting recognition systems require cropped text line images for both training and recognition.</s> <s>on the one hand, the annotation of position and transcript at line level is costly to obtain.</s> <s>on the other hand, automatic line segmentation algorithms are prone to errors, compromising the subsequent recognition.</s> <s>in this paper, we propose a modification of the popular and efficient multi-dimensional long short-term memory recurrent neural networks (mdlstm-rnns) to enable end-to-end processing of handwritten paragraphs.</s> <s>more particularly, we replace the collapse layer transforming the two-dimensional representation into a sequence of predictions by a recurrent version which can select one line at a time.</s> <s>in the proposed model, a neural network performs a kind of implicit line segmentation by computing attention weights on the image representation.</s> <s>the experiments on paragraphs of rimes and iam databases yield results that are competitive with those of networks trained at line level, and constitute a significant step towards end-to-end transcription of full documents.</s></p></d>", "label": ["<d><p><s>joint line segmentation and transcription for end-to-end handwritten paragraph recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recognizing facial action units (aus) from spontaneous facial expressions is still a challenging problem.</s> <s>most recently, cnns have shown promise on facial au recognition.</s> <s>however, the learned cnns are often overfitted and do not generalize well to unseen subjects due to limited au-coded training images.</s> <s>we proposed a novel incremental boosting cnn (ib-cnn) to integrate boosting into the cnn via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches.</s> <s>in addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the ib-cnn.</s> <s>experimental results on four benchmark au databases have demonstrated that the ib-cnn yields significant improvement over the traditional cnn and the boosting cnn without incremental learning, as well as outperforming the state-of-the-art cnn-based methods in au recognition.</s> <s>the improvement is more impressive for the aus that have the lowest frequencies in the databases.</s></p></d>", "label": ["<d><p><s>incremental boosting convolutional neural network for facial action unit recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational approaches are often used to approximate intractable posteriors or normalization constants in hierarchical latent variable models.</s> <s>while often effective in practice, it is known that the approximation error can be arbitrarily large.</s> <s>we propose a new class of bounds on the marginal log-likelihood of directed latent variable models.</s> <s>our approach relies on random projections to simplify the posterior.</s> <s>in contrast to standard variational methods, our bounds are guaranteed to be tight with high probability.</s> <s>we provide a new approach for learning latent variable models based on optimizing our new bounds on the log-likelihood.</s> <s>we demonstrate empirical improvements on benchmark datasets in vision and language for sigmoid belief networks, where a neural network is used to approximate the posterior.</s></p></d>", "label": ["<d><p><s>variational bayes on monte carlo steroids</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains.</s> <s>however, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters.</s> <s>in this paper, we propose learning from the feature-level supervision.</s> <s>we show that this kind of supervision can be easily obtained in the form of perception vectors in many applications.</s> <s>then we present novel algorithms, called perception embedded (pe) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user.</s> <s>extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of pe empirically.</s></p></d>", "label": ["<d><p><s>learning user perceived clusters with feature-level supervision</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>our method aims at reasoning over natural language questions and visual images.</s> <s>given a natural language question about an image, our model updates the question representation iteratively by selecting image regions relevant to the query and learns to give the correct answer.</s> <s>our model contains several reasoning layers, exploiting complex visual relations in the visual question answering (vqa) task.</s> <s>the proposed network is end-to-end trainable through back-propagation, where its weights are initialized using pre-trained convolutional neural network (cnn) and gated recurrent unit (gru).</s> <s>our method is evaluated on challenging datasets of coco-qa and vqa and yields state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>visual question answering with question representation update (qru)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider empirical risk minimization for large-scale datasets.</s> <s>we introduce ada newton as an adaptive algorithm that uses newton's method with adaptive sample sizes.</s> <s>the main idea of ada newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set.</s> <s>this allows to exploit the quadratic convergence property of newton's method and reach the statistical accuracy of each training set with only one iteration of newton's method.</s> <s>we show theoretically that we can iteratively increase the sample size while applying single newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk.</s> <s>in particular, we can double the size of the training set in each iteration when the number of samples is sufficiently large.</s> <s>numerical experiments on various datasets confirm the possibility of increasing the sample size by factor 2 at each iteration which implies that ada newton achieves the statistical accuracy of the full training set with about two passes over the dataset.</s></p></d>", "label": ["<d><p><s>adaptive newton method for empirical risk minimization to statistical accuracy</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations.</s> <s>towards this goal, we propose a clustering based regularization that encourages parsimonious representations.</s> <s>our k-means style objective is easy to optimize and flexible  supporting various forms of clustering, including sample and spatial clustering as well as co-clustering.</s> <s>we demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization and zero-shot learning.</s></p></d>", "label": ["<d><p><s>learning deep parsimonious representations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a long-term goal of machine learning research is to build an intelligent dialog agent.</s> <s>most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation).</s> <s>this kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication.</s> <s>in this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation.</s> <s>we study this setup in two domains: the babi dataset of (weston et al., 2015) and large-scale question answering from (dodge et al., 2015).</s> <s>we evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response.</s> <s>in particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.</s></p></d>", "label": ["<d><p><s>dialog-based language learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matrix completion methods can benefit from side information besides the partially observed matrix.</s> <s>the use of side features describing the row and column entities of a matrix has been shown to reduce the sample complexity for completing the matrix.</s> <s>we propose a novel sparse formulation that explicitly models the interaction between the row and column side features to approximate the matrix entries.</s> <s>unlike early methods, this model does not require the low-rank condition on the model parameter matrix.</s> <s>we prove that when the side features can span the latent feature space of the matrix to be recovered, the number of observed entries needed for an exact recovery is $o(\\log n)$ where $n$ is the size of the matrix.</s> <s>when the side features are corrupted latent features of the matrix with a small perturbation, our method can achieve an $\\epsilon$-recovery with $o(\\log n)$ sample complexity, and maintains a $\\o(n^{3/2})$ rate similar to classfic methods with no side information.</s> <s>an efficient linearized lagrangian algorithm is developed with a strong guarantee of convergence.</s> <s>empirical results show that our approach outperforms three state-of-the-art methods both in simulations and on real world datasets.</s></p></d>", "label": ["<d><p><s>a sparse interactive model for matrix completion with side information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions.</s> <s>several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently.</s> <s>though promising, existing approaches can still be greatly improved in terms of accuracy and scalability.</s> <s>we present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time.</s> <s>additionally, we propose a method to score causal predictions based on their confidence.</s> <s>crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge.</s> <s>we prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-of-the-art on synthetic data, achieving a speedup of several orders of magnitude.</s> <s>we illustrate its practical feasibility by applying it on a challenging protein data set.</s></p></d>", "label": ["<d><p><s>ancestral causal inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we describe a convergence acceleration technique for generic optimization problems.</s> <s>our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method.</s> <s>the weights in this average are computed via a simple and small linear system, whose solution can be updated online.</s> <s>this acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running.</s> <s>numerical experiments are detailed on classical classification problems.</s></p></d>", "label": ["<d><p><s>regularized nonlinear acceleration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in online convex optimization it is well known that certain subclasses of objective functions are much easier than arbitrary convex functions.</s> <s>we are interested in designing adaptive methods that can automatically get fast rates in as many such subclasses as possible, without any manual tuning.</s> <s>previous adaptive methods are able to interpolate between strongly convex and general convex functions.</s> <s>we present a new method, metagrad, that adapts to a much broader class of functions, including exp-concave and strongly convex functions, but also various types of stochastic and non-stochastic functions without any curvature.</s> <s>for instance, metagrad can achieve logarithmic regret on the unregularized hinge loss, even though it has no curvature, if the data come from a favourable probability distribution.</s> <s>metagrad's main feature is that it simultaneously considers multiple learning rates.</s> <s>unlike all previous methods with provable regret guarantees, however, its learning rates are not monotonically decreasing over time and are not tuned based on a theoretically derived bound on the regret.</s> <s>instead, they are weighted directly proportional to their empirical performance on the data using a tilted exponential weights master algorithm.</s></p></d>", "label": ["<d><p><s>metagrad: multiple learning rates in online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dynamic time warping (dtw) is a fundamental technique in time series analysis for comparing one curve to another using a flexible time-warping function.</s> <s>however, it was designed to compare a single pair of curves.</s> <s>in many applications, such as in metabolomics and image series analysis, alignment is simultaneously needed for multiple pairs.</s> <s>because the underlying warping functions are often related, independent application of dtw to each pair is a sub-optimal solution.</s> <s>yet, it is largely unknown how to efficiently conduct a joint alignment with all warping functions simultaneously considered, since any given warping function is constrained by the others and dynamic programming cannot be applied.</s> <s>in this paper, we show that the joint alignment problem can be transformed into a network flow problem and thus can be exactly and efficiently solved by the max flow algorithm, with a guarantee of global optimality.</s> <s>we name the proposed approach graphical time warping (gtw), emphasizing the graphical nature of the solution and that the dependency structure of the warping functions can be represented by a graph.</s> <s>modifications of dtw, such as windowing and weighting, are readily derivable within gtw.</s> <s>we also discuss optimal tuning of parameters and hyperparameters in gtw.</s> <s>we illustrate the power of gtw using both synthetic data and a real case study of an astrocyte calcium movie.</s></p></d>", "label": ["<d><p><s>graphical time warping for joint alignment of multiple curves</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many practical perception systems exist within larger processes which often include interactions with users or additional components that are capable of evaluating the quality of predicted solutions.</s> <s>in these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction.</s> <s>in this work, we pose the task of producing multiple outputs as a learning problem over an ensemble of deep networks -- introducing a novel stochastic gradient descent based approach to minimize the loss with respect to an oracle.</s> <s>our method is simple to implement, agnostic to both architecture and loss function, and parameter-free.</s> <s>our approach achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures.</s> <s>we also show qualitatively that solutions produced from our approach often provide interpretable representations of task ambiguity.</s></p></d>", "label": ["<d><p><s>stochastic multiple choice learning for training diverse deep ensembles</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements.</s> <s>with noisy measurements we show all local minima are very close to a global optimum.</s> <s>together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\\em  from random initialization}.</s></p></d>", "label": ["<d><p><s>global optimality of local search for low rank matrix recovery</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values.</s> <s>our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings.</s> <s>thus, attempts to closely match the recorded scores may lead to overfitting and impair generalization performance.</s> <s>instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low--rank parameters.</s> <s>besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations.</s> <s>one consequence is that  the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (dag), generalizing standard techniques that can only fit preference scores.</s> <s>despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a $\\log$ factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion.</s> <s>we further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain--regions and cognitive neuroscience terms.</s></p></d>", "label": ["<d><p><s>preference completion from partial rankings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>digital crowdsourcing (cs) is a modern approach to perform certain large projects using small contributions of a large crowd.</s> <s>in cs, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels.</s> <s>the crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project.</s> <s>in this work, the cs problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework.</s> <s>the purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget.</s> <s>the results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels.</s> <s>we also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.</s></p></d>", "label": ["<d><p><s>fundamental limits of budget-fidelity trade-off in label crowdsourcing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper presents generalized correspondence-lda (gc-lda), a generalization of the correspondence-lda model that allows for variable spatial representations to be associated with topics, and increased flexibility in terms of the strength of the correspondence between data types induced by the model.</s> <s>we present three variants of gc-lda, each of which associates topics with a different spatial representation, and apply them to a corpus of neuroimaging data.</s> <s>in the context of this dataset, each topic corresponds to a functional brain region, where the region's spatial extent is captured by a probability distribution over neural activity, and the region's cognitive function is captured by a probability distribution over linguistic terms.</s> <s>we illustrate the qualitative improvements offered by gc-lda in terms of the types of topics extracted with alternative spatial representations, as well as the model's ability to incorporate a-priori knowledge from the neuroimaging literature.</s> <s>we furthermore demonstrate that the novel features of gc-lda improve predictions for missing data.</s></p></d>", "label": ["<d><p><s>generalized correspondence-lda models (gc-lda) for identifying functional regions in the brain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational autoencoders are powerful models for unsupervised learning.</s> <s>however deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models.</s> <s>we propose a new inference model, the ladder variational autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed ladder network.</s> <s>we show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered variational autoencoders and other generative models.</s> <s>we provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables.</s> <s>finally, we observe that batch-normalization and deterministic warm-up (gradually turning on the kl-term) are crucial for training variational models with many stochastic layers.</s></p></d>", "label": ["<d><p><s>ladder variational autoencoders</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic inference serves as a popular model for neural processing.</s> <s>it is still unclear, however, how approximate probabilistic inference can be accurate and scalable to very high-dimensional continuous latent spaces.</s> <s>especially as typical posteriors for sensory data can be expected to exhibit complex latent dependencies including multiple modes.</s> <s>here, we study an approach that can efficiently be scaled while maintaining a richly structured posterior approximation under these conditions.</s> <s>as example model we use spike-and-slab sparse coding for v1 processing, and combine latent subspace selection with gibbs sampling (select-and-sample).</s> <s>unlike factored variational approaches, the method can maintain large numbers of posterior modes and complex latent dependencies.</s> <s>unlike pure sampling, the method is scalable to very high-dimensional latent spaces.</s> <s>among all sparse coding approaches with non-trivial posterior approximations (map or ica-like models), we report the largest-scale results.</s> <s>in applications we firstly verify the approach by showing competitiveness in standard denoising benchmarks.</s> <s>secondly, we use its scalability to, for the first time, study highly-overcomplete settings for v1 encoding using sophisticated posterior representations.</s> <s>more generally, our study shows that very accurate probabilistic inference for multi-modal posteriors with complex dependencies is tractable, functionally desirable and consistent with models for neural inference.</s></p></d>", "label": ["<d><p><s>select-and-sample for spike-and-slab sparse coding</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms.</s> <s>the new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight.</s> <s>besides this we prove two impossibility results.</s> <s>first, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case.</s> <s>second, the regret cannot scale with the effective range of the losses.</s> <s>in contrast, both results are possible in the full-information setting.</s></p></d>", "label": ["<d><p><s>refined lower bounds for adversarial bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep convolutional neural networks (cnn) have achieved great success.</s> <s>on the other hand, modeling structural information has been proved critical in many vision problems.</s> <s>it is of great interest to integrate them effectively.</s> <s>in a classical neural network, there is no message passing between neurons in the same layer.</s> <s>in this paper, we propose a crf-cnn framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation.</s> <s>a message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way.</s> <s>such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks.</s> <s>finally, a neural network implementation of end-to-end learning crf-cnn is provided.</s> <s>its effectiveness is demonstrated through experiments on two benchmark datasets.</s></p></d>", "label": ["<d><p><s>crf-cnn: modeling structured information in human pose estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural networks (nn) have achieved state-of-the-art performance in various applications.</s> <s>unfortunately in applications where training data is insufficient, they are often prone to overfitting.</s> <s>one effective way to alleviate this problem is to exploit the bayesian approach by using bayesian neural networks (bnn).</s> <s>another shortcoming of nn is the lack of flexibility to customize different distributions for the weights and neurons according to the data, as is often done in probabilistic graphical models.</s> <s>to address these problems, we propose a class of probabilistic neural networks, dubbed natural-parameter networks (npn), as a novel and lightweight bayesian treatment of nn.</s> <s>npn allows the usage of arbitrary exponential-family distributions to model the weights and neurons.</s> <s>different from traditional nn and bnn, npn takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions.</s> <s>as a bayesian treatment, efficient backpropagation (bp) is performed to learn the natural parameters for the distributions over both the weights and neurons.</s> <s>the output distributions of each layer, as byproducts, may be used as second-order representations for the associated tasks such as link prediction.</s> <s>experiments on real-world datasets show that npn can achieve state-of-the-art performance.</s></p></d>", "label": ["<d><p><s>natural-parameter networks: a class of probabilistic neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the effectiveness of neural sequence models for premise selection in automated theorem proving, a key bottleneck for progress in formalized mathematics.</s> <s>we propose a two stage approach for this task that yields good results for the premise selection task on the mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models.</s> <s>to our knowledge, this is the first time deep learning has been applied  theorem proving on a large scale.</s></p></d>", "label": ["<d><p><s>deepmath - deep sequence models for premise selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval.</s> <s>this typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved.</s> <s>much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori.</s> <s>this gives reasonable results but is quite suboptimal.</s> <s>recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal.</s> <s>we propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates.</s> <s>this closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other.</s> <s>the resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function.</s> <s>compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets.</s> <s>in addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.</s></p></d>", "label": ["<d><p><s>optimizing affinity-based binary hashing using auxiliary coordinates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose two stochastic gradient mcmc methods for sampling from bayesian posterior distributions defined on riemann manifolds with a known geodesic flow, e.g.</s> <s>hyperspheres.</s> <s>our methods are the first scalable sampling methods on these manifolds, with the aid of stochastic gradients.</s> <s>novel dynamics are conceived and 2nd-order integrators are developed.</s> <s>by adopting embedding techniques and the geodesic integrator, the methods do not require a global coordinate system of the manifold and do not involve inner iterations.</s> <s>synthetic experiments show the validity of the method, and its application to the challenging inference for spherical topic models indicate practical usability and efficiency.</s></p></d>", "label": ["<d><p><s>stochastic gradient geodesic mcmc methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>collaborative filtering is a popular technique to infer users' preferences on new content based on the collective information of all users preferences.</s> <s>recommender systems then use this information to make personalized suggestions to users.</s> <s>when users accept these recommendations it creates a feedback loop in the recommender system, and these loops iteratively influence the collaborative filtering algorithm's predictions over time.</s> <s>we investigate whether it is possible to identify items affected by these feedback loops.</s> <s>we state sufficient assumptions to deconvolve the feedback loops while keeping the inverse solution tractable.</s> <s>we furthermore develop a metric to unravel the recommender system's influence on the entire user-item rating matrix.</s> <s>we use this metric on synthetic and real-world datasets to (1) identify the extent to which the recommender system affects the final rating matrix, (2) rank frequently recommended items, and (3) distinguish whether a user's rated item was recommended or an intrinsic preference.</s> <s>our results indicate that it is possible to recover the ratings matrix of intrinsic user preferences using a single snapshot of the ratings matrix without any temporal information.</s></p></d>", "label": ["<d><p><s>deconvolving feedback loops in recommender systems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>automatic translation from natural language descriptions into programs is a long-standing challenging problem.</s> <s>in this work, we consider a simple yet important sub-problem: translation from textual  descriptions to if-then programs.</s> <s>we devise a novel neural network architecture for this task which we train end-to-end.</s> <s>specifically, we introduce latent attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements.</s> <s>our architecture reduces the error rate by 28.57% compared to prior art.</s> <s>we also propose a one-shot learning scenario of if-then program synthesis and simulate it with our existing dataset.</s> <s>we demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data.</s></p></d>", "label": ["<d><p><s>latent attention for if-then program synthesis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we propose a multi-step inertial forward--backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a lipschitz continuous gradient.</s> <s>we first prove global convergence of the scheme with the help of the kurdyka?</s> <s>?ojasiewicz property.</s> <s>then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis.</s> <s>the proposed method is illustrated on a few problems arising from statistics and machine learning.</s></p></d>", "label": ["<d><p><s>a multi-step inertial forward-backward splitting method for non-convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one of the major issues in stochastic gradient descent (sgd) methods is how to choose an appropriate step size while running the algorithm.</s> <s>since the traditional line search technique does not apply for stochastic optimization methods, the common practice in sgd is either to use a diminishing step size, or to tune a step size by hand, which can be time consuming in practice.</s> <s>in this paper, we propose to use the barzilai-borwein (bb) method to automatically compute step sizes for sgd and its variant: stochastic variance reduced gradient (svrg) method, which leads to two algorithms: sgd-bb and svrg-bb.</s> <s>we prove that svrg-bb converges linearly for strongly convex objective functions.</s> <s>as a by-product, we prove the linear convergence result of svrg with option i proposed in [10], whose convergence result has been missing in the literature.</s> <s>numerical experiments on standard data sets show that the performance of sgd-bb and svrg-bb is comparable to and sometimes even better than sgd and svrg with best-tuned step sizes, and is superior to some advanced sgd variants.</s></p></d>", "label": ["<d><p><s>barzilai-borwein step size for stochastic gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>as datasets capturing human choices grow in richness and scale, particularly in online domains, there is an increasing need for choice models flexible enough to handle data that violate traditional choice-theoretic axioms such as regularity, stochastic transitivity, or luce's choice axiom.</s> <s>in this work we introduce the pairwise choice markov chain (pcmc) model of discrete choice, an inferentially tractable model that does not assume these traditional axioms while still satisfying the foundational axiom of uniform expansion, which can be viewed as a weaker version of luce's axiom.</s> <s>we show that the pcmc model significantly outperforms the multinomial logit (mnl) model in prediction tasks on two empirical data sets known to exhibit violations of luce's axiom.</s> <s>our analysis also synthesizes several recent observations connecting the multinomial logit model and markov chains; the pcmc model retains the multinomial logit model as a special case.</s></p></d>", "label": ["<d><p><s>pairwise choice markov chains</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>an iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the linearized bregman iteration, hence called \\emph{split lbi}.</s> <s>despite its simplicity, split lbi outperforms the popular generalized lasso in both theory and experiments.</s> <s>a theory of path consistency is presented that equipped with a proper early stopping, split lbi may achieve model selection consistency under a family of irrepresentable conditions which can be weaker than the necessary and sufficient condition for generalized lasso.</s> <s>furthermore, some $\\ell_2$ error bounds are also given at the minimax optimal rates.</s> <s>the utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.</s></p></d>", "label": ["<d><p><s>split lbi: an iterative regularization path with structural sparsity</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval.</s> <s>much work has focused on affinity-based objective functions involving the hash functions or binary codes.</s> <s>these objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms.</s> <s>they ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization.</s> <s>we propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles.</s> <s>surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.</s></p></d>", "label": ["<d><p><s>an ensemble diversity approach to supervised binary hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov chain monte carlo (mcmc) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples.</s> <s>this challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures.</s> <s>in this work, we extend the recently introduced bidirectional monte carlo technique to evaluate mcmc-based posterior inference algorithms.</s> <s>by running annealed importance sampling (ais) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized kl divergence between the true posterior distribution and the distribution of approximate samples.</s> <s>we integrate our method into two probabilistic programming languages, webppl and stan, and validate it on several models and datasets.</s> <s>as an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in webppl and stan.</s></p></d>", "label": ["<d><p><s>measuring the reliability of mcmc inference with bidirectional monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>complex networks play an important role in a plethora of disciplines in natural sciences.</s> <s>cleaning up noisy observed networks, poses an important challenge in network analysis existing methods utilize labeled data to alleviate the noise effect in the network.</s> <s>however, labeled data is usually expensive to collect while unlabeled data can be gathered cheaply.</s> <s>in this paper, we propose an optimization framework to mine useful structures from noisy networks in an unsupervised manner.</s> <s>the key feature of our optimization framework is its ability to utilize local structures as well as global patterns in the network.</s> <s>we extend our method to incorporate multi-resolution networks in order to add further resistance to high-levels of noise.</s> <s>we also generalize our framework to utilize partial labels to enhance the performance.</s> <s>we specifically focus our method on multi-resolution hi-c data by recovering clusters of genomic regions that co-localize in 3d space.</s> <s>additionally, we use capture-c-generated partial labels to further denoise the hi-c network.</s> <s>we empirically demonstrate the effectiveness of our framework in denoising the network and improving community detection results.</s></p></d>", "label": ["<d><p><s>unsupervised learning from noisy networks with applications to hi-c data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the linear contextual bandit problem with resource consumption, in addition to reward generation.</s> <s>in each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions.</s> <s>the expected values of these outcomes depend linearly on the context of that arm.</s> <s>the budget/capacity constraints require that the sum of these vectors doesn't exceed the budget in each dimension.</s> <s>the objective is once again to maximize the total reward.</s> <s>this problem turns out to be a common generalization of classic linear contextual bandits  (lincontextual),  bandits with knapsacks (bwk), and the online stochastic packing problem (ospp).</s> <s>we present algorithms with near-optimal regret bounds for this problem.</s> <s>our bounds compare favorably to results on the unstructured version of the problem, where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies accessible through  an optimization oracle.</s> <s>we combine techniques from the work on lincontextual, bwk and ospp in a nontrivial manner while also tackling new difficulties that are not present in any of these special cases.</s></p></d>", "label": ["<d><p><s>linear contextual bandits with knapsacks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient-based monte carlo methods such as stochastic gradient langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications.</s> <s>these methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset.</s> <s>however, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing.</s> <s>in this paper, we present techniques for reducing variance in stochastic gradient langevin dynamics, yielding novel  stochastic monte carlo methods that improve performance by reducing the variance in the stochastic gradient.</s> <s>we show that our proposed method has better theoretical guarantees on convergence rate than stochastic langevin dynamics.</s> <s>this is complemented by impressive empirical results obtained on  a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling).</s> <s>these theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic monte carlo methods.</s></p></d>", "label": ["<d><p><s>variance reduction in stochastic gradient langevin dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an important problem in sequential decision-making under uncertainty is to use limited data to  compute a safe policy, i.e., a policy that is guaranteed to perform at least as well as a given baseline strategy.</s> <s>in this paper, we develop and analyze a new model-based approach to compute a safe policy when we have access to an inaccurate dynamics model of the system with known accuracy guarantees.</s> <s>our proposed robust method uses this (inaccurate) model to directly minimize the (negative) regret w.r.t.</s> <s>the baseline policy.</s> <s>contrary to the existing approaches, minimizing the regret allows one to improve the baseline policy in states with accurate dynamics and seamlessly fall back to the baseline policy, otherwise.</s> <s>we show that our formulation is np-hard and propose an approximate algorithm.</s> <s>our empirical results on several domains show that even this relatively simple approximate algorithm can significantly outperform standard approaches.</s></p></d>", "label": ["<d><p><s>safe policy improvement by minimizing robust baseline regret</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years.</s> <s>attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation.</s> <s>recently, similar improvements have been obtained using  alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way.</s> <s>such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling.</s> <s>so far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation.</s> <s>we analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences.</s> <s>we investigate this model and explain why previous active memory models did not succeed.</s> <s>finally, we discuss when active memory brings most benefits and where attention can be a better choice.</s></p></d>", "label": ["<d><p><s>can active memory replace attention?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>determinantal point processes (dpps) are probabilistic models over all subsets a ground set of n items.</s> <s>they have recently gained prominence in several applications that rely on diverse subsets.</s> <s>however, their applicability to large problems is still limited due to o(n^3) complexity of core tasks such as sampling and learning.</s> <s>we enable efficient sampling and learning for dpps by introducing krondpp, a dpp model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices.</s> <s>this decomposition immediately enables fast exact sampling.</s> <s>but contrary to what one may expect, leveraging the kronecker product structure for speeding up dpp learning turns out to be more difficult.</s> <s>we overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a krondpp.</s></p></d>", "label": ["<d><p><s>kronecker determinantal point processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>missing records are a perennial problem in analysis of complex data of all types, when the target of inference is some function of the full data law.</s> <s>in simple cases, where data is missing at random or completely at random (rubin, 1976), well-known adjustments exist that result in consistent estimators of target quantities.</s> <s>assumptions underlying these estimators are generally not realistic in practical missing data problems.</s> <s>unfortunately, consistent estimators in more complex cases where data is missing not at random, and where no ordering on variables induces monotonicity of missingness status are not known in general, with some notable exceptions (robins, 1997), (tchetgen tchetgen et al, 2016), (sadinle and reiter, 2016).</s> <s>in this paper, we propose a general class of consistent estimators for cases where data is missing not at random, and missingness status is non-monotonic.</s> <s>our estimators, which are generalized inverse probability weighting estimators, make no assumptions on the underlying full data law, but instead place independence restrictions, and certain other fairly mild assumptions, on the distribution of missingness status conditional on the data.</s> <s>the assumptions we place on the distribution of missingness status conditional on the data  can be viewed as a version of a conditional markov random field (mrf) corresponding to a chain graph.</s> <s>assumptions embedded in our model permit identification from the observed data law, and admit a natural fitting procedure based on the pseudo likelihood approach of (besag, 1975).</s> <s>we illustrate our approach with a simple simulation study, and an analysis of risk of premature birth in women in botswana exposed to highly active anti-retroviral therapy.</s></p></d>", "label": ["<d><p><s>consistent estimation of functions of data missing non-monotonically and not at random</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks.</s> <s>these models appear promising for applications such as language modeling and machine translation.</s> <s>however, they scale poorly in both space and time as the amount of memory grows --- limiting their applicability to real-world domains.</s> <s>here, we present an end-to-end differentiable memory access scheme, which we call sparse access memory (sam), that retains the representational power of the original approaches whilst training efficiently with very large memories.</s> <s>we show that sam achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs $1,\\!000\\times$ faster and with $3,\\!000\\times$ less physical memory than non-sparse models.</s> <s>sam learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot omniglot character recognition, and can scale to tasks requiring $100,\\!000$s of time steps and memories.</s> <s>as well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced differentiable neural computer.</s></p></d>", "label": ["<d><p><s>scaling memory-augmented neural networks with sparse reads and writes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimators of information theoretic measures such as entropy and mutual information from samples are a basic workhorse for many downstream applications in modern data science.</s> <s>state of the art approaches have been either geometric (nearest neighbor (nn) based) or kernel based (with bandwidth chosen to be data independent and vanishing sub linearly in the sample size).</s> <s>in this paper we combine both these approaches to design new estimators of entropy and mutual information that strongly outperform all state of the art methods.</s> <s>our estimator uses bandwidth choice of fixed $k$-nn distances; such a choice is both data dependent and linearly vanishing in the sample size and necessitates a bias cancellation term that  is  universal and independent of the underlying distribution.</s> <s>as a byproduct, we obtain a unified way of obtaining both kernel and nn estimators.</s> <s>the corresponding theoretical contribution relating the geometry of nn distances to asymptotic order statistics  is of independent mathematical interest.</s></p></d>", "label": ["<d><p><s>breaking the bandwidth barrier: geometrical adaptive entropy estimation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions.</s> <s>however, prototypes alone are rarely sufficient to represent the gist of the complexity.</s> <s>in order for users to construct better mental models and understand complex data distributions, we also need {\\em criticism} to explain what are \\textit{not} captured by prototypes.</s> <s>motivated by the bayesian model criticism framework, we develop \\texttt{mmd-critic} which efficiently learns prototypes and criticism, designed to aid human interpretability.</s> <s>a human subject pilot study shows that the \\texttt{mmd-critic} selects prototypes and criticism that are useful to facilitate human understanding and reasoning.</s> <s>we also evaluate the prototypes selected by \\texttt{mmd-critic} via a nearest prototype classifier, showing competitive performance compared to baselines.</s></p></d>", "label": ["<d><p><s>examples are not enough, learn to criticize! criticism for interpretability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models.</s> <s>though recent advances in regression technologies have made it possible to reveal price-demand relationship of a number of multiple products, most existing price optimization methods, such as mixed integer programming formulation, cannot handle tens or hundreds of products because of their high computational costs.</s> <s>to cope with this problem, this paper proposes a novel approach based on network flow algorithms.</s> <s>we reveal a connection between supermodularity of the revenue and cross elasticity of demand.</s> <s>on the basis of this connection, we propose an efficient algorithm that employs network flow algorithms.</s> <s>the proposed algorithm can handle hundreds or thousands of products, and returns an exact optimal solution under an assumption regarding cross elasticity of demand.</s> <s>even in case in which the assumption does not hold, the proposed algorithm can efficiently find approximate solutions as good as can other state-of-the-art methods, as empirical results show.</s></p></d>", "label": ["<d><p><s>large-scale price optimization via network flow</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes an efficient algorithm (holrr) to handle regression tasks where the outputs have a tensor structure.</s> <s>we formulate the regression problem as the minimization  of a least square criterion under a multilinear rank constraint, a difficult  non convex problem.</s> <s>holrr computes efficiently an approximate solution of this problem, with solid theoretical guarantees.</s> <s>a kernel extension is also presented.</s> <s>experiments on synthetic and real data show that holrr computes accurate solutions while being computationally very competitive.</s></p></d>", "label": ["<d><p><s>low-rank regression with tensor responses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we systematically analyze the connecting architectures of recurrent neural networks (rnns).</s> <s>our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of rnns in general.</s> <s>second, we propose three architecture complexity measures of rnns: (a) the recurrent depth, which captures the rnn?s over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the ?depth?</s> <s>in feedforward neural networks (fnns)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time.</s> <s>we rigorously prove each measure?s existence and computability.</s> <s>our experimental results show that rnns might benefit from larger recurrent depth and feedforward depth.</s> <s>we further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.</s></p></d>", "label": ["<d><p><s>architectural complexity measures of recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite the success of cnns, selecting the optimal architecture for a given task remains an open problem.</s> <s>instead of aiming to select a single optimal architecture, we propose a ``fabric'' that embeds an exponentially large number of architectures.</s> <s>the fabric consists of a 3d trellis that connects response maps at different layers, scales, and channels  with a sparse homogeneous local connectivity pattern.</s> <s>the only hyper-parameters of a fabric are the number of channels and layers.</s> <s>while individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their  paths overlap.</s> <s>parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size.</s> <s>we present benchmark results competitive with the state of the art for image classification on mnist and cifar10, and for semantic segmentation on the part labels dataset.</s></p></d>", "label": ["<d><p><s>convolutional neural fabrics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>feature construction is of vital importance in reinforcement learning, as the quality of a value function or policy is largely determined by the corresponding features.</s> <s>the recent successes of deep reinforcement learning (rl) only increase the importance of understanding feature construction.</s> <s>typical deep rl approaches use a linear output layer, which means that deep rl can be interpreted as a feature construction/encoding network followed by linear value function approximation.</s> <s>this paper develops and evaluates a theory of linear feature encoding.</s> <s>we extend theoretical results on feature quality for linear value function approximation from the uncontrolled case to the controlled case.</s> <s>we then develop a supervised linear feature encoding method that is motivated by insights from linear value function approximation theory, as well as empirical successes from deep rl.</s> <s>the resulting encoder is a surprisingly effective method for linear value function approximation using raw images as inputs.</s></p></d>", "label": ["<d><p><s>linear feature encoding for reinforcement learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>solving statistical learning problems often involves nonconvex optimization.</s> <s>despite the empirical success of nonconvex statistical optimization methods, their global dynamics, especially convergence to the desirable local minima, remain less well understood in theory.</s> <s>in this paper, we propose a new analytic paradigm based on diffusion processes to characterize the global dynamics of nonconvex statistical optimization.</s> <s>as a concrete example, we study stochastic gradient descent (sgd) for the tensor decomposition formulation of independent component analysis.</s> <s>in particular, we cast different phases of sgd into diffusion processes, i.e., solutions to stochastic differential equations.</s> <s>initialized from an unstable equilibrium, the global dynamics of sgd transit over three consecutive phases: (i) an unstable ornstein-uhlenbeck process slowly departing from the initialization, (ii) the solution to an ordinary differential equation, which quickly evolves towards the desirable local minimum, and (iii) a stable ornstein-uhlenbeck process oscillating around the desirable local minimum.</s> <s>our proof techniques are based upon stroock and varadhan?s weak convergence of markov chains to diffusion processes, which are of independent interest.</s></p></d>", "label": ["<d><p><s>online ica: understanding global dynamics of nonconvex optimization via diffusion processes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g.</s> <s>when evaluating the performances of several different neural network architectures in a parallel computing environment.</s> <s>in this paper, we develop a novel batch bayesian optimization algorithm --- the parallel knowledge gradient method.</s> <s>by construction, this method provides the one-step bayes optimal batch of points to sample.</s> <s>we provide an efficient strategy for computing this bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.</s></p></d>", "label": ["<d><p><s>the parallel knowledge gradient method for batch bayesian optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words -- i.e., words that only appear (with positive probability) in one topic.</s> <s>follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption.</s> <s>reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic.</s> <s>this paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework.</s> <s>the proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice.</s> <s>the associated algorithm only involves one eigen-decomposition and a few small linear programs.</s> <s>this makes it easy to implement and scale up to very large problem instances.</s> <s>experiments using the tdt2 and reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.</s></p></d>", "label": ["<d><p><s>anchor-free correlated topic modeling: identifiability and algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies the $k$-means++ algorithm for clustering as well as the class of $d^\\ell$ sampling algorithms to which $k$-means++ belongs.</s> <s>it is shown that for any constant factor $\\beta > 1$, selecting $\\beta k$ cluster centers by $d^\\ell$ sampling yields a constant-factor approximation to the optimal clustering with $k$ centers, in expectation and without conditions on the dataset.</s> <s>this result extends the previously known $o(\\log k)$ guarantee for the case $\\beta = 1$ to the constant-factor bi-criteria regime.</s> <s>it also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.</s></p></d>", "label": ["<d><p><s>a constant-factor bi-criteria approximation guarantee for k-means++</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent neural networks (rnns) have become the state-of-the-art choice for extracting patterns from temporal sequences.</s> <s>current rnn models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons.</s> <s>such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals.</s> <s>in this work, we introduce the phased lstm model, which extends the lstm unit by adding a new time gate.</s> <s>this gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle.</s> <s>even with the sparse updates imposed by the oscillation, the phased lstm network achieves faster convergence than regular lstms on tasks which require learning of long sequences.</s> <s>the model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information.</s> <s>it also greatly improves the performance of lstms in standard rnn applications, and does so with an order-of-magnitude fewer computes.</s></p></d>", "label": ["<d><p><s>phased lstm: accelerating recurrent network training for long or event-based sequences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods.</s> <s>recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained.</s> <s>however, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of monte carlo estimates.</s> <s>to address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods.</s> <s>the advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates.</s></p></d>", "label": ["<d><p><s>iterative refinement of the approximate posterior for directed belief networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we are interested in the computation of the transport map of an optimal transport problem.</s> <s>most of the computational approaches of optimal transport use the kantorovich relaxation of the problem to learn a probabilistic coupling $\\mgamma$ but do not address the problem of learning the underlying transport map $\\funct$ linked to the original monge problem.</s> <s>consequently, it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory.</s> <s>in this paper we propose a new way to jointly learn the coupling and an approximation of the transport map.</s> <s>we use a jointly convex formulation which can be efficiently optimized.</s> <s>additionally, jointly learning the coupling and the transport map allows to smooth the result of the optimal transport and generalize it to out-of-samples examples.</s> <s>empirically, we show the interest and the relevance of our method in two tasks: domain adaptation and image editing.</s></p></d>", "label": ["<d><p><s>mapping estimation for discrete optimal transport</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we present a scalable and robust bayesian method for demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics.</s> <s>inference is approximated by the newton-raphson algorithm, reduced to linear-time kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work.</s> <s>in a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.</s></p></d>", "label": ["<d><p><s>bayesian intermittent demand forecasting for large inventories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>unsupervised learning of structured predictors has been a long standing pursuit in machine learning.</s> <s>recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred.</s> <s>aside from being nonconvex, it also requires the demanding inference of normalization.</s> <s>in this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally.</s> <s>we further expand its applicability by resorting to a weaker form of inference---maximum a-posteriori.</s> <s>the flexibility of the model is demonstrated on two structures based on total unimodularity---graph matching and linear chain.</s> <s>experimental results confirm the promise of the method.</s></p></d>", "label": ["<d><p><s>convex two-layer modeling with latent structure</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate a reduction of supervised learning to game playing that reveals new connections and learning methods.</s> <s>for convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and nash equilibria in a simple game.</s> <s>we then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the nash equilibria and critical (or kkt) points of the deep learning problem.</s> <s>based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning approaches.</s></p></d>", "label": ["<d><p><s>deep learning games</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets.</s> <s>for example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall.</s> <s>other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training.</s> <s>in this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem.</s> <s>experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.</s></p></d>", "label": ["<d><p><s>satisfying real-world goals with dataset constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>experiments reveal that in the dorsal medial superior temporal (mstd) and the ventral intraparietal (vip) areas, where visual and vestibular cues are integrated to infer heading direction, there are two types of neurons with roughly the same number.</s> <s>one is ?congruent?</s> <s>cells, whose preferred heading directions are similar in response to visual and vestibular cues; and the other is ?opposite?</s> <s>cells, whose preferred heading directions are nearly ?opposite?</s> <s>(with an offset of 180 degree) in response to visual vs. vestibular cues.</s> <s>congruent neurons are known to be responsible for cue integration, but the computational role of opposite neurons remains largely unknown.</s> <s>here, we propose that opposite neurons may serve to encode the disparity information between cues necessary for multisensory segregation.</s> <s>we build a computational model composed of two reciprocally coupled modules, mstd and vip, and each module consists of groups of congruent and opposite neurons.</s> <s>in the model, congruent neurons in two modules are reciprocally connected with each other in the congruent manner, whereas opposite neurons are reciprocally connected in the opposite manner.</s> <s>mimicking the experimental protocol, our model reproduces the characteristics of congruent and opposite neurons, and demonstrates that in each module, the sisters of congruent and opposite neurons can jointly achieve optimal multisensory information integration and segregation.</s> <s>this study sheds light on our understanding of how the brain implements optimal multisensory integration and segregation concurrently in a distributed manner.</s></p></d>", "label": ["<d><p><s>?congruent? and ?opposite? neurons: sisters for multisensory integration and segregation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>markov chain monte carlo (mcmc) and belief propagation (bp) are the most popular algorithms for computational inference in graphical models (gm).</s> <s>in principle, mcmc is an exact probabilistic method which, however, often suffers from exponentially slow mixing.</s> <s>in contrast, bp  is a deterministic method, which is typically fast,  empirically very successful, however in general lacking control of accuracy over loopy graphs.</s> <s>in this paper, we introduce mcmc algorithms correcting the approximation error of bp, i.e.,  we provide a way to compensate for bp errors via a consecutive bp-aware mcmc.</s> <s>our framework is based on the loop calculus (lc) approach  which allows to express the bp error  as a sum of weighted generalized loops.</s> <s>although the full series is computationally intractable,  it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary gms and it also provides a highly accurate approximation empirically.</s> <s>motivated by this, we, first, propose a polynomial-time approximation mcmc scheme for the truncated series of general (non-planar) pair-wise binary models.</s> <s>our main idea here is to use the worm algorithm, known to provide fast mixing in other (related) problems, and then  design an appropriate rejection scheme to sample 2-regular loops.</s> <s>furthermore, we also design an efficient rejection-free mcmc scheme  for approximating the full series.</s> <s>the main novelty underlying our design  is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops.</s> <s>in essence, the proposed mcmc schemes run on transformed gm built upon  the non-trivial bp solution, and our experiments show that this synthesis of bp and mcmc  outperforms both direct mcmc and bare bp schemes.</s></p></d>", "label": ["<d><p><s>synthesis of mcmc and belief propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of recovering a vector $\\bx\\in \\bbr^n$ from its magnitude measurements $y_i=|\\langle \\ba_i, \\bx\\rangle|, i=1,..., m$.</s> <s>our work is along the line of the wirtinger flow (wf) approach \\citet{candes2015phase}, which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization.</s> <s>in contrast to the smooth loss function used in wf, we adopt a nonsmooth but lower-order loss function, and design a gradient-like algorithm (referred to as reshaped-wf).</s> <s>we show that for random gaussian measurements, reshaped-wf enjoys geometric convergence to a global optimal point as long as the number $m$ of measurements is at the order of $\\co(n)$, where $n$ is the dimension of the unknown $\\bx$.</s> <s>this improves the sample complexity of wf, and achieves  the same sample complexity as truncated-wf \\citet{chen2015solving} but without truncation at gradient step.</s> <s>furthermore, reshaped-wf costs less computationally than wf, and runs faster numerically than both wf and truncated-wf.</s> <s>bypassing higher-order variables in the loss function and truncations in the gradient loop, analysis of reshaped-wf is simplified.</s></p></d>", "label": ["<d><p><s>reshaped wirtinger flow for solving quadratic system of equations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>even in state-spaces of modest size, planning is plagued by the ?curse of dimensionality?.</s> <s>this problem is particularly acute in human and animal cognition given the limited capacity of working memory, and the time pressures under which planning often occurs in the natural environment.</s> <s>hierarchically organized modular representations have long been suggested to underlie the capacity of biological systems to efficiently and flexibly plan in complex environments.</s> <s>however, the principles underlying efficient modularization remain obscure, making it difficult to identify its behavioral and neural signatures.</s> <s>here, we develop a normative theory of efficient state-space representations which partitions an environment into distinct modules by minimizing the average (information theoretic) description length of planning within the environment, thereby optimally trading off the complexity of planning across and within modules.</s> <s>we show that such optimal representations provide a unifying account for a diverse range of hitherto unrelated phenomena at multiple levels of behavior and neural representation.</s></p></d>", "label": ["<d><p><s>efficient state-space modularization for planning: theory, behavioral and neural signatures</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>accuracy and interpretability are two dominant features of successful predictive models.</s> <s>typically, a choice must be made in favor of complex black box models such as recurrent neural networks (rnn) for accuracy versus less accurate but more interpretable traditional models such as logistic regression.</s> <s>this tradeoff poses challenges in medicine where both accuracy and interpretability are important.</s> <s>we addressed this challenge by developing the reverse time attention model (retain) for application to electronic health records (ehr) data.</s> <s>retain achieves high accuracy while remaining clinically interpretable and is based on a two-level neural attention model that detects influential past visits and significant clinical variables within those visits (e.g.</s> <s>key diagnoses).</s> <s>retain mimics physician practice by attending the ehr data in a reverse time order so that recent clinical visits are likely to receive higher attention.</s> <s>retain was tested on a large health system ehr dataset with 14 million visits completed by 263k patients over an 8 year period and demonstrated predictive accuracy and computational scalability comparable to state-of-the-art methods such as rnn, and ease of interpretability comparable to traditional models.</s></p></d>", "label": ["<d><p><s>retain: an interpretable predictive model for healthcare using reverse time attention mechanism</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we combine riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in deep neural networks with random weights.</s> <s>our results reveal a phase transition in the expressivity of random deep networks, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth, but not with width.</s> <s>we prove that this generic class of random functions cannot be efficiently computed by any shallow network, going beyond prior work that restricts their analysis to single functions.</s> <s>moreover, we formally quantify and demonstrate the long conjectured idea that deep networks can disentangle exponentially curved manifolds in input space into flat manifolds in hidden space.</s> <s>our theoretical framework for analyzing the expressive power of deep networks is broadly applicable and provides a basis for quantifying previously abstract notions about the geometry of deep functions.</s></p></d>", "label": ["<d><p><s>exponential expressivity in deep neural networks through transient chaos</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new approach to designing visual markers (analogous to qr-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks.</s> <s>in our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers.</s> <s>the two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and capture into account.</s> <s>additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype.</s> <s>in the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical.</s> <s>the ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach.</s> <s>as a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by convnets and on their ability to distinguish composite patterns.</s></p></d>", "label": ["<d><p><s>learnable visual markers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide two fundamental results on the population (infinite-sample) likelihood function of gaussian mixture models with $m \\geq 3$ components.</s> <s>our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical gaussians.</s> <s>we prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of srebro (2007).</s> <s>our second main result shows that the em algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least $1-e^{-\\omega(m)}$.</s> <s>we further establish that a first-order variant of em will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points.</s> <s>overall, our results highlight the necessity of careful initialization when using the em algorithm in practice, even when applied in highly favorable settings.</s></p></d>", "label": ["<d><p><s>local maxima in the likelihood of gaussian mixture models: structural results and algorithmic consequences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the cost function for  hierarchical clusterings introduced by [dasgupta, 2015]  where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters.</s> <s>it was also shown in [dasgupta, 2015] that a top-down algorithm  returns a hierarchical clustering of cost at most \\(o\\left(\\alpha_n \\log n\\right)\\) times the cost of the optimal hierarchical clustering, where \\(\\alpha_n\\) is the approximation ratio of the sparsest cut subroutine used.</s> <s>thus using the best known approximation algorithm for sparsest cut due to arora-rao-vazirani,  the top down algorithm returns a hierarchical clustering of cost at most  \\(o\\left(\\log^{3/2} n\\right)\\) times the cost of the optimal solution.</s> <s>we improve this by giving an \\(o(\\log{n})\\)-approximation algorithm for this problem.</s> <s>our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an integer linear programming (ilp) formulation for this family of ultrametrics, and showing how to iteratively round an lp relaxation of this formulation by  using the idea of \\emph{sphere growing} which has been extensively used in the context of graph  partitioning.</s> <s>we also prove that our algorithm returns an \\(o(\\log{n})\\)-approximate  hierarchical clustering for a generalization of this cost function also studied in [dasgupta, 2015].</s> <s>experiments show that the hierarchies found by using the ilp formulation as well  as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms.</s> <s>we conclude with an inapproximability result for this problem, namely that no polynomial sized lp or sdp can be used to obtain a constant factor approximation for this problem.</s></p></d>", "label": ["<d><p><s>hierarchical clustering via spreading metrics</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>new silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels.</s> <s>interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results.</s> <s>here we introduce kilosort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering.</s> <s>kilosort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved.</s> <s>unlike previous algorithms that compress the data with pca, kilosort operates on the raw data which allows it to construct a more accurate model of the waveforms.</s> <s>processing times are faster than in previous algorithms thanks to batch-based optimization on gpus.</s> <s>we compare kilosort to an established algorithm and show favorable performance, at much reduced processing times.</s> <s>a novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.</s></p></d>", "label": ["<d><p><s>fast and accurate spike sorting of high-channel count probes with kilosort</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems.</s> <s>unitary recurrent neural networks (urnns), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues.</s> <s>however, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned?</s> <s>to address this question, we propose full-capacity urnns that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over urnns that use a restricted-capacity recurrence matrix.</s> <s>our contribution consists of two main components.</s> <s>first, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity.</s> <s>using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7.</s> <s>second,we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices.</s> <s>the resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation.</s> <s>we confirm the utility of our claims by empirically evaluating our new full-capacity urnns on both synthetic and natural data, achieving superior performance compared to both lstms and the original restricted-capacity urnns.</s></p></d>", "label": ["<d><p><s>full-capacity unitary recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the reparameterization gradient has become a widely used method to obtain monte carlo gradients to optimize the variational objective.</s> <s>however, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit gaussian distributions.</s> <s>in this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions.</s> <s>generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters.</s> <s>this results in new monte carlo gradients that combine reparameterization gradients and score function gradients.</s> <s>we demonstrate our approach on variational inference for two complex probabilistic models.</s> <s>the generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.</s></p></d>", "label": ["<d><p><s>the generalized reparameterization gradient</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>faced with saturation of moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms.</s> <s>however, distributed computing is often bottle necked by a small fraction of slow processors called \"stragglers\" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing.</s> <s>to combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes.</s> <s>the fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers.</s> <s>in this paper, we propose a novel technique - that we call \"short-dot\" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors.</s> <s>instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors.</s> <s>further, only a subset of these short dot products are required at the fusion node to finish the computation successfully.</s> <s>we demonstrate through probabilistic analysis as well as experiments on computing clusters that short-dot offers significant speed-up compared to existing techniques.</s> <s>we also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.</s></p></d>", "label": ["<d><p><s>short-dot: computing large linear transforms distributedly using coded short dot products</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks have received a considerable attention due to the success of their training for real world machine learning applications.</s> <s>they are also of great interest to the understanding of sensory processing in cortical sensory hierarchies.</s> <s>the purpose of this work is to advance our theoretical understanding of the computational benefits of these architectures.</s> <s>using a simple model of clustered noisy inputs and a simple learning rule, we provide analytically derived recursion relations describing the propagation of the signals along the deep network.</s> <s>by analysis of these equations, and defining performance measures, we show that these model networks have optimal depths.</s> <s>we further explore the dependence of the optimal architecture on the system parameters.</s></p></d>", "label": ["<d><p><s>optimal architectures in a solvable model of deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints.</s> <s>on the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise.</s> <s>in this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes.</s> <s>we propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime.</s> <s>we establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary.</s> <s>our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise.</s> <s>moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes.</s> <s>we perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets.</s> <s>this result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.</s></p></d>", "label": ["<d><p><s>robustness of classifiers: from adversarial to random noise</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the latent dirichlet allocation (lda) model and its nonparametric extensions.</s> <s>to this end we study the optimization of a geometric loss function, which is a surrogate to the lda's likelihood.</s> <s>our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on gibbs sampling and variational inference, while achieving the accuracy comparable to that of a gibbs sampler.</s> <s>the topic estimates produced by our method are shown to be statistically consistent under some conditions.</s> <s>the algorithm is evaluated with extensive experiments on simulated and real data.</s></p></d>", "label": ["<d><p><s>geometric dirichlet means algorithm for topic inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>effective convolutional neural networks are trained on large sets of labeled data.</s> <s>however, creating large labeled datasets is a very costly and time-consuming task.</s> <s>semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available.</s> <s>in this paper, we consider the problem of semi-supervised learning with convolutional neural networks.</s> <s>techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent.</s> <s>multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques.</s> <s>we propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network.</s> <s>we evaluate the proposed method on several benchmark datasets.</s></p></d>", "label": ["<d><p><s>regularization with stochastic transformations and perturbations for deep semi-supervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points.</s> <s>finite mixture models, dirichlet process mixture models, and pitman--yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models.</s> <s>however, for some applications, this assumption is inappropriate.</s> <s>for example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points.</s> <s>these applications require models that yield clusters whose sizes grow sublinearly with the size of the data set.</s> <s>we address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property.</s> <s>we compare models within this class to two commonly used clustering models using four entity-resolution data sets.</s></p></d>", "label": ["<d><p><s>flexible models for microclustering with application to entity resolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>contexts are crucial for action recognition in video.</s> <s>current methods often mine contexts after extracting hierarchical local features and focus on their high-order encodings.</s> <s>this paper instead explores contexts as early as possible and leverages their evolutions for action recognition.</s> <s>in particular, we introduce a novel architecture called deep alternative neural network (dann) stacking alternative layers.</s> <s>each alternative layer consists of a volumetric convolutional layer followed by a recurrent layer.</s> <s>the former acts as local feature learner while the latter is used to collect contexts.</s> <s>compared with feed-forward neural networks, dann learns contexts of local features from the very beginning.</s> <s>this setting helps to preserve hierarchical context evolutions which we show are essential to recognize similar actions.</s> <s>besides, we present an adaptive method to determine the temporal size for network input based on optical flow energy, and develop a volumetric pyramid pooling layer to deal with input clips of arbitrary sizes.</s> <s>we demonstrate the advantages of dann on two benchmarks hmdb51 and ucf101 and report competitive or superior results to the state-of-the-art.</s></p></d>", "label": ["<d><p><s>deep alternative neural network: exploring contexts as early as possible for action recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost.</s> <s>at each round, our algorithm selects a pair of functions, a base predictor and a base abstention function.</s> <s>we define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the bayes solution.</s> <s>our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the rademacher complexities of the corresponding function classes.</s> <s>we give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps.</s> <s>we also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.</s></p></d>", "label": ["<d><p><s>boosting with abstention</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work on deriving $o(\\log t)$ anytime regret bounds for stochastic dueling bandit problems has considered mostly condorcet winners, which do not always exist, and more recently, winners defined by the copeland set, which do always exist.</s> <s>in this work, we consider a broad notion of winners defined by tournament solutions in social choice theory, which include the copeland set as a special case but also include several other notions of winners such as the top cycle, uncovered set, and banks set, and which, like the copeland set, always exist.</s> <s>we develop a family of ucb-style dueling bandit algorithms for such general tournament solutions, and show $o(\\log t)$ anytime regret bounds for them.</s> <s>experiments confirm the ability of our algorithms to achieve low regret relative to the target winning set of interest.</s></p></d>", "label": ["<d><p><s>dueling bandits: beyond condorcet winners to general tournament solutions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization.</s> <s>our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the kl divergence.</s> <s>empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods.</s> <s>the derivation of our method is based on a new theoretical result that connects the derivative of kl divergence under smooth transforms with stein?s identity and a recently proposed kernelized stein discrepancy, which is of independent interest.</s></p></d>", "label": ["<d><p><s>stein variational gradient descent: a general purpose bayesian inference algorithm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled.</s> <s>we propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program.</s> <s>we show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the mnist and cifar-10 datasets.</s> <s>our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms.</s> <s>furthermore, we show how existing approaches to improving robustness ?overfit?</s> <s>to adversarial examples generated using a specific algorithm.</s> <s>finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.</s></p></d>", "label": ["<d><p><s>measuring neural net robustness with constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (cnns).</s> <s>in this paper, we propose doubly convolutional neural networks (dcnns), which significantly improve the performance of cnns by further exploring this idea.</s> <s>in stead of allocating a set of convolutional filters that are independently learned, a dcnn maintains groups of filters where filters within each group are translated versions of each other.</s> <s>practically, a dcnn can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries.</s> <s>we perform extensive experiments on three image classification benchmarks: cifar-10, cifar-100 and imagenet, and show that dcnns consistently outperform other competing architectures.</s> <s>we have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a cnn can improve its performance.</s> <s>moreover, various design choices of dcnns are demonstrated, which shows that dcnn can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.</s></p></d>", "label": ["<d><p><s>doubly convolutional neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider sequential decision making problem in the adversarial setting, where regret is measured with respect to the optimal sequence of actions and the feedback adheres the bandit setting.</s> <s>it is well-known that obtaining sublinear regret in this setting is impossible in general, which arises the question of when can we do better than linear regret?</s> <s>previous works show that when the environment is guaranteed to vary slowly and furthermore we are given prior knowledge regarding its variation (i.e., a limit on the amount of changes suffered by the environment), then this task is feasible.</s> <s>the caveat however is that such prior knowledge is not likely to be available in practice, which causes the obtained regret bounds to be somewhat irrelevant.</s> <s>our main result is a regret guarantee that scales with the variation parameter of the environment, without requiring any prior knowledge about it whatsoever.</s> <s>by that, we also resolve an open problem posted by [gur, zeevi and besbes, nips' 14].</s> <s>an important key component in our result is a statistical test for identifying non-stationarity in a sequence of independent random variables.</s> <s>this test either identifies non-stationarity or upper-bounds the absolute deviation of the corresponding sequence of mean values in terms of its total variation.</s> <s>this test is interesting on its own right and has the potential to be found useful in additional settings.</s></p></d>", "label": ["<d><p><s>multi-armed bandits: competing with optimal sequences</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work, we propose an infinite restricted boltzmann machine (rbm), whose maximum likelihood estimation (mle) corresponds to a constrained convex optimization.</s> <s>we consider the frank-wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity.</s> <s>as a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization.</s> <s>the resulting model can also be used as an initialization for typical state-of-the-art rbm training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.</s></p></d>", "label": ["<d><p><s>learning infinite rbms with frank-wolfe</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>several works have shown that deep cnn classifiers can be easily transferred across datasets, e.g.</s> <s>the transfer of a cnn trained to recognize objects on imagenet to an object detector on pascal voc.</s> <s>less clear, however, is the ability of cnns to transfer knowledge across tasks.</s> <s>a common example of such transfer is the problem of scene classification that should leverage localized object detections to recognize holistic visual concepts.</s> <s>while this problem is currently addressed with fisher vector representations, these are now shown ineffective for the high-dimensional and highly non-linear features extracted by modern cnns.</s> <s>it is argued that this is mostly due to the reliance on a model, the gaussian mixture of diagonal covariances, which has a very limited ability to capture the second order statistics of cnn features.</s> <s>this problem is addressed by the adoption of a better model, the mixture of factor analyzers (mfa), which approximates the non-linear data manifold by a collection of local subspaces.</s> <s>the fisher score with respect to the mfa (mfa-fs) is derived and proposed as an image representation for holistic image classifiers.</s> <s>extensive experiments show that the mfa-fs has state of the art performance for object-to-scene transfer and this transfer actually  outperforms the training of a scene cnn from a large scene dataset.</s> <s>the two representations are also shown to be complementary, in the sense that their combination outperforms each of the representations by itself.</s> <s>when combined, they produce a state of the art scene classifier.</s></p></d>", "label": ["<d><p><s>object based scene representations using fisher scores of local subspace projections</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>neuroscience experiments often require training animals to perform tasks designed to elicit various sensory, cognitive, and motor behaviors.</s> <s>training typically involves a series of gradual adjustments of stimulus conditions and rewards in order to bring about learning.</s> <s>however, training protocols are usually hand-designed, relying on a combination of intuition, guesswork, and trial-and-error, and often require weeks or months to achieve a desired level of task performance.</s> <s>here we combine ideas from reinforcement learning and adaptive optimal experimental design to formulate methods for adaptive optimal training of animal behavior.</s> <s>our work addresses two intriguing problems at once: first, it seeks to infer the learning rules underlying an animal's behavioral changes during training; second, it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a desired objective.</s> <s>we develop and test these methods using data collected from rats during training on a two-interval sensory discrimination task.</s> <s>we show that we can accurately infer the parameters of a policy-gradient-based learning algorithm that describes how the animal's internal model of the task evolves over the course of training.</s> <s>we then formulate a theory for optimal training, which involves selecting sequences of stimuli that will drive the animal's internal policy toward a desired location in the parameter space.</s> <s>simulations show that our method can in theory provide a substantial speedup over standard training methods.</s> <s>we feel these results will hold considerable theoretical and practical implications both for researchers in reinforcement learning and for experimentalists seeking to train animals.</s></p></d>", "label": ["<d><p><s>adaptive optimal training of animal behavior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in high-dimensional settings, where the number of features p is typically much larger than the number of samples n, methods which can systematically examine arbitrary combinations of features, a huge 2^p-dimensional space, have recently begun to be explored.</s> <s>however, none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate, in order to correct for potential confounding effects.</s> <s>we propose the fast automatic conditional search (facs) algorithm, a significant discriminative itemset mining method which conditions on categorical covariates and only scales as o(k log k), where k is the number of states of the categorical covariate.</s> <s>based on the cochran-mantel-haenszel test, facs demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art, opening the door to numerous applications in biomedicine.</s></p></d>", "label": ["<d><p><s>finding significant combinations of features in the presence of categorical covariates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the iterations of many sparse estimation algorithms are comprised of a fixed linear filter cascaded with a thresholding nonlinearity, which collectively resemble a typical neural network layer.</s> <s>consequently, a lengthy sequence of algorithm iterations can be viewed as a deep network with shared, hand-crafted layer weights.</s> <s>it is therefore quite natural to examine the degree to which a learned network model might act as a viable surrogate for traditional sparse estimation in domains where ample training data is available.</s> <s>while the possibility of a reduced computational budget is readily apparent when a ceiling is imposed on the number of layers, our work primarily focuses on estimation accuracy.</s> <s>in particular, it is well-known that when a signal dictionary has coherent columns, as quantified by a large rip constant, then most tractable iterative algorithms are unable to find maximally sparse representations.</s> <s>in contrast, we demonstrate both theoretically and empirically the potential for a trained deep network to recover minimal $\\ell_0$-norm representations in regimes where existing methods fail.</s> <s>the resulting system, which can effectively learn novel iterative sparse estimation algorithms, is deployed on a practical photometric stereo estimation problem, where the goal is to remove sparse outliers that can disrupt the estimation of surface normals from a 3d scene.</s></p></d>", "label": ["<d><p><s>maximal sparsity with deep networks?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we establish upper and lower bounds for the influence of a set of nodes in certain types of contagion models.</s> <s>we derive two sets of bounds, the first designed for linear threshold models, and the second more broadly applicable to a general class of triggering models, which subsumes the popular independent cascade models, as well.</s> <s>we quantify the gap between our upper and lower bounds in the case of the linear threshold model and illustrate the gains of our upper bounds for independent cascade models in relation to existing results.</s> <s>importantly, our lower bounds are monotonic and submodular, implying that a greedy algorithm for influence maximization is guaranteed to produce a maximizer within a (1 - 1/e)-factor of the truth.</s> <s>although the problem of exact influence computation is np-hard in general, our bounds may be evaluated efficiently.</s> <s>this leads to an attractive, highly scalable algorithm for influence maximization with rigorous theoretical guarantees.</s></p></d>", "label": ["<d><p><s>computing and maximizing influence in linear threshold and triggering models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>bayesian nonparametric  methods based on the dirichlet process (dp), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning.</s> <s>however, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior.</s> <s>as such there is now considerable interest in models based on the stable processs (sp), generalized gamma process (ggp) and stable-beta process (sbp).</s> <s>these models present new challenges in terms of practical statistical implementation.</s> <s>in analogy to tractable processes such as the finite-dimensional dirichlet process, we describe a class of random processes, we call iid finite-dimensional bfry processes, that enables one to begin to develop efficient posterior inference algorithms such as variational bayes that readily scale to massive datasets.</s> <s>for illustrative purposes, we describe a simple variational bayes algorithm for normalized sp mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets.</s></p></d>", "label": ["<d><p><s>finite-dimensional bfry priors and variational bayesian inference for power law models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>fitting statistical models is computationally challenging when the sample size or the dimension of the dataset is huge.</s> <s>an attractive approach for down-scaling the problem size is to first partition the dataset into subsets and then fit using distributed algorithms.</s> <s>the dataset can be partitioned either horizontally (in the sample space) or vertically (in the feature space).</s> <s>while the majority of the literature focuses on sample space partitioning, feature space partitioning is more effective when p >> n. existing methods for partitioning features, however, are either vulnerable to high correlations or inefficient in reducing the model dimension.</s> <s>in this paper, we solve these problems through a new embarrassingly parallel framework named deco for distributed variable selection and parameter estimation.</s> <s>in deco, variables are first partitioned and allocated to m distributed workers.</s> <s>the decorrelated subset data within each worker are then fitted via any algorithm designed for high-dimensional problems.</s> <s>we show that by incorporating the decorrelation step, deco can achieve consistent variable selection and parameter estimation on each subset with (almost) no assumptions.</s> <s>in addition, the convergence rate is nearly minimax optimal for both sparse and weakly sparse models and does not depend on the partition number m. extensive numerical experiments are provided to illustrate the performance of the new framework.</s></p></d>", "label": ["<d><p><s>decorrelated feature space partitioning for distributed sparse regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the adaptive skills, adaptive partitions (asap) framework that (1) learns skills (i.e., temporally extended actions or options) as well as (2) where to apply them.</s> <s>we believe that both (1) and (2) are necessary for a truly general skill learning framework, which is a key building block needed to scale up to lifelong learning agents.</s> <s>the asap framework is also able to  solve related new tasks simply by adapting where it applies its existing learned skills.</s> <s>we prove that asap converges to a local optimum under natural conditions.</s> <s>finally, our experimental results, which include a robocup domain, demonstrate the ability of asap to learn where to reuse skills as well as solve multiple tasks with considerably less experience than solving each task from scratch.</s></p></d>", "label": ["<d><p><s>adaptive skills adaptive partitions (asap)</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the design of revenue-maximizing combinatorial auctions, i.e.</s> <s>multi item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale.</s> <s>in the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution.</s> <s>despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown.</s> <s>in recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions.</s> <s>the most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class.</s> <s>in this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design.</s> <s>in particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy.</s> <s>in addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory.</s> <s>in particular, the hypothesis functions used in our contexts are defined through multi stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.</s></p></d>", "label": ["<d><p><s>sample complexity of automated mechanism design</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms).</s> <s>sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros.</s> <s>this is imposed via an $\\ell_1$-norm constraint.</s> <s>by \"structured\", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation.</s> <s>we propose to use a sobolev (laplacian) penalty to impose this type of structure.</s> <s>combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images.</s> <s>this non-trivially extends the online dictionary-learning  work of mairal et al.</s> <s>(2010), at the price of only a factor of 2 or 3 on the overall running time.</s> <s>just like the mairal et al.</s> <s>(2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets.</s> <s>experiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.</s></p></d>", "label": ["<d><p><s>learning brain regions via large-scale online structured sparse dictionary learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic inference algorithms such as sequential monte carlo (smc) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results.</s> <s>in this paper, we show how to create procedural models which learn how to satisfy constraints.</s> <s>we augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far.</s> <s>we call such models neurally-guided procedural models.</s> <s>as a pre-computation, we train these models to maximize the likelihood of example outputs generated via smc.</s> <s>they are then used as efficient smc importance samplers, generating high-quality results with very few samples.</s> <s>we evaluate our method on l-system-like models with image-based constraints.</s> <s>given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.</s></p></d>", "label": ["<d><p><s>neurally-guided procedural models: amortized inference for procedural graphics programs using neural networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in pu learning, a binary classifier is trained from positive (p) and unlabeled (u) data without negative (n) data.</s> <s>although n data is missing, it sometimes outperforms pn learning (i.e., ordinary supervised learning).</s> <s>hitherto, neither theoretical nor experimental analysis has been given to explain this phenomenon.</s> <s>in this paper, we theoretically compare pu (and nu) learning against pn learning based on the upper bounds on estimation errors.</s> <s>we find simple conditions when pu and nu learning are likely to outperform pn learning, and we prove that, in terms of the upper bounds, either pu or nu learning (depending on the class-prior probability and the sizes of p and n data) given infinite u data will improve on pn learning.</s> <s>our theoretical findings well agree with the experimental results on artificial and benchmark data even when the experimental setup does not match the theoretical assumptions exactly.</s></p></d>", "label": ["<d><p><s>theoretical comparisons of positive-unlabeled learning against positive-negative learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the study of fairness in multi-armed bandit problems.</s> <s>our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm?s uncertainty over the true payoffs.</s> <s>in the classic stochastic bandits problem we provide a provably fair algorithm based on ?chained?</s> <s>confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms.</s> <s>we further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case.</s> <s>in the general contextual case, we prove a tight connection between fairness and the kwik (knows what it knows) learning model: a kwik algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa.</s> <s>this tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.</s></p></d>", "label": ["<d><p><s>fairness in learning: classic and contextual bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a derivation and theoretical investigation of the adams-bashforth and adams-moulton family of linear multistep methods for solving ordinary differential equations, starting from a gaussian process (gp) framework.</s> <s>in the limit, this formulation coincides with the classical deterministic methods, which have been used as higher-order initial value problem solvers for over a century.</s> <s>furthermore, the natural probabilistic framework provided by the gp formulation allows us to derive probabilistic versions of these methods, in the spirit of a number of other probabilistic ode solvers presented in the recent literature.</s> <s>in contrast to higher-order runge-kutta methods, which require multiple intermediate function evaluations per step, adams family methods make use of previous function evaluations, so that increased accuracy arising from a higher-order multistep approach comes at very little additional computational cost.</s> <s>we show that through a careful choice of covariance function for the gp, the posterior mean and standard deviation over the numerical solution can be made to exactly coincide with the value given by the deterministic method and its local truncation error respectively.</s> <s>we provide a rigorous proof of the convergence of these new methods, as well as an empirical investigation (up to fifth order) demonstrating their convergence rates in practice.</s></p></d>", "label": ["<d><p><s>probabilistic linear multistep methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose efficient algorithms for simultaneous clustering and completion of incomplete high-dimensional data that lie in a union of low-dimensional subspaces.</s> <s>we cast the problem as finding a completion of the data matrix so that each point can be reconstructed as a linear or affine combination of a few data points.</s> <s>since the problem is np-hard, we propose a lifting framework and reformulate the problem as a group-sparse recovery of each incomplete data point in a dictionary built using incomplete data, subject to rank-one constraints.</s> <s>to solve the problem efficiently, we propose a rank pursuit algorithm and a convex relaxation.</s> <s>the solution of our algorithms recover missing entries and provides a similarity matrix for clustering.</s> <s>our algorithms can deal with both low-rank and high-rank matrices, does not suffer from initialization, does not need to know dimensions of subspaces and can work with a small number of data points.</s> <s>by extensive experiments on synthetic data and real problems of video motion segmentation and completion of motion capture data, we show that when the data matrix is low-rank, our algorithm performs on par with or better than low-rank matrix completion methods, while for high-rank data matrices, our method significantly outperforms existing algorithms.</s></p></d>", "label": ["<d><p><s>high-rank matrix completion and clustering under self-expressive models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment.</s> <s>this is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment.</s> <s>in this paper, we address the problem of safely exploring finite markov decision processes (mdp).</s> <s>we define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a gaussian process prior.</s> <s>we develop a novel algorithm, safemdp, for this task and prove that it completely explores the safely reachable part of the mdp without violating the safety constraint.</s> <s>to achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment.</s> <s>moreover, the algorithm explicitly considers reachability when exploring the mdp, ensuring that it does not get stuck in any state with no safe way out.</s> <s>we demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.</s></p></d>", "label": ["<d><p><s>safe exploration in finite markov decision processes with gaussian processes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>stochastic gradient mcmc (sg-mcmc) has played an important role in large-scale bayesian learning, with well-developed theoretical convergence properties.</s> <s>in such applications of sg-mcmc, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients.</s> <s>while stale gradients could be directly used in sg-mcmc, their impact on convergence properties has not been well studied.</s> <s>in this paper we develop theory to show that while the bias and mse of an sg-mcmc algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it.</s> <s>in a simple bayesian distributed system with sg-mcmc, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t.</s> <s>the number of workers.</s> <s>experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of sg-mcmc with stale gradients.</s></p></d>", "label": ["<d><p><s>stochastic gradient mcmc with stale gradients</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition.</s> <s>however, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution.</s> <s>since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.</s> <s>nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage.</s> <s>in this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation.</s> <s>our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin.</s> <s>we will make our learned models as well as the source code available immediately upon acceptance.</s></p></d>", "label": ["<d><p><s>learning transferrable representations for unsupervised domain adaptation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose and study a new class of submodular functions called deep submodular functions (dsfs).</s> <s>we define dsfs and situate them within the broader context of classes of submodular functions in relationship both to various matroid ranks and sums of concave composed with modular functions (scms).</s> <s>notably, we find that dsfs constitute a strictly broader class than scms, thus motivating their use, but that they do not comprise all submodular functions.</s> <s>interestingly, some dsfs can be seen as special cases of certain deep neural networks (dnns), hence the name.</s> <s>finally, we provide a method to learn dsfs in a max-margin framework, and offer preliminary results applying this both to synthetic and real-world data instances.</s></p></d>", "label": ["<d><p><s>deep submodular functions: definitions and learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>many online communities present user-contributed responses, such as reviews of products and answers to questions.</s> <s>user-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes.</s> <s>we propose the chinese voting process (cvp) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases.</s> <s>we evaluate this model on amazon product reviews and more than 80 stackexchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities.</s></p></d>", "label": ["<d><p><s>beyond exchangeability: the chinese voting process</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider tractable representations of probability distributions and the polytime operations they support.</s> <s>in particular, we consider a recently proposed arithmetic circuit representation, the probabilistic sentential decision diagram (psdd).</s> <s>we show that psdd supports a polytime multiplication operator, while they do not support a polytime operator for summing-out variables.</s> <s>a polytime multiplication operator make psdds suitable for a broader class of applications compared to arithmetic circuits, which do not in general support multiplication.</s> <s>as one example, we show that psdd multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a psdd, and then multiply them.</s></p></d>", "label": ["<d><p><s>tractable operations for arithmetic circuits of probabilistic models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand.</s> <s>we reduce the complexity of algorithm design for machine learning by reductions:  we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications.</s> <s>furthermore, unlike existing results, our new reductions are optimal and more practical.</s> <s>we show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.</s></p></d>", "label": ["<d><p><s>optimal black-box reductions between optimization objectives</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the stochastic block model (sbm) has long been studied in machine learning and network science as a canonical model for clustering and community detection.</s> <s>in the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms.</s> <s>for the {\\it detection} problem in symmetric sbms, decelle et al.\\ conjectured that the so-called kesten-stigum (ks) threshold can be achieved efficiently.</s> <s>this was proved for two communities, but remained open from three communities.</s> <s>we prove this conjecture here, obtaining a more general result that applies to arbitrary sbms with linear size communities.</s> <s>the developed algorithm is a linearized acyclic belief propagation (abp) algorithm, which mitigates the effects of cycles while provably achieving the ks threshold in $o(n \\ln n)$ time.</s> <s>this extends prior methods by achieving universally the ks threshold while reducing or preserving the computational complexity.</s> <s>abp is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in krzakala et al., and extending results from bordenave et al.</s></p></d>", "label": ["<d><p><s>achieving the ks threshold in the general stochastic block model with linearized acyclic belief propagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep distributed decision trees and tree ensembles have grown in importance due to the need to model increasingly large datasets.</s> <s>however, planet, the standard distributed tree learning algorithm implemented in systems such as \\xgboost and spark mllib, scales poorly as data dimensionality and tree depths grow.</s> <s>we present yggdrasil, a new distributed tree learning method that outperforms existing methods by up to 24x.</s> <s>unlike planet, yggdrasil is based on vertical partitioning of the data (i.e., partitioning by feature), along with a set of optimized data structures to reduce the cpu and communication costs of training.</s> <s>yggdrasil (1) trains directly on compressed data for compressible features and labels; (2) introduces efficient data structures for training on uncompressed data; and (3) minimizes communication between nodes by using sparse bitvectors.</s> <s>moreover, while planet approximates split points through feature binning, yggdrasil does not require binning, and we analytically characterize the impact of this approximation.</s> <s>we evaluate yggdrasil against the mnist 8m dataset and a high-dimensional dataset at yahoo; for both, yggdrasil is faster by up to an order of magnitude.</s></p></d>", "label": ["<d><p><s>yggdrasil: an optimized system for training deep decision trees at scale</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>person re-identification is the task of matching images of a person across multiple camera views.</s> <s>almost all prior approaches address this challenge by attempting to learn the possible transformations that relate the different views of a person from a training corpora.</s> <s>then, they utilize these transformation patterns for matching a query image to those in a gallery image bank at test time.</s> <s>this necessitates learning good feature representations of the images and having a robust feature matching technique.</s> <s>deep learning approaches, such as convolutional neural networks (cnn), simultaneously do both and have shown great promise recently.</s> <s>in this work, we propose two cnn-based architectures for person re-identification.</s> <s>in the first, given a pair of images, we extract feature maps from these images via multiple stages of convolution and pooling.</s> <s>a novel inexact matching technique then matches pixels in the first representation with those of the second.</s> <s>furthermore, we search across a wider region in the second representation for matching.</s> <s>our novel matching technique allows us to tackle the challenges posed by large viewpoint variations, illumination changes or partial occlusions.</s> <s>our approach shows a promising performance and requires only about half the parameters as a current state-of-the-art technique.</s> <s>nonetheless, it also suffers from false matches at times.</s> <s>in order to mitigate this issue, we propose a fused architecture that combines our inexact matching pipeline with a state-of-the-art exact matching technique.</s> <s>we observe substantial gains with the fused model over the current state-of-the-art on multiple challenging datasets of varying sizes, with gains of up to about 21%.</s></p></d>", "label": ["<d><p><s>deep neural networks with inexact matching for person re-identification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>existing deep embedding methods in vision tasks are capable of learning a compact euclidean space from images, where euclidean distances correspond to a similarity metric.</s> <s>to make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the euclidean feature distance.</s> <s>however, the global euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions.</s> <s>in this paper, we introduce a position-dependent deep metric (pddm) unit, which is capable of learning a similarity metric adaptive to local feature structure.</s> <s>the metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner.</s> <s>the new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end.</s> <s>our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on imagenet 2010 and imagenet-10k datasets.</s></p></d>", "label": ["<d><p><s>local similarity-aware deep feature embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, partly due to a lack of practical estimators.</s> <s>they also include, as special cases, l^2 quantities which are used in many applications.</s> <s>we propose and analyze a family of estimators for sobolev quantities of unknown probability density functions.</s> <s>we bound the finite-sample bias and variance of our estimators, finding that they are generally minimax rate-optimal.</s> <s>our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints.</s> <s>we also draw theoretical connections to recent work on fast two-sample testing and empirically validate our estimators on synthetic data.</s></p></d>", "label": ["<d><p><s>efficient nonparametric smoothness estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a variant of the multiarmed bandit problem where jobs queue for service, and service rates of different servers may be unknown.</s> <s>we study algorithms that minimize queue-regret: the (expected) difference between the queue-lengths obtained by the algorithm, and those obtained by a genie-aided matching algorithm that knows exact service rates.</s> <s>a naive view of this problem would suggest that queue-regret should grow logarithmically: since queue-regret cannot be larger than classical regret, results for the standard mab problem give algorithms that ensure queue-regret increases no more than logarithmically in time.</s> <s>our paper shows surprisingly more complex behavior.</s> <s>in particular, the naive intuition is correct as long as the bandit algorithm's queues have relatively long regenerative cycles: in this case queue-regret is similar to cumulative regret, and scales (essentially) logarithmically.</s> <s>however, we show that this \"early stage\" of the queueing bandit eventually gives way to a \"late stage\", where the optimal queue-regret scaling is o(1/t).</s> <s>we demonstrate an algorithm that (order-wise) achieves this asymptotic queue-regret, and also exhibits close to optimal switching time from the early stage to the late stage.</s></p></d>", "label": ["<d><p><s>regret of queueing bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose an online convex optimization algorithm (rescaledexp) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions.</s> <s>we prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge.</s> <s>rescaledexp matches this lower bound asymptotically in the number of iterations.</s> <s>rescaledexp is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.</s></p></d>", "label": ["<d><p><s>online convex optimization with unconstrained domains and losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually.</s> <s>while very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms.</s> <s>these networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures.</s> <s>in this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning.</s> <s>to this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron.</s> <s>starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\\% while retaining or even improving the network accuracy.</s></p></d>", "label": ["<d><p><s>learning the number of neurons in deep networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the weighted k-nearest neighbors  algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning.</s> <s>the question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to  have remained unsettled.</s> <s>in this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit.</s> <s>our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors  efficiently and adaptively, for each data point whose value we wish to estimate.</s> <s>the applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.</s></p></d>", "label": ["<d><p><s>k*-nearest neighbors: from global to local</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features.</s> <s>assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition.</s> <s>our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.</s></p></d>", "label": ["<d><p><s>equality of opportunity in supervised learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of learning the underlying graph of an unknown ising model on p spins from a collection of i.i.d.</s> <s>samples generated from the model.</s> <s>we suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information theoretic lower-bound.</s> <s>our statistical estimator has a physical interpretation in terms of \"interaction screening\".</s> <s>the estimator is consistent and is efficiently implemented using convex optimization.</s> <s>we prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.</s></p></d>", "label": ["<d><p><s>interaction screening: efficient and sample-optimal learning of ising models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral clustering and co-clustering are well-known techniques in data analysis, and recent work has extended spectral clustering to square, symmetric tensors and hypermatrices derived from a network.</s> <s>we develop a new tensor spectral co-clustering method that simultaneously clusters the rows, columns, and slices of a nonnegative three-mode tensor and generalizes to tensors with any number of modes.</s> <s>the algorithm is based on a new random walk model which we call the super-spacey random surfer.</s> <s>we show that our method out-performs state-of-the-art co-clustering methods on several synthetic datasets with ground truth clusters and then use the algorithm to analyze several real-world datasets.</s></p></d>", "label": ["<d><p><s>general tensor spectral co-clustering for higher-order data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for many machine learning problems, there are some inputs that are known to be positively (or negatively) related to the output, and in such cases training the model to respect that monotonic relationship can provide regularization, and makes the model more interpretable.</s> <s>however, flexible monotonic functions are computationally challenging to learn beyond a few features.</s> <s>we break through this barrier by learning ensembles of monotonic calibrated interpolated look-up tables (lattices).</s> <s>a key contribution is an automated algorithm for selecting feature subsets for the ensemble base models.</s> <s>we demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation.</s></p></d>", "label": ["<d><p><s>fast and flexible monotonic functions with ensembles of lattices</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>learning accurate prior knowledge of natural images is of great importance for single image super-resolution (sr).</s> <s>existing sr methods either learn the prior from the low/high-resolution patch pairs or estimate the prior models from the input low-resolution (lr) image.</s> <s>specifically, high-frequency details are learned in the former methods.</s> <s>though effective, they are heuristic and have limitations in dealing with blurred lr images; while the latter suffers from the limitations of frequency aliasing.</s> <s>in this paper, we propose to combine those two lines of ideas for image super-resolution.</s> <s>more specifically, the parametric sparse prior of the desirable high-resolution (hr) image patches are learned from both the input low-resolution (lr) image and a training image dataset.</s> <s>with the learned sparse priors, the sparse codes and thus the hr image patches can be accurately recovered by solving a sparse coding problem.</s> <s>experimental results show that the proposed sr method outperforms existing state-of-the-art methods in terms of both subjective and objective image qualities.</s></p></d>", "label": ["<d><p><s>learning parametric sparse models for image super-resolution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods.</s> <s>our model family composes latent graphical models with neural network observation likelihoods.</s> <s>for inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms.</s> <s>all components are trained simultaneously with a single stochastic variational inference objective.</s> <s>we illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.</s></p></d>", "label": ["<d><p><s>composing graphical models with neural networks for structured representations and fast inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>factorizing low-rank matrices has many applications in machine learning and statistics.</s> <s>for probabilistic models in the bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases.</s> <s>here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case.</s> <s>this allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse pca.</s> <s>we also show that for a large set of parameters, an iterative algorithm called approximate message-passing is bayes optimal.</s> <s>there exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically.</s> <s>additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding.</s> <s>our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.</s></p></d>", "label": ["<d><p><s>mutual information for symmetric rank-one matrix estimation: a proof of the replica formula</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>decision tree (and its extensions such as gradient boosting decision trees and random forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability.</s> <s>with the emergence of big data, there is an increasing need to parallelize the training process of decision tree.</s> <s>however, most existing attempts along this line suffer from high communication costs.</s> <s>in this paper, we propose a new algorithm, called \\emph{parallel voting decision tree (pv-tree)}, to tackle this challenge.</s> <s>after partitioning the training data onto a number of (e.g., $m$) machines, this algorithm performs both local voting and global voting in each iteration.</s> <s>for local voting, the top-$k$ attributes are selected from each machine according to its local data.</s> <s>then, the indices of these top attributes are aggregated by a server, and the globally top-$2k$ attributes are determined by a majority voting among these local candidates.</s> <s>finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point.</s> <s>pv-tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well.</s> <s>furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability.</s> <s>our experiments on real-world datasets show that pv-tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency.</s></p></d>", "label": ["<d><p><s>a communication-efficient parallel algorithm for decision tree</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering.</s> <s>one difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset.</s> <s>this is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits.</s> <s>one limitation is that several strong assumptions were invoked to obtain provable approximation guarantees.</s> <s>in this paper we establish that these extra assumptions are not necessary?solving the sparsified problem will be almost optimal under the standard assumptions of the problem.</s> <s>we then analyze a different method of sparsification that is a better model for methods such as locality sensitive hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities.</s> <s>we validate our approach by demonstrating that it rapidly generates interpretable summaries.</s></p></d>", "label": ["<d><p><s>leveraging sparsity for efficient submodular data summarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states.</s> <s>specifically, we focus on the problem of exploration in non-tabular reinforcement learning.</s> <s>drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model.</s> <s>this technique enables us to generalize count-based exploration algorithms to the non-tabular case.</s> <s>we apply our ideas to atari 2600 games, providing sensible pseudo-counts from raw pixels.</s> <s>we transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult montezuma's revenge.</s></p></d>", "label": ["<d><p><s>unifying count-based exploration and intrinsic motivation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study causal subset selection with directed information as the measure of prediction causality.</s> <s>two typical tasks, causal sensor placement and covariate selection, are correspondingly formulated into cardinality constrained directed information maximizations.</s> <s>to attack the np-hard problems, we show that the first problem is submodular while not necessarily monotonic.</s> <s>and the second one is ``nearly'' submodular.</s> <s>to substantiate the idea of approximate submodularity, we introduce a novel quantity, namely submodularity index (smi), for general set functions.</s> <s>moreover, we show that based on smi, greedy algorithm has performance guarantee for the maximization of possibly non-monotonic and non-submodular functions, justifying its usage for a much broader class of problems.</s> <s>we evaluate the theoretical results with several case studies, and also illustrate the application of the subset selection to causal structure learning.</s></p></d>", "label": ["<d><p><s>causal meets submodular: subset selection with directed information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>learning from a few examples remains a key challenge in machine learning.</s> <s>despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data.</s> <s>in this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories.</s> <s>our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types.</s> <s>we then define one-shot learning problems on vision (using omniglot, imagenet) and language tasks.</s> <s>our algorithm improves one-shot accuracy on imagenet from 82.2% to 87.8% and from 88% accuracy to 95% accuracy on omniglot compared to competing approaches.</s> <s>we also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the penn treebank.</s></p></d>", "label": ["<d><p><s>matching networks for one shot learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we consider the problem of learning bayesian networks optimally, when subject to background knowledge in the form of ancestral constraints.</s> <s>our approach is based on a recently proposed framework for optimal structure learning based on non-decomposable scores, which is general enough to accommodate ancestral constraints.</s> <s>the proposed framework exploits oracles for learning structures using decomposable scores, which cannot accommodate ancestral constraints since they are non-decomposable.</s> <s>we show how to empower these oracles by passing them decomposable constraints that they can handle, which are inferred from ancestral constraints that they cannot handle.</s> <s>empirically, we demonstrate that our approach can be orders-of-magnitude more efficient than alternative frameworks, such as those based on integer linear programming.</s></p></d>", "label": ["<d><p><s>learning bayesian networks with ancestral constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>exemplar learning is a powerful paradigm for discovering visual similarities in an unsupervised manner.</s> <s>in this context, however, the recent breakthrough in deep learning could not yet unfold its full potential.</s> <s>with only a single positive sample, a great imbalance between one positive and many negatives, and unreliable relationships between most samples, training of convolutional neural networks is impaired.</s> <s>given weak estimates of local distance we propose a single optimization problem to extract batches of samples with mutually consistent relations.</s> <s>conflicting relations are distributed over different batches and similar samples are grouped into compact cliques.</s> <s>learning exemplar similarities is framed as a sequence of clique categorization tasks.</s> <s>the cnn then consolidates transitivity relations within and between cliques and learns a single representation for all samples without the need for labels.</s> <s>the proposed unsupervised approach has shown competitive performance on detailed posture analysis and object classification.</s></p></d>", "label": ["<d><p><s>cliquecnn: deep unsupervised exemplar learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>a central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli.</s> <s>in multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli.</s> <s>here we demonstrate that deep convolutional neural networks (cnns) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (ln) models and generalized linear models (glms).</s> <s>moreover, we find two additional surprising properties of cnns: they are less susceptible to overfitting than their ln counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g.</s> <s>between natural scenes and white noise).</s> <s>an examination of the learned cnns reveals several properties.</s> <s>first, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise.</s> <s>second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms.</s> <s>third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-poisson spiking variability observed in retinal ganglion cells.</s> <s>fourth, augmenting our cnns with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes.</s> <s>these methods can be readily generalized to other sensory modalities and stimulus ensembles.</s> <s>overall, this work demonstrates that cnns not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.</s></p></d>", "label": ["<d><p><s>deep learning models of the retinal response to natural scenes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering is an important unsupervised learning problem in machine learning and statistics.</s> <s>among many existing algorithms, kernel \\km has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity.</s> <s>there are two main approaches for kernel k-means: svd of the kernel matrix and convex relaxations.</s> <s>despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods.</s> <s>in this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both k-svd and sdp approaches are consistent in the limit, albeit sdp is strongly consistent, i.e.</s> <s>achieves exact recovery, whereas k-svd is weakly consistent, i.e.</s> <s>the fraction of misclassified nodes vanish.</s> <s>also the error bounds suggest that sdp is more resilient towards outliers, which we also demonstrate with experiments.</s></p></d>", "label": ["<d><p><s>on robustness of kernel clustering</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep convolutional neural networks (cnns) are successfully used in a number of applications.</s> <s>however, their storage and computational requirements have largely prevented their widespread use on mobile devices.</s> <s>here we present an effective cnn compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections.</s> <s>by treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals).</s> <s>a large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy.</s> <s>we relax the computational burden of convolution operations in cnns by linearly combining the convolution responses of discrete cosine transform (dct) bases.</s> <s>the compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>cnnpack: packing convolutional neural networks in the frequency domain</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal.</s> <s>one approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning.</s> <s>this approach is indirect and can be slow.</s> <s>we propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning.</s> <s>we show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free  methods in imitating complex behaviors in large, high-dimensional environments.</s></p></d>", "label": ["<d><p><s>generative adversarial imitation learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction.</s> <s>in this work, we introduce recursive maxima hunting (rmh) for variable selection in classification problems with functional data.</s> <s>in this context, variable selection techniques are especially attractive because they reduce the dimensionality, facilitate the interpretation and can improve the accuracy of the predictive models.</s> <s>the method, which is a recursive extension of maxima hunting (mh), performs variable selection by identifying the maxima of a relevance function, which measures the strength of the correlation of the predictor functional variable with the class label.</s> <s>at each stage, the information associated with the selected variable is removed by subtracting the conditional expectation of the process.</s> <s>the results of an extensive empirical evaluation are used to illustrate that, in the problems investigated, rmh has comparable or higher predictive accuracy than standard simensionality reduction techniques, such as pca and pls, and state-of-the-art feature selection methods for functional data, such as maxima hunting.</s></p></d>", "label": ["<d><p><s>feature selection in functional data classification with recursive maxima hunting</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc.</s> <s>however, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner.</s> <s>in order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework.</s> <s>we answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data.</s> <s>in this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.</s></p></d>", "label": ["<d><p><s>integrated perception with recurrent multi-task neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision.</s> <s>much of the existing work has focused on matrices with low-rank structure, and limited progress has been made on matrices with other types of structure.</s> <s>in this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized dantzig selector based on sub-gaussian measurements.</s> <s>we show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as gaussian widths of suitable sets associated with the structure of the underlying true matrix.</s> <s>further, we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms, a large family covering most matrix norms of practical interest.</s> <s>examples are provided to illustrate the utility of our theoretical development.</s></p></d>", "label": ["<d><p><s>structured matrix recovery via the generalized dantzig selector</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>nonlinear independent component analysis (ica) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable.</s> <s>here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data.</s> <s>our learning principle, time-contrastive learning (tcl),  finds a representation which allows optimal discrimination of time segments (windows).</s> <s>surprisingly, we show how tcl can be related to a nonlinear ica model, when ica is redefined to include temporal nonstationarities.</s> <s>in particular, we show that tcl combined with linear ica estimates the nonlinear ica model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ica which is rigorous, constructive, as well as very general.</s></p></d>", "label": ["<d><p><s>unsupervised feature extraction by time-contrastive learning and nonlinear ica</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of jointly inferring the $m$-best diverse labelings for a binary (high-order) submodular energy of a graphical model.</s> <s>recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures.</s> <s>it was noted that the labelings are, so-called, nested.</s> <s>this nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter $\\gamma$ give rise to different solutions.</s> <s>the popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings.</s> <s>as the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization.</s> <s>in particular, the joint $m$-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case - max-flow) solver for $m$ different values of $\\gamma$ in parallel, for certain diversity measures.</s> <s>importantly, the values for~$\\gamma$ can be computed in a closed form in advance, prior to any optimization.</s> <s>these theoretical results suggest two simple yet efficient algorithms for the joint $m$-best diverse problem, which outperform competitors in terms of runtime and quality of results.</s> <s>in particular, as we show in the paper, the new methods compute the exact $m$-best diverse labelings faster than a popular method of batra et al., which in some sense only obtains approximate solutions.</s></p></d>", "label": ["<d><p><s>joint m-best-diverse labelings as a parametric submodular minimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>minimizing a convex function over the spectrahedron, i.e., the set of all $d\\times d$ positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing.</s> <s>it is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions.</s> <s>an alternative, is the conditional gradient method (aka frank-wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting.</s> <s>the key benefit of the cg method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient.</s> <s>on the downside, the cg method, in general, converges with an inferior rate.</s> <s>the error for minimizing a $\\beta$-smooth function after $t$ iterations scales like $\\beta/t$.</s> <s>this rate does not improve even if the function is also strongly convex.</s> <s>in this work we present a modification of the cg method tailored for the spectrahedron.</s> <s>the per-iteration complexity of the method is essentially identical to that of the standard cg method: only a single eigenvecor computation is required.</s> <s>for minimizing an $\\alpha$-strongly convex and $\\beta$-smooth function, the \\textit{expected} error of the method after $t$ iterations is: $o\\left({\\min\\{\\frac{\\beta{}}{t} ,\\left({\\frac{\\beta\\sqrt{\\rank(\\x^*)}}{\\alpha^{1/4}t}}\\right)^{4/3}, \\left({\\frac{\\beta}{\\sqrt{\\alpha}\\lambda_{\\min}(\\x^*)t}}\\right)^{2}\\}}\\right)$.</s> <s>beyond the significant improvement in convergence rate,  it also follows that when the optimum is low-rank, our method provides better accuracy-rank tradeoff than the standard cg method.</s> <s>to the best of our knowledge, this is the first result that attains provably faster convergence rates for a cg variant for optimization over the spectrahedron.</s> <s>we also present encouraging preliminary empirical results.</s></p></d>", "label": ["<d><p><s>faster projection-free convex optimization over the spectrahedron</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many tasks in ai require the collaboration of multiple agents.</s> <s>typically, the communication protocol between agents is manually specified and not altered during training.</s> <s>in this paper we explore a simple neural model, called commnet, that uses continuous communication for fully cooperative tasks.</s> <s>the model consists of multiple agents and the communication between them is learned alongside their policy.</s> <s>we apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines.</s> <s>in some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.</s></p></d>", "label": ["<d><p><s>learning multiagent communication with backpropagation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper describes infogan, an information-theoretic extension to the generative adversarial network that is able to learn disentangled representations in a completely unsupervised manner.</s> <s>infogan is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation.</s> <s>we derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the wake-sleep algorithm.</s> <s>specifically, infogan successfully disentangles writing styles from digit shapes on the mnist dataset, pose from lighting of 3d rendered images, and background digits from the central digit on the svhn dataset.</s> <s>it also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the celeba face dataset.</s> <s>experiments show that infogan learns interpretable representations that are competitive with representations learned by existing fully supervised methods.</s></p></d>", "label": ["<d><p><s>infogan: interpretable representation learning by information maximizing generative adversarial nets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a new oracle-based algorithm, bistro+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d.</s> <s>or the sequence of contexts is known a priori, but where the losses are picked adversarially.</s> <s>our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order $o((kt)^{\\frac{2}{3}}(\\log n)^{\\frac{1}{3}})$, where $k$ is the number of actions, $t$ is the number of iterations, and $n$ is the number of baseline policies.</s> <s>our result is the first to break the $o(t^{\\frac{3}{4}})$ barrier achieved by recent algorithms, which was left as a major open problem.</s> <s>our analysis employs the recent relaxation framework of (rakhlin and sridharan, icml'16).</s></p></d>", "label": ["<d><p><s>improved regret bounds for oracle-based adversarial contextual bandits</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model.</s> <s>we develop two quantum algorithms for perceptron learning.</s> <s>the first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $n$, namely $o(\\sqrt{n})$.</s> <s>the second algorithm illustrates how the classical mistake bound of $o(\\frac{1}{\\gamma^2})$ can be further improved to $o(\\frac{1}{\\sqrt{\\gamma}})$ through quantum means, where $\\gamma$ denotes the margin.</s> <s>such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.</s></p></d>", "label": ["<d><p><s>quantum perceptron models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular.</s> <s>these models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation.</s> <s>in this paper, we focus primarily on parameter estimation in the models from  known upper-bounds on the intractable  log-partition function.</s> <s>we show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on ``perturb-and-map'' ideas.</s> <s>then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood.</s> <s>this can also be extended to conditional maximum likelihood.</s> <s>we illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.</s></p></d>", "label": ["<d><p><s>parameter learning for log-supermodular distributions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the amount of data available in the world is growing faster than our ability to deal with it.</s> <s>however, if we take advantage of the internal structure, data may become much smaller for machine learning purposes.</s> <s>in this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (erm), and provide faster algorithms with the help from the clustering structure of the data.</s> <s>we introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information.</s> <s>our accelerated algorithm clusteracdm is built on a novel haar transformation applied to the dual space of the erm problem, and our variance-reduction based algorithm clustersvrg introduces a new gradient estimator using clustering.</s> <s>our algorithms outperform their classical counterparts acdm and svrg respectively.</s></p></d>", "label": ["<d><p><s>exploiting the structure: stochastic gradient methods using raw clusters</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements.</s> <s>however, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception.</s> <s>here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task.</s> <s>we use feature congestion (rosenholtz et al.)</s> <s>as our non foveated clutter model, and we stack a peripheral architecture on top of feature congestion for our foveated model.</s> <s>we introduce the peripheral integration feature congestion (pifc) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity.</s> <s>we finally show that foveated feature congestion (ffc) clutter scores (r(44) = ?0.82 ?</s> <s>0.04, p < 0.0001) correlate better with target detection (hit rate) than regular feature congestion (r(44) = ?0.19 ?</s> <s>0.13, p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases.</s> <s>thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps.</s> <s>code for building peripheral representations is  available.</s></p></d>", "label": ["<d><p><s>can peripheral representations improve clutter metrics on complex scenes?</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for statistical learning in high dimension, sparse regularizations have proven useful to boost both computational and statistical efficiency.</s> <s>in some contexts, it is natural to handle more refined structures than pure sparsity, such as for instance group sparsity.</s> <s>sparse-group lasso has recently been introduced in the context of linear regression to enforce sparsity both at the feature and at the group level.</s> <s>we propose the first (provably) safe screening rules for sparse-group lasso, i.e., rules that allow to discard early in the solver features/groups that are inactive at optimal solution.</s> <s>thanks to efficient dual gap computations relying on the geometric properties of $\\epsilon$-norm, safe screening rules for sparse-group lasso lead to significant gains in term of computing time for our coordinate descent implementation.</s></p></d>", "label": ["<d><p><s>gap safe screening rules for sparse-group lasso</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>compressive sensing (cs) is an effective approach for fast magnetic resonance imaging (mri).</s> <s>it aims at reconstructing mr image from a small number of  under-sampled data in k-space, and accelerating the data acquisition in mri.</s> <s>to improve the current mri system in reconstruction accuracy and computational speed,  in this paper, we propose a novel deep architecture, dubbed admm-net.</s> <s>admm-net is defined over a data flow graph, which is derived from the iterative  procedures in alternating direction method of multipliers (admm) algorithm for optimizing a cs-based mri model.</s> <s>in the training phase, all parameters of the net, e.g., image transforms, shrinkage functions, etc., are discriminatively trained end-to-end using l-bfgs algorithm.</s> <s>in the testing phase, it has computational overhead similar to admm but uses optimized parameters learned from the  training data for cs-based reconstruction task.</s> <s>experiments on mri image  reconstruction under different sampling ratios in k-space demonstrate that it significantly improves the baseline admm algorithm and achieves high reconstruction  accuracies with fast computational speed.</s></p></d>", "label": ["<d><p><s>deep admm-net for compressive sensing mri</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we develop a novel {\\bf ho}moto{\\bf p}y  {\\bf s}moothing (hops) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and  a smooth term or  a simple non-smooth term whose proximal mapping is easy to compute.</s> <s>the best known iteration complexity for solving such non-smooth optimization problems is $o(1/\\epsilon)$ without any assumption on the strong convexity.</s> <s>in this work, we will show that the proposed  hops achieved a lower iteration complexity of $\\tilde o(1/\\epsilon^{1-\\theta})$ with $\\theta\\in(0,1]$ capturing the local sharpness of the objective function around the optimal solutions.</s> <s>to the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption.</s> <s>the hops algorithm employs nesterov's smoothing technique and nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function.</s> <s>we show that hops enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and $\\ell_1$ norm regularizer, finding a point in a polyhedron, cone programming, etc).</s> <s>experimental results verify the effectiveness of hops in comparison with nesterov's smoothing algorithm and the primal-dual style of first-order methods.</s></p></d>", "label": ["<d><p><s>homotopy smoothing for non-smooth problems with lower complexity than </s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work explores cnns for the recognition of novel categories from few examples.</s> <s>inspired by the transferability analysis of cnns, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images.</s> <s>by encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories.</s> <s>we propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators.</s> <s>the low-density separator (lds) modules can be plugged into any or all of the top layers of a standard cnn architecture.</s> <s>the resulting cnns, with enhanced generality, significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.</s></p></d>", "label": ["<d><p><s>combining low-density separators with cnns</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition.</s> <s>to examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an r-w model and a bayesian approach in a visual search task with different volatility levels.</s> <s>both r-w model and the bayesian approach reflected an individual's estimation of the environmental volatility, and there is a strong correlation between the learning rate in r-w model and the belief of stationarity in the bayesian approach in different volatility conditions.</s> <s>in a low volatility condition, r-w model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted u shape).</s> <s>the bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted u shape).</s> <s>in addition, we showed that comparing to expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from r-w model and lower belief of stationarity from the bayesian model.</s></p></d>", "label": ["<d><p><s>learning under uncertainty: a comparison between r-w and bayesian approach</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop an efficient alternating framework for learning a generalized version of factorization machine (gfm) on steaming data with provable guarantees.</s> <s>when the instances are sampled from $d$ dimensional random gaussian vectors and the target second order coefficient matrix in gfm is of rank $k$, our algorithm converges linearly, achieves $o(\\epsilon)$ recovery error after retrieving $o(k^{3}d\\log(1/\\epsilon))$ training instances, consumes $o(kd)$ memory in one-pass of dataset and only requires matrix-vector product operations in each iteration.</s> <s>the key ingredient of our framework is a construction of an estimation sequence  endowed with a so-called conditionally independent rip condition (ci-rip).</s> <s>as special cases of gfm, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.</s></p></d>", "label": ["<d><p><s>a non-convex one-pass framework for generalized factorization machine and rank-one matrix sensing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper proposes an adaptive neural-compilation framework to address the problem of learning efficient program.</s> <s>traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics.</s> <s>in contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution.</s> <s>our approach is inspired by the recent works on differentiable representations of programs.</s> <s>we show that it is possible to compile programs written in a low-level  language to a differentiable representation.</s> <s>we also show how programs in this representation can be optimised to make them efficient on a target distribution of inputs.</s> <s>experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.</s></p></d>", "label": ["<d><p><s>adaptive neural compilation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the recursive teaching dimension (rtd) of a concept class $c \\subseteq \\{0, 1\\}^n$, introduced by zilles et al.</s> <s>[zlhz11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of $c$ in the recursive teaching model.</s> <s>in this paper, we study the quantitative relation between rtd and the well-known learning complexity measure vc dimension (vcd), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the vc dimension.</s> <s>given a concept class $c \\subseteq \\{0, 1\\}^n$ with $vcd(c) = d$, we first show that $rtd(c)$ is at most $d 2^{d+1}$.</s> <s>this is the first upper bound for $rtd(c)$ that depends only on $vcd(c)$, independent of the size of the concept class $|c|$ and its~domain size $n$.</s> <s>before our work, the best known upper bound for $rtd(c)$ is $o(d 2^d \\log \\log |c|)$, obtained by moran et al.</s> <s>[mswy15].</s> <s>we remove the $\\log \\log |c|$ factor.</s> <s>we also improve the lower bound on the worst-case ratio of $rtd(c)$ to $vcd(c)$.</s> <s>we present a family of classes $\\{ c_k \\}_{k \\ge 1}$ with $vcd(c_k) = 3k$ and $rtd(c_k)=5k$, which implies that the ratio of $rtd(c)$ to $vcd(c)$ in the worst case can be as large as $5/3$.</s> <s>before our work, the largest ratio known was $3/2$ as obtained by kuhlmann [kuh99].</s> <s>since then, no finite concept class $c$ has been known to satisfy $rtd(c) > (3/2) vcd(c)$.</s></p></d>", "label": ["<d><p><s>on the recursive teaching dimension of vc classes</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>people often learn from others' demonstrations, and classic inverse reinforcement learning (irl) algorithms have brought us closer to realizing this capacity in machines.</s> <s>in contrast, teaching by demonstration has been less well studied computationally.</s> <s>here, we develop a novel bayesian model for teaching by demonstration.</s> <s>stark differences arise when demonstrators are intentionally teaching a task versus simply performing a task.</s> <s>in two experiments, we show that human participants systematically modify their teaching behavior consistent with the predictions of our model.</s> <s>further, we show that even standard irl algorithms benefit when learning from behaviors that are intentionally pedagogical.</s> <s>we conclude by discussing irl algorithms that can take advantage of intentional pedagogy.</s></p></d>", "label": ["<d><p><s>showing versus doing: teaching by demonstration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting.</s> <s>the network builds an internal plan, which is continuously updated upon observation of the next input from the environment.</s> <s>it can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to -- i.e.</s> <s>followed without replaning.</s> <s>combining these properties, the proposed model, dubbed strategic attentive writer (straw) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information.</s> <s>these macro-actions enable both structured exploration and economic computation.</s> <s>we experimentally demonstrate that straw delivers strong improvements on  several atari games by employing temporally extended planning strategies (e.g.</s> <s>ms. pacman and frostbite).</s> <s>it is at the same time a general algorithm that can be applied on any sequence data.</s> <s>to that end, we also show that when trained on text prediction task, straw naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.</s></p></d>", "label": ["<d><p><s>strategic attentive writer for learning macro-actions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a seller with an unlimited supply of a single good, who is faced with a stream of $t$ buyers.</s> <s>each buyer has a window of time in which she would like to purchase, and would buy at the lowest price in that window, provided that this price is lower than her private value (and otherwise, would not buy at all).</s> <s>in this setting, we give an algorithm that attains $o(t^{2/3})$ regret over any sequence of $t$ buyers with respect to the best fixed price in hindsight, and prove that no algorithm can perform better in the worst case.</s></p></d>", "label": ["<d><p><s>online pricing with strategic and patient buyers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>building discriminative representations for 3d data has been an important task in computer graphics and computer vision research.</s> <s>convolutional neural networks (cnns) have shown to operate on 2d images with great success for a variety of tasks.</s> <s>lifting convolution operators to 3d (3dcnns) seems like a plausible and promising next step.</s> <s>unfortunately, the computational complexity of 3d cnns grows cubically with respect to voxel resolution.</s> <s>moreover, since most 3d geometry representations are boundary based, occupied regions do not increase proportionately with the size of the discretization, resulting in wasted computation.</s> <s>in this work, we represent 3d spaces as volumetric fields, and propose a novel design that employs field probing filters to efficiently extract features from them.</s> <s>each field probing filter is a set of probing points -- sensors that perceive the space.</s> <s>our learning algorithm optimizes not only the weights associated with the probing points, but also their locations, which deforms the shape of the probing filters and adaptively distributes them in 3d space.</s> <s>the optimized probing points sense the 3d space \"intelligently\", rather than operating blindly over the entire domain.</s> <s>we show that field probing is significantly more efficient than 3dcnns, while providing state-of-the-art performance, on classification tasks for 3d object recognition benchmark datasets.</s></p></d>", "label": ["<d><p><s>fpnn: field probing neural networks for 3d data</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>non-negative matrix factorization is a popular tool for  decomposing data into feature and weight matrices under non-negativity constraints.</s> <s>it enjoys practical success but is poorly understood theoretically.</s> <s>this paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions.</s> <s>in particular, its only essential requirement on features is linear independence.</s> <s>furthermore, the algorithm uses relu to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal.</s> <s>the analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.</s></p></d>", "label": ["<d><p><s>recovery guarantee of non-negative matrix factorization  via alternating updates</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence.</s> <s>here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system.</s> <s>our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks.</s> <s>we evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics.</s> <s>our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations.</s> <s>our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.</s></p></d>", "label": ["<d><p><s>interaction networks for learning about objects, relations and physics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we investigate the statistical performance and computational efficiency of the  alternating minimization procedure for nonparametric tensor learning.</s> <s>tensor modeling has been widely used for capturing the higher order relations between  multimodal data sources.</s> <s>in addition to a linear model,  a nonlinear tensor model has been received much attention recently because of its high flexibility.</s> <s>we consider  an alternating minimization procedure for  a general nonlinear model where the true function  consists of components in a reproducing kernel hilbert space (rkhs).</s> <s>in this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm  and that the generalization error of the resultant estimator yields the minimax optimality.</s> <s>we apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.</s></p></d>", "label": ["<d><p><s>minimax optimal alternating minimization for kernel nonparametric tensor learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>for an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans.</s> <s>we propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (cirl).</s> <s>a cirl problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human?s reward function, but the robot does not initially know what this is.</s> <s>in contrast to classical irl, where the human is assumed to act optimally in isolation, optimal cirl solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment.</s> <s>we show that computing optimal joint policies in cirl games can be reduced to solving a pomdp, prove that optimality in isolation is suboptimal in cirl, and derive an approximate cirl algorithm.</s></p></d>", "label": ["<d><p><s>cooperative inverse reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables.</s> <s>by using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables.</s> <s>to carry out this optimization, we develop the first bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages.</s> <s>we present applications of our method to a number of tasks including engineering design and parameter optimization.</s></p></d>", "label": ["<d><p><s>bayesian optimization for probabilistic programs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many applications of machine learning involve structured output with large domain, where learning of structured predictor is prohibitive due to repetitive calls to expensive inference oracle.</s> <s>in this work, we show that, by decomposing training of structural support vector machine (svm) into a series of multiclass svm problems connected through messages, one can replace expensive structured oracle with factorwise maximization oracle (fmo) that allows efficient implementation of complexity sublinear to the factor domain.</s> <s>a greedy direction method of multiplier (gdmm) algorithm is proposed to exploit sparsity of messages which guarantees $\\epsilon$ sub-optimality after $o(log(1/\\epsilon))$ passes of fmo calls.</s> <s>we conduct experiments on chain-structured problems and fully-connected problems of large output domains.</s> <s>the proposed approach is orders-of-magnitude faster than the state-of-the-art training algorithms for structural svm.</s></p></d>", "label": ["<d><p><s>dual decomposed learning with factorwise oracle for structural svm of large output domain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a unified approach for learning the parameters of sum-product networks (spns).</s> <s>we prove that any complete and decomposable spn is equivalent to a mixture of trees where each tree corresponds to a product of univariate distributions.</s> <s>based on the mixture model perspective, we characterize the objective function when learning spns based on the maximum likelihood estimation (mle) principle and show that the optimization problem can be formulated as a signomial program.</s> <s>we construct two parameter learning algorithms for spns by using sequential monomial approximations (sma) and the concave-convex procedure (cccp), respectively.</s> <s>the two proposed methods naturally admit multiplicative updates, hence effectively avoiding the projection operation.</s> <s>with the help of the unified framework, we also show that, in the case of spns, cccp leads to the same algorithm as expectation maximization (em) despite the fact that they are different in general.</s></p></d>", "label": ["<d><p><s>a unified approach for learning the parameters of sum-product networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>joint matrix triangularization is often used for estimating the joint eigenstructure of a set m of matrices, with applications in signal processing and machine learning.</s> <s>we consider the problem of approximate joint matrix triangularization when the matrices in m are jointly diagonalizable and real, but we only observe a set m' of noise perturbed versions of the matrices in m. our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in m' and any exact joint triangularizer of the matrices in m. the bound depends only on the observable matrices in m' and the noise level.</s> <s>in particular, it does not depend on optimization specific properties of the triangularizer, such as its proximity to critical points, that are typical of existing bounds in the literature.</s> <s>to our knowledge, this is the first a posteriori bound for joint matrix decomposition.</s> <s>we demonstrate the bound on synthetic data for which the ground truth is known.</s></p></d>", "label": ["<d><p><s>a posteriori error bounds for joint matrix decomposition problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups.</s> <s>for very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require np hard projections when dealing with overlapping groups.</s> <s>in this paper, we show that such np-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e.</s> <s>say when two features have correlation  $\\geq 0.99$), which existing analyses cannot handle.</s> <s>these methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups.</s> <s>experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity.</s></p></d>", "label": ["<d><p><s>structured sparse regression via greedy hard thresholding</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures.</s> <s>we propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training.</s> <s>specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a gaussian process marginal likelihood objective.</s> <s>within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra.</s> <s>we show improved performance over stand alone deep networks, svms, and state of the art scalable gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, cifar, and imagenet.</s></p></d>", "label": ["<d><p><s>stochastic variational deep kernel learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a general duality between neural networks and compositional kernel hilbert spaces.</s> <s>we introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space.</s> <s>random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights.</s> <s>the kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions.</s> <s>we prove that random networks induce representations which approximate the kernel space.</s> <s>in particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks.</s></p></d>", "label": ["<d><p><s>toward deeper understanding of neural networks: the power of initialization and a dual view on expressivity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the stochastic asynchronous proximal alternating linearized minimization (sapalm) method, a block coordinate stochastic proximal-gradient method for solving nonconvex, nonsmooth optimization problems.</s> <s>sapalm is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex, nonsmooth problems.</s> <s>we prove that sapalm matches the best known rates of convergence --- among synchronous or asynchronous methods --- on this problem class.</s> <s>we provide upper bounds on the number of workers for which we can expect to see a linear speedup, which match the best bounds known for less complex problems, and show that in practice sapalm achieves this linear speedup.</s> <s>we demonstrate state-of-the-art performance on several matrix factorization problems.</s></p></d>", "label": ["<d><p><s>the sound of apalm clapping: faster nonsmooth nonconvex optimization with stochastic asynchronous palm</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce the general and powerful scheme of predicting information re-use in optimization algorithms.</s> <s>this allows us to devise a computationally efficient algorithm for bandit convex optimization with new state-of-the-art guarantees for both lipschitz loss functions and loss functions with lipschitz gradients.</s> <s>this is the first algorithm admitting both a polynomial time complexity and a regret that is polynomial in the dimension of the action space that improves upon the original regret bound for lipschitz loss functions, achieving a regret of $\\widetilde o(t^{11/16}d^{3/8})$.</s> <s>our algorithm further improves upon the best existing polynomial-in-dimension bound (both computationally and in terms of regret) for loss functions with lipschitz gradients, achieving a regret of $\\widetilde o(t^{8/13} d^{5/3})$.</s></p></d>", "label": ["<d><p><s>optimistic bandit convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a body of recent work in modeling neural activity focuses on recovering low- dimensional latent features that capture the statistical structure of large-scale neural populations.</s> <s>most such approaches have focused on linear generative models, where inference is computationally tractable.</s> <s>here, we propose flds, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state.</s> <s>this extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space.</s> <s>to fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time.</s> <s>we show that our techniques permit inference in a wide class of generative models.we also show in application to two neural datasets that, compared to state-of-the-art neural population models, flds captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability.</s></p></d>", "label": ["<d><p><s>linear dynamical neural population models through nonlinear embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimating optimal phylogenetic trees or hierarchical clustering trees from metric data is an important problem in evolutionary biology and data analysis.</s> <s>intuitively, the goodness-of-fit of a metric space to a tree depends on its inherent treeness, as well as other metric properties such as intrinsic dimension.</s> <s>existing algorithms for embedding metric spaces into tree metrics provide distortion bounds depending on cardinality.</s> <s>because cardinality is a simple property of any set, we argue that such bounds do not fully capture the rich structure endowed by the metric.</s> <s>we consider an embedding of a metric space into a tree proposed by gromov.</s> <s>by proving a stability result, we obtain an improved additive distortion bound depending only on the hyperbolicity and doubling dimension of the metric.</s> <s>we observe that gromov's method is dual to the well-known single linkage hierarchical clustering (slhc) method.</s> <s>by means of this duality, we are able to transport our results to the setting of slhc, where such additive distortion bounds were previously unknown.</s></p></d>", "label": ["<d><p><s>improved error bounds for tree representations of metric spaces</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the hard thresholding pursuit (htp) is a class of truncated gradient descent methods for finding sparse solutions of $\\ell_0$-constrained loss minimization problems.</s> <s>the htp-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications.</s> <s>however, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency.</s> <s>it remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original np-hard problem.</s> <s>in this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for htp-style methods under restricted strong condition number bounding conditions.</s> <s>we further show that htp-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number.</s> <s>numerical results on simulated data confirms our theoretical predictions.</s></p></d>", "label": ["<d><p><s>exact recovery of hard thresholding pursuit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>two-stream convolutional networks (convnets) have shown strong performance for human action recognition in videos.</s> <s>recently, residual networks (resnets) have arisen as a new technique to train extremely deep architectures.</s> <s>in this paper, we introduce spatiotemporal resnets as a combination of these two approaches.</s> <s>our novel architecture generalizes resnets for the spatiotemporal domain by introducing residual connections in two ways.</s> <s>first, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams.</s> <s>second, we transform pretrained image convnets into spatiotemporal networks by equipping these with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time.</s> <s>this approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image convnet design principles.</s> <s>the whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features.</s> <s>we evaluate our novel spatiotemporal resnet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.</s></p></d>", "label": ["<d><p><s>spatiotemporal residual networks for video action recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper addresses the challenge of jointly learning both the per-task model parameters and the inter-task relationships in a multi-task online learning setting.</s> <s>the proposed algorithm features probabilistic interpretation, efficient updating rules and flexible modulation on whether learners focus on their specific task or on jointly address all tasks.</s> <s>the paper also proves a sub-linear regret bound as compared to the best linear predictor in hindsight.</s> <s>experiments over three multi-task learning benchmark datasets show advantageous performance of the proposed approach over several state-of-the-art online multi-task learning baselines.</s></p></d>", "label": ["<d><p><s>adaptive smoothed online multi-task learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>commonly used in many applications, robust pca represents an algorithmic attempt to reduce the sensitivity of classical pca to outliers.</s> <s>the basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers.</s> <s>although the resulting problem is typically np-hard, convex relaxations provide a computationally-expedient alternative with theoretical support.</s> <s>however, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including bayesian-inspired models, have been proposed to boost estimation quality.</s> <s>unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible.</s> <s>into this mix we propose a novel pseudo-bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation.</s></p></d>", "label": ["<d><p><s>a pseudo-bayesian algorithm for robust pca</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor candecomp/parafac (cp) decomposition is a powerful but computationally challenging tool in modern data analytics.</s> <s>in this paper, we show ways of sampling intermediate steps of alternating minimization algorithms for computing low rank tensor cp decompositions, leading to the sparse alternating least squares (spals) method.</s> <s>specifically, we sample the the khatri-rao product, which arises as an intermediate object during the iterations of alternating least squares.</s> <s>this product captures the interactions between different tensor modes, and form the main computational bottleneck for solving many tensor related tasks.</s> <s>by exploiting the spectral structures of the matrix khatri-rao product, we provide efficient access to its statistical leverage scores.</s> <s>when applied to the tensor cp decomposition, our method leads to the first algorithm that runs in sublinear time per-iteration and approximates the output of deterministic alternating least squares algorithms.</s> <s>empirical evaluations of this approach show significantly speedups over existing randomized and deterministic routines for performing cp decomposition.</s> <s>on a tensor of the size 2.4m by 6.6m by 92k with over 2 billion nonzeros formed by amazon product reviews, our routine converges in two minutes to the same error as deterministic als.</s></p></d>", "label": ["<d><p><s>spals: fast alternating least squares via implicit leverage scores sampling</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables.</s> <s>our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression.</s> <s>we give numerical results to illustrate these tools on simulated data and on health record data.</s></p></d>", "label": ["<d><p><s>selective inference for group-sparse linear models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions.</s> <s>we propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (asc-pg) method, which updates based on queries to the sampling oracle using two different timescales.</s> <s>the asc-pg is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty.</s> <s>we show that the asc-pg exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases.</s> <s>we further demonstrate the application of asc-pg to reinforcement learning  and conduct numerical experiments.</s></p></d>", "label": ["<d><p><s>accelerating stochastic composition optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>an augmented lagrangian (al) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems which are then usually solved with local solvers.</s> <s>recently, surrogate-based bayesian optimization (bo) sub-solvers have been successfully deployed in the al framework for a more global search in the presence of inequality constraints; however a drawback was that expected improvement (ei) evaluations relied on monte carlo.</s> <s>here we introduce an alternative slack variable al, and show that in this formulation the ei may be evaluated with library routines.</s> <s>the slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof.</s> <s>we show our new slack \"albo\" compares favorably to the original.</s> <s>its superiority over conventional alternatives is reinforced on several new mixed constraint examples.</s></p></d>", "label": ["<d><p><s>bayesian optimization under mixed constraints  with a slack-variable augmented lagrangian</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers.</s> <s>an unknown set of $\\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially.</s> <s>the manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an fraction of low-quality items.</s> <s>perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with $\\tilde{o}(1/\\beta\\alpha\\epsilon^4)$ ratings per worker, and $\\tilde{o}(1/\\beta\\epsilon^2)$ ratings by the manager, where $\\beta$ is the fraction of high-quality items.</s> <s>our results extend to the more general setting of peer prediction, including peer grading in online classrooms.</s></p></d>", "label": ["<d><p><s>avoiding imposters and delinquents: adversarial crowdsourcing and peer prediction</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers.</s> <s>a recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward.</s> <s>in fact, random feedback weights work evenly well, because the network learns how to make the feedback useful.</s> <s>in this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition.</s> <s>the error is propagated through fixed random feedback connections directly from the output layer to each hidden layer.</s> <s>this simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation.</s> <s>the method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required.</s> <s>experiments show that the test performance on mnist and cifar is almost as good as those obtained with back-propagation for fully connected networks.</s> <s>if combined with dropout, the method achieves 1.45% error on the permutation invariant mnist task.</s></p></d>", "label": ["<d><p><s>direct feedback alignment provides learning in deep neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>for massive and heterogeneous modern  data sets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited.</s> <s>in the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data.</s> <s>this allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy.</s> <s>theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.</s></p></d>", "label": ["<d><p><s>computational and statistical tradeoffs in learning to rank</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a semi-parametric bayesian model for survival analysis.</s> <s>the model is centred on a parametric baseline hazard, and uses a gaussian process to model variations away from it nonparametrically, as well as  dependence on covariates.</s> <s>as opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function.</s> <s>furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis.</s> <s>we propose a mcmc algorithm to perform inference and an approximation scheme based on random fourier features to make computations faster.</s> <s>we report experimental results on synthetic and real data, showing that our model performs better than competing models such as cox proportional hazards, anova-ddp and random survival forests.</s></p></d>", "label": ["<d><p><s>gaussian processes for survival analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>feature selection is one of the most fundamental problems in machine learning.</s> <s>an extensive body of work on information-theoretic feature selection exists which is based on maximizing mutual information between subsets of features and class labels.</s> <s>practical methods are forced to rely on approximations due to the difficulty of estimating mutual information.</s> <s>we demonstrate that approximations made by existing methods are based on unrealistic assumptions.</s> <s>we formulate a more flexible and general class of assumptions based on variational distributions and use them to tractably generate lower bounds for mutual information.</s> <s>these bounds define a novel information-theoretic framework for feature selection, which we prove to be optimal under tree graphical models with proper choice of variational distributions.</s> <s>our experiments demonstrate that the proposed method strongly outperforms existing information-theoretic feature selection approaches.</s></p></d>", "label": ["<d><p><s>variational information maximization for feature selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of robust pca in the fully and partially observed settings.</s> <s>without corruptions, this is the well-known matrix completion problem.</s> <s>from a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood.</s> <s>this paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms.</s> <s>in particular, in the fully observed case, with $r$ denoting rank and $d$ dimension, we reduce the complexity from $o(r^2d^2\\log(1/\\epsilon))$ to $o(rd^2\\log(1/\\epsilon))$ -- a big savings when the rank is big.</s> <s>for the partially observed case, we show the complexity of our algorithm is no more than $o(r^4d\\log(d)\\log(1/\\epsilon))$.</s> <s>not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where $r$ is small compared to $d$, it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.</s></p></d>", "label": ["<d><p><s>fast algorithms for robust pca via gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods.</s> <s>however, applications of these methods to multimodality remain limited.</s> <s>we present multimodal residual networks (mrn) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning.</s> <s>unlike the deep residual learning, mrn effectively learns the joint representation from visual and language information.</s> <s>the main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies.</s> <s>various alternative models introduced by multimodality are explored based on our study.</s> <s>we achieve the state-of-the-art results on the visual qa dataset for both open-ended and multiple-choice tasks.</s> <s>moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.</s></p></d>", "label": ["<d><p><s>multimodal residual learning for visual qa</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider the problem of optimization from samples of monotone submodular functions with bounded curvature.</s> <s>in numerous applications, the function optimized is not known a priori, but instead learned from data.</s> <s>what are the guarantees we have when optimizing functions from sampled data?</s> <s>in this paper we show that for any monotone submodular function with curvature c there is a (1 - c)/(1 + c - c^2) approximation algorithm for maximization under cardinality constraints when polynomially-many samples are drawn from the uniform distribution over feasible sets.</s> <s>moreover, we show that this algorithm is optimal.</s> <s>that is, for any c < 1, there exists a submodular function with curvature c for which no algorithm can achieve a better approximation.</s> <s>the curvature assumption is crucial as for general monotone submodular functions no algorithm can obtain a constant-factor approximation for maximization under a cardinality constraint when observing polynomially-many samples drawn from any distribution over feasible sets, even when the function is statistically learnable.</s></p></d>", "label": ["<d><p><s>the power of optimization from samples</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>segmentation of 3d images is a fundamental problem in biomedical image analysis.</s> <s>deep learning (dl) approaches have achieved the state-of-the-art segmentation performance.</s> <s>to exploit the 3d contexts using neural networks, known dl segmentation methods, including 3d convolution, 2d convolution on the planes orthogonal to 2d slices, and lstm in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3d biomedical images.</s> <s>in this paper, we propose a new dl framework for 3d image segmentation, based on a combination of a fully convolutional network (fcn) and a recurrent neural network (rnn), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively.</s> <s>to our best knowledge, this is the first dl framework for 3d image segmentation that explicitly leverages 3d image anisotropism.</s> <s>evaluating using a dataset from the isbi neuronal structure segmentation challenge and in-house image stacks for 3d fungus segmentation, our approach achieves promising results, comparing to the known dl-based 3d segmentation approaches.</s></p></d>", "label": ["<d><p><s>combining fully convolutional and recurrent neural networks for 3d biomedical image segmentation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a framework for semi-supervised active clustering framework (ssac), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not.</s> <s>we study the query and computational complexity of clustering in this framework.</s> <s>we consider a setting where the expert conforms to a center-based clustering with a notion of margin.</s> <s>we show that there is a trade off between computational complexity and query complexity; we prove that for the case of $k$-means clustering (i.e., when the expert conforms to a solution of $k$-means), having access to relatively few such queries allows efficient solutions to otherwise np hard problems.</s> <s>in particular, we provide a probabilistic polynomial-time (bpp) algorithm  for clustering in this setting that asks $o\\big(k^2\\log k + k\\log n)$ same-cluster queries and runs with time complexity $o\\big(kn\\log n)$ (where $k$ is the number of clusters and $n$ is the number of instances).</s> <s>the success of the algorithm is guaranteed for data satisfying the margin condition under which, without queries, we show that the problem is np hard.</s> <s>we also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting.</s></p></d>", "label": ["<d><p><s>clustering with same-cluster queries</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round.</s> <s>specifically, we show that the sleeping versions of these problems are at least as hard as pac learning dnf expressions, a long standing open problem.</s> <s>we show hardness for the sleeping versions of online shortest paths, online minimum spanning tree, online k-subsets, online k-truncated permutations, online minimum cut, and online bipartite matching.</s> <s>the hardness result for the sleeping version of the online shortest paths problem resolves an open problem presented at colt 2015 [koolen et al., 2015].</s></p></d>", "label": ["<d><p><s>hardness of online sleeping combinatorial optimization problems</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions.</s> <s>the underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions.</s> <s>in this paper we incorporate the biologically plausible mechanism of inhibition of return into the learned region sparsity model, thereby imposing diversity on the selected regions.</s> <s>we investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks.</s> <s>we report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations.</s> <s>notably, the classification performance of the extended model  remains the same as the original.</s> <s>this work suggests a new computational perspective on visual attention mechanisms and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.</s></p></d>", "label": ["<d><p><s>learned region sparsity and diversity also predicts visual attention</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>gaussian process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions.</s> <s>one example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation.</s> <s>most methods for this so-called ?bayesian optimization?</s> <s>only allow sequential exploration of the parameter space.</s> <s>however, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal.</s> <s>batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios.</s> <s>in this paper, we propose a new approach for parallelizing bayesian optimization by modeling the diversity of a batch via determinantal point processes (dpps) whose kernels are learned automatically.</s> <s>this allows us to generalize a previous result as well as prove better regret bounds based on dpp sampling.</s> <s>our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our dpp-based methods, especially those based on dpp sampling, outperform state-of-the-art methods.</s></p></d>", "label": ["<d><p><s>batched gaussian process bandit optimization via determinantal point processes</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems.</s> <s>in particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level.</s> <s>examples of such events include disease transmission, opinion transition in elections, and rumor propagation.</s> <s>unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level.</s> <s>in order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly ?</s> <s>rather than exponentially?</s> <s>with the number of individuals.</s> <s>to validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years.</s> <s>the proposed algorithm was used to track disease transmission and predict the probability of infection for each individual.</s> <s>our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.</s></p></d>", "label": ["<d><p><s>using social dynamics to make individual predictions: variational inference with a stochastic kinetic model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital.</s> <s>in this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient.</s> <s>the algorithm addresses several known challenges with clinical state estimation such as eliminating bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states.</s> <s>these benefits are obtained by combining the tools of non-parametric bayesian inference, permutation testing, and generalizations of the empirical bernstein inequality.</s> <s>the algorithm is validated using real-world data from a cancer ward in a large academic hospital.</s></p></d>", "label": ["<d><p><s>a non-parametric learning method for confidently estimating patient's clinical state and dynamics</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the follow the leader (ftl) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are positively curved.</s> <s>in this paper we ask whether there are other \"lucky\" settings when ftl achieves sublinear, \"small\" regret.</s> <s>in particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain.</s> <s>amongst other results, we prove that the curvature of  the boundary of the domain can act as if the losses were curved: in this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, ftl enjoys a logarithmic growth rate of regret, while, e.g., for polyhedral domains and stochastic data it enjoys finite expected regret.</s> <s>building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for ftl.</s></p></d>", "label": ["<d><p><s>following the leader and fast rates in linear prediction: curved constraint sets and other regularities</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose probabilistic latent variable models for multi-view anomaly detection, which is the task of finding instances that have inconsistent views given multi-view data.</s> <s>with the proposed model, all views of a non-anomalous instance are assumed to be generated from a single latent vector.</s> <s>on the other hand, an anomalous instance is assumed to have multiple latent vectors, and its different views are generated from different latent vectors.</s> <s>by inferring the number of latent vectors used for each instance with dirichlet process priors, we obtain multi-view anomaly scores.</s> <s>the proposed model can be seen as a robust extension of probabilistic canonical correlation analysis for noisy multi-view data.</s> <s>we present bayesian inference procedures for the proposed model based on a stochastic em algorithm.</s> <s>the effectiveness of the proposed model is demonstrated in terms of performance when detecting multi-view anomalies.</s></p></d>", "label": ["<d><p><s>multi-view anomaly detection via robust probabilistic latent variable models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the covariance matrix adaptation evolution strategy (cma-es) is arguably one of the most powerful real-valued derivative-free optimization algorithms, finding many applications in machine learning.</s> <s>the cma-es is a monte carlo method, sampling from a sequence of multi-variate gaussian distributions.</s> <s>given the function values at the sampled points, updating and storing the covariance matrix dominates the time and space complexity in each iteration of the algorithm.</s> <s>we propose a numerically stable quadratic-time covariance matrix update scheme with minimal memory requirements based on maintaining triangular cholesky factors.</s> <s>this requires a modification of the cumulative step-size adaption (csa) mechanism in the cma-es, in which we replace the inverse of the square root of the covariance matrix by the inverse of the triangular cholesky factor.</s> <s>because the triangular cholesky factor changes smoothly with the matrix square root, this modification does not change the behavior of the cma-es in terms of required objective function evaluations as verified empirically.</s> <s>thus, the described algorithm can and should replace the standard cma-es if updating and storing the covariance matrix matters.</s></p></d>", "label": ["<d><p><s>cma-es with optimal covariance update and storage complexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper we establish a duality between boosting and svm, and use this to derive a novel discriminant dimensionality reduction algorithm.</s> <s>in particular, using the multiclass formulation of boosting and svm we note that both use a combination of mapping and linear classification to maximize the multiclass margin.</s> <s>in svm this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers.</s> <s>in boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through combination of weak learners.</s> <s>we argue that the intermediate mapping, e.g.</s> <s>boosting predictor, is preserving the discriminant aspects of the data and by controlling the dimension of this mapping it is possible to achieve discriminant low dimensional representations for the data.</s> <s>we use the aforementioned duality and propose a new method, large margin discriminant dimensionality reduction (ladder) that jointly learns the mapping and the linear classifiers in an efficient manner.</s> <s>this leads to a data-driven mapping which can embed data into any number of dimensions.</s> <s>experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.</s></p></d>", "label": ["<d><p><s>large margin discriminant dimensionality reduction in prediction space</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the stochastic optimization of canonical correlation analysis (cca), whose objective is nonconvex and does not decouple over training samples.</s> <s>although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them.</s> <s>inspired by the alternating least squares/power iterations formulation of cca, and the shift-and-invert preconditioning method for pca, we propose two globally convergent meta-algorithms for cca, both of which transform the original problem into sequences of least squares problems that need only be solved approximately.</s> <s>we instantiate the meta-algorithms with state-of-the-art sgd methods and obtain time complexities that significantly improve upon that of previous work.</s> <s>experimental results demonstrate their superior performance.</s></p></d>", "label": ["<d><p><s>efficient globally convergent stochastic optimization for canonical correlation analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>low-rank matrix factorizations arise in a wide variety of applications -- including recommendation systems, topic models, and source separation, to name just a few.</s> <s>in these and many other applications, it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models, significant improvements are possible in practice.</s> <s>however, despite the reported superior empirical performance of these dynamic models over their static counterparts, there is limited theoretical justification for introducing these more complex models.</s> <s>in this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations.</s> <s>first, we propose the locally weighted matrix smoothing (lowems) framework as one possible approach to dynamic matrix recovery.</s> <s>we then establish error bounds for lowems in both the {\\em matrix sensing} and {\\em matrix completion} observation models.</s> <s>our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity.</s> <s>to illustrate these benefits we provide both synthetic and real-world experimental results.</s></p></d>", "label": ["<d><p><s>dynamic matrix recovery from incomplete observations under an exact low-rank constraint</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the move from hand-designed features to learned features in machine learning has been wildly successful.</s> <s>in spite of this, optimization algorithms are still designed by hand.</s> <s>in this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way.</s> <s>our learned algorithms, implemented by lstms, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure.</s> <s>we demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.</s></p></d>", "label": ["<d><p><s>learning to learn by gradient descent by gradient descent</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>arising from many applications at the intersection of decision-making and machine learning, marginal maximum a posteriori (marginal map) problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them.</s> <s>we propose xor_mmap, a novel approach to solve the marginal map problem, which represents the intractable counting subproblem with queries to np oracles, subject to additional parity constraints.</s> <s>xor_mmap provides a constant factor approximation to the marginal map problem, by encoding it as a single optimization in a  polynomial size of the original problem.</s> <s>we evaluate our approach in several machine learning and decision-making applications, and show that our approach outperforms several state-of-the-art marginal map solvers.</s></p></d>", "label": ["<d><p><s>solving marginal map problems with np oracles and parity constraints</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose a novel approach to reduce the computational cost of evaluation of convolutional neural networks, a factor that has hindered their deployment in low-power devices such as mobile phones.</s> <s>inspired by the loop perforation technique from source code optimization, we speed up the bottleneck convolutional layers by skipping their evaluation in some of the spatial positions.</s> <s>we propose and analyze several strategies of choosing these positions.</s> <s>we demonstrate that perforation can accelerate modern convolutional networks such as alexnet and vgg-16 by a factor of 2x - 4x.</s> <s>additionally, we show that perforation is complementary to the recently proposed acceleration method of zhang et al.</s></p></d>", "label": ["<d><p><s>perforatedcnns: acceleration through elimination of redundant convolutions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we suggest a new loss for learning deep embeddings.</s> <s>the key characteristics of the new loss is the absence of tunable parameters and very good results obtained across a range of datasets and problems.</s> <s>the loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) point pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on these probability estimates.</s> <s>we show that these operations can be performed in a simple and piecewise-differentiable manner using 1d histograms with soft assignment operations.</s> <s>this makes the proposed loss suitable for learning deep embeddings using stochastic optimization.</s> <s>the experiments reveal favourable results compared to recently proposed loss functions.</s></p></d>", "label": ["<d><p><s>learning deep embeddings with histogram loss</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we present region-based, fully convolutional networks for accurate and efficient object detection.</s> <s>in contrast to previous region-based detectors such as fast/faster r-cnn that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image.</s> <s>to achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection.</s> <s>our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest residual networks (resnets), for object detection.</s> <s>we show competitive results on the pascal voc datasets (e.g., 83.6% map on the 2007 set) with the 101-layer resnet.</s> <s>meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the faster r-cnn counterpart.</s> <s>code is made publicly available at: https://github.com/daijifeng001/r-fcn.</s></p></d>", "label": ["<d><p><s>r-fcn: object detection via region-based fully convolutional networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>despite the success of kernel-based nonparametric methods, kernel selection still requires considerable expertise, and is often described as a ?black art.?</s> <s>we present a sophisticated method for automatically searching for an appropriate kernel from an infinite space of potential choices.</s> <s>previous efforts in this direction have focused on traversing a kernel grammar, only examining the data via computation of marginal likelihood.</s> <s>our proposed search method is based on bayesian optimization in model space, where we reason about model evidence as a function to be maximized.</s> <s>we explicitly reason about the data distribution and how it induces similarity between potential model choices in terms of the explanations they can offer for observed data.</s> <s>in this light, we construct a novel kernel between models to explain a given dataset.</s> <s>our method is capable of finding a model that explains a given dataset well without any human assistance, often with fewer computations of model evidence than previous approaches, a claim we demonstrate empirically.</s></p></d>", "label": ["<d><p><s>bayesian optimization for automated model selection</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in stochastic convex optimization the goal is to minimize a convex function $f(x) \\doteq \\e_{f\\sim d}[f(x)]$ over a convex set $\\k \\subset \\r^d$ where $d$ is some unknown distribution and each $f(\\cdot)$ in the support of $d$ is convex over $\\k$.</s> <s>the optimization is based on i.i.d.~samples $f^1,f^2,\\ldots,f^n$ from $d$.</s> <s>a common approach to such problems is empirical risk minimization (erm) that optimizes $f_s(x) \\doteq \\frac{1}{n}\\sum_{i\\leq n} f^i(x)$.</s> <s>here we consider the question of how many samples are necessary for erm to succeed and the closely related question of uniform convergence of $f_s$ to $f$ over $\\k$.</s> <s>we demonstrate that in the standard $\\ell_p/\\ell_q$ setting of lipschitz-bounded functions over a $\\k$ of bounded radius, erm requires sample size that scales linearly with the dimension $d$.</s> <s>this nearly matches standard upper bounds and improves on $\\omega(\\log d)$ dependence proved for $\\ell_2/\\ell_2$ setting in (shalev-shwartz et al.</s> <s>2009).</s> <s>in stark contrast, these problems can be solved using dimension-independent number of samples for $\\ell_2/\\ell_2$ setting and $\\log d$ dependence for $\\ell_1/\\ell_\\infty$ setting using other approaches.</s> <s>we also demonstrate that for a more general class of range-bounded (but not lipschitz-bounded) stochastic convex programs an even stronger gap appears already in dimension 2.</s></p></d>", "label": ["<d><p><s>generalization of erm in stochastic convex optimization: the dimension strikes back</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>the softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems.</s> <s>however, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant.</s> <s>here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability.</s> <s>this bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization.</s> <s>it allows us to perform doubly stochastic estimation by subsampling both training instances and class labels.</s> <s>we show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems.</s></p></d>", "label": ["<d><p><s>one-vs-each approximation to softmax for scalable estimation of probabilities</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>while neural machine translation (nmt) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training.</s> <s>however, human labeling is very costly.</s> <s>to tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an nmt system to automatically learn from unlabeled data through a dual-learning game.</s> <s>this mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., english-to-french translation (primal) versus french-to-english translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler.</s> <s>in the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process.</s> <s>based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods).</s> <s>we call the corresponding approach to neural machine translation \\emph{dual-nmt}.</s> <s>experiments show that dual-nmt works very well on english$\\leftrightarrow$french translation; especially, by learning from monolingual data (with 10\\% bilingual data for warm start), it achieves a comparable accuracy to nmt trained from the full bilingual data for the french-to-english translation task.</s></p></d>", "label": ["<d><p><s>dual learning for machine translation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>neural codes are inevitably shaped by various kinds of biological constraints, \\emph{e.g.}</s> <s>noise and metabolic cost.</s> <s>here we formulate a coding framework which explicitly deals with noise and the metabolic costs associated with the neural representation of information, and analytically derive the optimal neural code for monotonic response functions and arbitrary stimulus distributions.</s> <s>for a single neuron, the theory predicts a family of optimal response functions depending on the metabolic budget and noise characteristics.</s> <s>interestingly, the well-known histogram equalization solution can be viewed as a special case when metabolic resources are unlimited.</s> <s>for a pair of neurons, our theory suggests that under more severe metabolic constraints, on-off coding is an increasingly more efficient coding scheme compared to on-on or off-off.</s> <s>the advantage could be as large as one-fold, substantially larger than the previous estimation.</s> <s>some of these predictions could be generalized to the case of large neural populations.</s> <s>in particular, these analytical results may provide a theoretical basis for the predominant segregation into on- and off-cells in early visual processing areas.</s> <s>overall, we provide a unified framework for optimal neural codes with monotonic tuning curves in the brain, and makes predictions that can be directly tested with physiology experiments.</s></p></d>", "label": ["<d><p><s>efficient neural codes under metabolic constraints</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we consider convex-concave saddle-point problems where the objective functions may be split in many components, and extend recent stochastic variance reduction methods (such as svrg or saga) to provide the first  large-scale linearly convergent algorithms for this class of problems which are common in machine learning.</s> <s>while the algorithmic extension is straightforward, it comes with challenges and opportunities: (a) the convex minimization analysis does not apply and we use the notion of monotone operators to prove convergence, showing in particular that the same algorithm applies to a larger class of problems, such as variational inequalities,  (b) there are two notions of splits, in terms of functions, or in terms of partial derivatives, (c) the split does need to be done with convex-concave terms, (d) non-uniform sampling is key to an efficient algorithm, both in theory and practice, and (e)  these incremental algorithms can be easily accelerated using a simple extension of the \"catalyst\" framework,  leading to an algorithm which is always superior to accelerated batch algorithms.</s></p></d>", "label": ["<d><p><s>stochastic variance reduction methods for saddle-point problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>weighted minwise hashing (wmh) is one of the fundamental subroutine, required by many celebrated approximation algorithms, commonly adopted in industrial practice for large -scale search and learning.</s> <s>the resource bottleneck with wmh is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data.</s> <s>we propose a simple rejection type sampling scheme based on a carefully designed red-green map, where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling.</s> <s>the running time of our method,  for many practical datasets, is an order of magnitude smaller than existing methods.</s> <s>experimental evaluations, on real datasets, show that for computing 500 wmh, our proposal can be 60000x faster than the ioffe's method without losing any accuracy.</s> <s>our method is also around 100x faster than approximate heuristics capitalizing on the efficient ``densified\" one permutation hashing schemes~\\cite{proc:onehashlsh_icml14,proc:shrivastava_uai14}.</s> <s>given the simplicity of our approach and its significant advantages, we hope that it will replace existing implementations in practice.</s></p></d>", "label": ["<d><p><s>simple and efficient weighted minwise hashing</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recent work on scaling up gaussian process regression (gpr) to large datasets has primarily focused on sparse gpr, which leverages a small set of basis functions to approximate the full gaussian process during inference.</s> <s>however, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory.</s> <s>although previous work has considered incrementally solving variational sparse gpr, most algorithms fail to update the basis functions and therefore perform suboptimally.</s> <s>we propose a novel incremental learning algorithm for variational sparse gpr based on stochastic mirror ascent of probability densities in reproducing kernel hilbert space.</s> <s>this new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence.</s> <s>we conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than  the recent state-of-the-art incremental solutions to variational sparse gpr.</s></p></d>", "label": ["<d><p><s>incremental variational sparse gaussian process regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance).</s> <s>we quantify the friendliness of stochastic environments by means of the well-known bernstein (a.k.a.</s> <s>generalized tsybakov margin) condition.</s> <s>for two recent algorithms (squint for the hedge setting and metagrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the bernstein parameters of the stochastic environment.</s> <s>we prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.</s></p></d>", "label": ["<d><p><s>combining adversarial guarantees and stochastic fast rates in online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a general framework for classification of sparse and irregularly-sampled time series.</s> <s>the properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixed-dimensional feature spaces.</s> <s>to address these challenges, we propose an uncertainty-aware classification framework based on a special computational layer we refer to as the gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent.</s> <s>we show how to scale up the required computations based on combining the structured kernel interpolation framework and the lanczos approximation method, and how to discriminatively train the gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.</s></p></d>", "label": ["<d><p><s>a scalable end-to-end gaussian process adapter for irregularly sampled time series classification</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>behavioral experiments on humans and animals suggest that the brain performs probabilistic inference to interpret its environment.</s> <s>here we present a new general-purpose, biologically-plausible neural implementation of approximate inference.</s> <s>the neural network represents uncertainty using probabilistic population codes (ppcs), which are distributed neural representations that naturally encode probability distributions, and support marginalization and evidence integration in a biologically-plausible manner.</s> <s>by connecting multiple ppcs together as a probabilistic graphical model, we represent multivariate probability distributions.</s> <s>approximate inference in graphical models can be accomplished by message-passing algorithms that disseminate local information throughout the graph.</s> <s>an attractive and often accurate example of such an algorithm is loopy belief propagation (lbp), which uses local marginalization and evidence integration operations to perform approximate inference efficiently even for complex models.</s> <s>unfortunately, a subtle feature of lbp renders it neurally implausible.</s> <s>however, lbp can be elegantly reformulated as a sequence of tree-based reparameterizations (trp) of the graphical model.</s> <s>we re-express the trp updates as a nonlinear dynamical system with both fast and slow timescales, and show that this produces a neurally plausible solution.</s> <s>by combining all of these ideas, we show that a network of ppcs can represent multivariate probability distributions and implement the trp updates to perform probabilistic inference.</s> <s>simulations with gaussian graphical models demonstrate that the neural network inference quality is comparable to the direct evaluation of lbp and robust to noise, and thus provides a promising mechanism for general probabilistic inference in the population codes of the brain.</s></p></d>", "label": ["<d><p><s>inference by reparameterization in neural population codes</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>good sparse approximations are essential for practical inference in gaussian processes as the computational cost of exact methods is prohibitive for large datasets.</s> <s>the fully independent training conditional (fitc) and the variational free energy (vfe) approximations are two recent popular methods.</s> <s>despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice.</s> <s>we thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.</s></p></d>", "label": ["<d><p><s>understanding probabilistic sparse gaussian process approximations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-means.</s> <s>however, k-means++ seeding, the state of the art algorithm, does not scale well to massive datasets as it is inherently sequential and requires k full passes through the data.</s> <s>it was recently shown that markov chain monte carlo sampling can be used to efficiently approximate the seeding step of k-means++.</s> <s>however, this result requires assumptions on the data generating distribution.</s> <s>we propose a simple yet fast seeding algorithm that produces *provably* good clusterings even *without assumptions* on the data.</s> <s>our analysis shows that the algorithm allows for a favourable trade-off between solution quality and computational cost, speeding up k-means++ seeding by up to several orders of magnitude.</s> <s>we validate our theoretical results in extensive experiments on a variety of real-world data sets.</s></p></d>", "label": ["<d><p><s>fast and provably good seedings for k-means</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates.</s> <s>in particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra.</s> <s>the typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise.</s> <s>as such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit.</s> <s>we address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise.</s> <s>building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy.</s> <s>equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of dirac vectors located at the target fundamental frequencies (musical pitch values).</s> <s>this in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.</s></p></d>", "label": ["<d><p><s>optimal spectral transportation with application to music transcription</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>matching users to the right items at the right time is a fundamental task in recommendation systems.</s> <s>as users interact with different items over time, users' and items' feature may evolve and co-evolve over time.</s> <s>traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions.</s> <s>we propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature.</s> <s>to learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints.</s> <s>extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.</s></p></d>", "label": ["<d><p><s>coevolutionary latent feature processes for continuous-time user-item interactions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a new algorithm is proposed which accelerates the mini-batch k-means algorithm of sculley (2010) by using the distance bounding approach of elkan (2003).</s> <s>we argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused.</s> <s>to this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t+1.</s> <s>using nested mini-batches presents two difficulties.</s> <s>the first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids.</s> <s>the second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down.</s> <s>experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1\\% of the empirical minimum 100 times earlier than the standard mini-batch algorithm.</s></p></d>", "label": ["<d><p><s>nested mini-batch k-means</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the importance of studying the robustness of learners to malicious data is well established.</s> <s>while much work has been done establishing both robust estimators and effective data injection attacks when the attacker is omniscient, the ability of an attacker to provably harm learning while having access to little information is largely unstudied.</s> <s>we study the potential of a ?blind attacker?</s> <s>to provably limit a learner?s performance by data injection attack without observing the learner?s training set or any parameter of the distribution from which it is drawn.</s> <s>we provide examples of simple yet effective attacks in two settings: firstly, where an ?informed learner?</s> <s>knows the strategy chosen by the attacker, and secondly, where a ?blind learner?</s> <s>knows only the proportion of malicious data and some family to which the malicious distribution chosen by the attacker belongs.</s> <s>for each attack, we analyze minimax rates of convergence and establish lower bounds on the learner?s minimax risk, exhibiting limits on a learner?s ability to learn under data injection attack even when the attacker is ?blind?.</s></p></d>", "label": ["<d><p><s>blind attacks on machine learners</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>maximum mean discrepancy (mmd) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing.</s> <s>this distance is based on the notion of embedding probabilities in a reproducing kernel hilbert space.</s> <s>in this paper, we present the first known lower bounds for the estimation of mmd based on finite samples.</s> <s>our lower bounds hold for any radial universal kernel on $\\r^d$ and match the existing upper bounds up to constants that depend only on the properties of the kernel.</s> <s>using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its $u$-statistic variant, which are usually employed in applications.</s></p></d>", "label": ["<d><p><s>minimax estimation of maximum mean discrepancy with radial kernels</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of recovering a high-dimensional but structured vector from linear observations in a general setting where the vector can come from an arbitrary union of subspaces.</s> <s>this setup includes well-studied problems such as compressive sensing and low-rank matrix recovery.</s> <s>we show how to design more efficient algorithms for the union-of subspace recovery problem by using *approximate* projections.</s> <s>instantiating our general framework for the low-rank matrix recovery problem gives the fastest provable running time for an algorithm with optimal sample complexity.</s> <s>moreover, we give fast approximate projections for 2d histograms, another well-studied low-dimensional model of data.</s> <s>we complement our theoretical results with experiments demonstrating that our framework also leads to improved time and sample complexity empirically.</s></p></d>", "label": ["<d><p><s>fast recovery from a union of subspaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a general theoretical analysis of structured prediction with a series of new results.</s> <s>we give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition.</s> <s>these are the tightest margin bounds known for both standard multi-class and general structured prediction problems.</s> <s>our guarantees are expressed in terms of a data-dependent complexity measure, \\emph{factor graph complexity}, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets, and a sparsity measure for features and graphs.</s> <s>our proof techniques include generalizations of talagrand's contraction lemma that can be of independent interest.</s> <s>we further extend our theory by leveraging the principle of voted risk minimization (vrm) and show that learning is possible even with complex factor graphs.</s> <s>we present new learning bounds for this advanced setting, which we use to devise two new algorithms, \\emph{voted conditional random field} (vcrf) and \\emph{voted structured boosting} (structboost).</s> <s>these algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees.</s> <s>we also report the results of experiments with vcrf on several datasets to validate our theory.</s></p></d>", "label": ["<d><p><s>structured prediction theory based on factor graph complexity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the use of bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide.</s> <s>standard bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible.</s> <s>recent work on scaling bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration.</s> <s>we leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset.</s> <s>we can then use this small coreset in any number of existing posterior inference algorithms without modification.</s> <s>in this paper, we develop an efficient coreset construction algorithm for bayesian logistic regression models.</s> <s>we provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models.</s> <s>crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort.</s> <s>we demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size.</s> <s>furthermore, constructing the coreset takes a negligible amount of time compared to that required to run mcmc on it.</s></p></d>", "label": ["<d><p><s>coresets for scalable bayesian logistic regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations.</s> <s>in contrast to previous cnn-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity.</s> <s>our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with $o(n)$ feedforward passes for n keypoints, instead of $o(n^2)$ for typical patch similarity methods.</s> <s>we propose a convolutional spatial transformer to mimic patch normalization in traditional features like sift, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations.</s> <s>extensive experiments on kitti, pascal and cub-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features.</s></p></d>", "label": ["<d><p><s>universal correspondence network</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>proteins are the \"building blocks of life\", the most abundant organic molecules, and the central focus of most areas of biomedicine.</s> <s>protein structure is strongly related to protein function, thus structure prediction is a crucial task on the way to solve many biological questions.</s> <s>a contact map is a compact representation of the three-dimensional structure of a protein via the pairwise contacts between the amino acid constituting the protein.</s> <s>we use a convolutional network to calculate protein contact maps from inferred statistical coupling between positions in the protein sequence.</s> <s>the input to the network has an image-like structure amenable to convolutions, but every \"pixel\" instead of color channels contains a bipartite undirected edge-weighted graph.</s> <s>we propose several methods for treating such \"graph-valued images\" in a convolutional network.</s> <s>the proposed method outperforms state-of-the-art methods by a large margin.</s> <s>it also allows for a great flexibility with regard to the input data, which makes it useful for studying a wide range of problems.</s></p></d>", "label": ["<d><p><s>protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings.</s> <s>we introduce a new dataset ?depth in the wild?</s> <s>consisting of images in the wild annotated with relative depth between pairs of random points.</s> <s>we also propose a new algorithm that learns to estimate metric depth using annotations of relative depth.</s> <s>compared to the state of the art, our algorithm is simpler and performs better.</s> <s>experiments show that our algorithm, combined with existing rgb-d data and our new relative depth annotations, significantly improves single-image depth perception in the wild.</s></p></d>", "label": ["<d><p><s>single-image depth perception in the wild</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification.</s> <s>we first extend the investigation to multiclass categorization: we prove that in this case learnability is equivalent to compression of logarithmic sample size and that the uniform convergence property implies compression of constant size.</s> <s>we use the compressibility-learnability equivalence to show that (i) for multiclass categorization, pac and agnostic pac learnability are equivalent, and (ii) to derive a compactness theorem for learnability.</s> <s>we then consider supervised learning under general loss functions: we show that in this case, in order to maintain the compressibility-learnability equivalence, it is necessary to consider an approximate variant of compression.</s> <s>we use it to show that pac and agnostic pac are not equivalent, even when the loss function has only three values.</s></p></d>", "label": ["<d><p><s>supervised learning through the lens of compression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>spectral methods are popular in detecting global structures in the given data that can be represented as a matrix.</s> <s>however when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise.</s> <s>in this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors.</s> <s>using matrix perturbation analysis, we demonstrate that the learned  regularizations suppress down the eigenvalues associated with localized  eigenvectors and enable us to recover the informative eigenvectors representing the global structure.</s> <s>we show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems.</s> <s>using extensive experiments, we illustrate that our method solves the localization problem and works down to the  theoretical detectability limits in different kinds of synthetic data.</s> <s>this is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.</s></p></d>", "label": ["<d><p><s>robust spectral detection of global structures in the data by learning a regularization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems.</s> <s>in the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to $b$ bits.</s> <s>we here argue that the maximum likelihood estimator (mle) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties.</s> <s>a specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission.</s> <s>along the way, we also touch upon the existence of a qualitative counterpart to the johnson-lindenstrauss lemma in the presence of quantization.</s></p></d>", "label": ["<d><p><s>quantized random projections and non-linear estimation of cosine similarity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees.</s> <s>we introduce hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples.</s> <s>our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems.</s> <s>in particular, we apply them to sequential hypothesis testing, best arm identification, and sorting.</s> <s>the resulting algorithms rival or exceed the state of the art both theoretically and empirically.</s></p></d>", "label": ["<d><p><s>adaptive concentration inequalities for sequential decision problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>decision making under uncertainty is commonly modelled as a process of competitive stochastic evidence accumulation to threshold (the drift-diffusion model).</s> <s>however, it is unknown how animals learn these decision thresholds.</s> <s>we examine threshold learning by constructing a reward function that averages over many trials to wald's cost function that defines decision optimality.</s> <s>these rewards are highly stochastic and hence challenging to optimize, which we address in two ways: first, a simple two-factor reward-modulated learning rule derived from williams' reinforce method for neural networks; and second, bayesian optimization of the reward function with a gaussian process.</s> <s>bayesian optimization converges in fewer trials than reinforce but is slower computationally with greater variance.</s> <s>the reinforce method is also a better model of acquisition behaviour in animals and a similar learning rule has been proposed for modelling basal ganglia function.</s></p></d>", "label": ["<d><p><s>threshold learning for optimal decision making</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a surprising phenomenon related to the representation of a cloud of data points using polynomials.</s> <s>we start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately.</s> <s>this distinguished polynomial is a sum-of-squares (sos) derived in a simple manner from the inverse of the empirical moment matrix.</s> <s>in fact, this sos polynomial is directly related to orthogonal polynomials and the christoffel function.</s> <s>this allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon.</s> <s>among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.</s></p></d>", "label": ["<d><p><s>sorting out typicality with the inverse moment matrix sos polynomial</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a recent work (wang et.</s> <s>al., nips 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees.</s> <s>their algorithm is based on computing sketches of the input tensor, which requires reading the entire input.</s> <s>we show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor.</s> <s>instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling.</s> <s>to achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases.</s> <s>for symmetric tensors $ t = \\sum_{i=1}^k \\lambda_i u_i^{\\otimes p}$ with $\\lambda_i > 0$ for all i, we estimate such norms in sublinear time whenever p is even.</s> <s>for the important case of p = 3 and small values of k, we can also estimate such norms.</s> <s>for asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below $\\| t \\|_f$ then sublinear time is again possible.</s> <s>one of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy.</s></p></d>", "label": ["<d><p><s>sublinear time orthogonal tensor decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a new framework of applying deep neural networks (dnn) to devise a universal discrete denoiser.</s> <s>unlike other approaches that utilize supervised learning for denoising, we do not require any additional training data.</s> <s>in such setting, while the ground-truth label, i.e., the clean data, is not available, we devise ``pseudo-labels'' and a novel objective function such that dnn can be trained in a same way as supervised learning to become a discrete denoiser.</s> <s>we experimentally show that our resulting algorithm, dubbed as neural dude, significantly outperforms the previous state-of-the-art in several applications with a systematic rule of choosing the hyperparameter, which is an attractive feature in practice.</s></p></d>", "label": ["<d><p><s>neural universal discrete denoiser</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor decomposition is positioned to be a pervasive tool in the era of big data.</s> <s>in this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition.</s> <s>we propose simple variants of the tensor power method which enjoy these strong properties.</s> <s>we propose the first streaming method with a linear memory requirement.</s> <s>moreover, we present a noise calibrated tensor power method with efficient privacy guarantees.</s> <s>at the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.</s></p></d>", "label": ["<d><p><s>online and differentially-private tensor decomposition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the task of clustering items using answers from non-expert crowd workers.</s> <s>in such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not.</s> <s>an important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is.</s> <s>since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint.</s> <s>when a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost.</s> <s>in addition to theoretical justification, through several simulations and experiments on two real data sets on amazon mechanical turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries.</s> <s>even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them.</s> <s>we also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.</s></p></d>", "label": ["<d><p><s>crowdsourced clustering: querying edges vs triangles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>choosing a diverse subset of a large collection of points in a metric space is a fundamental problem, with applications in feature selection, recommender systems, web search, data summarization, etc.</s> <s>various notions of diversity have been proposed, tailored to different applications.</s> <s>the general algorithmic goal is to find a subset of points that maximize diversity, while obeying a cardinality (or more generally, matroid) constraint.</s> <s>the goal of this paper is to develop a novel linear programming (lp) framework that allows us to design approximation algorithms for such problems.</s> <s>we study an objective known as {\\em sum-min} diversity, which is known to be effective in many applications, and give the first constant factor approximation algorithm.</s> <s>our lp framework allows us to easily incorporate additional constraints, as well as secondary objectives.</s> <s>we also prove a hardness result for two natural diversity objectives, under the  so-called {\\em planted clique} assumption.</s> <s>finally, we study the empirical performance of our algorithm on several standard datasets.</s> <s>we first study the approximation quality of the algorithm by comparing with the lp objective.</s> <s>then, we compare the quality of the solutions produced by our method with other popular diversity maximization algorithms.</s></p></d>", "label": ["<d><p><s>linear relaxations for finding diverse elements in metric spaces</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>efficient exploration remains a major challenge for reinforcement learning (rl).</s> <s>common dithering strategies for exploration, such as epsilon-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements.</s> <s>however, most algorithms for statistically efficient rl are not computationally tractable in complex environments.</s> <s>randomized value functions offer a promising approach to efficient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions.</s> <s>as a first step towards addressing such contexts we develop bootstrapped dqn.</s> <s>we demonstrate that bootstrapped dqn can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy.</s> <s>in the arcade learning environment bootstrapped dqn substantially improves learning speed and cumulative performance across most games.</s></p></d>", "label": ["<d><p><s>deep exploration via bootstrapped dqn</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this paper introduces an approach to regularize 2.5d surface normal and depth predictions at each pixel given a single input image.</s> <s>the approach infers and reasons about the underlying 3d planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects.</s> <s>our approach comprises two components: (i) a fourstream convolutional neural network (cnn) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (dcrf) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information.</s> <s>the dcrf is formulated such that gradients can be passed to the surface normal and depth cnns via backpropagation.</s> <s>in addition, we propose new planar wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3d editing applications.</s> <s>we show that our regularization yields a 30% relative improvement in planar consistency on the nyu v2 dataset.</s></p></d>", "label": ["<d><p><s>surge: surface regularized geometry estimation from a single image</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying euclidean metric.</s> <s>we suggest to replace this metric with a locally adaptive, smoothly changing (riemannian) metric that favors regions of high local density.</s> <s>the resulting locally adaptive normal distribution (land) is a generalization of the normal distribution to the \"manifold\" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in r^d.</s> <s>the land is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric.</s> <s>the underlying metric is, however, non-parametric.</s> <s>we develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and monte carlo integration.</s> <s>we further extend the land to mixture models, and provide the corresponding em algorithm.</s> <s>we demonstrate the efficiency of the land to fit non-trivial probability distributions over both synthetic data, and eeg measurements of human sleep.</s></p></d>", "label": ["<d><p><s>a locally adaptive normal distribution</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>high demand for computation resources severely hinders deployment of large-scale deep neural networks (dnn) in resource constrained devices.</s> <s>in this work, we propose a structured sparsity learning (ssl) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of dnns.</s> <s>ssl can: (1) learn a compact structure from a bigger dnn to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of dnn to efficiently accelerate the dnn?s evaluation.</s> <s>experimental results show that ssl achieves on average 5.1x and 3.1x speedups of convolutional layer computation of alexnet against cpu and gpu, respectively, with off-the-shelf libraries.</s> <s>these speedups are about twice speedups of non-structured sparsity; (3) regularize the dnn structure to improve classification accuracy.</s> <s>the results show that for cifar-10, regularization on layer depth reduces a 20-layer deep residual network (resnet) to 18 layers while improves the accuracy from 91.25% to 92.60%, which is still higher than that of original resnet with 32 layers.</s> <s>for alexnet, ssl reduces the error by ~1%.</s></p></d>", "label": ["<d><p><s>learning structured sparsity in deep neural networks</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations.</s> <s>unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem.</s> <s>we present a fast online active set method to solve this sparse nonnegative deconvolution problem.</s> <s>importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session.</s> <s>our algorithm is a generalization of the pool adjacent violators algorithm (pava) for isotonic regression and inherits its linear-time computational complexity.</s> <s>we gain remarkable increases in processing speed:  more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods.</s> <s>our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data.</s> <s>the algorithm enables real-time simultaneous deconvolution of $o(10^5)$ traces of whole-brain zebrafish imaging data on a laptop.</s></p></d>", "label": ["<d><p><s>fast active set methods for online spike inference from calcium imaging</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a stochastic and distributed algorithm for nonconvex  problems whose objective consists a sum $n$ nonconvex $l_i/n$-smooth functions, plus a  nonsmooth regularizer.</s> <s>the proposed nonconvex primal-dual splitting (nestt) algorithm splits the problem into $n$ subproblems, and utilizes an augmented lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner.</s> <s>with a special non-uniform sampling, a version of nestt achieves $\\epsilon$-stationary solution  using $\\mathcal{o}((\\sum_{i=1}^n\\sqrt{l_i/n})^2/\\epsilon)$ gradient evaluations, which can be up to $\\mathcal{o}(n)$ times better than the (proximal) gradient descent methods.</s> <s>it also achieves q-linear convergence rate for nonconvex $\\ell_1$ penalized quadratic problems with polyhedral constraints.</s> <s>further, we reveal  a fundamental connection between {\\it primal-dual} based methods and a few {\\it primal only} methods such as iag/sag/saga.</s></p></d>", "label": ["<d><p><s>nestt: a nonconvex primal-dual splitting method for distributed and stochastic optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study k-svd that is to obtain the first k singular vectors of a matrix a approximately.</s> <s>recently, a few breakthroughs have been discovered on $k$-svd: musco and musco [1] provided the first gap-free theorem for the block krylov method, shamir [2] discovered the first variance-reduction stochastic method, and bhojanapalli et al.</s> <s>[3] provided the fastest $o(nnz(a) + poly(1/eps))$-type of algorithm using alternating minimization.</s> <s>in this paper, we improve the above breakthroughs by providing a new framework for solving k-svd.</s> <s>in particular, we obtain faster gap-free convergence speed outperforming [1], we obtain the first accelerated and stochastic method outperforming [3].</s> <s>in the nnz running-time regime, we outperform [3] without even using alternating minimization for certain parameter regimes.</s></p></d>", "label": ["<d><p><s>even faster svd decomposition yet without agonizing pain</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a cluster tree provides an intuitive summary of a density function that reveals essential structure about the high-density clusters.</s> <s>the true cluster tree is estimated from a finite sample from an unknown true density.</s> <s>this paper addresses the basic question of quantifying our  uncertainty by assessing the statistical significance of different features of an empirical cluster tree.</s> <s>we first study a variety of metrics that can be used to compare different trees, analyzing their properties and assessing their suitability for our inference task.</s> <s>we then propose methods to construct and summarize confidence sets for the unknown true cluster tree.</s> <s>we introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees.</s> <s>finally, we provide a variety of simulations to illustrate our proposed methods and furthermore demonstrate their utility in the analysis of a graft-versus-host disease (gvhd) data set.</s></p></d>", "label": ["<d><p><s>statistical inference for cluster trees</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>predicting the behavior of human participants in strategic settings is an important problem in many domains.</s> <s>most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics.</s> <s>in this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge.</s> <s>we introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.</s></p></d>", "label": ["<d><p><s>deep learning for predicting human strategic behavior</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a single color image can contain many cues informative towards different aspects of local geometric structure.</s> <s>we approach the problem of monocular depth estimation by using a neural network to produce a mid-level representation that summarizes these cues.</s> <s>this network is trained to characterize local scene geometry by predicting, at every image location, depth derivatives of different orders, orientations and scales.</s> <s>however, instead of a single estimate for each derivative, the network outputs probability distributions that allow it to express confidence about some coefficients, and ambiguity about others.</s> <s>scene depth is then estimated by harmonizing this overcomplete set of network predictions, using a globalization procedure that finds a single consistent depth map that best matches all the local derivative distributions.</s> <s>we demonstrate the efficacy of this approach through evaluation on the nyu v2 depth data set.</s></p></d>", "label": ["<d><p><s>depth from a single image by harmonizing overcomplete local network predictions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the stochastic combinatorial multi-armed bandit (cmab) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables.</s> <s>our framework enables a much larger class of reward functions such as the $\\max()$ function and nonlinear utility functions.</s> <s>existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (ucb) technique, do not work directly on these functions.</s> <s>we propose a new algorithm called stochastically dominant confidence bound (sdcb), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds.</s> <s>we prove that sdcb can achieve $o(\\log t)$ distribution-dependent regret and $\\tilde{o}(\\sqrt{t})$ distribution-independent regret, where $t$ is the time horizon.</s> <s>we apply our results to the $k$-max problem and expected utility maximization problems.</s> <s>in particular, for $k$-max, we provide the first polynomial-time approximation scheme (ptas) for its offline problem, and give the first $\\tilde{o}(\\sqrt t)$ bound on the $(1-\\epsilon)$-approximation regret of its online problem, for any $\\epsilon>0$.</s></p></d>", "label": ["<d><p><s>combinatorial multi-armed bandit with general reward functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>recurrent neural networks (rnns) have achieved state-of-the-art performances in many natural language processing tasks, such as language modeling and machine translation.</s> <s>however, when the vocabulary is large, the rnn model will become very big (e.g., possibly beyond the memory capacity of a gpu device) and its training will become very inefficient.</s> <s>in this work, we propose a novel technique to tackle this challenge.</s> <s>the key idea is to use 2-component (2c) shared embedding for word representations.</s> <s>we allocate every word in the vocabulary into a table, each row of which is associated with a vector, and each column associated with another vector.</s> <s>depending on its position in the table, a word is jointly represented by two components: a row vector and a column vector.</s> <s>since the words in the same row share the row vector and the words in the same column share the column vector, we only need $2 \\sqrt{|v|}$ vectors to represent a vocabulary of $|v|$ unique words, which are far less than the $|v|$ vectors required by existing approaches.</s> <s>based on the 2-component shared embedding, we design a new rnn algorithm and evaluate it using the language modeling task on several benchmark datasets.</s> <s>the results show that our algorithm significantly reduces the model size and speeds up the training process, without sacrifice of accuracy (it achieves similar, if not better, perplexity as compared to state-of-the-art language models).</s> <s>remarkably, on the one-billion-word benchmark dataset, our algorithm achieves comparable perplexity to previous language models, whilst reducing the model size by a factor of 40-100, and speeding up the training process by a factor of 2.</s> <s>we name our proposed algorithm \\emph{lightrnn} to reflect its very small model size and very high training speed.</s></p></d>", "label": ["<d><p><s>lightrnn: memory and computation-efficient recurrent neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback.</s> <s>these problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains.</s> <s>this paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting.</s> <s>our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret.</s> <s>we show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms.</s> <s>our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown.</s> <s>our regret guarantees are superior to prior techniques that ignore the feedback.</s></p></d>", "label": ["<d><p><s>contextual semibandits via supervised learning oracles</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>stochastic gradient markov chain monte carlo (sg-mcmc) algorithms have become increasingly popular for bayesian inference in large-scale applications.</s> <s>even though these methods have proved useful in several scenarios, their performance is often limited by their bias.</s> <s>in this study, we propose a novel sampling algorithm that aims to reduce the bias of sg-mcmc while keeping the variance at a reasonable level.</s> <s>our approach is based on a numerical sequence acceleration method, namely the richardson-romberg extrapolation, which simply boils down   to running almost the same sg-mcmc algorithm twice in parallel with different step sizes.</s> <s>we illustrate our framework on the popular stochastic gradient langevin dynamics (sgld) algorithm and propose a novel sg-mcmc algorithm referred to as stochastic gradient richardson-romberg langevin dynamics (sgrrld).</s> <s>we provide formal theoretical analysis and show that sgrrld is asymptotically consistent, satisfies a central limit theorem, and its non-asymptotic bias and the mean squared-error can be bounded.</s> <s>our results show that sgrrld attains higher rates of convergence than sgld in both finite-time and asymptotically, and it achieves the theoretical   accuracy of the methods that are based on higher-order integrators.</s> <s>we support our findings using both synthetic and real data experiments.</s></p></d>", "label": ["<d><p><s>stochastic gradient richardson-romberg markov chain monte carlo</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study optimization of finite sums of \\emph{geodesically} smooth functions on riemannian manifolds.</s> <s>although variance reduction techniques for optimizing finite-sums have witnessed tremendous attention in the recent years, existing work is limited to vector space problems.</s> <s>we introduce \\emph{riemannian svrg} (\\rsvrg), a new variance reduced riemannian optimization method.</s> <s>we analyze \\rsvrg for both geodesically  \\emph{convex} and \\emph{nonconvex} (smooth) functions.</s> <s>our analysis reveals that \\rsvrg inherits  advantages of the usual svrg method, but with factors depending on curvature of the manifold that influence its convergence.</s> <s>to our knowledge, \\rsvrg is the first \\emph{provably fast} stochastic riemannian method.</s> <s>moreover, our paper presents the first non-asymptotic complexity analysis (novel even for the batch setting) for nonconvex riemannian optimization.</s> <s>our results have several implications; for instance, they offer a riemannian perspective on variance reduced pca, which promises a short, transparent convergence analysis.</s></p></d>", "label": ["<d><p><s>riemannian svrg: fast stochastic optimization on riemannian manifolds</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present a novel neural network algorithm, the tensor switching (ts) network, which generalizes the rectified linear unit (relu) nonlinearity to tensor-valued hidden units.</s> <s>the ts network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity.</s> <s>in this way, even a simple linear readout from the ts representation can implement a highly expressive deep-network-like function.</s> <s>the ts network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size.</s> <s>we develop several methods to train the ts network, including equivalent kernels for infinitely wide and deep ts networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms.</s> <s>our experimental results demonstrate that the ts network is indeed more expressive and consistently learns faster than standard relu networks.</s></p></d>", "label": ["<d><p><s>tensor switching networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>semidefinite programs (sdp's) can be solved in polynomial time by interior point methods, but scalability can be an issue.</s> <s>to address this shortcoming, over a decade ago, burer and monteiro proposed to solve sdp's with few equality constraints via rank-restricted, non-convex surrogates.</s> <s>remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably.</s> <s>although some theory supports this empirical success, a complete explanation of it remains an open question.</s> <s>in this paper, we consider a class of sdp's which includes applications such as max-cut, community detection in the stochastic block model, robust pca, phase retrieval and synchronization of rotations.</s> <s>we show that the low-rank burer-monteiro formulation of sdp's in that class almost never has any spurious local optima.</s></p></d>", "label": ["<d><p><s>the non-convex burer-monteiro approach works on smooth semidefinite programs</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the weakly supervised binary classification problem where the labels are randomly flipped with probability $1-\\alpha$.</s> <s>although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by $\\alpha$.</s> <s>in this paper, we characterize the effect of $\\alpha$ by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model.</s> <s>for small $\\alpha$, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision.</s> <s>interestingly, we also show that this gap narrows as $\\alpha$ increases.</s> <s>in other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.</s></p></d>", "label": ["<d><p><s>more supervision, less computation: statistical-computational tradeoffs in weakly supervised learning</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>deep neural networks (dnns) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems.</s> <s>understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right---similar to why we study the human brain---and will enable researchers to further improve dnns.</s> <s>one path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect.</s> <s>one such method is called activation maximization, which synthesizes an input (e.g.</s> <s>an image) that highly activates a neuron.</s> <s>here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network.</s> <s>the algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).</s></p></d>", "label": ["<d><p><s>synthesizing the preferred inputs for neurons in neural networks via deep generator networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations.</s> <s>for instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position.</s> <s>conventional policy learning approaches, such as those based on markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior.</s> <s>the key difficulty is that conventional models are ``single-scale'' and only learn a single state-action policy.</s> <s>we instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network.</s> <s>we showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.</s></p></d>", "label": ["<d><p><s>generating long-term trajectories using deep hierarchical networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical methods for network data often parameterize the edge-probability by attributing latent traits such as block structure to the vertices and assume exchangeability in the sense of the aldous-hoover representation theorem.</s> <s>these assumptions are however incompatible with traits found in real-world networks such as a power-law degree-distribution.</s> <s>recently, caron & fox (2014) proposed the use of a different notion of exchangeability after kallenberg (2005) and obtained a network model which permits edge-inhomogeneity, such as a power-law degree-distribution whilst retaining desirable statistical properties.</s> <s>however, this model does not capture latent vertex traits such as block-structure.</s> <s>in this work we re-introduce the use of block-structure for network models obeying kallenberg?s notion of exchangeability and thereby obtain a collapsed model which both admits the inference of block-structure and edge inhomogeneity.</s> <s>we derive a simple expression for the likelihood and an efficient sampling method.</s> <s>the obtained model is not significantly more difficult to implement than existing approaches to block-modelling and performs well on real network datasets.</s></p></d>", "label": ["<d><p><s>completely random measures for modelling block-structured sparse networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of efficiently estimating the coefficients of generalized linear models (glms) in the large-scale setting where the number of observations $n$ is much larger than the number of predictors $p$, i.e.</s> <s>$n\\gg p \\gg 1$.</s> <s>we show that in glms with random (not necessarily gaussian) design, the glm coefficients are approximately proportional to the corresponding ordinary least squares (ols) coefficients.</s> <s>using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (mle)  through iterations that  attain up to a cubic convergence rate, and that are cheaper than  any batch optimization algorithm by at least a factor of $\\mathcal{o}(p)$.</s> <s>we provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions.</s> <s>% finally, we demonstrate the performance of  our algorithm through extensive numerical studies  on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.</s></p></d>", "label": ["<d><p><s>scaled least squares estimator for glms in large-scale problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques.</s> <s>for some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning.</s> <s>we therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label subsets of the data, but that are noisy and may conflict.</s> <s>by viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \"denoise\" the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings.</s> <s>we then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and lstms.</s> <s>experimentally, on the 2014 tac-kbp slot filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an lstm model leads to a tac-kbp score almost 6 f1 points over a state-of-the-art lstm baseline (and into second place in the competition).</s> <s>additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.</s></p></d>", "label": ["<d><p><s>data programming: creating large training sets, quickly</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>this work is motivated by the engineering task of achieving a near state-of-the-art face recognition on a minimal computing budget running on an embedded system.</s> <s>our main technical contribution centers around a novel training method, called multibatch, for similarity learning, i.e., for the task of generating an invariant ``face signature'' through training pairs of ``same'' and ``not-same'' face images.</s> <s>the multibatch method first generates signatures for a mini-batch of $k$ face images and then constructs an unbiased estimate of the full gradient by relying on all $k^2-k$ pairs from the mini-batch.</s> <s>we prove that the variance of the multibatch estimator is bounded by $o(1/k^2)$, under some mild conditions.</s> <s>in contrast, the standard gradient estimator that relies on random $k/2$ pairs has a variance of order $1/k$.</s> <s>the smaller variance of the multibatch estimator significantly speeds up the convergence rate of stochastic gradient descent.</s> <s>using the multibatch method we train a deep convolutional neural network that achieves an accuracy of $98.2\\%$ on the lfw benchmark, while its prediction runtime takes only $30$msec on a single arm cortex a9 core.</s> <s>furthermore, the entire training process took only 12 hours on a single titan x gpu.</s></p></d>", "label": ["<d><p><s>learning a metric embedding  for face recognition using the multibatch method</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of finding the best arm in a stochastic mutli-armed bandit (mab) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic mab problem.</s> <s>in these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be  easier than discovering the best arm.</s> <s>our results are focused on the scenario where the failure probability $\\delta$ must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification.</s> <s>we demonstrate the effectiveness of our framework by applying it, and improving the state-of-the art results in the problems of: linear bandits, dueling bandits with the condorcet assumption, copeland dueling bandits, unimodal bandits and graphical bandits.</s></p></d>", "label": ["<d><p><s>verification based solution for structured mab problems</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning.</s> <s>we investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions.</s> <s>in both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively.</s> <s>this implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget.</s> <s>we discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.</s></p></d>", "label": ["<d><p><s>adaptive maximization of pointwise submodular functions with budget constraint</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this work explores conditional image generation with a new image density model based on the pixelcnn architecture.</s> <s>the model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks.</s> <s>when conditioned on class labels from the imagenet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures.</s> <s>when conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions.</s> <s>we also show that conditional pixelcnn can serve as a powerful decoder in an image autoencoder.</s> <s>additionally, the gated convolutional layers in the proposed model improve the log-likelihood of pixelcnn to match the state-of-the-art performance of pixelrnn on imagenet, with greatly reduced computational cost.</s></p></d>", "label": ["<d><p><s>conditional image generation with pixelcnn decoders</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a novel variational autoencoder is developed to model images, as well as associated labels or captions.</s> <s>the deep generative deconvolutional network (dgdn) is used as a decoder of the latent image features, and a deep convolutional neural network (cnn) is used as an image encoder; the cnn is used to approximate a distribution for the latent dgdn features/code.</s> <s>the latent code is also linked to generative models for labels (bayesian support vector machine) or captions (recurrent neural network).</s> <s>when predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned cnn-based encoder.</s> <s>since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for cnn learning with images; the framework even allows unsupervised cnn learning, based on images alone.</s></p></d>", "label": ["<d><p><s>variational autoencoder for deep learning of images, labels and captions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in many cases of network analysis, it is more attractive to study how a network varies under  different conditions than an individual static network.</s> <s>we propose a novel graphical model, namely latent differential graph model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices.</s> <s>we propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization.</s> <s>we show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys oracle property under mild conditions.</s> <s>thorough experiments on both synthetic and real world data support our theory.</s></p></d>", "label": ["<d><p><s>semiparametric differential graph models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries.</s> <s>as a result, there is a large body of literature focused on consistent model selection.</s> <s>however, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly.</s> <s>in this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on hyv\\\"arinen scoring rule.</s> <s>hyv\\\"arinen scoring rule is especially useful in cases where the normalizing constant cannot be  obtained efficiently in a closed form.</s> <s>we prove that the estimator is $\\sqrt{n}$-consistent and asymptotically normal.</s> <s>this result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests.</s> <s>we establish our results under conditions that are typically assumed in the literature for consistent estimation.</s> <s>however, we do not require that the estimator consistently recovers the graph structure.</s> <s>in particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes.</s> <s>we illustrate validity of our estimator through extensive simulation studies.</s></p></d>", "label": ["<d><p><s>statistical inference for pairwise graphical models using score matching</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>functional brain networks are well described and estimated from data with gaussian graphical models (ggms), e.g.\\ using sparse inverse covariance estimators.</s> <s>comparing functional connectivity of subjects in two populations calls for comparing these estimated ggms.</s> <s>our goal is to identify differences in ggms known to have similar structure.</s> <s>we characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator.</s> <s>sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings.</s> <s>characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator.</s> <s>recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso.</s> <s>these distributions can be used to give confidence intervals on edges in ggms, and by extension their differences.</s> <s>however, in the case of comparing ggms, these estimators do not make use of any assumed joint structure among the ggms.</s> <s>inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference.</s> <s>this leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner.</s> <s>we then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in ggms.</s> <s>we validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.</s></p></d>", "label": ["<d><p><s>testing for differences in gaussian graphical models:  applications to brain connectivity</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>existing object proposal algorithms usually search for possible object regions over multiple locations and scales \\emph{ separately}, which ignore the interdependency among different objects and deviate from the human perception procedure.</s> <s>to incorporate global interdependency between objects into object localization, we propose an effective tree-structured reinforcement learning (tree-rl) approach to sequentially search for objects by fully exploiting both the current observation and  historical search paths.</s> <s>the tree-rl approach learns multiple  searching policies through maximizing the long-term reward that reflects localization accuracies over all the objects.</s> <s>starting with taking the entire image as a proposal, the tree-rl approach allows the agent to sequentially discover multiple objects via a  tree-structured traversing scheme.</s> <s>allowing multiple near-optimal policies, tree-rl  offers more diversity in search paths and is able to find multiple objects with a single feed-forward pass.</s> <s>therefore, tree-rl can better cover different objects with various scales which is quite appealing in the context of object proposal.</s> <s>experiments on pascal voc 2007 and 2012 validate the effectiveness of the tree-rl, which can achieve comparable recalls with current object proposal algorithms via much fewer candidate windows.</s></p></d>", "label": ["<d><p><s>tree-structured reinforcement learning for sequential object localization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics.</s> <s>first, it does not assume any generative model and based on a worst-case performance metric.</s> <s>second, it is comparative, namely performance is measured with respect to a given hypothesis class.</s> <s>this allows to avoid known computational hardness results and improper algorithms based on convex relaxations.</s> <s>we show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.</s></p></d>", "label": ["<d><p><s>a non-generative framework and convex relaxations for unsupervised learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>factorial hidden markov models (fhmms) are powerful models for sequential data but they do not scale well with long sequences.</s> <s>we propose a scalable inference and learning algorithm for fhmms that draws on ideas from the stochastic variational inference, neural network and copula literatures.</s> <s>unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning.</s> <s>our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large fhmms.</s></p></d>", "label": ["<d><p><s>scaling factorial hidden markov models: stochastic variational inference without messages</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many manifold learning algorithms aim to create embeddings with low or no distortion (i.e.</s> <s>isometric).</s> <s>if the data has intrinsic dimension d, it is often impossible to obtain an isometric embedding in d dimensions, but possible in s > d dimensions.</s> <s>yet, most geometry preserving algorithms cannot do the latter.</s> <s>this paper proposes an embedding algorithm that overcomes this problem.</s> <s>the algorithm directly computes, for any data embedding y, a distortion loss(y), and iteratively updates y in order to decrease it.</s> <s>the distortion measure we propose is based on the push-forward riemannian metric associated with the coordinates y.</s> <s>the experiments confirm the superiority of our algorithm in obtaining low distortion embeddings.</s></p></d>", "label": ["<d><p><s>nearly isometric embedding by relaxation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments.</s> <s>we introduce a new parameter $\\w$, which measures the total statistical variance of the loss distributions over $t$ rounds of the process, and study how this amount affects the regret.</s> <s>we investigate the interaction between $\\w$ and $\\gamma$, which counts the number of times the distributions change, as well as $\\w$ and $v$, which measures how far the distributions deviates over time.</s> <s>one striking result we find is that even when $\\gamma$, $v$, and $\\lambda$ are all restricted to constant, the regret lower bound in the bandit setting still grows with $t$.</s> <s>the other highlight is that in the full-information setting, a constant regret becomes achievable with constant $\\gamma$ and $\\lambda$, as it can be made independent of $t$, while with constant $v$ and $\\lambda$, the regret still has a $t^{1/3}$ dependency.</s> <s>we not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well.</s></p></d>", "label": ["<d><p><s>tracking the best expert in non-stationary stochastic environments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans.</s> <s>here we adopt the hypothesis that when we are in an interactive social setting, our brains perform bayesian inference of the intentions and cooperativeness of others using probabilistic representations.</s> <s>we employ the framework of partially observable markov decision processes (pomdps) to model human decision making in a social context, focusing specifically on the volunteer's dilemma in a version of the classic public goods game.</s> <s>we show that the pomdp model explains both the behavior of subjects as well as neural activity recorded using fmri during the game.</s> <s>the decisions of subjects can be modeled across all trials using two interpretable parameters.</s> <s>furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions.</s> <s>our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.</s></p></d>", "label": ["<d><p><s>a probabilistic model of social decision making based on reward maximization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning.</s> <s>expressing these in a common form, we derive a novel algorithm, retrace(lambda), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies.</s> <s>we analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms.</s> <s>we believe this is the first return-based off-policy control algorithm converging a.s. to q* without the glie assumption (greedy in the limit with infinite exploration).</s> <s>as a corollary, we prove the convergence of watkins' q(lambda), which was  an open problem since 1989.</s> <s>we illustrate the benefits of retrace(lambda) on a standard suite of atari 2600 games.</s></p></d>", "label": ["<d><p><s>safe and efficient off-policy reinforcement learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>most robots lack the ability to learn new objects from past experiences.</s> <s>to migrate a robot to a new environment one must often completely re-generate the knowledge- base that it is running with.</s> <s>since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots.</s> <s>therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion.</s> <s>this paper proposes an open-ended 3d object recognition system which concurrently learns both the object categories and the statistical features for encoding objects.</s> <s>in particular, we propose an extension of latent dirichlet allocation to learn structural semantic features (i.e.</s> <s>topics) from low-level feature co-occurrences for each category independently.</s> <s>moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views.</s> <s>the approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations.</s> <s>results show the fulfilling performance of this approach on different types of objects.</s> <s>moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.</s></p></d>", "label": ["<d><p><s>hierarchical object representation for open-ended object category learning and recognition</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services.</s> <s>such systems have usually access to massive data generated by a large pool of users.</s> <s>a major fraction of the data is public and is visible to (and can be used for) all users.</s> <s>however, each user can also contribute some private data that should not be shared with other users to ensure her privacy.</s> <s>the goal is to provide a succinct summary of massive dataset, ideally as small as possible, from which customized summaries can be built for each user, i.e.</s> <s>it can contain elements from the public data (for diversity) and users' private data (for personalization).</s> <s>to formalize the above challenge, we assume that the scoring function according to which a user evaluates the utility of her summary satisfies submodularity, a widely used notion in data summarization applications.</s> <s>thus, we model the data summarization targeted to each user as an instance of a submodular cover problem.</s> <s>however, when the data is massive it is infeasible to use the centralized greedy algorithm to find a customized summary even for a single user.</s> <s>moreover, for a large pool of users, it is too time consuming to find such summaries separately.</s> <s>instead, we develop a fast distributed algorithm for submodular cover, fastcover, that provides a succinct summary in one shot and for all users.</s> <s>we show that the solution provided by fastcover is competitive with that of the centralized algorithm with the number of rounds that is exponentially smaller than state of the art results.</s> <s>moreover, we have implemented fastcover with spark to demonstrate its practical performance on a number of concrete applications, including personalized location recommendation, personalized movie recommendation, and dominating set on tens of millions of data points and varying number of users.</s></p></d>", "label": ["<d><p><s>fast distributed submodular cover: public-private data summarization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of building continuous occupancy representations in  dynamic environments for robotics applications.</s> <s>the problem has hardly been discussed previously due to the complexity of patterns in urban environments,  which have both spatial and temporal dependencies.</s> <s>we address the problem  as learning a kernel classifier on an efficient feature space.</s> <s>the key novelty of  our approach is the incorporation of variations in the time domain into the spatial  domain.</s> <s>we propose a method to propagate motion uncertainty into the kernel using a hierarchical model.</s> <s>the main benefit of this approach is that it can directly predict  the occupancy state of the map in the future from past observations, being a valuable  tool for robot trajectory planning under uncertainty.</s> <s>our approach preserves the  main computational benefits of static hilbert maps ?</s> <s>using stochastic gradient  descent for fast optimization of model parameters and incremental updates as  new data are captured.</s> <s>experiments conducted in road intersections of an urban  environment demonstrated that spatio-temporal hilbert maps can accurately model  changes in the map while outperforming other techniques on various aspects.</s></p></d>", "label": ["<d><p><s>spatio-temporal hilbert maps for continuous occupancy representation in dynamic environments</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce convolutional draw, a homogeneous deep generative model achieving state-of-the-art performance in latent variable image modeling.</s> <s>the algorithm naturally stratifies information into higher and lower level details, creating abstract features and as such addressing one of the fundamentally desired properties of representation learning.</s> <s>furthermore, the hierarchical ordering of its latents creates the opportunity to selectively store global information about an image, yielding a high quality 'conceptual compression' framework.</s></p></d>", "label": ["<d><p><s>towards conceptual compression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a central challenge in neuroscience is understanding how neural system implements computation through its dynamics.</s> <s>we propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories.</s> <s>our model assumes low-dimensional continuous dynamics in a finite volume.</s> <s>it incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories.</s> <s>we show that our model can recover qualitative features of the phase portrait such as attractors, slow points, and bifurcations, while also producing reliable long-term future predictions in a variety of dynamical models and in real neural data.</s></p></d>", "label": ["<d><p><s>interpretable nonlinear dynamic modeling of neural trajectories</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose the coupled generative adversarial nets (cogan) framework for generating pairs of corresponding images in two different domains.</s> <s>the framework consists of a pair of generative adversarial nets, each responsible for generating images in one domain.</s> <s>we show that by enforcing a simple weight-sharing constraint, the cogan learns to generate pairs of corresponding images without existence of any pairs of corresponding images in the two domains in the training set.</s> <s>in other words, the cogan learns a joint distribution of images in the two domains from images drawn separately from the marginal distributions of the individual domains.</s> <s>this is in contrast to the existing multi-modal generative models, which require corresponding images for training.</s> <s>we apply the cogan to several pair image generation tasks.</s> <s>for each task, the cogan learns to generate convincing pairs of corresponding images.</s> <s>we further demonstrate the applications of the cogan framework for the domain adaptation and cross-domain image generation tasks.</s></p></d>", "label": ["<d><p><s>coupled generative adversarial networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings.</s> <s>we show that for any continuous function f, consistent estimators of the mean embedding of a random variable x lead to consistent estimators of the mean embedding of f(x).</s> <s>for matern kernels and sufficiently smooth functions we also provide rates of convergence.</s> <s>our results extend to functions of multiple random variables.</s> <s>if the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions.</s> <s>in either case, our results cover both mean embeddings based on i.i.d.</s> <s>samples as well as \"reduced set\" expansions in terms of dependent expansion points.</s> <s>the latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.</s></p></d>", "label": ["<d><p><s>consistent kernel mean estimation for functions of random variables</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting.</s> <s>however, a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable.</s> <s>indeed, much of the content may have been simply ignored by the user.</s> <s>the present work proposes to exploit available information regarding the display position bias under the so-called position-based click model (pbm).</s> <s>we first discuss how this model differs from the cascade model and its variants considered in several recent works on multiple-play bandits.</s> <s>we then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance.</s></p></d>", "label": ["<d><p><s>multiple-play bandits in the position-based model</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a key problem in structured output prediction is enabling direct optimization of the task reward function that matters for test evaluation.</s> <s>this paper presents a simple and computationally efficient method that incorporates task reward into maximum likelihood training.</s> <s>we establish a connection between maximum likelihood and regularized expected reward, showing that they are approximately equivalent in the vicinity of the optimal solution.</s> <s>then we show how maximum likelihood can be generalized by optimizing the conditional probability of auxiliary outputs that are sampled proportional to their exponentiated scaled rewards.</s> <s>we apply this framework to optimize edit distance in the output space, by sampling from edited targets.</s> <s>experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over maximum likelihood baseline by simply sampling from target output augmentations.</s></p></d>", "label": ["<d><p><s>reward augmented maximum likelihood for neural structured prediction</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>two seemingly contradictory theories attempt to explain how humans move to intercept an airborne ball.</s> <s>one theory posits that humans predict the ball trajectory to optimally plan future actions; the other claims that, instead of performing such complicated computations, humans employ heuristics to reactively choose appropriate actions based on immediate visual feedback.</s> <s>in this paper, we show that interception strategies appearing to be heuristics can be understood as computational solutions to the optimal control problem faced by a ball-catching agent acting under uncertainty.</s> <s>modeling catching as a continuous partially observable markov decision process and employing stochastic optimal control theory, we discover that the four main heuristics described in the literature are optimal solutions if the catcher has sufficient time to continuously visually track the ball.</s> <s>specifically, by varying model parameters such as noise, time to ground contact, and perceptual latency, we show that different strategies arise under different circumstances.</s> <s>the catcher's policy switches between generating reactive and predictive behavior based on the ratio of system to observation noise and the ratio between reaction time and task duration.</s> <s>thus, we provide a rational account of human ball-catching behavior and a unifying explanation for seemingly contradictory theories of target interception on the basis of stochastic optimal control.</s></p></d>", "label": ["<d><p><s>catching heuristics are optimal control policies</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>reconstruction of neuroanatomy is a fundamental problem in neuroscience.</s> <s>stochastic expression of colors in individual cells is a promising tool, although its use in the nervous system has been limited due to various sources of variability in expression.</s> <s>moreover, the intermingled anatomy of neuronal trees is challenging for existing segmentation algorithms.</s> <s>here, we propose a method to automate the segmentation of neurons in such (potentially pseudo-colored) images.</s> <s>the method uses spatio-color relations between the voxels, generates supervoxels to reduce the problem size by four orders of magnitude before the final segmentation, and is parallelizable over the supervoxels.</s> <s>to quantify performance and gain insight, we generate simulated images, where the noise level and characteristics, the density of expression, and the number of fluorophore types are variable.</s> <s>we also present segmentations of real brainbow images of the mouse hippocampus, which reveal many of the dendritic segments.</s></p></d>", "label": ["<d><p><s>automated scalable segmentation of neurons from multispectral images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>clustering, in particular $k$-means clustering, is a central topic in data analysis.</s> <s>clustering with bregman divergences is a recently proposed generalization of $k$-means clustering which has already been widely used in applications.</s> <s>in this paper we analyze theoretical properties of bregman clustering when the number of the clusters $k$ is large.</s> <s>we establish quantization rates and describe the limiting distribution of the centers as $k\\to \\infty$, extending well-known results for  $k$-means clustering.</s></p></d>", "label": ["<d><p><s>clustering with bregman divergences: an asymptotic analysis</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>asynchronous parallel optimization received substantial successes and extensive attention recently.</s> <s>one of core theoretical questions is how much speedup (or benefit) the asynchronous parallelization can bring to us.</s> <s>this paper provides a comprehensive and generic analysis to study the speedup property for a broad range of asynchronous parallel stochastic algorithms from the zeroth order to the first order methods.</s> <s>our result recovers or improves existing analysis on special cases, provides more insights for understanding the asynchronous parallel behaviors, and suggests a novel asynchronous parallel zeroth order method for the first time.</s> <s>our experiments provide novel applications of the proposed asynchronous parallel zeroth order method on hyper parameter tuning and model blending problems.</s></p></d>", "label": ["<d><p><s>a comprehensive linear speedup analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of synthesizing a number of likely future frames from a single input image.</s> <s>in contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach which models future frames in a probabilistic manner.</s> <s>our proposed method is therefore able to synthesize multiple possible next frames using the same model.</s> <s>solving this challenging problem involves low- and high-level image and motion understanding for successful image synthesis.</s> <s>here, we propose a novel network structure, namely a cross convolutional network, that encodes images as feature maps and motion information as convolutional kernels to aid in synthesizing future frames.</s> <s>in experiments, our model performs well on both synthetic data, such as 2d shapes and animated game sprites, as well as on real-wold video data.</s> <s>we show that our model can also be applied to tasks such as visual analogy-making, and present analysis of the learned network representations.</s></p></d>", "label": ["<d><p><s>visual dynamics: probabilistic future frame synthesis via cross convolutional networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study accelerated descent dynamics for constrained convex optimization.</s> <s>this dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate $\\eta(t)$, and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights $w(t)$.</s> <s>using a lyapunov argument, we give sufficient conditions on $\\eta$ and $w$ to achieve a desired convergence rate.</s> <s>as an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme.</s> <s>we then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the lyapunov function.</s> <s>we provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting.</s> <s>the experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases.</s></p></d>", "label": ["<d><p><s>adaptive averaging in accelerated descent dynamics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the goal of ordinal embedding is to represent items as points in a low-dimensional euclidean space given a set of constraints like ``item $i$ is closer to item $j$ than item $k$''.</s> <s>ordinal   constraints like this often come from human judgments.</s> <s>the classic approach to solving this problem is known as non-metric multidimensional scaling.</s> <s>to account for errors and variation in judgments, we consider the noisy situation in which the given constraints are independently corrupted by reversing the correct constraint with some probability.</s> <s>the ordinal embedding problem has been studied for decades, but most past work pays little attention to the question of whether accurate embedding is possible, apart from empirical studies.</s> <s>this paper shows that under a generative data model it is possible to learn the correct embedding from noisy distance comparisons.</s> <s>in establishing this fundamental result, the paper makes several new contributions.</s> <s>first, we derive prediction error bounds for embedding from noisy distance comparisons by exploiting the fact that the rank of a distance matrix of points in $\\r^d$ is at most $d+2$.</s> <s>these bounds characterize how well a learned embedding predicts new comparative judgments.</s> <s>second, we show that the underlying embedding can be recovered by solving a simple convex optimization.</s> <s>this result is highly non-trivial since we show that the linear map corresponding to distance comparisons is non-invertible, but there exists a nonlinear map that is invertible.</s> <s>third, two new algorithms for ordinal embedding are proposed and evaluated in experiments.</s></p></d>", "label": ["<d><p><s>finite sample prediction and recovery bounds for ordinal embedding</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring.</s> <s>in this problem the goal is to estimate the energy consumption of each appliance based on the total energy-consumption signal of a household.</s> <s>the current state of the art models the problem as inference in factorial hmms, and finds an approximate solution to the resulting quadratic integer program via quadratic programming.</s> <s>here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations with randomized rounding, as well as with a scalable admm method that exploits the special structure of the resulting semidefinite program.</s> <s>simulation results demonstrate the superiority of our methods both in synthetic and real-world datasets.</s></p></d>", "label": ["<d><p><s>sdp relaxation with randomized rounding for energy disaggregation</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length.</s> <s>moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training.</s> <s>to support this observation, we rewrite residual networks as an explicit collection of paths.</s> <s>unlike traditional models, paths through residual networks vary in length.</s> <s>further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other.</s> <s>finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient.</s> <s>for example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep.</s> <s>our results reveal one of the key characteristics that seem to enable the training of very deep networks: residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.</s></p></d>", "label": ["<d><p><s>residual networks behave like ensembles of relatively shallow networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present an effective method for supervised feature construction.</s> <s>the main goal of the approach is to construct a feature representation for which a set of linear hypotheses is of sufficient capacity -- large enough to contain a satisfactory solution to the considered problem and small enough to allow good generalization from a small number of training examples.</s> <s>we achieve this goal with a greedy procedure that constructs features by empirically fitting squared error residuals.</s> <s>the proposed constructive procedure is consistent and can output a rich set of features.</s> <s>the effectiveness of the approach is evaluated empirically by fitting a linear ridge regression model in the constructed feature space and our empirical results indicate a superior performance of our approach over competing methods.</s></p></d>", "label": ["<d><p><s>greedy feature construction</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>we demonstrate a spiking neural circuit for azimuth angle detection  inspired by the  echolocation circuits of the horseshoe bat  rhinolophus ferrumequinum and utilize it to devise a  model for  navigation and target tracking, capturing several key aspects of information transmission in biology.</s> <s>our network,  using only  a simple local-information based sensor implementing the cardioid angular gain function, operates at biological  spike rate of  10 hz.</s> <s>the network  tracks large angular targets (60 degrees) within 1 sec with a 10%  rms error.</s> <s>we study the navigational ability of our model for foraging and target localization tasks in  a forest of obstacles  and show that our network requires less than 200x   spike-triggered decisions, while suffering only a 1% loss in performance compared to a  proportional-integral-derivative controller, in the presence of 50% additive noise.</s> <s>superior performance can be obtained at  a higher average spike rate of  100 hz  and  1000 hz, but even the accelerated networks requires 20x and 10x lesser decisions respectively, demonstrating the superior computational efficiency of bio-inspired information processing systems.</s></p></d>", "label": ["<d><p><s>efficient and robust spiking neural circuit for navigation inspired by echolocating bats</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we study the support recovery guarantees of underdetermined sparse regression using the $\\ell_1$-norm as a regularizer and a non-smooth loss function for data fidelity.</s> <s>more precisely, we focus in detail on the cases of $\\ell_1$ and $\\ell_\\infty$ losses, and contrast them with the usual $\\ell_2$ loss.while these losses are routinely used to account for either sparse ($\\ell_1$ loss) or uniform ($\\ell_\\infty$ loss) noise models, a theoretical analysis of their performance is still lacking.</s> <s>in this article, we extend the existing theory from the smooth $\\ell_2$ case to these non-smooth cases.</s> <s>we derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations, as long as the loss constraint size is tuned proportionally to the noise level.</s> <s>a distinctive feature of our theory is that it also explains what happens when the support is unstable.</s> <s>while the support is not stable anymore, we identify an \"extended support\" and show that this extended support is stable to small additive noise.</s> <s>to exemplify the usefulness of our theory, we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses.</s> <s>this highlights different parameter regimes, ranging from total support stability to progressively increasing support instability.</s></p></d>", "label": ["<d><p><s>sparse support recovery with non-smooth loss functions</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>one crucial goal in kernel online learning is to bound the model size.</s> <s>common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies.</s> <s>although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors.</s> <s>an alternative way to address the model size problem is to apply random features to approximate the kernel function.</s> <s>this allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization.</s> <s>however, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation.</s> <s>consequently, it leads to a significant increase in the computational cost.</s> <s>to address all of these aforementioned challenges, we present in this paper the dual space gradient descent (dualsgd), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance.</s> <s>consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance.</s> <s>we further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.</s></p></d>", "label": ["<d><p><s>dual space gradient descent for online learning</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>dropout has been witnessed with great success in training deep neural networks by independently   zeroing  out the outputs of neurons at random.</s> <s>it has also received a surge of interest for shallow learning, e.g., logistic regression.</s> <s>however, the independent  sampling for dropout could be suboptimal for the sake of convergence.</s> <s>in this paper, we propose to use multinomial  sampling for dropout, i.e., sampling features or neurons according to  a multinomial distribution with different probabilities for different features/neurons.</s> <s>to exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial  dropout and establish the risk bound for stochastic optimization.</s> <s>by minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution.</s> <s>to tackle the issue of evolving  distribution of neurons in deep learning, we propose an efficient adaptive  dropout (named \\textbf{evolutional dropout}) that computes the sampling probabilities on-the-fly from a mini-batch of examples.</s> <s>empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve  not only much faster convergence and  but also a smaller testing error than the standard dropout.</s> <s>for example, on the cifar-100 data, the evolutional  dropout achieves relative improvements  over 10\\% on the prediction performance and over 50\\% on the convergence speed compared to the standard dropout.</s></p></d>", "label": ["<d><p><s>improved dropout for shallow and deep learning</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>clustering large datasets is a fundamental problem with a number of applications in machine learning.</s> <s>data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication.</s> <s>we would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site.</s> <s>in this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel.</s> <s>we give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds.</s> <s>our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to cluster n points or n vertices in a graph distributed across s servers, for a worst-case partitioning the communication complexity in a point-to-point model is n*s, while in the broadcast model it is n + s. we implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice.</s></p></d>", "label": ["<d><p><s>communication-optimal distributed clustering</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>this paper addresses the problem of 3d human pose estimation in the wild.</s> <s>a significant challenge is the lack of training data, i.e., 2d images of humans annotated with 3d poses.</s> <s>such data is necessary to train state-of-the-art cnn architectures.</s> <s>here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3d pose annotations.</s> <s>we introduce an image-based synthesis engine that artificially augments a dataset of real images with 2d human pose annotations using 3d motion capture (mocap) data.</s> <s>given a candidate 3d pose our algorithm selects for each joint an image whose 2d pose locally matches the projected 3d pose.</s> <s>the selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner.</s> <s>the resulting images are used to train an end-to-end cnn for full-body 3d pose estimation.</s> <s>we cluster the training data into a large number of pose classes and tackle pose estimation as a k-way classification problem.</s> <s>such an approach is viable only with large training sets such as ours.</s> <s>our method outperforms the state of the art in terms of 3d pose estimation in controlled environments (human3.6m) and shows promising results for in-the-wild images (lsp).</s> <s>this demonstrates that cnns trained on artificial images generalize well to real images.</s></p></d>", "label": ["<d><p><s>mocap-guided data augmentation for 3d pose estimation in the wild</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>sensing is the process of deriving signals from the environment that allows artificial systems to interact with the physical world.</s> <s>the shannon theorem specifies the maximum rate at which information can be acquired.</s> <s>however, this upper bound is hard to achieve in many man-made systems.</s> <s>the biological visual systems, on the other hand, have highly efficient signal representation and processing mechanisms that allow precise sensing.</s> <s>in this work, we argue that redundancy is one of the critical characteristics for such superior performance.</s> <s>we show architectural advantages by utilizing redundant sensing, including correction of mismatch error and significant precision enhancement.</s> <s>for a proof-of-concept demonstration, we have designed a heuristic-based analog-to-digital converter - a zero-dimensional quantizer.</s> <s>through monte carlo simulation with the error probabilistic distribution as a priori, the performance approaching the shannon limit is feasible.</s> <s>in actual measurements without knowing the error distribution, we observe at least 2-bit extra precision.</s> <s>the results may also help explain biological processes including the dominance of binocular vision, the functional roles of the fixational eye movements, and the structural mechanisms allowing hyperacuity.</s></p></d>", "label": ["<d><p><s>a bio-inspired redundant sensing architecture</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we consider a non-convex loss-minimization problem of learning supervised pagerank models, which can account for features of nodes and edges.</s> <s>we propose gradient-based and random gradient-free methods to solve this problem.</s> <s>our algorithms are based on the concept of an inexact oracle and unlike the state-of-the-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them.</s> <s>finally, we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task.</s></p></d>", "label": ["<d><p><s>learning supervised pagerank with gradient-based and gradient-free optimization methods</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>optimal transport (ot) defines a powerful framework to compare probability distributions in a geometrically faithful way.</s> <s>however, the practical impact of ot is still limited because of its computational burden.</s> <s>we propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications.</s> <s>these methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems.</s> <s>this alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error.</s> <s>these algorithms rely on two main ideas: (a) the dual ot problem can be re-cast as the maximization of an expectation; (b) entropic regularization of the primal ot problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence.</s> <s>we instantiate these ideas in three different computational setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat the current state of the art finite dimensional ot solver (sinkhorn's algorithm) ; (ii) when comparing a discrete distribution to a continuous density, a re-formulation (semi-discrete) of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel hilbert space (rkhs).</s> <s>this is currently the only known method to solve this problem, and is more efficient than discretizing beforehand the two densities.</s> <s>we backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.</s></p></d>", "label": ["<d><p><s>stochastic optimization for large-scale optimal transport</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of completing a binary matrix in an online learning setting.</s> <s>on each trial we predict a matrix entry and then receive the true entry.</s> <s>we propose a matrix exponentiated gradient algorithm [1] to solve this problem.</s> <s>we provide a mistake bound for the algorithm, which scales with the margin complexity [2, 3] of the underlying matrix.</s> <s>the bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects, the columns.</s> <s>using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the kernel perceptron with an optimal kernel in hindsight.</s> <s>we discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.</s></p></d>", "label": ["<d><p><s>mistake bounds for binary matrix completion</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>to what extent is the success of deep visualization due to the training?</s> <s>could we do deep visualization using untrained, random weight networks?</s> <s>to address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks.</s> <s>first we invert representations in feature spaces and reconstruct images from white noise inputs.</s> <s>the reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture.</s> <s>next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network.</s> <s>third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of gatys et al.</s> <s>on pretrained networks.</s> <s>to our knowledge this is the first demonstration of image representations using untrained deep neural networks.</s> <s>our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.</s> <s>it may possibly lead to a way to compare network architectures without training.</s></p></d>", "label": ["<d><p><s>a powerful generative model using random weights for the deep image representation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we exhibit a strong link between frequentist pac-bayesian bounds and the bayesian marginal likelihood.</s> <s>that is, for the negative log-likelihood loss function, we show that the minimization of pac-bayesian generalization bounds maximizes the bayesian marginal likelihood.</s> <s>this provides an alternative explanation to the bayesian occam's razor criteria, under the assumption that the data is generated by an i.i.d.</s> <s>distribution.</s> <s>moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a pac-bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical bayesian linear regression tasks.</s></p></d>", "label": ["<d><p><s>pac-bayesian theory meets bayesian inference</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we consider the problem of estimating a function defined over $n$ locations on a $d$-dimensional grid (having all side lengths equal to $n^{1/d}$).</s> <s>when the function is constrained to have discrete total variation bounded by $c_n$, we derive the minimax optimal (squared) $\\ell_2$ estimation error rate, parametrized by $n, c_n$.</s> <s>total variation denoising, also known as the fused lasso, is seen to be rate optimal.</s> <s>several simpler estimators exist, such as laplacian smoothing and laplacian eigenmaps.</s> <s>a natural question is: can these simpler estimators perform just as well?</s> <s>we prove that these estimators, and more broadly all estimators given by linear transformations of the input data, are suboptimal over the class of functions with bounded variation.</s> <s>this extends fundamental findings of donoho and johnstone (1998) on 1-dimensional total variation spaces to higher dimensions.</s> <s>the implication is that the computationally simpler methods cannot be used for such sophisticated denoising tasks, without sacrificing statistical accuracy.</s> <s>we also derive minimax rates for discrete sobolev spaces over $d$-dimensional grids, which are, in some sense, smaller than the total variation function spaces.</s> <s>indeed, these are small enough spaces that linear estimators can be optimal---and a few well-known ones are, such as laplacian smoothing and laplacian eigenmaps, as we show.</s> <s>lastly, we investigate the adaptivity of the total variation denoiser to these smaller sobolev function spaces.</s></p></d>", "label": ["<d><p><s>total variation classes beyond 1d: minimax rates, and the limitations of linear smoothers</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>word embeddings are a powerful approach to capturing semantic similarity among terms in a vocabulary.</s> <s>in this paper, we develop exponential family embeddings, which extends the idea of word embeddings to other types of high-dimensional data.</s> <s>as examples, we studied several types of data: neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system.</s> <s>the main idea is that each observation is modeled conditioned on a set of latent embeddings and other observations, called the context, where the way the context is defined depends on the problem.</s> <s>in language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart.</s> <s>each instance of an embedding defines the context, the exponential family of conditional distributions, and how the embedding vectors are shared across data.</s> <s>we infer the embeddings with stochastic gradient descent, with an algorithm that connects closely to generalized linear models.</s> <s>on all three of our applications?neural activity of zebrafish, users?</s> <s>shopping behavior, and movie ratings?we found that exponential family embedding models are more effective than other dimension reduction methods.</s> <s>they better reconstruct held-out data and find interesting qualitative structure.</s></p></d>", "label": ["<d><p><s>exponential family embeddings</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>it has recently been shown that supervised learning linear classifiers with two of the most popular losses, the logistic and square loss, is equivalent to optimizing an equivalent loss over sufficient statistics about the class: rademacher observations (rados).</s> <s>it has also been shown that learning over rados brings solutions to two prominent problems for which the state of the art of learning from examples can be comparatively inferior and in fact less convenient: protecting and learning from private examples, learning from distributed datasets without entity resolution.</s> <s>bis repetita placent: the two proofs of equivalence are different and rely on specific properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind.</s> <s>this is our first contribution: we show how they can be fit into the same theory for the equivalence between example and rado losses.</s> <s>as a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (i.e.</s> <s>the data) in the equivalent rado loss, in such a way that an efficient algorithm for one regularized rado loss may be as efficient when changing the regularizer.</s> <s>this is our third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, \\slope, l_\\infty, or elastic nets, using the same master routine for all.</s> <s>because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples we obtain the first efficient proxy to the minimisation of the regularized logistic loss over examples using such a wide spectrum of regularizers.</s> <s>experiments with a readily available code display that regularization significantly improves rado-based learning and compares favourably with example-based learning.</s></p></d>", "label": ["<d><p><s>on regularizing rademacher observation losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we introduce a method to train binarized neural networks (bnns) - neural networks with binary weights and activations at run-time.</s> <s>at train-time the binary weights and activations are used for computing the parameter gradients.</s> <s>during the forward pass, bnns drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to  substantially improve power-efficiency.</s> <s>to validate the effectiveness of bnns, we conducted two sets of experiments on the torch7 and theano frameworks.</s> <s>on both, bnns achieved nearly state-of-the-art results over the mnist, cifar-10 and svhn datasets.</s> <s>we also report our preliminary results on the challenging imagenet dataset.</s> <s>last but not least, we wrote a binary matrix multiplication gpu kernel with which it is possible to run our mnist bnn 7 times faster  than with an unoptimized gpu kernel, without suffering any loss in classification accuracy.</s> <s>the code for training and running our bnns is available on-line.</s></p></d>", "label": ["<d><p><s>binarized neural networks</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the stochastic block model (sbm) is a widely used random graph model for networks with communities.</s> <s>despite the recent burst of interest in community detection under the sbm from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery.</s> <s>in this paper, we consider the sbm in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities.</s> <s>for such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on sbm parameters for exact recoverability.</s> <s>our results exploit the tradeoffs among the various parameters of heterogenous sbm and provide recovery guarantees for many new interesting sbm configurations.</s></p></d>", "label": ["<d><p><s>exploiting tradeoffs for exact recovery in heterogeneous stochastic block models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making.</s> <s>these models require an agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies.</s> <s>to avoid barriers to sample-efficient learning associated with large observation spaces and general pomdps, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class.</s> <s>in this setting, we design and analyze a new reinforcement learning algorithm, least squares value elimination by exploration.</s> <s>we prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space.</s> <s>our result provides theoretical justification for reinforcement learning with function approximation.</s></p></d>", "label": ["<d><p><s>pac reinforcement learning with rich observations</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in recent years, a rapidly increasing number of applications in practice requires solving non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation etc.</s> <s>though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak.</s> <s>we consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are ``approximately convex'', i.e.</s> <s>functions $\\tf: \\real^d \\to \\real$ for which there exists a \\emph{convex function} $f$ such that for all $x$, $|\\tf(x) - f(x)| \\le \\errnoise$ for a fixed value $\\errnoise$.</s> <s>we then want to minimize $\\tf$, i.e.</s> <s>output a point $\\tx$ such that $\\tf(\\tx) \\le \\min_{x} \\tf(x) + \\err$.</s> <s>it is quite natural to conjecture that for fixed $\\err$, the problem gets harder for larger $\\errnoise$, however, the exact dependency of $\\err$ and $\\errnoise$ is not known.</s> <s>in this paper, we strengthen the known \\emph{information theoretic} lower bounds on the trade-off between $\\err$ and $\\errnoise$ substantially, and exhibit an algorithm that matches these lower bounds for a large class of convex bodies.</s></p></d>", "label": ["<d><p><s>algorithms and matching lower bounds for approximately-convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we present the first application of the median of means in a pac exploration algorithm for mdps.</s> <s>using the median of means allows us to significantly reduce the dependence of our bounds on the range of values that the value function can take, while introducing a dependence on the (potentially much smaller) variance of the bellman operator.</s> <s>additionally, our algorithm is the first algorithm with pac bounds that can be applied to mdps with unbounded rewards.</s></p></d>", "label": ["<d><p><s>improving pac exploration using the median of means</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in a traditional convolutional layer, the learned filters stay fixed after training.</s> <s>in contrast, we introduce a new framework, the dynamic filter network, where filters are generated dynamically conditioned on an input.</s> <s>we show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters.</s> <s>a wide variety of filtering operation can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction.</s> <s>moreover, multiple such layers can be combined, e.g.</s> <s>in a recurrent architecture.</s> <s>we demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving mnist dataset with a much smaller model.</s> <s>by visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data.</s> <s>this suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.</s></p></d>", "label": ["<d><p><s>dynamic filter networks</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>in modern data analysis, random sampling is an efficient and widely-used strategy to overcome the computational difficulties brought by large sample size.</s> <s>in previous studies, researchers conducted random sampling which is according to the input data but independent on the response variable, however the response variable may also be informative for sampling.</s> <s>in this paper we propose an adaptive sampling called the gradient-based sampling which is dependent on both the input data and the output for fast solving of least-square (ls) problems.</s> <s>we draw the data points by random sampling from the full data according to their gradient values.</s> <s>this sampling is computationally saving, since the running time of computing the sampling probabilities is reduced to o(nd) where n is the full sample size and d is the dimension of the input.</s> <s>theoretically, we establish an error bound analysis of the general importance sampling with respect to ls solution from full data.</s> <s>the result establishes an improved performance of the use of our gradient-based sampling.</s> <s>synthetic and real data sets are used to empirically argue that the gradient-based sampling has an obvious advantage over existing sampling methods from two aspects of statistical efficiency and computational saving.</s></p></d>", "label": ["<d><p><s>gradient-based sampling: an adaptive importance sampling for least-squares</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study the problem of recovering an incomplete $m\\times n$ matrix of rank $r$ with columns arriving online over time.</s> <s>this is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc.</s> <s>the challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity.</s> <s>in this work, we give algorithms achieving strong guarantee under two realistic noise models.</s> <s>in bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column.</s> <s>for this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case.</s> <s>for sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by probability at least $1-\\delta$ with sample complexity as small as $o(\\mu_0rn\\log(r/\\delta))$.</s> <s>this result advances the state-of-the-art work and matches the lower bound in a worst case.</s> <s>we also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller.</s> <s>our proposed algorithms perform well experimentally in both synthetic and real-world datasets.</s></p></d>", "label": ["<d><p><s>noise-tolerant life-long matrix completion via adaptive sampling</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we propose a simple and scalable method for improving the flexibility of variational inference through a transformation with autoregressive neural networks.</s> <s>autoregressive neural networks, such as rnns or the pixelcnn, are very powerful models and potentially interesting for use as variational posterior approximation.</s> <s>however, ancestral sampling in such networks is a long sequential operation, and therefore typically very slow on modern parallel hardware, such as gpus.</s> <s>we show that by inverting autoregressive neural networks we can obtain equally powerful posterior models from which we can sample efficiently on modern hardware.</s> <s>we show that such data transformations, inverse autoregressive flows (iaf), can be used to transform a simple distribution over the latent variables into a much more flexible distribution, while still allowing us to compute the resulting variables' probability density function.</s> <s>the method is simple to implement, can be made arbitrarily flexible and, in contrast with previous work, is well applicable to models with high-dimensional latent spaces, such as convolutional generative models.</s> <s>the method is applied to a novel deep architecture of variational auto-encoders.</s> <s>in experiments with natural images, we demonstrate that autoregressive flow leads to significant performance gains.</s></p></d>", "label": ["<d><p><s>improving variational autoencoders with inverse autoregressive flow</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>experience constantly shapes neural circuits through a variety of plasticity mechanisms.</s> <s>while the functional roles of some plasticity mechanisms are well-understood, it remains unclear how changes in neural excitability contribute to learning.</s> <s>here, we develop a normative interpretation of intrinsic plasticity (ip) as a key component of unsupervised learning.</s> <s>we introduce a novel generative mixture model that accounts for the class-specific statistics of stimulus intensities, and we derive a neural circuit that learns the input classes and their intensities.</s> <s>we will analytically show that inference and learning for our generative model can be achieved by a neural circuit with intensity-sensitive neurons equipped with a specific form of ip.</s> <s>numerical experiments verify our analytical derivations and show robust behavior for artificial and natural stimuli.</s> <s>our results link ip to non-trivial input statistics, in particular the statistics of stimulus intensities for classes to which a neuron is sensitive.</s> <s>more generally, our work paves the way toward new classification algorithms that are robust to intensity variations.</s></p></d>", "label": ["<d><p><s>neurons equipped with intrinsic plasticity learn stimulus intensity statistics</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a spectral analysis of the koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations.</s> <s>in this paper, we consider a spectral analysis of the koopman operator in a reproducing kernel hilbert space (rkhs).</s> <s>we propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system.</s> <s>the algorithm is in essence reduced to the calculation of a set of orthogonal bases for the krylov matrix in rkhs and the eigendecomposition of the projection of the koopman operator onto the subspace spanned by the bases.</s> <s>the algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system.</s> <s>therefore, we further consider applications in machine learning using extracted features with the presented analysis.</s> <s>we illustrate the method on the applications using synthetic and real-world data.</s></p></d>", "label": ["<d><p><s>dynamic mode decomposition with reproducing kernels for koopman spectral analysis</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>this study introduces a novel feature selection approach cmicot, which is a further evolution of filter methods with sequential  forward selection (sfs) whose scoring functions are based on conditional mutual information (mi).</s> <s>we state and study a novel saddle point (max-min) optimization problem to build a scoring function that is able to identify joint interactions between several  features.</s> <s>this method fills the gap of mi-based sfs techniques with high-order dependencies.</s> <s>in this high-dimensional case, the estimation of mi has prohibitively high sample complexity.</s> <s>we mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used.</s> <s>the superiority of our approach is demonstrated by comparison with recently proposed interaction-aware filters and several interaction-agnostic state-of-the-art ones on ten publicly available benchmark datasets.</s></p></d>", "label": ["<d><p><s>efficient high-order interaction-aware feature selection based on conditional mutual information</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>tensor factorization is a powerful tool to analyse multi-way data.</s> <s>recently proposed nonlinear factorization methods, although capable of capturing complex relationships, are computationally quite expensive and may suffer a severe learning bias in case of extreme data sparsity.</s> <s>therefore, we propose a distributed, flexible nonlinear tensor factorization model, which avoids the expensive computations and structural restrictions of the kronecker-product in the existing tgp formulations, allowing an arbitrary subset of tensor entries to be selected for training.</s> <s>meanwhile, we derive a tractable and tight variational evidence lower bound (elbo) that enables highly decoupled, parallel computations and high-quality inference.</s> <s>based on the new bound, we develop a distributed, key-value-free inference algorithm in the mapreduce framework, which can fully exploit the memory cache mechanism in fast mapreduce systems such as spark.</s> <s>experiments demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency.</s></p></d>", "label": ["<d><p><s>distributed flexible nonlinear tensor factorization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices.</s> <s>however, the aldous-hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse.</s> <s>we present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges.</s> <s>we demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity.</s> <s>to do so, we outline a general framework for graph generative models; by contrast to the pioneering work of caron and fox (2015), models within our framework are stationary across steps of the graph sequence.</s> <s>in particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.</s></p></d>", "label": ["<d><p><s>edge-exchangeable graphs and sparsity</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>graphical models with latent count variables arise in a number of fields.</s> <s>standard exact inference techniques such as variable elimination and belief propagation do not apply to these models because the latent variables have countably infinite support.</s> <s>as a result, approximations such as truncation or mcmc are employed.</s> <s>we present the first exact inference algorithms for a class of models with latent count variables by developing a novel representation of countably infinite factors as probability generating functions, and then performing variable elimination with generating functions.</s> <s>our approach is exact, runs in pseudo-polynomial time, and is much faster than existing approximate techniques.</s> <s>it leads to better parameter estimates for problems in population ecology by avoiding error introduced by approximate likelihood computations.</s></p></d>", "label": ["<d><p><s>probabilistic inference with generating functions for poisson latent variable models</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>the problem of predicting connections between a set of data points finds many applications, in systems biology and social network analysis among others.</s> <s>this paper focuses on the \\textit{graph reconstruction} problem, where the prediction rule is obtained by minimizing the average error over all n(n-1)/2 possible pairs of the n nodes of a training graph.</s> <s>our first contribution is to derive learning rates of order o(log n / n) for this problem, significantly improving upon the slow rates of order o(1/?n) established in the seminal work of biau & bleakley (2006).</s> <s>strikingly, these fast rates are universal, in contrast to similar results known for other statistical learning problems (e.g., classification, density level set estimation, ranking, clustering) which require strong assumptions on the distribution of the data.</s> <s>motivated by applications to large graphs, our second contribution deals with the computational complexity of graph reconstruction.</s> <s>specifically, we investigate to which extent the learning rates can be preserved when replacing the empirical reconstruction risk by a computationally cheaper monte-carlo version, obtained by sampling with replacement b << n?</s> <s>pairs of nodes.</s> <s>finally, we illustrate our theoretical results by numerical experiments on synthetic and real graphs.</s></p></d>", "label": ["<d><p><s>on graph reconstruction via empirical risk minimization: fast learning rates and scalability</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>gibbs sampling is a markov chain monte carlo sampling technique that iteratively samples variables from their conditional distributions.</s> <s>there are two common scan orders for the variables: random scan and systematic scan.</s> <s>due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan.</s> <s>while it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions.</s> <s>to prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.</s></p></d>", "label": ["<d><p><s>scan order in gibbs sampling: models in which it matters and bounds on how much</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>in this paper, we focus on training and evaluating effective word embeddings with both text and visual information.</s> <s>more specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available pins (i.e.</s> <s>an image with sentence descriptions uploaded by users) on pinterest.</s> <s>this dataset is more than 200 times larger than ms coco, the standard large-scale image dataset with sentence descriptions.</s> <s>in addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases.</s> <s>the word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships.</s> <s>based on these datasets, we propose and compare several recurrent neural networks (rnns) based multimodal (text and image) models.</s> <s>experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings.</s> <s>the project page is: http://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html (the datasets introduced in this work will be gradually released on the project page.</s> <s>).</s></p></d>", "label": ["<d><p><s>training and evaluating multimodal word embeddings with large-scale web annotated images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>scalable and effective exploration remains a key challenge in reinforcement learning (rl).</s> <s>while there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep rl scenarios.</s> <s>as such, most contemporary rl relies on simple heuristics such as epsilon-greedy exploration or adding gaussian noise to the controls.</s> <s>this paper introduces variational information maximizing exploration (vime), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics.</s> <s>we propose a practical implementation, using variational inference in bayesian neural networks which efficiently handles continuous state and action spaces.</s> <s>vime modifies the mdp reward function, and can be applied with several different underlying rl algorithms.</s> <s>we demonstrate that vime achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.</s></p></d>", "label": ["<d><p><s>vime: variational information maximizing exploration</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we study a variant of the classical stochastic $k$-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available.</s> <s>for example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences.</s> <s>we formalise this task as a \\emph{multi-fidelity} bandit, where, at each time step, the forecaster may choose to play an arm at any one of $m$ fidelities.</s> <s>the highest fidelity (desired outcome) expends cost $\\costm$.</s> <s>the $m$\\ssth fidelity (an approximation) expends $\\costm < \\costm$ and returns a biased estimate of the highest fidelity.</s> <s>we develop \\mfucb, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations.</s> <s>for instance, in the above online advertising example, \\mfucbs would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates.</s> <s>we complement this result with a lower bound and show that \\mfucbs is nearly optimal under certain conditions.</s></p></d>", "label": ["<d><p><s>the multi-fidelity multi-armed bandit</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>cross-region dynamic connectivity, which describes spatio-temporal dependence of neural activity among multiple brain regions of interest (rois), can provide important information for understanding cognition.</s> <s>for estimating such connectivity, magnetoencephalography (meg) and electroencephalography (eeg) are well-suited tools because of their millisecond temporal resolution.</s> <s>however, localizing source activity in the brain requires solving an under-determined linear problem.</s> <s>in typical two-step approaches, researchers first solve the linear problem with general priors assuming independence across rois, and secondly quantify cross-region connectivity.</s> <s>in this work, we propose a one-step state-space model to improve estimation of dynamic connectivity.</s> <s>the model treats the mean activity in individual rois as the state variable, and describes non-stationary dynamic dependence across rois using time-varying auto-regression.</s> <s>compared with a two-step method, which first obtains the commonly used minimum-norm estimates of source activity, and then fits the auto-regressive model, our state-space model yielded smaller estimation errors on simulated data where the model assumptions held.</s> <s>when applied on empirical meg data from one participant in a scene-processing experiment, our state-space model also demonstrated intriguing preliminary results, indicating leading and lagged linear dependence between the early visual cortex and a higher-level scene-sensitive region, which could reflect feed-forward and feedback information flow within the visual cortex during scene processing.</s></p></d>", "label": ["<d><p><s>a state-space model of cross-region dynamic connectivity in meg/eeg</s></p></d>"], "set": "test"},
  {"data": "<d><p><s>we introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image.</s> <s>our approach, combinatorial energy learning for image segmentation (celis) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems.</s> <s>we propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations.</s> <s>we extensively evaluate our method on a publicly available 3-d microscopy dataset with 25 billion voxels of ground truth data.</s> <s>on an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-d convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-d convolutional neural network.</s></p></d>", "label": ["<d><p><s>combinatorial energy learning for image segmentation</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>in this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices.</s> <s>we show applications of our approach to computing the principle component analysis (pca) of any $n\\times d$ matrix, using one pass over the stream of its rows.</s> <s>our solution uses coresets: a scaled subset of the $n$ rows that approximates their sum of squared distances to \\emph{every} $k$-dimensional \\emph{affine} subspace.</s> <s>an open theoretical problem has been to compute such a coreset that is independent of both $n$ and $d$.</s> <s>an open practical problem has been to compute a non-trivial approximation to the pca of very large but sparse databases such as the wikipedia document-term matrix in a reasonable time.</s> <s>we answer both of these questions affirmatively.</s> <s>our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream.</s></p></d>", "label": ["<d><p><s>dimensionality reduction of massive sparse datasets using coresets</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data.</s> <s>we find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error.</s> <s>the result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations.</s> <s>their decision rules take a form familiar in decision theory -- applying sigmoid functions to a notion of ensemble margin -- without the assumptions typically made in margin-based learning.</s></p></d>", "label": ["<d><p><s>optimal binary classifier aggregation for general losses</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>given a matrix of observed data, principal components analysis (pca) computes a small number of orthogonal directions that contain most of its variability.</s> <s>provably accurate solutions for pca have been in use for a long time.</s> <s>however, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise  are mutually independent, or at least uncorrelated.</s> <s>this is valid in practice often, but not always.</s> <s>in this paper, we study the pca problem in the setting where the data and noise can be correlated.</s> <s>such noise is often also referred to as ``data-dependent noise\".</s> <s>we obtain a correctness result for the standard eigenvalue decomposition (evd) based solution to pca under simple assumptions on the data-noise correlation.</s> <s>we also develop and analyze a generalization of evd, cluster-evd, that improves upon evd in certain regimes.</s></p></d>", "label": ["<d><p><s>correlated-pca: principal components' analysis when data and noise are correlated</s></p></d>"], "set": "dev"},
  {"data": "<d><p><s>due to the computational difficulty of performing mmse (minimum mean squared error) inference, maximum a posteriori (map) is often used as a surrogate.</s> <s>however, the accuracy of map is suboptimal for high dimensional inference, where the number of model parameters is of the same order as the number of samples.</s> <s>in this work we demonstrate how mmse performance is asymptotically achievable via optimization with an appropriately selected convex penalty and regularization function which are a smoothed version of the widely applied map algorithm.</s> <s>our findings provide a new derivation and interpretation for recent optimal m-estimators discovered by el karoui, et.</s> <s>al.</s> <s>pnas 2013 as well as extending to non-additive noise models.</s> <s>we demonstrate the performance of these optimal m-estimators with numerical simulations.</s> <s>overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional bayesian integration, and high dimensional convex optimization.</s> <s>in essence we show that the former computationally difficult integral may be computed by solving the latter, simpler optimization problem.</s></p></d>", "label": ["<d><p><s>an equivalence between high dimensional bayes optimal inference and m-estimation</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>a key goal of computer vision is to recover the underlying 3d structure that gives rise to 2d observations of the world.</s> <s>if endowed with 3d understanding, agents can abstract away from the complexity of the rendering process to form stable, disentangled representations of scene elements.</s> <s>in this paper we learn strong deep generative models of 3d structures, and recover these structures from 2d images via probabilistic inference.</s> <s>we demonstrate high-quality samples and report log-likelihoods on several datasets, including shapenet, and establish the first benchmarks in the literature.</s> <s>we also show how these models and their inference networks can be trained jointly, end-to-end, and directly from 2d images without any use of ground-truth 3d labels.</s> <s>this demonstrates for the first time the feasibility of learning to infer 3d representations of the world in a purely unsupervised manner.</s></p></d>", "label": ["<d><p><s>unsupervised learning of 3d structure from images</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>we extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions.</s> <s>our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its ``hardest local alternative'' to a given numerical precision.</s> <s>the bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis.</s> <s>we show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum.</s> <s>we also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the fisher information in statistical estimation.</s> <s>the nature and practical implications of the results are demonstrated in simulations.</s></p></d>", "label": ["<d><p><s>local minimax complexity of stochastic convex optimization</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>nystr\\\"{o}m method has been used successfully to improve the computational efficiency of  kernel ridge regression (krr).</s> <s>recently, theoretical analysis of nystr\\\"{o}m krr, including generalization bound and convergence rate, has been established based on  reproducing kernel hilbert space (rkhs) associated with the symmetric positive semi-definite kernel.</s> <s>however, in real world applications, rkhs is not always  optimal  and  kernel function is not necessary to be  symmetric or positive semi-definite.</s> <s>in this paper, we consider  the generalized nystr\\\"{o}m  kernel regression (gnkr) with $\\ell_2$ coefficient regularization, where the kernel just requires the continuity and boundedness.</s> <s>error analysis is provided to characterize its generalization performance  and the column norm sampling is introduced to construct the refined hypothesis space.</s> <s>in particular,  the fast learning rate with polynomial decay is reached for the gnkr.</s> <s>experimental analysis demonstrates the satisfactory performance of gnkr with the column norm sampling.</s></p></d>", "label": ["<d><p><s>error analysis of generalized nystr?m kernel regression</s></p></d>"], "set": "train"},
  {"data": "<d><p><s>statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models.</s> <s>the goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole.</s> <s>in this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant.</s> <s>we show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain.</s> <s>this includes an open problem called s4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox.</s> <s>we further identify new classes s2fo2 and s2ru of domain-liftable theories, which respectively subsume fo2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.</s></p></d>", "label": ["<d><p><s>new liftable classes for first-order probabilistic inference</s></p></d>"], "set": "train"},
]